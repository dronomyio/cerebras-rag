[
  {
    "content": "",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "develops methods for informal, often graphical, analysis of data. More formal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "Chapter 4\ndevelops methods for informal, often graphical, analysis of data. More formal\nmethods based on statistical inference, that is, estimation and testing, are\nintroduced in Chap. 5. The chapters that follow Chap. 5 cover a variety of\nmore advanced statistical techniques: ARIMA models, regression, multivari-\nate models, copulas, GARCH models, factor models, cointegration, Bayesian\nstatistics, and nonparametric regression.\n    Much of \ufb01nance is concerned with \ufb01nancial risk. The return on an\ninvestment is its revenue expressed as a fraction of the initial investment.\nIf one invests at time t1 in an asset with price Pt1 and the price later at\ntime t2 is Pt2 , then the net return for the holding period from t1 to t2 is\n(Pt2 \u2212 Pt1 )/Pt1 . For most assets, future returns cannot be known exactly\nand therefore are random variables. Risk means uncertainty in future returns\nfrom an investment, in particular, that the investment could earn less than\nthe expected return and even result in a loss, that is, a negative return. Risk\nis often measured by the standard deviation of the return, which we also\ncall the volatility. Recently there has been a trend toward measuring risk by\nvalue-at-risk (VaR) and expected shortfall (ES). These focus on large losses\nand are more direct indications of \ufb01nancial risk than the standard deviation\nof the return. Because risk depends upon the probability distribution of a re-\nturn, probability and statistics are fundamental tools for \ufb01nance. Probability\nis needed for risk calculations, and statistics is needed to estimate parame-\nters such as the standard deviation of a return or to test hypotheses such\nas the so-called random walk hypothesis which states that future returns are\nindependent of the past.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "develops methods for informal, often graphical, analysis of data. More formal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "\u00a9 Springer Science+Business Media New York 2015                              1\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 1\n\f2      1 Introduction",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "develops methods for informal, often graphical, analysis of data. More formal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "In \ufb01nancial engineering there are two kinds of probability distributions\nthat can be estimated. Objective probabilities are the true probabilities of\nevents. Risk-neutral or pricing probabilities give model outputs that agree\nwith market prices and re\ufb02ect the market\u2019s beliefs about the probabilities\nof future events. The statistical techniques in this book can be used to esti-\nmate both types of probabilities. Objective probabilities are usually estimated\nfrom historical data, whereas risk-neutral probabilities are estimated from the\nprices of options and other \ufb01nancial instruments.\n    Finance makes extensive use of probability models, for example, those\nused to derive the famous Black\u2013Scholes formula. Use of these models raises\nimportant questions of a statistical nature such as: Are these models supported\nby \ufb01nancial markets data? How are the parameters in these models estimated?\nCan the models be simpli\ufb01ed or, conversely, should they be elaborated?\n    After Chaps. 4\u20138 develop a foundation in probability, statistics, and ex-\nploratory data analysis, Chaps. 12 and 13 look at ARIMA models for time\nseries. Time series are sequences of data sampled over time, so much of the\ndata from \ufb01nancial markets are time series. ARIMA models are stochas-\ntic processes, that is, probability models for sequences of random variables.\nIn Chap. 16 we study optimal portfolios of risky assets (e.g., stocks) and\nof risky assets and risk-free assets (e.g., short-term U.S. Treasury bills).\nChapters 9\u201311 cover one of the most important areas of applied statistics,\nregression.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "develops methods for informal, often graphical, analysis of data. More formal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "Chapter 15 introduces cointegration analysis. In Chap. 17 portfo-\nlio theory and regression are applied to the CAPM.",
    "metadata": {
      "chapter_number": "15",
      "chapter_title": "introduces cointegration analysis. In Chap. 17 portfo-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1 Bibliographic Notes\nThe dictum that \u201cAll models are false but some models are useful\u201d is from\nBox (1976).\n\n\nReferences\n\nBox, G. E. P. (1976) Science and statistics, Journal of the American Statistical\n  Association, 71, 791\u2013799.\n\f2\nReturns\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1 Introduction\n\nThe goal of investing is, of course, to make a pro\ufb01t. The revenue from investing,\nor the loss in the case of negative revenue, depends upon both the change in\nprices and the amounts of the assets being held. Investors are interested in\nrevenues that are high relative to the size of the initial investments. Returns\nmeasure this, because returns on an asset, e.g., a stock, a bond, a portfolio\nof stocks and bonds, are changes in price expressed as a fraction of the initial\nprice.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1.1 Net Returns\n\nLet Pt be the price of an asset at time t. Assuming no dividends, the net\nreturn over the holding period from time t \u2212 1 to time t is\n                                 Pt      Pt \u2212 Pt\u22121\n                         Rt =        \u22121=           .\n                                Pt\u22121        Pt\u22121\n\nThe numerator Pt \u2212 Pt\u22121 is the revenue or pro\ufb01t during the holding period,\nwith a negative pro\ufb01t meaning a loss. The denominator, Pt\u22121 , was the initial\ninvestment at the start of the holding period. Therefore, the net return can\nbe viewed as the relative revenue or pro\ufb01t rate.\n   The revenue from holding an asset is\n\n                  revenue = initial investment \u00d7 net return.\n\n    For example, an initial investment of $10,000 and a net return of 6 % earns\na revenue of $600. Because Pt \u2265 0,\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                                5\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 2\n\f6       2 Returns\n\n                                     Rt \u2265 \u22121,                                 (2.1)\n\nso the worst possible return is \u22121, that is, a 100 % loss, and occurs if the asset\nbecomes worthless.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "1 Net Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1.2 Gross Returns\n\nThe simple gross return is\n                                  Pt\n                                      = 1 + Rt .\n                                 Pt\u22121\n\nFor example, if Pt = 2 and Pt+1 = 2.1, then 1 + Rt+1 = 1.05, or 105 %, and\nRt+1 = 0.05, or 5 %. One\u2019s \ufb01nal wealth at time t is one\u2019s initial wealth at time\nt \u2212 1 times the gross return. Stated di\ufb00erently, if X0 is the initial at time t \u2212 1,\nthen X0 (1 + Rt ) is one\u2019s wealth at time t.\n    Returns are scale-free, meaning that they do not depend on units (dollars,\ncents, etc.). Returns are not unitless. Their unit is time; they depend on the\nunits of t (hour, day, etc.). In this example, if t is measured in years, then,\nstated more precisely, the net return is 5 % per year.\n    The gross return over the most recent k periods is the product of the k\nsingle-period gross returns (from time t \u2212 k to time t):\n                                   \u0007      \b\u0007          \b \u0007           \b\n                            Pt        Pt        Pt\u22121        Pt\u2212k+1\n             1 + Rt (k) =       =                       \u00b7\u00b7\u00b7\n                          Pt\u2212k       Pt\u22121       Pt\u22122          Pt\u2212k\n                                = (1 + Rt ) \u00b7 \u00b7 \u00b7 (1 + Rt\u2212k+1 ).\n\nThe k-period net return is Rt (k).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "2 Gross Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1.3 Log Returns\n\nLog returns, also called continuously compounded returns, are denoted by rt\nand de\ufb01ned as\n                                         \u0007      \b\n                                            Pt\n                 rt = log(1 + Rt ) = log          = pt \u2212 pt\u22121 ,\n                                           Pt\u22121\n\nwhere pt = log(Pt ) is called the log price.\n    Log returns are approximately equal to returns because if x is small, then\nlog(1 + x) \u2248 x, as can been seen in Fig. 2.1, where log(1 + x) is plotted. Notice\nin that \ufb01gure that log(1 + x) is very close to x if |x| < 0.1, e.g., for returns\nthat are less than 10 %.\n    For example, a 5 % return equals a ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "3 Log Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.88 % log return since log(1 + 0.05) =\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.88",
      "section_title": "% log return since log(1 + 0.05) =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0488. Also, a \u22125 % return equals a \u22125.13 % log return since log(1 \u2212 0.05) =\n\u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0488",
      "section_title": "Also, a \u22125 % return equals a \u22125.13 % log return since log(1 \u2212 0.05) =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0513. In both cases, rt = log(1 + Rt ) \u2248 Rt . Also, log(1 + 0.01) = 0.00995\nand log(1 \u2212 0.01) = \u22120.01005, so log returns of \u00b11 % are very close to the\n\f                                                                                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0513",
      "section_title": "In both cases, rt = log(1 + Rt ) \u2248 Rt . Also, log(1 + 0.01) = 0.00995",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1 Introduction   7\n\ncorresponding net returns. Since returns are smaller in magnitude over shorter\nperiods, we can expect returns and log returns to be similar for daily returns,\nless similar for yearly returns, and not necessarily similar for longer periods\nsuch as 10 years.\n\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "Introduction   7",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                           0.1\n              log(x + 1)\n                           \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 \u22120.2 \u22120.1 0.0\n\n\n\n\n                                                                                     log(x+1)\n                                                                                     x\n\n                                                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "\u22120.2 \u22120.1 0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2     \u22120.1      0.0      0.1     0.2\n                                                                     x\n\n                 Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "\u22120.1      0.0      0.1     0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1. Comparison of functions log(1 + x) and x.\n\n\n\n\n    The return and log return have the same sign. The magnitude of the log\nreturn is smaller (larger) than that of the return if they are both positive (neg-\native). The di\ufb00erence between a return and a log return is most pronounced\nwhen both are very negative. Returns close to the lower bound of \u22121, that is\ncomplete losses, correspond to log return close to \u2212\u221e.\n    One advantage of using log returns is simplicity of multiperiod returns. A\nk-period log return is simply the sum of the single-period log returns, rather\nthan the product as for gross returns. To see this, note that the k-period log\nreturn is\n\n                             rt (k) = log{1 + Rt (k)}\n                                                = log {(1 + Rt ) \u00b7 \u00b7 \u00b7 (1 + Rt\u2212k+1 )}\n                                                = log(1 + Rt ) + \u00b7 \u00b7 \u00b7 + log(1 + Rt\u2212k+1 )\n                                                = rt + rt\u22121 + \u00b7 \u00b7 \u00b7 + rt\u2212k+1 .\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "Comparison of functions log(1 + x) and x.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1.4 Adjustment for Dividends\n\nMany stocks, especially those of mature companies, pay dividends that must\nbe accounted for when computing returns. Similarly, bonds pay interest. If a\n\f8       2 Returns\n\ndividend (or interest) Dt is paid prior to time t, then the gross return at time\nt is de\ufb01ned as\n                                         Pt + D t\n                              1 + Rt =            ,                         (2.2)\n                                          Pt\u22121\nand so the net return is Rt = (Pt + Dt )/Pt\u22121 \u2212 1 and the log return is\nrt = log(1 + Rt ) = log(Pt + Dt ) \u2212 log(Pt\u22121 ). Multiple-period gross returns are\nproducts of single-period gross returns so that\n                   \u0007          \b\u0007                  \b \u0007                    \b\n                     Pt + D t     Pt\u22121 + Dt\u22121           Pt\u2212k+1 + Dt\u2212k+1\n      1 + Rt (k) =                                  \u00b7\u00b7\u00b7\n                       Pt\u22121            Pt\u22122                   Pt\u2212k\n                 = (1 + Rt )(1 + Rt\u22121 ) \u00b7 \u00b7 \u00b7 (1 + Rt\u2212k+1 ),                (2.3)\n\nwhere, for any time s, Ds = 0 if there is no dividend between s \u2212 1 and s.\nSimilarly, a k-period log return is\n\n        rt (k) = log{1 + Rt (k)} = log(1 + Rt ) + \u00b7 \u00b7 \u00b7 + log(1 + Rt\u2212k+1 )\n                    \u0007          \b               \u0007                  \b\n                      Pt + D t                   Pt\u2212k+1 + Dt\u2212k+1\n              = log              + \u00b7 \u00b7 \u00b7 + log                      .\n                       Pt\u22121                            Pt\u2212k\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "4 Adjustment for Dividends",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 The Random Walk Model\nThe random walk hypothesis states that the single-period log returns, rt =\nlog(1 + Rt ), are independent. Because\n\n                     1 + Rt (k) = (1 + Rt ) \u00b7 \u00b7 \u00b7 (1 + Rt\u2212k+1 )\n                                = exp(rt ) \u00b7 \u00b7 \u00b7 exp(rt\u2212k+1 )\n                                 = exp(rt + \u00b7 \u00b7 \u00b7 + rt\u2212k+1 ),\n\nwe have\n                       log{1 + Rt (k)} = rt + \u00b7 \u00b7 \u00b7 + rt\u2212k+1 .                (2.4)\n                                                                       2\nIt is sometimes assumed further that the log returns are N (\u03bc, \u03c3 ) for some\nconstant mean and variance. Since sums of normal random variables are\nthemselves normal, normality of single-period log returns implies normality\nof multiple-period log returns. Under these assumptions, log{1 + Rt (k)} is\nN (k\u03bc, k\u03c3 2 ).\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "The Random Walk Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2.1 Random Walks\n\nModel (2.4) is an example of a random walk model. Let Z1 , Z2 , . . . be i.i.d. (in-\ndependent and identically distributed) with mean \u03bc and standard deviation \u03c3.\nLet S0 be an arbitrary starting point and\n\n                        St = S0 + Z 1 + \u00b7 \u00b7 \u00b7 + Z t ,   t \u2265 1.                (2.5)\n\f                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "1 Random Walks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 The Random Walk Model               9\n\nFrom (2.5), St is the position of the random walker after t steps starting at S0 .\n    The process S0 , S1 , . . . is called a random walk and Z1 , Z2 , . . . are its steps.\nIf the steps are normally distributed, then the process is called a normal\nrandom walk. The expectation and variance of St , conditional given S0 , are\nE(St |S0 ) = S0 + \u03bct and Var(St |S0 ) = \u03c3 2 t. The parameter \u03bc is called the drift\nand determines the general direction of the random walk. The parameter \u03c3\nis the volatility and determines how much the random walk \ufb02uctuates about\nthe\n  \u221a conditional mean\u221a S0 + \u03bct. Since the standard deviation of St given S0 is\n\u03c3 t, (S0 + \u03bct) \u00b1 \u03c3 t gives the mean plus and minus one standard deviation,\nwhich, for a normal random walk, gives a range\u221acontaining 68 % probability.\nThe width of this range grows proportionally to t, as is illustrated in Fig. 2.2,\nshowing that at time t = 0 we know far less about where the random walk\nwill be in the distant future compared to where it will be in the immediate\nfuture.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "The Random Walk Model               9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2.2 Geometric Random Walks\n\nRecall that log{1 + Rt (k)} = rt + \u00b7 \u00b7 \u00b7 + rt\u2212k+1 . Therefore,\n                    Pt\n                        = 1 + Rt (k) = exp(rt + \u00b7 \u00b7 \u00b7 + rt\u2212k+1 ),                   (2.6)\n                   Pt\u2212k\nso taking k = t, we have\n\n                         Pt = P0 exp(rt + rt\u22121 + \u00b7 \u00b7 \u00b7 + r1 ).                      (2.7)\n\nWe call such a process whose logarithm is a random walk a geometric random\nwalk or an exponential random walk. If r1 , r2 , . . . are i.i.d. N (\u03bc, \u03c3 2 ), then Pt is\nlognormal for all t and the process is called a lognormal geometric random walk\nwith parameters (\u03bc, \u03c3 2 ). As discussed in Appendix A.9.4, \u03bc is called the log-\nmean and \u03c3 is called the log-standard deviation of the log-normal distribution\nof exp(rt ). Also, \u03bc is sometimes called the log-drift of the lognormal geometric\nrandom walk.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "2 Geometric Random Walks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2.3 Are Log Prices a Lognormal Geometric Random Walk?\n\nMuch work in mathematical \ufb01nance assumes that prices follow a lognormal\ngeometric random walk or its continuous-time analog, geometric Brownian\nmotion. So a natural question is whether this assumption is usually true.\nThe quick answer is \u201cno.\u201d The lognormal geometric random walk makes two\nassumptions: (1) the log returns are normally distributed and (2) the log\nreturns are mutually independent.\n    In Chaps. 4 and 5, we will investigate the marginal distributions of several\nseries of log returns. The conclusion will be that, though the return density\nhas a bell shape somewhat like that of normal densities, the tails of the log\nreturn distributions are generally much heavier than normal tails. Typically, a\n\f10      2 Returns\n\n                        mean\n                        mean + SD\n\n\n\n\n              8\n                        mean \u2212 SD\n\n\n\n              6\n              4\n              2\n              0\n\n\n\n\n                    0        2         4          6       8         10\n                                           time\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "3 Are Log Prices a Lognormal Geometric Random Walk?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2. Mean and bounds (mean plus and minus one standard deviation) on a\nrandom walk with S0 = 0, \u03bc = 0.5, and \u03c3 = 1. At any given time, the probability\nof being between the bounds (dashed curves) is 68 % if the distribution of the steps\nis normal. Since \u03bc > 0, there is an overall positive trend that would be reversed if \u03bc\nwere negative.\n\n\nt-distribution with a small degrees-of-freedom parameter, say 4\u20136, is a much\nbetter \ufb01t than the normal model. However, the log-return distributions do\nappear to be symmetric, or at least nearly so.\n    The independence assumption is also violated. First, there is some corre-\nlation between returns. The correlations, however, are generally small. More\nseriously, returns exhibit volatility clustering, which means that if we see high\nvolatility in current returns then we can expect this higher volatility to con-\ntinue, at least for a while. Volatility clustering can be detected by checking\nfor correlations between the squared returns.\n    Before discarding the assumption that the prices of an asset are a lognor-\nmal geometric random walk, it is worth remembering Box\u2019s dictum that \u201call\nmodels are false, but some models are useful.\u201d This assumption is sometimes\nuseful, e.g., for deriving the famous Black\u2013Scholes formula.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "Mean and bounds (mean plus and minus one standard deviation) on a",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, there is an overall positive trend that would be reversed if \u03bc\nwere negative.",
        "start": 255,
        "end": 339
      }
    ]
  },
  {
    "content": "2.3 Bibliographic Notes\n\nThe random walk hypothesis is related to the so-called e\ufb03cient market hy-\npothesis; see Ruppert et al. (2003) for discussion and further references. Bodie\net al. (1999) and Sharpe et al. (1995) are good introductions to the random\nwalk hypothesis and market e\ufb03ciency. A more advanced discussion of the\nrandom walk hypothesis is found in Chap. 2 of Campbell et al. (1997) and\nLo and MacKinlay (1999). Much empirical evidence about the behavior of\n\f                                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.3",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4 R Lab     11\n\nreturns is reviewed by Fama (1965, 1970, 1991, 1998). Evidence against the\ne\ufb03cient market hypothesis can be found in the \ufb01eld of behavioral \ufb01nance\nwhich uses the study of human behavior to understand market behavior; see\nShefrin (2000), Shleifer (2000), and Thaler (1993). One indication of market\nine\ufb03ciency is excess volatility of market prices; see Shiller (1992) or Shiller\n(2000) for a less technical discussion.\n   R will be used extensively in what follows. Dalgaard (2008) and Zuur et al.\n(2009) are good places to start learning R.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "R Lab     11",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4 R Lab\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.1 Data Analysis\n\nObtain the data set Stock_bond.csv from the book\u2019s website and put it in\nyour working directory. Start R1 and you should see a console window open\nup. Use Change Dir in the \u201cFile\u201d menu to change to the working directory.\nRead the data with the following command:\n    dat = read.csv(\"Stock_bond.csv\", header = TRUE)\n\nThe data set Stock_bond.csv contains daily volumes and adjusted closing\n(AC) prices of stocks and the S&P 500 (columns B\u2013W) and yields on bonds\n(columns X\u2013AD) from 2-Jan-1987 to 1-Sep-2006.\n    This book does not give detailed information about R functions since\nthis information is readily available elsewhere. For example, you can use R\u2019s\nhelp to obtain more information about the read.csv() function by typing\n\u201c?read.csv\u201d in your R console and then hitting the Enter key. You should\nalso use the manual An Introduction to R that is available on R\u2019s help \ufb01le and\nalso on CRAN. Another resource for those starting to learn R is Zuur et al.\n(2009).\n    An alternative to typing commands in the console is to start a new script\nfrom the \u201cfile\u201d menu, put code into the editor, highlight the lines, and then\npress Ctrl-R to run the code that has been highlighted.2 This technique is\nuseful for debugging. You can save the script \ufb01le and then reuse or modify it.\n    Once a \ufb01le is saved, the entire \ufb01le can be run by \u201csourcing\u201d it. You can\nuse the \u201cfile\u201d menu in R to source a \ufb01le or use the source() function. If\nthe \ufb01le is in the editor, then it can be run by hitting Ctrl-A to highlight the\nentire \ufb01le and then Ctrl-R.\n    The next lines of code print the names of the variables in the data set,\nattach the data, and plot the adjusted closing prices of GM and Ford.\n\n1\n  You can also run R from Rstudio and, in fact, Rstudio is highly recommended.\n  The authors switched from R to Rstudio while the second edition of this book\n  was being written.\n2\n  Or click the \u201crun\u201d button in Rstudio.\n\f12     2 Returns\n\n1 names(dat)\n2 attach(dat)\n3 par(mfrow = c(1, 2))\n\n4 plot(GM_AC)\n\n5 plot(F_AC)\n\n\n\nHere and elsewhere in this book, line numbers are often added when listing R\ncode. The line numbers are not part of the code.\n    By default, as in lines 4 and 5, points are plotted with the character \u201co\u201d.\nTo plot a line instead, use, for example plot(GM_AC, type = \"l\"). Similarly,\nplot(GM_AC, type = \"b\") plots both points and a line.\n    The R function attach() puts a database into the R search path. This\nmeans that the database is searched by R when evaluating a variable, so objects\nin the database can be accessed by simply giving their names. If dat was not\nattached, then line 4 would be replaced by plot(dat$GM AC) and similarly\nfor line 5.\n    The function par() speci\ufb01es plotting parameters and mfrow=c(n1,n2)\nspeci\ufb01es \u201cmake a \ufb01gure, \ufb01ll by rows, n1 rows and n2 columns.\u201d Thus, the \ufb01rst\nn1 plots \ufb01ll the \ufb01rst row and so forth. mfcol(n1,n2) \ufb01lls by columns and so\nwould put the \ufb01rst n2 plots in the \ufb01rst column. As mentioned before, more\ninformation about these and other R functions can be obtained from R\u2019s online\nhelp or the manual An Introduction to R.\n    Run the code below to \ufb01nd the sample size (n), compute GM and Ford\nreturns, and plot GM net returns versus the Ford returns.\n1 n = dim(dat)[1]\n2 GMReturn = GM_AC[-1] / GM_AC[-n] - 1\n3 FReturn = F_AC[-1] / F_AC[-n] - 1\n\n4 par(mfrow = c(1, 1))\n\n5 plot(GMReturn,FReturn)\n\n\n\n   On lines 2 and 3, the index -1 means all indices except the \ufb01rst and\nsimilarly -n means all indices except the last.\n\nProblem 1 Do the GM and Ford returns seem positively correlated? Do you\nnotice any outlying returns? If \u201cyes,\u201d do outlying GM returns seem to occur\nwith outlying Ford returns?\n\n\nProblem 2 Compute the log returns for GM and plot the returns versus the\nlog returns. How highly correlated are the two types of returns? (The R function\ncor() computes correlations.)\n\n\nProblem 3 Repeat Problem 1 with Microsoft (MSFT) and Merck (MRK).\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "1 Data Analysis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4 R Lab     13\n\n    When you exit R, you can \u201cSave workspace image,\u201d which will create an\nR workspace \ufb01le in your working directory. Later, you can restart R and load\nthis workspace image into memory by right-clicking on the R workspace \ufb01le.\nWhen R starts, your working directory will be the folder containing the R\nworkspace that was opened. A useful trick when starting a project in a new\nfolder is to put an empty saved workspace into this folder. Double-clicking on\nthe workspace starts R with the folder as the working directory.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "R Lab     13",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.2 Simulations\n\nHedge funds can earn high pro\ufb01ts through the use of leverage, but leverage\nalso creates high risk. The simulations in this section explore the e\ufb00ects of\nleverage in a simpli\ufb01ed setting.\n    Suppose a hedge fund owns $1,000,000 of stock and used $50,000 of its\nown capital and $950,000 in borrowed money for the purchase. Suppose that\nif the value of the stock falls below $950,000 at the end of any trading day,\nthen the hedge fund will sell all the stock and repay the loan. This will wipe\nout its $50,000 investment. The hedge fund is said to be leveraged 20:1 since\nits position is 20 times the amount of its own capital invested.\n    Suppose that the daily log returns on the stock have a mean of 0.05/year\nand a standard deviation of 0.23/year.\u221a These can be converted to rates per\ntrading day by dividing by 253 and 253, respectively.\n\nProblem 4 What is the probability that the value of the stock will be below\n$950,000 at the close of at least one of the next 45 trading days? To answer\nthis question, run the code below.\n\n\n1  niter = 1e5           # number of iterations\n2  below = rep(0, niter) # set up storage\n 3 set.seed(2009)\n\n 4 for (i in 1:niter)\n\n 5 {\n\n 6    r = rnorm(45, mean = 0.05/253,\n 7       sd = 0.23/sqrt(253)) # generate random numbers\n 8    logPrice = log(1e6) + cumsum(r)\n 9     minlogP = min(logPrice) # minimum price over next 45 days\n10    below[i] = as.numeric(minlogP < log(950000))\n11 }\n\n12 mean(below)\n\n\n\nOn line 10, below[i] equals 1 if, for the ith simulation, the minimum price\nover 45 days is less that 950,000. Therefore, on line 12, mean(below) is the\nproportion of simulations where the minimum price is less than 950,000.\n   If you are unfamiliar with any of the R functions used here, then use R\u2019s\nhelp to learn about them; e.g., type ?rnorm to learn that rnorm() generates\n\f14     2 Returns\n\nnormally distributed random numbers. You should study each line of code,\nunderstand what it is doing, and convince yourself that the code estimates\nthe probability being requested. Note that anything that follows a pound sign\nis a comment and is used only to annotate the code.\n    Suppose the hedge fund will sell the stock for a pro\ufb01t of at least $100,000\nif the value of the stock rises to at least $1,100,000 at the end of one of the\n\ufb01rst 100 trading days, sell it for a loss if the value falls below $950,000 at the\nend of one of the \ufb01rst 100 trading days, or sell after 100 trading days if the\nclosing price has stayed between $950,000 and $1,100,000.\n    The following questions can be answered by simulations much like the one\nabove. Ignore trading costs and interest when answering these questions.\n\nProblem 5 What is the probability that the hedge fund will make a pro\ufb01t of\nat least $100,000?\n\n\nProblem 6 What is the probability the hedge fund will su\ufb00er a loss?\n\n\nProblem 7 What is the expected pro\ufb01t from this trading strategy?\n\n\nProblem 8 What is the expected return? When answering this question, re-\nmember that only $50,000 was invested. Also, the units of return are time,\ne.g., one can express a return as a daily return or a weekly return. Therefore,\none must keep track of how long the hedge fund holds its position before selling.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "2 Simulations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.3 Simulating a Geometric Random Walk\n\nIn this section you will use simulations to see how stock prices evolve when the\nlog-returns are i.i.d. normal, which implies that the price series is a geometric\nrandom walk.\n    Run the following R code. The set.seed() command insures that everyone\nusing this code will have the same random numbers and will obtain the same\nprice series. There are 253 trading days per year, so you are simulating 1 year\nof daily returns nine times. The price starts at 120.\n    The code par(mfrow=c(3,3)) on line 3 opens a graphics window with\nthree rows and three columns and rnorm() on line 6 generates normally dis-\ntributed random numbers.\n1 set.seed(2012)\n2 n = 253\n3 par(mfrow=c(3,3))\n\n4 for (i in (1:9))\n\n5 {\n\n6    logr = rnorm(n, ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "3 Simulating a Geometric Random Walk",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 / 253, 0.2 / sqrt(253))\n\f                                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "/ 253, 0.2 / sqrt(253))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4 R Lab      15\n\n7       price = c(120, 120 * exp(cumsum(logr)))\n8       plot(price, type = \"b\")\n9   }\n\nProblem 9 In this simulation, what are the mean and standard deviation of\nthe log-returns for 1 year?\n\n\nProblem 10 Discuss how the price series appear to have momentum. Is the\nappearance of momentum real or an illusion?\n\n\nProblem 11 Explain what the code c(120,120*exp(cumsum(logr))) does.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "R Lab      15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.4 Let\u2019s Look at McDonald\u2019s Stock\n\nIn this section we will be looking at daily returns on McDonald\u2019s stock over\nthe period 2010\u20132014. To start the lab, run the following commands to get\ndaily adjusted prices over this period:\n1 data = read.csv(\u2019MCD_PriceDaily.csv\u2019)\n2 head(data)\n3 adjPrice = data[, 7]\n\n\n\nProblem 12 Compute the returns and log returns and plot them against each\nother. As discussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "4 Let\u2019s Look at McDonald\u2019s Stock",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1.3, does it seem reasonable that the two types\nof daily returns are approximately equal?\n\n\nProblem 13 Compute the mean and standard deviation for both the returns\nand the log returns. Comment on the similarities and di\ufb00erences you perceive\nin the \ufb01rst two moments of each random variable. Does it seem reasonable\nthat they are the same?\n\n\nProblem 14 Perform a t-test to compare the means of the returns and the\nlog returns. Comment on your \ufb01ndings. Do you reject the null hypothesis that\nthey are the same mean at 5 % signi\ufb01cance? Or do you accept it? [Hint: Should\nyou be using an independent samples t-test or a paired-samples t-test?]\nWhat are the assumptions behind the t-test? Do you think that they are met\nin this example? If the assumptions made by the t-test are not met, how would\nthis a\ufb00ect your interpretation of the results of the test?\n\n\nProblem 15 After looking at return and log return data for McDonald\u2019s, are\nyou satis\ufb01ed that for small values, log returns and returns are interchangeable?\n\f16     2 Returns\n\nProblem 16 Assume that McDonald\u2019s log returns are normally distributed\nwith mean and standard deviation equal to their estimates and that you have\nbeen made the following proposition by a friend: If at any point within the\nnext 20 trading days, the price of McDonald\u2019s falls below 85 dollars, you will\nbe paid $100, but if it does not, you have to pay him $1. The current price\nof McDonald\u2019s is at the end of the sample data, $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "3, does it seem reasonable that the two types",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "93.07. Are you willing to\nmake the bet? (Use 10,000 iterations in your simulation and use the command\nset.seed(2015) to ensure your results are the same as the answer key)\n\n\nProblem 17 After coming back to your friend with an unwillingness to make\nthe bet, he asks you if you are willing to try a slightly di\ufb00erent deal. This time\nthe o\ufb00er stays the same as before, except he would pay an additional $25 if\nthe price ever fell below $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "93.07",
      "section_title": "Are you willing to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "84.50. You still only pay him $1 for losing. Do you\nnow make the bet?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "84.50",
      "section_title": "You still only pay him $1 for losing. Do you",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 Exercises\n\n 1. Suppose that the daily log returns on a stock are independent and nor-\n    mally distributed with mean ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 and standard deviation 0.015. Suppose\n    you buy $1,000 worth of this stock.\n    (a) What is the probability that after one trading day your investment is\n        worth less than $990? (Note: The R function pnorm() will compute a\n        normal CDF, so, for example, pnorm(0.3, mean = 0.1, sd = 0.2)\n        is the normal CDF with mean ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.001",
      "section_title": "and standard deviation 0.015. Suppose",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 and standard deviation 0.2 evaluated\n        at ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "and standard deviation 0.2 evaluated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3.)\n    (b) What is the probability that after \ufb01ve trading days your investment\n        is worth less than $990?\n 2. The yearly log returns on a stock are normally distributed with mean ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": ")",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n    and standard deviation 0.2. The stock is selling at $100 today. What is\n    the probability that 1 year from now it is selling at $110 or more?\n 3. The yearly log returns on a stock are normally distributed with mean ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "and standard deviation 0.2. The stock is selling at $100 today. What is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n    and standard deviation 0.15. The stock is selling at $80 today. What is\n    the probability that 2 years from now it is selling at $90 or more?\n 4. Suppose the prices of a stock at times 1, 2, and 3 are P1 = 95, P2 = 103,\n    and P3 = 98. Find r3 (2).\n 5. The prices and dividends of a stock are given in the table below.\n    (a) What is R2 ?\n    (b) What is R4 (3)?\n    (c) What is r3 ?\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08",
      "section_title": "and standard deviation 0.15. The stock is selling at $80 today. What is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 Exercises      17\n\n                                      t Pt D t\n                                      1 52 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "Exercises      17",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                      2 54 0.2\n                                      3 53 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "2 54 0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                      4 59 0.25\n 6. The prices and dividends of a stock are given in the table below.\n    (a) Find R3 (2),\n    (b) Find r4 (3).\n                                      t Pt D t\n                                      1 82 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "4 59 0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                      2 85 0.1\n                                      3 83 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "2 85 0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                      4 87 0.125\n\n 7. Let rt be a log return. Suppose that r1 , r2 , . . . are i.i.d. N (0.06, 0.47).\n    (a) What is the distribution of rt (4) = rt + rt\u22121 + rt\u22122 + rt\u22123 ?\n    (b) What is P {r1 (4) < 2}?\n    (c) What is the covariance between r2 (1) and r2 (2)?\n    (d) What is the conditional distribution of rt (3) given rt\u22122 = 0.6?\n 8. Suppose that X1 , X2 , . . . is a lognormal geometric random walk with pa-\n    rameters (\u03bc, \u03c3 2 ). More speci\ufb01cally, suppose that Xk = X0 exp(r1 + \u00b7 \u00b7 \u00b7 +\n    rk ), where X0 is a \ufb01xed constant and r1 , r2 , . . . are i.i.d. N (\u03bc, \u03c3 2 ).\n    (a) Find P (X2 > ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "4 87 0.125",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.3 X0 ).\n    (b) Use (A.4) to \ufb01nd the density of X1 .\n    (c) Find a formula for the ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.3",
      "section_title": "X0 ).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 quantile of Xk for all k.\n    (d) What is the expected value of Xk2 for any k? (Find a formula giving\n         the expected value as a function of k.)\n    (e) Find the variance of Xk for any k.\n 9. Suppose that X1 , X2 , . . . is a lognormal geometric random walk with pa-\n    rameters \u03bc = 0.1, \u03c3 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "quantile of Xk for all k.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2.\n    (a) Find P (X3 > 1.2X0 ).\n    (b) Find the conditional variance of Xk /k given X0 for any k.\n    (c) Find the minimum number of days before the probability is at least\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "(a) Find P (X3 > 1.2X0 ).",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1.2X0 ).\n    (b) Find the conditional variance of Xk /k given X0 for any k.\n    (c) Find the minimum number of days before the probability is at least",
        "start": 24,
        "end": 186
      }
    ]
  },
  {
    "content": "0.9 of doubling one\u2019s money, that is, \ufb01nd the small value of t such that\n         P (Pt /P0 \u2265 2) \u2265 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "of doubling one\u2019s money, that is, \ufb01nd the small value of t such that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9.\n10. The daily log returns on a stock are normally distributed with mean 0.0002\n    and standard deviation ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "10. The daily log returns on a stock are normally distributed with mean 0.0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03. The stock price is now $97. What is the\n    probability that it will exceed $100 after 20 trading days?\n11. Suppose that daily log-returns are N (0.0005, 0.012). Find the smallest\n    value of t such that P (Pt /P0 \u2265 2) \u2265 0.9, that is, that after t days the\n    probability the price has doubled is at least 90 %.\n\f18     2 Returns\n\nReferences\nBodie, Z., Kane, A., and Marcus, A. (1999) Investments, 4th ed., Irwin/\n  McGraw-Hill, Boston.\nCampbell, J., Lo, A., and MacKinlay, A. (1997) The Econometrics of Finan-\n  cial Markets, Princeton University Press, Princeton, NJ.\nDalgaard, P. (2008) Introductory Statistics with R, 2nd ed., Springer.\nFama, E. (1965) The behavior of stock market prices. Journal of Business,\n  38, 34\u2013105.\nFama, E. (1970) E\ufb03cient capital markets: A review of theory and empirical\n  work. Journal of Finance, 25, 383\u2013417.\nFama, E. (1991) E\ufb03cient Capital Markets: II. Journal of Finance. 46,\n  1575\u20131618.\nFama, E. (1998) Market e\ufb03ciency, long-term returns, and behavioral \ufb01nance.\n  Journal of Financial Economics, 49, 283\u2013306.\nLo, A. W., and MacKinlay, A. C. (1999) A Non-Random Walk Down Wall\n  Street, Princeton University Press, Princeton and Oxford.\nRuppert, D. (2003) Statistics and Finance: An Introduction, Springer,\n  New York.\nSharpe, W. F., Alexander, G. J., and Bailey, J. V. (1995) Investments, 6th\n  ed., Simon and Schuster, Upper Saddle River, NJ.\nShefrin, H. (2000) Beyond Greed and Fear: Understanding Behavioral Finance\n  and the Psychology of Investing, Harvard Business School Press, Boston.\nShiller, R. (1992) Market Volatility, Reprint ed., MIT Press, Cambridge, MA.\nShiller, R. (2000) Irrational Exuberance, Broadway, New York.\nShleifer, A. (2000) Ine\ufb03cient Markets: An Introduction to Behavioral Finance,\n  Oxford University Press, Oxford.\nThaler, R. H. (1993) Advances in Behavioral Finance, Russell Sage Founda-\n  tion, New York.\nZuur, A., Ieno, E., Meesters, E., and Burg, D. (2009) A Beginner\u2019s Guide to\n  R, Springer, New York.\n\f3\nFixed Income Securities\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "The stock price is now $97. What is the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1 Introduction\n\nCorporations \ufb01nance their operations by selling stock and bonds. Owning a\nshare of stock means partial ownership of the company. Stockholders share\nin both the pro\ufb01ts and losses of the company. Owning a bond is di\ufb00erent.\nWhen you buy a bond you are loaning money to the corporation, though\nbonds, unlike loans, are tradeable. The corporation is obligated to pay back\nthe principal and to pay interest as stipulated by the bond. The bond owner\nreceives a \ufb01xed stream of income, unless the corporation defaults on the bond.\nFor this reason, bonds are called \u201c\ufb01xed income\u201d securities.\n    It might appear that bonds are risk-free, almost stodgy, but this is not\nthe case. Many bonds are long-term, e.g., 5, 10, 20, or even 30 years. Even\nif the corporation stays solvent or if you buy a U.S. Treasury bond, where\ndefault is for all intents and purposes impossible, your income from the bond\nis guaranteed only if you keep the bond to maturity. If you sell the bond before\nmaturity, your return will depend on changes in the price of the bond. Bond\nprices move in opposite direction to interest rates, so a decrease in interest\nrates will cause a bond \u201crally,\u201d where bond prices increase. Long-term bonds\nare more sensitive to interest-rate changes than short-term bonds. The interest\nrate on your bond is \ufb01xed, but in the market interest rates \ufb02uctuate. Therefore,\nthe market value of your bond \ufb02uctuates too. For example, if you buy a bond\npaying 5 % and the rate of interest increases to 6 %, then your bond is inferior\nto new bonds o\ufb00ering 6 %. Consequently, the price of your bond will decrease.\nIf you sell the bond, you could lose money.\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                              19\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 3\n\f20        3 Fixed Income Securities\n\n    The interest rate of a bond depends on its maturity. For example, on\nMarch 28, 2001, the interest rate of Treasury bills1 was ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.23 % for 3-month\nbills. The yields on Treasury notes and bonds were ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.23",
      "section_title": "% for 3-month",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.41 %, 5.01 %, and 5.46 %\nfor 2-, 10-, and 30-year maturities, respectively. The term structure of interest\nrates describes how rates change with maturity.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.41",
      "section_title": "%, 5.01 %, and 5.46 %",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2 Zero-Coupon Bonds\nZero-coupon bonds, also called pure discount bonds and sometimes known as\n\u201czeros,\u201d pay no principal or interest until maturity. A \u201czero\u201d has a par value\nor face value, which is the payment made to the bondholder at maturity.\nThe zero sells for less than the par value, which is the reason it is a discount\nbond.\n    For example, consider a 20-year zero with a par value of $1,000 and 6 %\ninterest compounded annually. The market price is the present value of $1,000\nwith an annual interest rate of 6 % with annual discounting. That is, the\nmarket price is\n                               $1, 000\n                                       = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Zero-Coupon Bonds",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "311.80.\n                              (1.06)20\n\nIf the annual interest rate is 6 % but compounded every 6 months, then the\nprice is\n                               $1, 000\n                                       = $306.56,\n                              (1.03)40\n\nand if the annual rate is 6 % compounded continuously, then the price is\n\n                                 $1, 000\n                                             = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "311.80",
      "section_title": "(1.06)20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "301.19.\n                             exp{(0.06)(20)}\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "301.19",
      "section_title": "exp{(0.06)(20)}",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2.1 Price and Returns Fluctuate with the Interest Rate\n\nFor concreteness, assume semiannual compounding. Suppose you bought the\nzero for $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "1 Price and Returns Fluctuate with the Interest Rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "306.56 and then 6 months later the interest rate increased to 7 %.\nThe market price would now be\n\n                                 $1, 000\n                                          = $261.41,\n                                (1.035)39\n\nso the value of your investment would drop by ($",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "306.56",
      "section_title": "and then 6 months later the interest rate increased to 7 %.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "306.56 \u2212 $261.41) = $45.15.\nYou will still get your $1,000 if you keep the bond for 20 years, but if you sold\nit now, you would lose $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "306.56",
      "section_title": "\u2212 $261.41) = $45.15.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "45.15. This is a return of\n1\n     Treasury bills have maturities of 1 year or less, Treasury notes have maturities\n     from 1 to 10 years, and Treasury bonds have maturities from 10 to 30 years.\n\f                                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "45.15",
      "section_title": "This is a return of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2 Zero-Coupon Bonds         21\n\n                                \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Zero-Coupon Bonds         21",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "45.15\n                                       = \u221214.73 %\n                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "45.15",
      "section_title": "= \u221214.73 %",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "306.56\nfor a half-year, or \u221229.46,% per year. And the interest rate only changed from\n6 % to 7 %!2 Notice that the interest rate went up and the bond price went\ndown. This is a general phenomenon. Bond prices always move in the opposite\ndirection of interest rates.\n    If the interest rate dropped to 5 % after 6 months, then your bond would\nbe worth\n                               $1, 000\n                                        = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "306.56",
      "section_title": "for a half-year, or \u221229.46,% per year. And the interest rate only changed from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "381.74.\n                              (1.025)39\nThis would be an annual rate of return of\n                      \u0007                  \b\n                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "381.74",
      "section_title": "(1.025)39",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "381.74 \u2212 306.56\n                    2                      = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "381.74",
      "section_title": "\u2212 306.56",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "49.05 %.\n                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "49.05",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "306.56\n\nIf the interest rate remained unchanged at 6 %, then the price of the bond\nwould be\n                              $1, 000\n                                      = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "306.56",
      "section_title": "If the interest rate remained unchanged at 6 %, then the price of the bond",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "315.75.\n                             (1.03)39\nThe annual rate of return would be\n                          \u0007                 \b\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "315.75",
      "section_title": "(1.03)39",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "315.75 \u2212 306.56\n                        2                     = 6 %.\n                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "315.75",
      "section_title": "\u2212 306.56",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "306.56\n\nThus, if the interest rate does not change, you can earn a 6 % annual rate of\nreturn, the same return rate as the interest rate, by selling the bond before\nmaturity. If the interest rate does change, however, the 6 % annual rate of\nreturn is guaranteed only if you keep the bond until maturity.\n\nGeneral Formula\n\nThe price of a zero-coupon bond is given by\n\n                             PRICE = PAR(1 + r)\u2212T\n\nif T is the time to maturity in years and the annual rate of interest is r with\nannual compounding. If we assume semiannual compounding, then the price is\n\n                           PRICE = PAR(1 + r/2)\u22122T .                            (3.1)\n\n\n\n\n2\n    Fortunately for investors, a rate change as large as going from 6 % to 7 % is rare\n    on a 20-year bond.\n\f22         3 Fixed Income Securities\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "306.56",
      "section_title": "Thus, if the interest rate does not change, you can earn a 6 % annual rate of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.3 Coupon Bonds\nCoupon bonds make regular interest payments. Coupon bonds generally sell\nat or near the par value when issued. At maturity, one receives a principal\npayment equal to the par value of the bond and the \ufb01nal interest payment.\n    As an example, consider a 20-year coupon bond with a par value of $1,000\nand 6 % annual coupon rate with semiannual coupon payments, so e\ufb00ectively\nthe 6 % is compounded semiannually. Each coupon payment will be $30. Thus,\nthe bondholder receives 40 payments of $30, one every 6 months plus a prin-\ncipal payment of $1,000 after 20 years. One can check that the present value\nof all payments, with discounting at the 6 % annual rate (3 % semiannual),\nequals $1,000:\n                            40\n                                  30         1000\n                                       t\n                                         +        40\n                                                     = 1000.\n                            t=1\n                                (1.03)     (1.03)\n\nAfter 6 months, if the interest rate is unchanged, then the bond (including\nthe \ufb01rst coupon payment, which is now due) is worth\n          39                                    40\n                30         1000                      30         1000\n                     t\n                       +          = (1.03)                  +               = 1030,\n          t=0\n              (1.03)     (1.03)39              t=1\n                                                   (1.03) t   (1.03)40\n\nwhich is a semiannually compounded 6 % annual return as expected. If the\ninterest rate increases to 7 %, then after 6 months the bond (plus the interest\ndue) is only worth\n     39                                         40\n         30         1000                              30         1000\n              t\n                +         39\n                             = (1.035)                     t\n                                                             +         40\n                                                                             = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.3",
      "section_title": "Coupon Bonds",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "924.49.\n  t=0\n      (1.035)     (1.035)                      t=1\n                                                   (1.035)     (1.035)\n\nThis is an annual return of\n                         \u0007               \b\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "924.49",
      "section_title": "t=0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "924.49 \u2212 1000\n                       2                   = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "924.49",
      "section_title": "\u2212 1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1 %.\n                               1000\n\nIf the interest rate drops to 5 % after 6 months, then the investment is worth\n\n 39                                            40\n        30         1000                              30         1000\n             t\n               +           = (1.025)                        +               = 1,153.70,\n t=0\n     (1.025)     (1.025)39                    t=1\n                                                  (1.025) t   (1.025)40\n                                                                                      (3.2)\nand the annual return is\n                                \u0007                   \b\n                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "15.1",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1153.7 \u2212 1000\n                            2                           = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1153.7",
      "section_title": "\u2212 1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "30.72 %.\n                                        1000\n\f                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "30.72",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4 Yield to Maturity     23\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "Yield to Maturity     23",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.3.1 A General Formula\n\nLet\u2019s derive some useful formulas. If a bond with a par value of PAR matures\nin T years and makes semiannual coupon payments of C and the yield (rate\nof interest) is r per half-year, then the value of the bond when it is issued is\n         2T\n                C           PAR       C                        PAR\n                     t\n                       +         2T\n                                    =    1 \u2212 (1 + r)\u22122T +\n         t=1\n             (1 + r)     (1 + r)      r                     (1 + r)2T\n                                          \u000e           \u000f\n                                      C             C\n                                    =   + PAR \u2212         (1 + r)\u22122T .              (3.3)\n                                      r             r\n\nDerivation of (3.3)\n\nThe summation formula for a \ufb01nite geometric series is\n                                T\n                                            1 \u2212 rT +1\n                                     ri =             ,                           (3.4)\n                               i=0\n                                              1\u2212r\n\nprovided that r = 1. Therefore,\n        2T                    2T \u22121 \u0007          \bt\n               C          C              1                C{1 \u2212 (1 + r)\u22122T }\n                      =                             =\n        t=1\n            (1 + r) t   1 + r t=0       1+r             (1 + r){1 \u2212 (1 + r)\u22121 }\n                         C\n                     =     {1 \u2212 (1 + r)\u22122T }.                                     (3.5)\n                         r\nThe remainder of the derivation is straightforward algebra.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.3",
      "section_title": "1 A General Formula",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4 Yield to Maturity\n\nSuppose a bond with T = 30 and C = 40 is selling for $1,200, $200 above par\nvalue. If the bond were selling at par value, then the interest rate would be\n0.04/half-year (= 0.08/year). The 4 %/half-year rate is called the coupon rate.\n    But the bond is not selling at par value. If you purchase the bond at\n$1,200, you will make less than 8 % per year interest. There are two reasons\nthat the rate of interest is less than 8 %. First, the coupon payments are $40\nor 40/1200 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "Yield to Maturity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.333 %/half-year (or 6.67 %/year) for the $1,200 investment;\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.333",
      "section_title": "%/half-year (or 6.67 %/year) for the $1,200 investment;",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.67 %/year is called the current yield. Second, at maturity you only get back\n$1,000, not the entire $1,200 investment. The current yield of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.67",
      "section_title": "%/year is called the current yield. Second, at maturity you only get back",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.67 %/year,\nthough less than the coupon rate of 8 %/year, overestimates the return since\nit does not account for this loss of capital.\n\f24      3 Fixed Income Securities\n\n   The yield to maturity, often shortened to simply yield, is the average rate of\nreturn, including the loss (or gain) of capital because the bond was purchased\nabove (or below) par. For this bond, the yield to maturity is the value of r\nthat solves                        \u000e            \u000f\n                             40              40\n                    1200 =      + 1000 \u2212          (1 + r)\u221260 .              (3.6)\n                              r               r\nThe right-hand side of (3.6) is (3.3) with C = 40, T = 30, and PAR = 1000.\nIt is easy to solve equation (3.6) numerically. The R program in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.67",
      "section_title": "%/year,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10.1\ndoes the following:\n\u2022    computes the bond price for each r value on a grid;\n\u2022    graphs bond price versus r (this is not necessary, but it is fun to see the\n     graph); and\n\u2022    interpolates to \ufb01nd the value of r such that the bond value equals $1,200.\n\n\n                                        par = 1000, coupon payment = 40, T = 30\n                         1600\n                         1400\n         price of bond\n                         1200\n                         1000\n                         800\n\n\n\n\n                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020    0.025   0.030    0.035      0.040   0.045   0.050\n                                                     yield to maturity\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "0.025   0.030    0.035      0.040   0.045   0.050",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. Bond price versus yield to maturity. The horizontal red line is at the bond\nprice of $1,200. The price/yield curve intersects this line at ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Bond price versus yield to maturity. The horizontal red line is at the bond",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0324 as indicated by\nthe vertical red line. Therefore, ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0324",
      "section_title": "as indicated by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0324 is the bond\u2019s yield.\n\n\n\n\nOne \ufb01nds that the yield to maturity is 0.0324, that is, ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0324",
      "section_title": "is the bond\u2019s yield.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.24 %/half-year. Fig-\nure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.24",
      "section_title": "%/half-year. Fig-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1 shows the graph of bond price versus the yield (r) and shows that\nr = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "shows the graph of bond price versus the yield (r) and shows that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0324 maps to a bond price of $1,200.\n   The yield to maturity of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0324",
      "section_title": "maps to a bond price of $1,200.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0324 is less than the current yield of 0.0333,\nwhich is less than the coupon rate of 40/1000 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0324",
      "section_title": "is less than the current yield of 0.0333,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04. (All three rates are rates\nper half-year.) Whenever, as in this example, the bond is selling above par\n\f                                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "(All three rates are rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4 Yield to Maturity      25\n\nvalue, then the coupon rate is greater than the current yield because the bond\nsells above par value, and the current yield is greater than the yield to maturity\nbecause the yield to maturity accounts for the loss of capital when at the\nmaturity date you get back only the par value, not the entire investment. In\nsummary,\n\n      price > par \u21d2 coupon rate > current yield > yield to maturity.\n\nEverything is reversed if the bond is selling below par value. For example, if the\nprice of the bond were only $900, then the yield to maturity would be ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "Yield to Maturity      25",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "par \u21d2 coupon rate > current yield > yield to maturity.",
        "start": 367,
        "end": 425
      }
    ]
  },
  {
    "content": "0.0448\n(as before, this value can be determined by interpolation), the current yield\nwould be 40/900 = 0.0444, and the coupon rate would still be 40/1000 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0448",
      "section_title": "(as before, this value can be determined by interpolation), the current yield",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04.\nIn general,\n\n      price < par \u21d2 coupon rate < current yield < yield to maturity.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "In general,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4.1 General Method for Yield to Maturity\n\nThe yield to maturity (on a semiannual basis) of a coupon bond is the value\nof r that solves\n                                \u000e           \u000f\n                            C            C\n                 PRICE =       + PAR \u2212        (1 + r)\u22122T .            (3.7)\n                             r            r\n\nHere PRICE is the market price of the bond, PAR is the par value, C is\nthe semiannual coupon payment, and T is the time to maturity in years and\nassumed to be a multiple of 1/2.\n   For a zero-coupon bond, C = 0 and (3.7) becomes\n\n                          PRICE = PAR(1 + r)\u22122T .                           (3.8)\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "1 General Method for Yield to Maturity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4.2 Spot Rates\n\nThe yield to maturity of a zero-coupon bond of maturity n years is called\nthe n-year spot rate and is denoted by yn . One uses the n-year spot rate to\ndiscount a payment n years from now, so a payment of $1 to be made n years\nfrom now has a net present value (NPV) of $1/(1 + yn )n if yn is the spot rate\nper annum or $1/(1 + yn )2n if yn is a semiannual rate.\n    A coupon bond is a bundle of zero-coupon bonds, one for each coupon\npayment and a \ufb01nal one for the principal payment. The component zeros\nhave di\ufb00erent maturity dates and therefore di\ufb00erent spot rates. The yield to\nmaturity of the coupon bond is, thus, a complex \u201caverage\u201d of the spot rates\nof the zeros in this bundle.\n\f26     3 Fixed Income Securities\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "2 Spot Rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. Finding the price and yield to maturity of a coupon bond using\nspot rates\n\n   Consider the simple example of 1-year coupon bond with semiannual\ncoupon payments of $40 and a par value of $1,000. Suppose that the one-half-\nyear spot rate is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Finding the price and yield to maturity of a coupon bond using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 %/half-year and the 1-year spot rate is 3 %/half-year.\nThink of the coupon bond as being composed of two zero-coupon bonds, one\nwith T = 1/2 and a par value of $40 and the second with T = 1 and a par\nvalue of $1,040. The price of the bond is the sum of the prices of these two\nzeros. Applying (3.8) twice to obtain the prices of these zeros and summing,\nwe obtain the price of the zero-coupon bond:\n                           40      1040\n                                +         = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "%/half-year and the 1-year spot rate is 3 %/half-year.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1019.32.\n                         1.025 (1.03)2\nThe yield to maturity on the coupon bond is the value of y that solves\n                          40     1040\n                              +        = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1019.32",
      "section_title": "1.025 (1.03)2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1019.32.\n                        1 + y (1 + y)2\nThe solution is y = 0.0299/half-year. Thus, the annual yield to maturity is\ntwice 0.0299, or ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1019.32",
      "section_title": "1 + y (1 + y)2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.98 %/year.                                            \u0002\n\nGeneral Formula\nIn this section we will \ufb01nd a formula that generalizes Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.98",
      "section_title": "%/year.                                            \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. Suppose\nthat a coupon bond pays semiannual coupon payments of C, has a par value\nof PAR, and has T years until maturity. Let y1 , y2 , . . . , y2T be the half-year\nspot rates for zero-coupon bonds of maturities 1/2, 1, 3/2, . . . , T years. Then\nthe yield to maturity (on a half-year basis) of the coupon bond is the value\nof y that solves\n           C          C                     C              PAR + C\n                +            + \u00b7\u00b7\u00b7 +                    +\n         1 + y1   (1 + y2 )2         (1 + y2T \u22121 )2T \u22121   (1 + yn )2T\n                  C          C                 C          PAR + C\n             =        +         2\n                                  + \u00b7\u00b7\u00b7 +         2T \u22121\n                                                        +             .     (3.9)\n                1 + y (1 + y)             (1 + y)          (1 + y)2T\nThe left-hand side of Eq. (3.9) is the price of the coupon bond, and the yield\nto maturity is the value of y that makes the right-hand side of (3.9) equal to\nthe price.\n   Methods for solving (3.9) are explored in the R lab in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Suppose",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10.\n\n\n3.5 Term Structure\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "3.5 Term Structure",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5.1 Introduction: Interest Rates Depend Upon Maturity\nOn January 26, 2001, the 1-year T-bill rate was ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "1 Introduction: Interest Rates Depend Upon Maturity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.83 % and the 30-year\nTreasury bond rate was ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.83",
      "section_title": "% and the 30-year",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.11 %. This is typical. Short- and long-term rates usu-\nally di\ufb00er. Often short-term rates are lower than long-term rates. This makes\n\f                                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.11",
      "section_title": "%. This is typical. Short- and long-term rates usu-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 Term Structure       27\n\nsense since long-term bonds are riskier, because long-term bond prices \ufb02uc-\ntuate more with interest-rate changes. However, during periods of very high\nshort-term rates, the short-term rates may be higher than the long-term rates.\nThe reason is that the market believes that rates will return to historic lev-\nels and no one will commit to the high interest rate for, say, 20 or 30 years.\nFigure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "Term Structure       27",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2 shows weekly values of the 90-day, 10-year, and 30-year Treasury\nrates from 1970 to 1993, inclusive. Notice that the 90-day rate is more volatile\nthan the longer-term rates and is usually less than them. However, in the early\n1980s, when interest rates were very high, the short-term rates were higher\nthan the long-term rates. These data were taken from the Federal Reserve\nBank of Chicago\u2019s website.\n    The term structure of interest rates is a description of how, at a given\ntime, yield to maturity depends on maturity.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "shows weekly values of the 90-day, 10-year, and 30-year Treasury",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5.2 Describing the Term Structure\n\nTerm structure for all maturities up to n years can be described by any one\nof the following:\n                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "2 Describing the Term Structure",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.16\n\n\n\n\n                                                                3\u2212month\n                                                                10\u2212year\n                                                                30\u2212year\n                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.16",
      "section_title": "3\u2212month",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n         rate\n                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.12",
      "section_title": "rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n                0.04\n\n\n\n\n                       1980            1985            1990\n                                          year\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2. Treasury rates of three maturities. Weekly time series. The data were\ntaken from the website of the Federal Reserve Bank of Chicago.\n\n\n\n\n\u2022   prices of zero-coupon bonds of maturities 1-year, 2-years, . . . , n-years are\n    denoted here by P (1), P (2), . . . , P (n);\n\u2022   spot rates (yields of maturity of zero-coupon bonds) of maturities 1-year,\n    2-years, . . . , n-years are denoted by y1 , . . . , yn ;\n\f28       3 Fixed Income Securities\n\n\u2022    forward rates r1 , . . . , rn , where ri is the forward rate that can be locked in\n     now for borrowing in the ith future year (i = 1 for next year, and so on).\n\nAs discussed in this section, each of the sets {P (1), . . . , P (n)}, {y1 , . . . , yn },\nand {r1 , . . . , rn } can be computed from either of the other sets. For example,\nequation (3.11) ahead gives {P (1), . . . , P (n)} in terms of {r1 , . . . , rn }, and\nequations (3.12) and (3.13) ahead give {y1 , . . . , yn } in terms of {P (1), . . . ,\nP (n)} or {r1 , . . . , rn }, respectively.\n    Term structure can be described by breaking down the time interval be-\ntween the present time and the maturity time of a bond into short time\nsegments with a constant interest rate within each segment, but with interest\nrates varying between segments. For example, a 3-year loan can be considered\nas three consecutive 1-year loans, or six consecutive half-year loans, and so\nforth.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Treasury rates of three maturities. Weekly time series. The data were",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2. Finding prices from forward rates\n\n   As an illustration, suppose that loans have the forward interest rates listed\nin Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Finding prices from forward rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. Using the forward rates in the table, we see that a par $1,000\n1-year zero would sell for\n                          1000    1000\n                                =      = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Using the forward rates in the table, we see that a par $1,000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "943.40 = P (1).\n                         1 + r1   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "943.40",
      "section_title": "= P (1).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.06\nA par $1,000 2-year zero would sell for\n                      1000               1000\n                                   =              = $",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.06",
      "section_title": "A par $1,000 2-year zero would sell for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "881.68 = P (2),\n                (1 + r1 )(1 + r2 )   (1.06)(1.07)\n\nsince the rate r1 is paid the \ufb01rst year and r2 the following year. Similarly, a\npar $1,000 3-year zero would sell for\n                    1000                      1000\n                                     =                    = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "881.68",
      "section_title": "= P (2),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "816.37 = P (3).\n         (1 + r1 )(1 + r2 )(1 + r3 )   (1.06)(1.07)(1.08)\n\n\n\n          Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "816.37",
      "section_title": "= P (3).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. Forward interest rates used in Examples 3.2 and 3.3\n                           Year (i) Interest rate (ri )(%)\n                              1              6\n                              2              7\n                              3              8\n\n\n\n                                                                                       \u0002\n\f                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.1",
      "section_title": "Forward interest rates used in Examples 3.2 and 3.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 Term Structure      29\n\n   The general formula for the present value of $1 paid n periods from now is\n                                       1\n                                                            .                       (3.10)\n                         (1 + r1 )(1 + r2 ) \u00b7 \u00b7 \u00b7 (1 + rn )\nHere ri is the forward interest rate during the ith period. If the periods are\nyears, then the price of an n-year par $1,000 zero-coupon bond P (n) is $1,000\ntimes the discount factor in (3.10); that is,\n                                            1000\n                         P (n) =                              .                     (3.11)\n                                    (1 + r1 ) \u00b7 \u00b7 \u00b7 (1 + rn )\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "Term Structure      29",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.3. Back to Example 3.2: Finding yields to maturity from prices and\nfrom the forward rates\n\n    In this example, we \ufb01rst \ufb01nd the yields to maturity from the prices derived\nin Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.3",
      "section_title": "Back to Example 3.2: Finding yields to maturity from prices and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2 using the interest rates from Table 3.1. For a 1-year zero, the\nyield to maturity y1 solves\n                                  1000\n                                          = 943.40,\n                                (1 + y1 )\nwhich implies that y1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "using the interest rates from Table 3.1. For a 1-year zero, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06. For a 2-year zero, the yield to maturity y2 solves\n                                  1000\n                                           = 881.68,\n                                (1 + y2 )2\nso that                         \u0010\n                                     1000\n                         y2 =              \u2212 1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "For a 2-year zero, the yield to maturity y2 solves",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0650.\n                                    881.68\nFor a 3-year zero, the yield to maturity y3 solves\n                                  1000\n                                           = 816.37,\n                                (1 + y3 )3\nand equals ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0650",
      "section_title": "881.68",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.070.\n   The yields can also be found from the forward rates. First, trivially, y1 =\nr1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.070",
      "section_title": "The yields can also be found from the forward rates. First, trivially, y1 =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06. Next, y2 is given by\n              \u0011                       \u0011\n         y2 = (1 + r1 )(1 + r2 ) \u2212 1 = (1.06)(1.07) \u2212 1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "Next, y2 is given by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0650.\nAlso,\n                                                         1/3\n                  y3 = {(1 + r1 )(1 + r2 )(1 + r3 )}           \u22121\n                                                 1/3\n                     = {(1.06)(1.07)(1.08)}            \u2212 1 = 0.0700,\nor, more precisely, ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0650",
      "section_title": "Also,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06997. Thus, (1 + y3 ) is the geometric average of 1.06,\n1.07, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06997",
      "section_title": "Thus, (1 + y3 ) is the geometric average of 1.06,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.08 and very nearly equal to their arithmetic average, which is 1.07.\n\u0002\n\f30     3 Fixed Income Securities\n\n   Recall that P (n) is the price of a par $1,000 n-year zero-coupon bond. The\ngeneral formulas for the yield to maturity yn of an n-year zero are\n                                    \u000e           \u000f1/n\n                                        1000\n                             yn =                      \u2212 1,                (3.12)\n                                        P (n)\n\nto calculate the yield from the price, and\n                                                          1/n\n                      yn = {(1 + r1 ) \u00b7 \u00b7 \u00b7 (1 + rn )}          \u22121         (3.13)\n\nto obtain the yield from the forward rate.\n    Equations (3.12) and (3.13) give the yields to maturity in terms of the\nbond prices and forward rates, respectively. Also, inverting (3.12) gives the\nformula\n                                         1000\n                              P (n) =                                  (3.14)\n                                      (1 + yn )n\nfor P (n) as a function of the yield to maturity.\n    As mentioned before, interest rates for future years are called forward\nrates. A forward contract is an agreement to buy or sell an asset at some \ufb01xed\nfuture date at a \ufb01xed price. Since r2 , r3 , . . . are rates that can be locked in\nnow for future borrowing, they are forward rates.\n    The general formulas for determining forward rates from yields to maturity\nare\n                                    r1 = y1 ,                               (3.15)\nand\n                             (1 + yn )n\n                   rn =                  \u2212 1,          n = 2, 3, . . . .   (3.16)\n                          (1 + yn\u22121 )n\u22121\nNow suppose that we only observed bond prices. Then we can calculate yields\nto maturity and forward rates using (3.12) and then (3.16).\n\n\n                  Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.08",
      "section_title": "and very nearly equal to their arithmetic average, which is 1.07.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2. Bond prices used in Example 3.4\n                                 Maturity Price\n                                  1 Year $920\n                                  2 Years $830\n                                  3 Years $760\n\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Bond prices used in Example 3.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4. Finding yields and forward rates from prices\n\n    Suppose that one-, two-, and three-year par $1,000 zeros are priced as\ngiven in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "Finding yields and forward rates from prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2. Using (3.12), the yields to maturity are\n\f                                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "Using (3.12), the yields to maturity are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 Term Structure      31\n\n                              1000\n                         y1 =       \u2212 1 = 0.087,\n                               920\n                              \u000e      \u000f1/2\n                                1000\n                         y2 =             \u2212 1 = 0.0976,\n                                 830\n                              \u000e      \u000f1/3\n                                1000\n                         y3 =             \u2212 1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "Term Structure      31",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.096.\n                                 760\nThen, using (3.15) and (3.16),\n               r1 = y1 = 0.087,\n                    (1 + y2 )2       (1.0976)2\n               r2 =            \u22121=             \u2212 1 = 0.108, and\n                     (1 + y1 )         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.096",
      "section_title": "760",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0876\n                    (1 + y3 )3        (1.096)3\n               r3 =            \u2212 1 =           \u2212 1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0876",
      "section_title": "(1 + y3 )3        (1.096)3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.092.\n                    (1 + y2 )2       (1.0976)2\n                                                                                       \u0002\n    The formula for \ufb01nding rn from the prices of zero-coupon bonds is\n                                        P (n \u2212 1)\n                                 rn =             \u2212 1,                             (3.17)\n                                          P (n)\nwhich can be derived from\n                                             1000\n                      P (n) =                                       ,\n                                 (1 + r1 )(1 + r2 ) \u00b7 \u00b7 \u00b7 (1 + rn )\nand\n                                                1000\n                   P (n \u2212 1) =                                         .\n                                  (1 + r1 )(1 + r2 ) \u00b7 \u00b7 \u00b7 (1 + rn\u22121 )\nTo calculate r1 using (3.17), we need P (0), the price of a 0-year bond, but\nP (0) is simply the par value.3\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.092",
      "section_title": "(1 + y2 )2       (1.0976)2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5. Forward rates from prices\n\n    Thus, using (3.17) and the prices in Table 3.2, the forward rates are\n                                     1000\n                             r1 =         \u2212 1 = 0.087,\n                                     920\n                                      920\n                             r2 =         \u2212 1 = 0.108,\n                                      830\nand\n                                      830\n                             r3 =         \u2212 1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "Forward rates from prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.092.\n                                      760\n                                                                                       \u0002\n3\n    Trivially, a bond that must be paid back immediately is worth exactly its par\n    value.\n\f32      3 Fixed Income Securities\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.092",
      "section_title": "760",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.6 Continuous Compounding\nNow assume continuous compounding with forward rates r1 , . . . , rn . Using\ncontinuously compounded rates simpli\ufb01es the relationships among the forward\nrates, the yields to maturity, and the prices of zero-coupon bonds.\n   If P (n) is the price of a $1,000 par value n-year zero-coupon bond, then\n                                             1000\n                          P (n) =                              .                   (3.18)\n                                    exp(r1 + r2 + \u00b7 \u00b7 \u00b7 + rn )\nTherefore,\n                   P (n \u2212 1)    exp(r1 + \u00b7 \u00b7 \u00b7 + rn )\n                             =                         = exp(rn ),                 (3.19)\n                     P (n)     exp(r1 + \u00b7 \u00b7 \u00b7 + rn\u22121 )\nand                                 \u000e              \u000f\n                                    P (n \u2212 1)\n                                log                     = rn .                     (3.20)\n                                      P (n)\nThe yield to maturity of an n-year zero-coupon bond solves the equation\n                                               1000\n                                  P (n) =              ,\n                                             exp(nyn )\nand is easily seen to be\n                               yn = (r1 + \u00b7 \u00b7 \u00b7 + rn )/n.                          (3.21)\nTherefore, {r1 , . . . , rn } is easily found from {y1 , . . . , yn } by the relationship\n                                         r1 = yn ,\nand\n                         rn = nyn \u2212 (n \u2212 1)yn\u22121 for n > 1.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.6",
      "section_title": "Continuous Compounding",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1.",
        "start": 1521,
        "end": 1527
      }
    ]
  },
  {
    "content": "3.6. Continuously compounded forward rates and yields from prices\n\n    Using the prices in Table 3.2, we have P (1) = 920, P (2) = 830, and\nP (3) = 760. Therefore, using (3.20),\n                                   \u000e      \u000f\n                                     1000\n                          r1 = log          = 0.083,\n                                      920\n                                    \u000e     \u000f\n                                      920\n                           r2 = log         = 0.103,\n                                      830\nand                                      \u000e         \u000f\n                                             830\n                              r3 = log                 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.6",
      "section_title": "Continuously compounded forward rates and yields from prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.088.\n                                             760\nAlso, y1 = r1 = 0.083, y2 = (r1 + r2 )/2 = 0.093, and y3 = (r1 + r2 + r3 )/3 =\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.088",
      "section_title": "760",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.091.                                                                       \u0002\n\f                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.091",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.7 Continuous Forward Rates       33\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.7",
      "section_title": "Continuous Forward Rates       33",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.7 Continuous Forward Rates\nSo far, we have assumed that forward interest rates vary from year to year but\nare constant within each year. This assumption is, of course, unrealistic and\nwas made only to simplify the introduction of forward rates. Forward rates\nshould be modeled as a function varying continuously in time.\n    To specify the term structure in a realistic way, we assume that there is a\nfunction r(t) called the forward-rate function such that the current price of a\nzero-coupon bond of maturity T and with par value equal to 1 is given by\n                                     \u0012 \u0013            \u0014\n                                                T\n                         D(T ) = exp \u2212              r(t)dt .               (3.22)\n                                            0\n\n\nD(T ) is called the discount function and the price of any zero-coupon bond is\ngiven by discounting its par value by multiplication with the discount function;\nthat is,\n                             P (T ) = PAR \u00d7 D(T ),                        (3.23)\nwhere P (T ) is the price of a zero-coupon bond of maturity T with par value\nequal to PAR. Also,\n                                              \u0013 T\n                      log P (T ) = log(PAR) \u2212     r(t)dt,\n                                                      0\n\nso that\n                           d\n                        \u2212     log P (T ) = r(T ) for all T.          (3.24)\n                          dT\n   Formula (3.22) is a generalization of formula (3.18). To appreciate this,\nsuppose that r(t) is the piecewise constant function\n\n                            r(t) = rk for k \u2212 1 < t \u2264 k.\n\nWith this piecewise constant r, for any integer T , we have\n                      \u0013 T\n                          r(t)dr = r1 + r2 + \u00b7 \u00b7 \u00b7 + rT ,\n                         0\n\nso that             \u0012 \u0013         \u0014\n                        T\n                 exp \u2212    r(t)dt = exp{\u2212(r1 + \u00b7 \u00b7 \u00b7 + rT )}\n                             0\n\nand therefore (3.18) agrees with (3.22) in this special situation. However, (3.22)\nis a more general formula since it applies to noninteger T and to arbitrary\nr(t), not only to piecewise constant functions.\n    The yield to maturity of a zero-coupon bond with maturity date T is\nde\ufb01ned to be\n\f34     3 Fixed Income Securities\n                                         \u0013 T\n                                     1\n                              yT =             r(t) dt.                    (3.25)\n                                     T    0\n\nThinking of the right-hand side of (3.25) as the average of r(t) over the interval\n0 \u2264 t \u2264 T , we see that (3.25) is the analog of (3.21). From (3.22) and (3.25) it\nfollows that the discount function can be obtained from the yield to maturity\nby the formula\n                             D(T ) = exp{\u2212T yT },                           (3.26)\nso that the price of a zero-coupon bond maturing at time T is the same as it\nwould be if there were a constant forward interest rate equal to yT . It follows\nfrom (3.26) that\n                             yT = \u2212 log{D(T )}/T.                         (3.27)\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.7",
      "section_title": "Continuous Forward Rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.7. Finding continuous yield and discount functions from forward\nrates\n\n   Suppose the forward rate is the linear function r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.7",
      "section_title": "Finding continuous yield and discount functions from forward",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + 0.0005 t. Find\nr(15), y15 , and D(15).\nAnswer: r(15) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ 0.0005 t. Find",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + (0.0005)(15) = 0.0375,\n                          \u0013 15\n                       \u22121\n            y15 = (15)         (",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ (0.0005)(15) = 0.0375,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + 0.0005 t)dt\n                               0\n                                                   \u001515\n                                                   \u0015\n                                                   \u0015\n                   = (15)\u22121 (",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ 0.0005 t)dt",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 t + 0.0005 t2 /2)\u0015 = 0.03375,\n                                                   \u0015\n                                                          0\n\nand D(15) = exp(\u221215y15 ) = exp{\u2212(15)(0.03375)} = exp(\u22120.5055) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "t + 0.0005 t2 /2)\u0015 = 0.03375,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.603.\n\u0002\n    The linear forward rate in Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.603",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.7 was chosen for simplicity and is\nnot realistic. The Nelson-Siegel and Svensson parametric families of curves\nintroduced in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.7",
      "section_title": "was chosen for simplicity and is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3 are used in practice to model forward rates and yield\ncurves. The European Community Bank uses the Svensson family. Nonpara-\nmetric estimation of a forward rate by local polynomial and spline estimation\nis discussed in Examples ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "11.3",
      "section_title": "are used in practice to model forward rates and yield",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1 and 21.3, respectively. The Federal Reserve, the\nBank of England, and the Bank of Canada use splines. The European Central\nBank uses the Svensson family.\n    The discount function D(T ) and forward-rate function r(t) in formula (3.22)\ndepend on the current time, which is taken to be zero in that formula. How-\never, we could be interested in how the discount function and forward rate\nfunction change over time. In that case we de\ufb01ne the discount function D(s, T )\nto be the price at time s of a zero-coupon bond, with a par value of $1, ma-\nturing at time T . Also, if the forward-rate curve at time s is r(s, t), t \u2265 s,\nthen\n\f                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "21.1",
      "section_title": "and 21.3, respectively. The Federal Reserve, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.8 Sensitivity of Price to Yield      35\n                                     \u0012 \u0013            \u0014\n                                         T\n                       D(s, T ) = exp \u2212    r(s, t)dt .                        (3.28)\n                                             s\n\nThe yield at time s of a bond maturing at time T > s is\n                                             \u0013 T\n                      y(s, T ) = (T \u2212 s)\u22121           r(s, u)du.\n                                                 s\n\n    Since r(t) and D(t) in (3.22) are r(0, t) and D(0, t) in our new nota-\ntion, (3.22) is the special case of (3.28) with s = 0. Similarly, yT is equal to\ny(0, T ) in the new notation. However, for the remainder of this chapter we\nassume that s = 0 and return to the simpler notation of r(t) and D(t).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.8",
      "section_title": "Sensitivity of Price to Yield      35",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "s is\n                                             \u0013 T\n                      y(s, T ) = (T \u2212 s)\u22121           r(s, u)du.\n                                                 s",
        "start": 321,
        "end": 493
      }
    ]
  },
  {
    "content": "3.8 Sensitivity of Price to Yield\nAs we have seen, bonds are risky because bond prices are sensitive to inter-\nest rates. This problem is called interest-rate risk. This section describes a\ntraditional method of quantifying interest-rate risk.\n    Using Eq. (3.26), we can approximate how the price of a zero-coupon bond\nchanges if there is a small change in yield. Suppose that yT changes to yT + \u03b4,\nwhere the change in yield \u03b4 is small. Then the change in D(T ) is approximately\n\u03b4 times\n                 d\n                     exp{\u2212T yT } \u2248 \u2212T exp{\u2212T yT } = \u2212T D(T ).             (3.29)\n               dyT\nTherefore, by Eq. (3.23), for a zero-coupon bond of maturity T ,\n\n                 change bond price\n                                   \u2248 \u2212T \u00d7 change in yield.                    (3.30)\n                    bond price\nIn this equation \u201c\u2248\u201d means that the ratio of the right- to left-hand sides\nconverges to 1 as \u03b4 \u2192 0.\n    Equation (3.30) is worth examining. The minus sign on the right-hand side\nshows us something we already knew, that bond prices move in the opposite\ndirection to interest rates. Also, the relative change in the bond price, which\nis the left-hand side of the equation, is proportional to T , which quanti\ufb01es the\nprinciple that longer-term bonds have higher interest-rate risks than short-\nterm bonds.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.8",
      "section_title": "Sensitivity of Price to Yield",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.8.1 Duration of a Coupon Bond\n\nRemember that a coupon bond can be considered a bundle of zero-coupon\nbonds of various maturities. The duration of a coupon bond, which we will\ndenote by DUR, is the weighted average of these maturities with weights in\n\f36       3 Fixed Income Securities\n\nproportion to the net present value of the cash \ufb02ows (coupon payments and\npar value at maturity).\n   Now assume that all yields change by a constant amount \u03b4, that is, yT\nchanges to yT + \u03b4 for all T . This restrictive assumption is needed to de\ufb01ne\nduration. Because of this assumption, Eq. (3.30) applies to each of these cash\n\ufb02ows and averaging them with these weights gives us that for a coupon bond,\n                        change bond price\n                                          \u2248 \u2212DUR \u00d7 \u03b4.                       (3.31)\n                           bond price\n\nThe details of the derivation of (3.31) are left as an exercise (Exercise 15).\nDuration analysis uses (3.31) to approximate the e\ufb00ect of a change in yield\non bond prices.\n   We can rewrite (3.31) as\n\n                                  \u22121    change in price\n                        DUR \u2248         \u00d7                                     (3.32)\n                                 price change in yield\n\nand use (3.32) as a de\ufb01nition of duration. Notice that \u201cbond price\u201d has been\nreplaced by \u201cprice.\u201d The reason for this is that (3.32) can de\ufb01ne the durations\nof not only bonds but also of derivative securities whose prices depend on\nyield, for example, call options on bonds. When this de\ufb01nition is extended\nto derivatives, duration has nothing to do with maturities of the underlying\nsecurities. Instead, duration is solely a measure of sensitivity of price to yield.\nTuckman (2002) gives an example of a 10-year coupon bond with a duration\nof ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.8",
      "section_title": "1 Duration of a Coupon Bond",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.79 years and a call option on this bond with a duration of 120.82 years.\nThese durations show that the call is much riskier than the bond since it is\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.79",
      "section_title": "years and a call option on this bond with a duration of 120.82 years.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5 (= 129.82/7.79) times more sensitive to changes in yield.\n    Unfortunately, the underlying assumption behind (3.31) that all yields\nchange by the same amount is not realistic, so duration analysis is falling into\ndisfavor and value-at-risk is replacing duration analysis as a method for eval-\nuating interest-rate risk.4 Value-at-risk and other risk measures are covered\nin Chap. 19.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "15.5",
      "section_title": "(= 129.82/7.79) times more sensitive to changes in yield.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.9 Bibliographic Notes\n\nTuckman (2002) is an excellent comprehensive treatment of \ufb01xed income\nsecurities; it is written at an elementary mathematical level and is highly rec-\nommended for readers wishing to learn more about this topic. Bodie, Kane,\nand Marcus (1999), Sharpe, Alexander, and Bailey (1999), and Campbell,\nLo, and MacKinlay (1997) provide good introductions to \ufb01xed income securi-\nties, with the last-named being at a more advanced level. James and Webber\n(2000) is an advanced book on interest rate modeling. Jarrow (2002) covers\n4\n     See Dowd (1998).\n\f                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.9",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10 R Lab     37\n\nmany advanced topics that are not included in this book, including modeling\nthe evolution of term structure, bond trading strategies, options and futures\non bonds, and interest-rate derivatives.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "R Lab     37",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10 R Lab\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10.1 Computing Yield to Maturity\n\nThe following R function computes the price of a bond given its coupon pay-\nment, maturity, yield to maturity, and par value.\n   bondvalue = function(c, T, r, par)\n   {\n   #       Computes bv = bond values (current prices) corresponding\n   #       to all values of yield to maturity in the\n   #       input vector r\n   #\n   #       INPUT\n   #        c = coupon payment (semiannual)\n   #        T = time to maturity (in years)\n   #        r = vector of yields to maturity (semiannual rates)\n   #        par = par value\n   #\n   bv = c / r + (par - c / r) * (1 + r)^(-2 * T)\n   bv\n   }\n\nThe R code that follows computes the price of a bond for 300 semiannual\ninterest rates between ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "1 Computing Yield to Maturity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02 and 0.05 for a 30-year par $1,000 bond with coupon\npayments of $40. Then interpolation is used to \ufb01nd the yield to maturity if\nthe current price is $1,200.\n\n   price = 1200      #   current price of the bond\n   C = 40            #   coupon payment\n   T= 30             #   time to maturity\n   par = 1000        #   par value of the bond\n\n   r = seq(0.02, 0.05, length = 300)\n   value = bondvalue(C, T, r, par)\n   yield2M = spline(value, r, xout = price) # spline interpolation\n\nThe \ufb01nal bit of R code below plots price as a function of yield to maturity\nand graphically interpolates to show the yield to maturity when the price is\n$1,200.\n   plot(r, value, xlab = \u2019yield to maturity\u2019, ylab = \u2019price of bond\u2019,\n      type = \"l\", main = \"par = 1000, coupon payment = 40,\n\f38      3 Fixed Income Securities\n\n        T = 30\", lwd = 2)\n     abline(h = 1200)\n     abline(v = yield2M)\n\nProblem 1 Use the plot to estimate graphically the yield to maturity. Does\nthis estimate agree with that from spline interpolation?\n\n\n    As an alternative to interpolation, the yield to maturity can be found\nusing a nonlinear root \ufb01nder (equation solver) such as uniroot(), which is\nillustrated here:\n     uniroot(function(r) r^2 - .5, c(0.7, 0.8))\n\nProblem 2 What does the code\n     uniroot(function(r) r^2 - 0.5, c(0.7, 0.8))\n\ndo?\n\n\nProblem 3 Use uniroot() to \ufb01nd the yield to maturity of the 30-year par\n$1,000 bond with coupon payments of $40 that is selling at $1,200.\n\n\nProblem 4 Find the yield to maturity of a par $10,000 bond selling at $9,800\nwith semiannual coupon payments equal to $280 and maturing in 8 years.\n\n\nProblem 5 Use uniroot() to \ufb01nd the yield to maturity of the 20-year par\n$1,000 bond with semiannual coupon payments of $35 that is selling at $1,050.\n\n\nProblem 6 The yield to maturity is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "and 0.05 for a 30-year par $1,000 bond with coupon",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.035 on a par $1,000 bond selling at\n$",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.035",
      "section_title": "on a par $1,000 bond selling at",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "950.10 and maturing in 5 years. What is the coupon payment?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "950.10",
      "section_title": "and maturing in 5 years. What is the coupon payment?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10.2 Graphing Yield Curves\n\nR\u2019s fEcofin package had many interesting \ufb01nancial data sets but is no longer\navailable. The data sets mk.maturity.csv and mk.zero2.csv used in this\nexample were taken from this package and are now available on this book\u2019s\nwebpage. The data set mk.zero2 has yield curves of U.S. zero coupon bonds\nrecorded monthly at 55 maturities. These maturities are in the data set\nmk.maturity. The following code plots the yield curves on four consecutive\nmonths.\n\f                                                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "2 Graphing Yield Curves",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.10 R Lab         39\n\n    mk.maturity = read.csv(\"mk.maturity.csv\", header = T)\n    mk.zero2 = read.csv(\"mk.zero2.csv\", header = T)\n    plot(mk.maturity[,1], mk.zero2[5,2:56], type = \"l\",\n      xlab = \"maturity\", ylab = \"yield\")\n    lines(mk.maturity[,1], mk.zero2[6,2:56], lty = 2, type = \"l\")\n    lines(mk.maturity[,1], mk.zero2[7,2:56], lty = 3, type = \"l\")\n    lines(mk.maturity[,1], mk.zero2[8,2:56], lty = 4, type = \"l\")\n    legend(\"bottomright\", c(\"1985-12-01\", \"1986-01-01\",\n       \"1986-02-01\", \"1986-03-01\"), lty = 1:4)\n\n    Run the code above and then, to zoom in on the short end of the curves,\nrerun the code with maturities restricted to 0 to 3 years; to do that, use xlim\nin the plot function.\n\nProblem 7 Describe how the yield curve changes between December 1, 1985\nand March 1, 1986. Describe the behavior of both the short and long ends of\nthe yield curves.\n\n\nProblem 8 Plot the yield curves from December 1, 1986 to March 1, 1987\nand describe how the yield curve changes during this period.\n\n\n    The next set of code estimates the forward rate for 1 month. Line 1 es-\ntimates the integrated forward rate, called intForward, which is T yT =\n\u0016T\n 0\n    r(t)dt where r(t) is the forward rate. Line 3 interpolates the estimated\nintegrated forward rate onto a grid of 200 points from 0 to 20. This grid is\ncreated on line 2.\n    If a function f is evaluated on a grid, t1 , . . . , tL , then {f (t\u0002 )\u2212f (t\u0002\u22121 )}/(t\u0002 \u2212\nt\u0002\u22121 ) approximates f \u0003 ((t\u0002 + t\u0002\u22121 )/2) for \u0007 = 2, . . . , L. Line 4 numerically dif-\nferentiates the integrated forward rate to approximate the forward rate on the\ngrid calculated at Line 5.\n1 intForward = mk.maturity[, 1] * mk.zero2[6, 2:56]\n2 xout = seq(0, 20, length = 200)\n3 z1 = spline(mk.maturity[ ,1], intForward, xout = xout)\n\n4 forward = diff(z1$y) / diff(z1$x)\n\n5 T_grid = (xout[-1] + xout[-200]) / 2\n\n6 plot(T_grid, forward, type = \"l\", lwd = 2, ylim = c(0.06, 0.11))\n\n\n\nProblem 9 Plot the forward rates on the same dates used before, 1985-12-\n01, 1986-01-01, 1986-02-01, and 1986-03-01. Describe how the forward rates\nchanged from month to month.\n\n\nThe approximate forward rates found by numerically di\ufb00erentiating a inter-\npolating spline are \u201cwiggly.\u201d The wiggles can be removed, or at least reduced,\nby using a penalized spline instead of an interpolating spline. See Chap. 21.\n\f40    3 Fixed Income Securities\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.10",
      "section_title": "R Lab         39",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.11 Exercises\n1. Suppose that the forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.11",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.028 + 0.00042t.\n   (a) What is the yield to maturity of a bond maturing in 20 years?\n   (b) What is the price of a par $1,000 zero-coupon bond maturing in\n       15 years?\n2. Suppose that the forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.028",
      "section_title": "+ 0.00042t.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 + 0.0002t \u2212 0.00003t2 .\n   (a) What is the yield to maturity of a bond maturing in 8 years?\n   (b) What is the price of a par $1,000 zero-coupon bond maturing in\n       5 years?\n   (c) Plot the forward rate and the yield curve. Describe the two curves.\n       Which are convex and which are concave? How do they di\ufb00er?\n   (d) Suppose you buy a 10-year zero-coupon bond and sell it after 1 year.\n       What will be the return if the forward rate does not change during\n       that year?\n3. A coupon bond has a coupon rate of 3 % and a current yield of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "+ 0.0002t \u2212 0.00003t2 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.8 %.\n   (a) Is the bond selling above or below par? Why or why not?\n   (b) Is the yield to maturity above or below ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.8",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.8 %? Why or why not?\n4. Suppose that the forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.8",
      "section_title": "%? Why or why not?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.032 + 0.001t + 0.0002t2 .\n   (a) What is the 5-year continuously compounded spot rate?\n   (b) What is the price of a zero-coupon bond that matures in 5 years?\n5. The 1/2-, 1-, 1.5-, and 2-year semiannually compounded spot rates are\n   0.025, 0.028, 0.032, and 0.033, respectively. A par $1,000 coupon bond\n   matures in 2 years and has semiannual coupon payments of $35. What is\n   the price of this bond?\n6. Verify the following equality:\n           2T                               \u000e         \u000f\n                  C           PAR       C           C\n                       t\n                         +         2T\n                                      =   +   PAR \u2212     (1 + r)\u22122T .\n           t=1\n               (1 + r)     (1 + r)      r           r\n\n7. One year ago a par $1,000 20-year coupon bond with semiannual coupon\n   payments was issued. The annual interest rate (that is, the coupon rate)\n   at that time was ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.032",
      "section_title": "+ 0.001t + 0.0002t2 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5 %. Now, a year later, the annual interest rate is 7.6 %.\n   (a) What are the coupon payments?\n   (b) What is the bond worth now? Assume that the second coupon pay-\n       ment was just received, so the bondholder receives an additional 38\n       coupon payments, the next one in 6 months.\n   (c) What would the bond be worth if instead the second payment were\n       just about to be received?\n8. A par $1,000 zero-coupon bond that matures in 5 years sells for $828.\n   Assume that there is a constant continuously compounded forward rate r.\n   (a) What is r?\n   (b) Suppose that 1 year later the forward rate r is still constant but has\n       changed to be ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "%. Now, a year later, the annual interest rate is 7.6 %.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.042. Now what is the price of the bond?\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.042",
      "section_title": "Now what is the price of the bond?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.11 Exercises      41\n\n    (c) If you bought the bond for the original price of $828 and sold it 1 year\n         later for the price computed in part (b), then what is the net return?\n 9. A coupon bond with a par value of $1,000 and a 10-year maturity pays\n    semiannual coupons of $21.\n    (a) Suppose the yield for this bond is 4 % per year compounded semian-\n         nually. What is the price of the bond?\n    (b) Is the bond selling above or below par value? Why?\n10. Suppose that a coupon bond with a par value of $1,000 and a maturity of\n    7 years is selling for $1,040. The semiannual coupon payments are $23.\n    (a) Find the yield to maturity of this bond.\n    (b) What is the current yield on this bond?\n    (c) Is the yield to maturity less or greater than the current yield? Why?\n11. Suppose that the continuous forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.11",
      "section_title": "Exercises      41",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.033 + 0.0012t. What\n    is the current value of a par $100 zero-coupon bond with a maturity of 15\n    years?\n12. Suppose the continuous forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.033",
      "section_title": "+ 0.0012t. What",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 + 0.001t when a 8-year\n    zero coupon bond is purchased. Six months later the forward rate is r(t) =\n    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "+ 0.001t when a 8-year",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + 0.0013t and bond is sold. What is the return?\n13. Suppose that the continuous forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ 0.0013t and bond is sold. What is the return?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + 0.001t \u2212\n    0.00021(t \u2212 10)+ . What is the yield to maturity on a 20-year zero-coupon\n    bond? Here x+ is the positive part function de\ufb01ned by\n                                       \u000e\n                                         x, x > 0,\n                                 x+ =\n                                         0, x \u2264 0.\n\n14. An investor is considering the purchase of zero-coupon bonds with maturi-\n    ties of one, three, or 5 years. Currently the spot rates for 1-, 2-, 3-, 4-, and\n    5-year zero-coupon bonds are, respectively, 0.031, 0.035, 0.04, 0.042, and\n    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ 0.001t \u2212",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0,\n                                 x+ =\n                                         0, x \u2264 0.",
        "start": 239,
        "end": 334
      }
    ]
  },
  {
    "content": "0.043 per year with semiannual compounding. A \ufb01nancial analyst has ad-\n    vised this investor that interest rates will increase during the next year\n    and the analyst expects all spot rates to increase by the amount 0.005,\n    so that the 1-year spot rate will become 0.036, and so forth. The investor\n    plans to sell the bond at the end of 1 year and wants the greatest return\n    for the year. This problem does the bond math to see which maturity, 1,\n    3, or 5 years, will give the best return under two scenarios: interest rates\n    are unchanged and interest rates increase as forecast by the analyst.\n    (a) What are the current prices of 1-, 3-, and 5-year zero-coupon bonds\n         with par values of $1,000?\n    (b) What will be the prices of these bonds 1 year from now if spot rates\n         remain unchanged?\n    (c) What will be the prices of these bonds 1 year from now if spot rates\n         each increase by 0.005?\n    (d) If the analyst is correct that spot rates will increase by ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.043",
      "section_title": "per year with semiannual compounding. A \ufb01nancial analyst has ad-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005 in 1 year,\n         which maturity, 1, 3, or 5 years, will give the investor the greatest\n         return when the bond is sold after 1 year? Justify your answer.\n\f42      3 Fixed Income Securities\n\n    (e) If instead the analyst is incorrect and spot rates remain unchanged,\n        then which maturity, 1, 3, or 5 years, earns the highest return when\n        the bond is sold after 1 year? Justify your answer.\n    (f) The analyst also said that if the spot rates remain unchanged, then the\n        bond with the highest spot rate will earn the greatest 1-year return.\n        Is this correct? Why?\n    (Hint: Be aware that a bond will not have the same maturity in 1 year as\n    it has now, so the spot rate that applies to that bond will change.)\n15. Suppose that a bond pays a cash \ufb02ow Ci at time Ti for i = 1, . . . , N . Then\n    the net present value (NPV) of cash \ufb02ow Ci is\n\n                             NPVi = Ci exp(\u2212Ti yTi ).\n\n     De\ufb01ne the weights\n                                         NPVi\n                                 \u03c9 i = \u0017N\n                                        j=1 NPVj\n\n     and de\ufb01ne the duration of the bond to be\n                                            N\n                                    DUR =         \u03c9i Ti ,\n                                            i=1\n\n     which is the weighted average of the times of the cash \ufb02ows. Show that\n                                       \u0015\n          d\n              N\n                                       \u0015             N\n                 Ci exp{\u2212Ti (yTi + \u03b4)}\u0015\u0015     = \u2212DUR     Ci exp{\u2212Ti yTi }\n          d\u03b4 i=1                         \u03b4=0        i=1\n\n    and use this result to verify Eq. (3.31).\n16. Assume that the yield curve is YT = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.005",
      "section_title": "in 1 year,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 + 0.001 T .\n    (a) What is the price of a par-$1,000 zero-coupon bond with a maturity\n        of 10 years?\n    (b) Suppose you buy this bond. If 1 year later the yield curve is YT =\n        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "+ 0.001 T .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.042 + 0.001 T , then what will be the net return on the bond?\n17. A coupon bond has a coupon rate of 3 % and a current yield of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.042",
      "section_title": "+ 0.001 T , then what will be the net return on the bond?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.8 %.\n    (a) Is the bond selling above or below par? Why or why not?\n    (b) Is the yield to maturity above or below ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.8",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.8 %? Why or why not?\n18. Suppose that the forward rate is r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.8",
      "section_title": "%? Why or why not?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 + 0.001t + 0.0002t2\n    (a) What is the 5-year spot rate?\n    (b) What is the price of a zero-coupon bond that matures in 5 years?\n19. The 1/2-, 1-, 1.5-, and 2-year spot rates are 0.025, 0.029, 0.031, and 0.035,\n    respectively. A par $1,000 coupon bond matures in 2 years and has semi-\n    annual coupon payments of $35. What is the price of this bond?\n20. Par $1,000 zero-coupon bonds of maturities of 0.5-, 1-, 1.5-, and 2-years\n    are selling at $980.39, $957.41, $923.18, and $888.489, respectively.\n    (a) Find the 0.5-, 1-, 1.5-, and 2-year semiannual spot rates.\n\f                                                              References   43\n\n    (b) A par $1,000 coupon bond has a maturity of 2 years. The semiannual\n        coupon payment is $21. What is the price of this bond?\n21. A par $1,000 bond matures in 4 years and pays semiannual coupon pay-\n    ments of $25. The price of the bond is $1,015. What is the semiannual\n    yield to maturity of this bond?\n22. A coupon bond matures in 4 years. Its par is $1,000 and it makes eight\n    coupon payments of $21, one every one-half year. The continuously com-\n    pounded forward rate is\n\n                  r(t) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "+ 0.001t + 0.0002t2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.022 + 0.005 t \u2212 0.004 t2 + 0.0003 t3 .\n\n   (a) Find the price of the bond.\n   (b) Find the duration of this bond.\n\n\nReferences\n\nBodie, Z., Kane, A., and Marcus, A. (1999) Investments, 4th ed., Irwin/\n  McGraw-Hill, Boston.\nCampbell, J. Y., Lo, A. W., and MacKinlay, A. C. (1997) Econometrics of\n  Financial Markets, Princeton University Press, Princeton, NJ.\nDowd, K. (1998) Beyond Value at Risk, Wiley, Chichester.\nJames, J., and Webber, N. (2000) Interest Rate Modeling, Wiley, Chichester.\nJarrow, R. (2002) Modeling Fixed-Income Securities and Interest Rate\n  Options, 2nd ed., Stanford University Press, Stanford, CA.\nSharpe, W., Alexander, G., and Bailey, J. (1999) Investments, 6th ed.,\n  Prentice-Hall, Englewood Cli\ufb00s, NJ.\nTuckman, B. (2002) Fixed Income Securities, 2nd ed., Wiley, Hoboken, NJ.\n\f4\nExploratory Data Analysis\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.022",
      "section_title": "+ 0.005 t \u2212 0.004 t2 + 0.0003 t3 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 Introduction\nThis book is about the statistical analysis of \ufb01nancial markets data such as\nequity prices, foreign exchange rates, and interest rates. These quantities vary\nrandomly thereby causing \ufb01nancial risk as well as the opportunity for pro\ufb01t.\nFigures 4.1, 4.2, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3 show, respectively, time series plots of daily log returns\non the S&P 500 index, daily changes in the Deutsch Mark (DM) to U.S. dollar\nexchange rate, and changes in the monthly risk-free return, which is 1/12th\nthe annual risk-free interest rate. A time series is a sequence of observations\n\n\n\n                                      S&P 500 daily returns\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "show, respectively, time series plots of daily log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n         log return\n                      \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "log return",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n                      \u22120.20\n\n\n\n\n                              1982   1984    1986      1988    1990\n                                              year\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "\u22120.20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1. Daily log returns on the S&P 500 index from January 1981 to April 1991.\nThis data set is the variable r500 in the SP500 series in the Ecdat package in R.\nNotice the extreme volatility in October 1987.\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                                  45\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 4\n\f46        4 Exploratory Data Analysis\n\nof some quantity or quantities, e.g., equity prices, taken over time, and a time\nseries plot is a plot of a time series in chronological order. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "Daily log returns on the S&P 500 index from January 1981 to April 1991.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 was\nproduced by the following code:\n     data(SP500, package = \"Ecdat\")\n     SPreturn = SP500$r500\n     n = length(SPreturn)\n     year_SP = 1981 + (1:n) * (",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "was",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1991.25 - 1981) / n\n     plot(year_SP, SPreturn, main = \"S&P 500 daily returns\",\n          xlab = \"year\", type = \"l\", ylab = \"log return\")\n\n\n\n                                            changes inDM/dollar exchange rate\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1991.25",
      "section_title": "- 1981) / n",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n                            0.010\n           change in rate\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n                            \u22120.015\n\n\n\n\n                                     1980      1982         1984        1986\n                                                          year\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000",
      "section_title": "\u22120.015",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2. Daily changes in the DM/dollar exchange rate, January 2, 1980, to May 21,\n1987. The data come from the Garch series in the Ecdat package in R. The DM/dollar\nexchange rate is the variable dm.\n\n\n    Despite the large random \ufb02uctuations in all three time series, we can see\nthat each series appears stationary, meaning that the nature of its random\nvariation is constant over time. In particular, the series \ufb02uctuate about means\nthat are constant, or nearly so. We also see volatility clustering, because there\nare periods of higher, and of lower, variation within each series. Volatility\nclustering does not indicate a lack of stationarity but rather can be viewed\nas a type of dependence in the conditional variance of each series. This point\nwill be discussed in detail in Chap. 14.\n    Each of these time series will be modeled as a sequence Y1 , Y2 , . . . of random\nvariables, each with a CDF that we will call F .1 F will vary between series\nbut, because of stationarity, is assumed to be constant within each series. F is\nalso called the marginal distribution function. By the marginal distribution of\na stationary time series, we mean the distribution of Yt given no knowledge\n1\n     See Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Daily changes in the DM/dollar exchange rate, January 2, 1980, to May 21,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1 for de\ufb01nitions of CDF, PDF, and other terms in probability\n     theory.\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.1",
      "section_title": "for de\ufb01nitions of CDF, PDF, and other terms in probability",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 Histograms and Kernel Density Estimation   47\n\n                                       changes in risk\u2212free interest return\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Histograms and Kernel Density Estimation   47",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n        change in rate\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "change in rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                         \u22120.2\n                         \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                1960     1970        1980        1990         2000\n                                                       year\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "1960     1970        1980        1990         2000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3. Monthly changes in the risk-free rate, January 1960 to December 2002.\nThe rates are the variable rf in the Capm series in the Ecdat package in R.\n\n\nof the other observations, that is, no knowledge of Ys for any s = t. Thus,\nwhen modeling a marginal distribution, we disregard dependencies in the time\nseries. For this reason, a marginal distribution is also called an unconditional\ndistribution. Dependencies such as autocorrelation and volatility clustering\nwill be discussed in later chapters.\n    In this chapter, we explore various methods for modeling and estimating\nmarginal distributions, in particular, graphical methods such as histograms,\ndensity estimates, sample quantiles, and probability plots.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "Monthly changes in the risk-free rate, January 1960 to December 2002.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 Histograms and Kernel Density Estimation\nAssume that the marginal CDF F has a probability density function f .\nThe histogram is a simple and well-known estimator of probability density\nfunctions. Panel (a) of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Histograms and Kernel Density Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4 is a histogram of the S&P 500 log returns\nusing 30 cells (or bins). There are some outliers in this series, especially a\nreturn near \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "is a histogram of the S&P 500 log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.23 that occurred on Black Monday, October 19, 1987. Note\nthat a return of this size means that the market lost 23 % of its value in a\nsingle day. The outliers are di\ufb03cult, or perhaps impossible, to see in the his-\ntogram, except that they have caused the x-axis to expand. The reason that\nthe outliers are di\ufb03cult to see is the large sample size. When the sample size\nis in the thousands, a cell with a small frequency is essentially invisible. Panel\n(b) of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.23",
      "section_title": "that occurred on Black Monday, October 19, 1987. Note",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4 zooms in on the high-probability region. Note that only a few\nof the 30 cells are in this area.\n    The histogram is a fairly crude density estimator. A typical histogram\nlooks more like a big city skyline than a density function and its appearance\nis sensitive to the number and locations of its cells\u2014see Fig. 4.4, where panels\n(b), (c), and (d) di\ufb00er only in the number of cells. A much better estimator is\n\f48                       4 Exploratory Data Analysis\n\na                               30 cells, full range                        b                     30 cells, central range\n             800\n\n\n\n\n                                                                                        800\nFrequency\n\n\n\n\n                                                                            Frequency\n             400\n\n\n\n\n                                                                                        400\n             0\n\n\n\n\n                                                                                        0\n                           \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "zooms in on the high-probability region. Note that only a few",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20           \u22120.10            0.00     0.10                     \u22120.04       \u22120.02   0.00     0.02    0.04\n                                                   return                                                         return\n\nc                           20 cells, central range                         d                     50 cells, central range\n             800 1200\n\n\n\n\n                                                                                        600\n                                                                            Frequency\nFrequency\n\n\n\n\n                                                                                        400\n                                                                                        200\n             400\n             0\n\n\n\n\n                                                                                        0\n\n\n\n\n                        \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.20",
      "section_title": "\u22120.10            0.00     0.10                     \u22120.04       \u22120.02   0.00     0.02    0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04      \u22120.02      0.00           0.02   0.04                      \u22120.04       \u22120.02   0.00     0.02    0.04\n                                             return                                                               return\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "\u22120.02      0.00           0.02   0.04                      \u22120.04       \u22120.02   0.00     0.02    0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4. Histograms of the daily log returns on the S&P 500 index from January\n1981 to April 1991. This data set is the SP500 series in the Ecdat package in R.\n\n\nthe kernel density estimator (KDE). The estimator takes its name from the\nso-called kernel function, denoted here by K, which is a probability density\nfunction that is symmetric about 0. The standard2 normal density function is\na common choice for K and will be used here. The kernel density estimator\nbased on Y1 , . . . , Yn is\n                                                                       n            \u0007                 \b\n                                                                1                       y \u2212 Yi\n                                                        f\u0002(y) =        K                                                          (4.1)\n                                                                nb i=1                    b\n\nwhere b, which is called the bandwidth, determines the resolution of the\nestimator.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "Histograms of the daily log returns on the S&P 500 index from January",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5 illustrates the construction of kernel density estimates using a\nsmall simulated data set of six observations from a standard normal distribu-\ntion. The small sample size is needed for visual clarity but, of course, does not\nlead to an accurate estimate of the underlying normal density. The six data\npoints are shown at the bottom of the \ufb01gure as short vertical lines called a\n\u201crug.\u201d The bandwidth in the top plot is 0.4, and so each of the six dashed\nlines is 1/6 times a normal density with standard deviation equal to ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.5",
      "section_title": "illustrates the construction of kernel density estimates using a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 and\n2\n            \u201cStandard\u201d means having expectation 0 and variance 1.\n\f                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 Histograms and Kernel Density Estimation   49\n\n                                                            b = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Histograms and Kernel Density Estimation   49",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                    0.0 0.1 0.2 0.3 0.4\n              KDE\n\n\n\n\n                                          \u22123      \u22122       \u22121         0       1\n                                                                x\n                                                            b = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0 0.1 0.2 0.3 0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                    0.6\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n              KDE\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "KDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                    0.0\n\n\n\n\n                                          \u22123      \u22122       \u22121         0       1\n                                                                x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5. Illustration of kernel density estimates using a sample of size 6 and two\nbandwidths. The six dashed curves are the kernels centered at the data points, which\nare indicated by vertical lines at the bottom. The solid curve is the kernel density\nestimate created by adding together the six kernels. Although the same data are\nused in the top and bottom panels, the density estimates are di\ufb00erent because of the\ndi\ufb00erent bandwidths.\n\n\ncentered at one of the data points. The solid curve is the superposition, that\nis, the sum as in Eq. (4.1), of the six dashed curves and estimates the density\nof the data.\n     A small value of b allows the density estimator to detect \ufb01ne features in\nthe true density, but it also permits a high degree of random variation. This\ncan be seen in the plot in the bottom of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.5",
      "section_title": "Illustration of kernel density estimates using a sample of size 6 and two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5 where the bandwidth is\nonly half as large as in the plot on the top. Conversely, a large value of b\ndampens random variation but obscures \ufb01ne detail in the true density. Stated\ndi\ufb00erently, a small value of b causes the kernel density estimator to have high\nvariance and low bias, and a large value of b results in low variance and high\nbias.\n     Choosing b requires one to make a tradeo\ufb00 between bias and variance.\nAppropriate values of b depend on both the sample size n and the true den-\nsity and, of course, the latter is unknown, though it can be estimated. Roughly\nspeaking, nonsmooth or \u201cwiggly\u201d densities require a smaller bandwidth.\n\f50      4 Exploratory Data Analysis\n\nFortunately, a large amount of research has been devoted to automatic se-\nlection of b, which, in e\ufb00ect, estimates the roughness of the true density. As\na result of this research, modern statistical software can select the bandwidth\nautomatically. However, automatic bandwidth selectors are not foolproof and\ndensity estimates should be checked visually and, if necessary, adjusted as\ndescribed below.\n    The solid curve in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.5",
      "section_title": "where the bandwidth is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6 has the default bandwidth from the density()\nfunction in R. The dashed and dotted curves have the default bandwidth mul-\ntiplied by 1/3 and 3, respectively. The tuning parameter adjust in R is the\nmultiplier of the default bandwidth, so that adjust is 1, 1/3, and 3 in the\nthree curves. The solid curve with adjust equal to 1 appears to have a proper\namount of smoothness. The dashed curve corresponding to adjust = 1/3 is\nwiggly, indicating too much random variability; such a curve is called under-\nsmoothed and over\ufb01t. The dotted curve is very smooth but underestimates\nthe peak near 0, a sign of bias. Such a curve is called oversmoothed or un-\nder\ufb01t. Here over\ufb01t means that the density estimate adheres too closely to the\ndata and so is unduly in\ufb02uenced by random variation. Conversely, under\ufb01t\nmeans that the density estimate does not adhere closely enough to the data\nand misses features in the true density. Stated di\ufb00erently, over- and under\ufb01t-\nting means a poor bias\u2013variance tradeo\ufb00 with an over\ufb01tted curve having too\nmuch variance and an under\ufb01tted curve having too much bias.\n    Automatic bandwidth selectors are very useful, but there is nothing mag-\nical about them, and often one will use an automatic selector as a starting\n\n                                             S&P 500 daily returns\n                   60\n                   50\n                   40\n\n\n\n\n                                adjust=1\n         Density\n\n\n\n\n                                adjust=1/3\n                   30\n\n\n\n\n                                adjust=3\n                   20\n                   10\n                   0\n\n\n\n\n                        \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "has the default bandwidth from the density()",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04       \u22120.02            0.00            0.02   0.04\n                                                    return\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "\u22120.02            0.00            0.02   0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6. Kernel density estimates of the daily log returns on the S&P 500 index\nusing three bandwidths. Each bandwidth is the default bandwidth times adjust and\nadjust is 1/3, 1, and 3. This data set is the SP500 series in the Ecdat package in R.\nThe KDE is plotted only for a limited range of returns to show detail in the middle\nof the distribution.\n\f                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "Kernel density estimates of the daily log returns on the S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 Histograms and Kernel Density Estimation                      51\n\npoint and then \u201c\ufb01ne-tune\u201d the bandwidth; this is the point of the adjust pa-\nrameter. Generally, adjust will be much closer to 1 than the values, 1/3 and\n3, used above. The reason for using 1/3 and 3 in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Histograms and Kernel Density Estimation                      51",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6 was to emphasize\nthe e\ufb00ects of under- and oversmoothing.\n    Often a kernel density estimate is used to suggest a parametric statisti-\ncal model. The density estimates in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "was to emphasize",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6 are bell-shaped, suggesting that\na normal distribution might be a suitable model. To further investigate the\nsuitability of the normal model, Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "are bell-shaped, suggesting that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7 compares the kernel density estimate\nwith adjust = 1 with normal densities. In panel (a), the normal density has\nmean and standard deviation equal to the sample mean and standard devia-\ntion of the returns. We see that the kernel estimate and the normal density\nare somewhat dissimilar. The reason is that the outlying returns in\ufb02ate the\nsample standard deviation and cause the \ufb01tted normal density to be too dis-\npersed in the middle of the data. Panel (b) shows a normal density that is\nmuch closer to the kernel estimator. This normal density uses robust estima-\ntors which are less sensitive to outliers\u2014the mean is estimated by the sample\nmedian and the MAD estimator is used for the standard deviation. The MAD\nestimator is the median absolute deviation from the median but scaled so\nthat it estimates the standard deviation of a normal population.3 The sample\n\n\n     a                       standard estimates          b                      robust estimates\n                60\n\n\n\n\n                                                                   60\n\n\n\n\n                             estimated density                                  estimated density\n                             normal density                                     normal density\n                50\n\n\n\n\n                                                                   50\n                40\n\n\n\n\n                                                                   40\n      Density\n\n\n\n\n                                                         Density\n                30\n\n\n\n\n                                                                   30\n                20\n\n\n\n\n                                                                   20\n                10\n\n\n\n\n                                                                   10\n                0\n\n\n\n\n                                                                   0\n\n\n\n\n                     \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.7",
      "section_title": "compares the kernel density estimate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06    \u22120.02      0.02     0.06                  \u22120.06    \u22120.02      0.02    0.06\n                       N = 2783 Bandwidth = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "\u22120.02      0.02     0.06                  \u22120.06    \u22120.02      0.02    0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001459                      N = 2783 Bandwidth = 0.001459\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.001459",
      "section_title": "N = 2783 Bandwidth = 0.001459",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7. Kernel density estimates (solid) of the daily log returns on the S&P 500\nindex compared with normal densities (dashed). (a) The normal density uses the\nsample mean and standard deviation. (b) The normal density uses the sample me-\ndian and MAD estimate of standard deviation. This data set is the SP500 series in\nthe Ecdat package in R.\n3\n    See Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.7",
      "section_title": "Kernel density estimates (solid) of the daily log returns on the S&P 500",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.16 for more discussion of robust estimation and the precise de\ufb01nition\n    of MAD.\n\f52     4 Exploratory Data Analysis\n\nstandard deviation is 0.011, but the MAD is smaller, 0.0079; these values were\ncomputed using the R functions sd() and mad(). Even the normal density in\npanel (b) shows some deviation from the kernel estimator, and, as we will soon\nsee, the t-distribution provides a better model for the return distribution than\ndoes the normal distribution. The need for robust estimators is itself a sign\nof nonnormality.\n    We have just seen a problem with using a KDE to suggest a good model\nfor the distribution of the data in a sample\u2014the parameters in the model\nmust be estimated properly. Normal probability plots and, more generally,\nquantile\u2013quantile plots, which will be discussed in Sects. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.16",
      "section_title": "for more discussion of robust estimation and the precise de\ufb01nition",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.2 and 4.3.4, are\nbetter methods for comparing a sample with a theoretical distribution.\n    Though simple to compute, the KDE has some problems. In particular, it\nis often too bumpy in the tails. An improvement to the KDE is discussed in\nSect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "2 and 4.3.4, are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8.\n\n\n4.3 Order Statistics, the Sample CDF, and Sample\nQuantiles\nSuppose that Y1 , . . . , Yn is a random sample from a probability distribution\nwith CDF F . In this section we estimate F and its quantiles. The sample or\nempirical CDF Fn (y) is de\ufb01ned to be the proportion of the sample that is less\nthan or equal to y. For example, if 10 out of 40 (= n) elements of a sample\nare 3 or less, then Fn (3) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "4.3 Order Statistics, the Sample CDF, and Sample",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25. More generally,\n                                      \u0017n\n                                           I{Yi \u2264 y}\n                              Fn (y) = i=1           ,                    (4.2)\n                                            n\nwhere I{\u00b7} is the indicator function so that I{Yi \u2264 y} is 1 if Yi \u2264 y and is\n0 otherwise. Therefore, the sum in the numerator in (4.2) counts the number\nof Yi that are less than or equal to y. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "More generally,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 shows Fn for a sample of\nsize 150 from an N (0, 1) distribution. The true CDF (\u03a6) is shown as well.\nThe sample CDF di\ufb00ers from the true CDF because of random variation. The\nsample CDF is also called the empirical distribution function, or EDF.\n    The function ecdf() computes a sample CDF. The code to produce\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "shows Fn for a sample of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 is:\n1  set.seed(\"991155\")\n2  edf_norm = ecdf(rnorm(150))\n 3 pdf(\"normalcdfplot.pdf\", width = 6, height = 5)  ## Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "is:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8\n 4 par(mfrow = c(1, 1))\n\n 5 plot(edf_norm, verticals = TRUE, do.p = FALSE, main = \"EDF and CDF\")\n\n 6 tt = seq(from = -3, to = 3, by = 0.01)\n\n 7 lines(tt, pnorm(tt), lty = 2, lwd = 2, col = \"red\")\n\n 8 legend(1.5, 0.2, c(\"EDF\", \"CDF\"), lty = c(1, 2),\n\n 9    lwd = c(1.5, 2), col = c(\"black\", \"red\"))\n10 graphics.off()\n\f                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "4 par(mfrow = c(1, 1))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles         53\n\n                                          EDF and CDF\n\n\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "Order Statistics, the Sample CDF, and Sample Quantiles         53",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                  0.8\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n          Fn(x)\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "Fn(x)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                  0.2\n\n\n\n\n                                                                      EDF\n                                                                      CDF\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                          \u22123      \u22122     \u22121       0      1        2         3\n                                                 x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22123      \u22122     \u22121       0      1        2         3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8. The EDF Fn (solid) and the true CDF (dashed) for a simulated random\nsample from an N (0, 1) population. The sample size is 150.\n\n\n    The order statistics Y(1) , Y(2) , . . . , Y(n) are the values Y1 , . . . , Yn ordered\nfrom smallest to largest. The subscripts of the order statistics are in paren-\ntheses to distinguish them from the unordered sample. For example, Y1 is\nsimply the \ufb01rst observation in the original sample while Y(1) is the smallest\nobservation in that sample. The sample quantiles are de\ufb01ned in slightly di\ufb00er-\nent ways by di\ufb00erent authors, but roughly the q-sample quantile, 0 < q < 1,\nis Y(k) , where k is qn rounded to an integer. Some authors round up, oth-\ners round to the nearest integer, and still others interpolate. The function\nquantile() in R has nine di\ufb00erent types of sample quantiles, the three used\nby SASTM , S-PLUSTM , and SPSSTM and MinitabTM , plus six others. With\nthe large sample sizes typical of \ufb01nancial markets data, the di\ufb00erent choices\nlead to nearly identical estimates, but for small samples they can be somewhat\ndi\ufb00erent.\n    The qth quantile is also called the 100qth percentile. Certain quantiles have\nspecial names. The ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "The EDF Fn (solid) and the true CDF (dashed) for a simulated random",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 sample quantile is the 50th percentile and is usually\ncalled the median. The ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "sample quantile is the 50th percentile and is usually",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 and 0.75 sample quantiles are called the \ufb01rst and\nthird quartiles, and the median is also called the second quartile. The 0.2, 0.4,\n0.6, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "and 0.75 sample quantiles are called the \ufb01rst and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8 quantiles are the quintiles since they divide the data into \ufb01ve\nequal-size subsets, and the 0.1, 0.2, . . ., ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "quantiles are the quintiles since they divide the data into \ufb01ve",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 quantiles are the deciles.4\n4\n    Somewhat confusingly, the bottom 10 % of the data is also called the \ufb01rst decile\n    and similarly for the second 10 %, and so forth. Thus, the \ufb01rst decile could refer\n    to the 10th percentile of the data or to all of the data at or below this percentile.\n    In like fashion, the bottom 20 % of the sample is called the \ufb01rst quintile and the\n    second to \ufb01fth quantiles are de\ufb01ned analogously.\n\f54     4 Exploratory Data Analysis\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "quantiles are the deciles.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.1 The Central Limit Theorem for Sample Quantiles\n\nMany estimators have an approximate normal distribution if the sample size\nis su\ufb03ciently large. This is true of sample quantiles by the following central\nlimit theorem.\n\nResult ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "1 The Central Limit Theorem for Sample Quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 Let Y1 , . . . , Yn be an i.i.d. sample with a CDF F . Suppose that F\nhas a density f that is continuous and positive at F \u22121 (q), 0 < q < 1. Then for\nlarge n, the qth sample quantile is approximately normally distributed with\nmean equal to the population quantile F \u22121 (q) and variance equal to\n                                    q(1 \u2212 q)\n                                                   2.                       (4.3)\n                                n [f {F \u22121 (q)}]\n\n\n   This result is not immediately applicable, for example, for constructing\n                                                          \u0018           \u00192\na con\ufb01dence interval for a population quantile, because f {F \u22121 (q)} is un-\nknown. However, f can be estimated by kernel density estimation (Sect. 4.2)\nand F \u22121 (q) can be estimated by the qth sample quantile. Alternatively, a con-\n\ufb01dence interval can be constructed by resampling. Resampling is introduced\nin Chap. 6.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "Let Y1 , . . . , Yn be an i.i.d. sample with a CDF F . Suppose that F",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.2 Normal Probability Plots\n\nMany statistical models assume that a random sample comes from a normal\ndistribution. Normal probability plots are used to check this assumption, and,\nif the normality assumption seems false, to investigate how the distribution\nof the data di\ufb00ers from a normal distribution. If the normality assumption is\ntrue, then the qth sample quantile will be approximately equal to \u03bc+\u03c3 \u03a6\u22121 (q),\nwhich is the population quantile. Therefore, except for sampling variation, a\nplot of the sample quantiles versus \u03a6\u22121 will be linear. One version of the\nnormal probability plot is a plot of Y(i) versus \u03a6\u22121 {(i \u2212 1/2)/n}. These are\nthe (i\u22121/2)/n sample and population quantiles, respectively. The subtraction\nof 1/2 from i in the numerator is used to avoid \u03a6\u22121 (1) = +\u221e when i = n.\n    Systematic deviation of the plot from a straight line is evidence of non-\nnormality. There are other versions of the normal plot, e.g., a plot of the\norder statistics versus their expectations under normality, but for large sam-\nples these will all be similar, except perhaps in the extreme tails.\n    Statistical software di\ufb00ers about whether the data are on the x-axis (hor-\nizontal axis) and the theoretical quantiles on the y-axis (vertical axis) or vice\nversa. The qqnorm() function in R allows the data to be on either axis depend-\ning on the choice of the parameter datax. When interpreting a normal plot\nwith a nonlinear pattern, it is essential to know which axis contains the data.\nIn this book, the data will always be plotted on the x-axis and the theoretical\nquantiles on the y-axis, so in R, datax = TRUE was used to construct the plots\nrather than the default, which is datax = FALSE.\n\f                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "2 Normal Probability Plots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                      55\n\n    If the pattern in a normal plot is nonlinear, then to interpret the pattern\none checks where the plot is convex and where it is concave. A convex curve\nis one such that as one moves from left to right, the slope of the tangent line\nincreases; see Fig. 4.9a. Conversely, if the slope decreases as one moves from\nleft to right, then the curve is concave; see Fig. 4.9b. A convex-concave curve\nis convex on the left and concave on the right and, similarly, a concave-convex\ncurve is concave on the left and convex on the right; see Fig. 4.9c and d.\n    A convex, concave, convex-concave, or concave-convex normal plot indi-\ncates, respectively, left skewness, right skewness, heavy tails (compared to the\nnormal distribution), or light tails (compared to the normal distribution)\u2014\nthese interpretations require that the sample quantiles are on the horizontal\naxis and need to be changed if the sample quantiles are plotted on the vertical\naxis. Tails of a distribution are the regions far from the center. Reasonable\nde\ufb01nitions of the \u201ctails\u201d would be that the left tail is the region from \u2212\u221e to\n\u03bc \u2212 2\u03c3 and the right tail is the region from \u03bc + 2\u03c3 to +\u221e, though the choices\nof \u03bc \u2212 2\u03c3 and \u03bc + 2\u03c3 are somewhat arbitrary. Here \u03bc and \u03c3 are the mean and\nstandard deviation, though they might be replaced by the median and MAD\nestimator, which are less sensitive to tail weight.\n\n\na                                       Convex                     b                                    Concave\n                   20\n\n\n\n\n                                                                                     0\n normal quantile\n\n\n\n\n                                                                   normal quantile\n                                                                                     \u22122\n                   10\n                   5\n\n\n\n\n                                                                                     \u22124\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "Order Statistics, the Sample CDF, and Sample Quantiles                                      55",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2     0.4    0.6      0.8   1.0                          0.0   0.2     0.4    0.6      0.8   1.0\n                                     sample quantile                                                  sample quantile\n\nc                               Convex\u2212concave                     b                            Concave\u2212convex\n normal quantile\n\n\n\n\n                                                                   normal quantile\n                                                                                     2\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.2     0.4    0.6      0.8   1.0                          0.0   0.2     0.4    0.6      0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                                     0\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                     \u22122\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                         0.0   0.2     0.4    0.6      0.8   1.0                          0.0   0.2     0.4    0.6      0.8   1.0\n                                     sample quantile                                                  sample quantile\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0   0.2     0.4    0.6      0.8   1.0                          0.0   0.2     0.4    0.6      0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.9. As one moves from (a) to (d), the curves are convex, concave, convex-\nconcave, and concave-convex. Normal plots with these patterns indicate left skewness,\nright skewness, heavier tails than a normal distribution, and lighter tails than a\nnormal distribution, respectively, assuming that the data are on the x-axis and the\nnormal quantiles on the y-axis, as will always be the case in this textbook.\n\f56                                   4 Exploratory Data Analysis\n\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.9",
      "section_title": "As one moves from (a) to (d), the curves are convex, concave, convex-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10 contains normal plots of samples of size 20, 150, and 1000\nfrom a normal distribution. To show the typical amount of random variation\nin normal plots, two independent samples are shown for each sample size. The\nplots are only close to linear because of random variation. Even for normally\ndistributed data, some deviation from linearity is to be expected, especially\nfor smaller sample sizes. With larger sample sizes, the only deviations from\nlinearity are in the extreme left and right tails, where the plots are more\nvariable.\n    Often, a reference line is added to the normal plot to help the viewer\ndetermine whether the plot is reasonably linear. One choice for the reference\nline goes through the pair of \ufb01rst quartiles and the pair of third quartiles;\nthis is what R\u2019s qqline() function uses. Other possibilities would be a least-\nsquares \ufb01t to all of the quantiles or, to avoid the in\ufb02uence of outliers, some\nsubset of the quantiles, e.g., all between the ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "contains normal plots of samples of size 20, 150, and 1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 and 0.9-quantiles.\n\n                                                          n = 20                                                                                        n = 20\n                        2\n\n\n\n\n                                                                                                                   2\nTheoretical Quantiles\n\n\n\n\n                                                                                           Theoretical Quantiles\n                        1\n\n\n\n\n                                                                                                                   1\n                        0\n\n\n\n\n                                                                                                                   0\n                        \u22121\n\n\n\n\n                                                                                                                   \u22121\n                        \u22122\n\n\n\n\n                                                                                                                   \u22122\n\n\n\n\n                                \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "and 0.9-quantiles.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0    \u22120.5        0.0        0.5       1.0         1.5                                    \u22121.5        \u22121.0   \u22120.5      0.0       0.5       1.0   1.5\n                                                Sample Quantiles                                                                                    Sample Quantiles\n\n\n                                                          n = 150                                                                                      n = 150\nTheoretical Quantiles\n\n\n\n\n                                                                                           Theoretical Quantiles\n                        2\n\n\n\n\n                                                                                                                   2\n                        1\n\n\n\n\n                                                                                                                   1\n                        0\n\n\n\n\n                                                                                                                   0\n                        \u22122\n\n\n\n\n                                                                                                                   \u22122\n\n\n\n\n                                \u22123      \u22122      \u22121             0         1       2                                                 \u22122          \u22121         0              1         2\n                                                Sample Quantiles                                                                                    Sample Quantiles\n\n\n                                                      n = 1000                                                                                        n = 1000\nTheoretical Quantiles\n\n\n\n\n                                                                                           Theoretical Quantiles\n                        1 2 3\n\n\n\n\n                                                                                                                   1 2 3\n                        \u22121\n\n\n\n\n                                                                                                                   \u22121\n                        \u22123\n\n\n\n\n                                                                                                                   \u22123\n\n\n\n\n                                 \u22123     \u22122     \u22121          0         1       2   3                                         \u22123           \u22122      \u22121             0         1         2\n                                                Sample Quantiles                                                                                    Sample Quantiles\n\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "\u22120.5        0.0        0.5       1.0         1.5                                    \u22121.5        \u22121.0   \u22120.5      0.0       0.5       1.0   1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10. Normal probability plots of random samples of size 20, 150, and 1000\nfrom an N (0, 1) population. The plots were produced by the R function qqnorm().\nThe reference lines pass through the \ufb01rst and third quartiles and were produced by\nR\u2019s qqline() function.\n\f                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "Normal probability plots of random samples of size 20, 150, and 1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                                                                   57\n\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "Order Statistics, the Sample CDF, and Sample Quantiles                                                                                   57",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11 contains normal probability plots of samples of size 150 from\nlognormal (0, \u03c3 2 ) distributions,5 with the log-standard deviation \u03c3 = 1, 1/2,\nand 1/5. The concave shapes in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "contains normal probability plots of samples of size 150 from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11 indicate right skewness. The skewness\nwhen \u03c3 = 1 is quite strong, and when \u03c3 = 1/2, the skewness is still very\nnoticeable. With \u03c3 reduced to 1/5, the right skewness is much less pronounced\nand might not be discernable with smaller sample sizes.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "indicate right skewness. The skewness",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.12 contains normal plots of samples of size 150 from t-distributions\nwith 4, 10, and 30 degrees of freedom. The \ufb01rst two distributions have heavy\ntails or, stated di\ufb00erently, are outlier-prone, meaning that the extreme obser-\nvations on both the left and right sides are signi\ufb01cantly more extreme than\nwould be expected for a normal distribution. One can see that the tails are\nheavier in the sample with 4 degrees of freedom compared to the sample with\n\n                                                     n=150, \u03c3 = 1                                                                               n=1000, \u03c3 = 1\nTheoretical Quantiles\n\n\n\n\n                                                                                    Theoretical Quantiles\n                                                                                                            1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                            \u22121\n                        \u22122\n\n\n\n\n                                                                                                            \u22123\n\n\n\n\n                             0                   5             10          15                                           0                   5               10                   15\n                                                     Sample Quantiles                                                                           Sample Quantiles\n\n\n                                                     n=150, \u03c3 = 1/2                                                                         n=1000, \u03c3 = 1/2\nTheoretical Quantiles\n\n\n\n\n                                                                                    Theoretical Quantiles\n                                                                                                            1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                            \u22121\n                        \u22122\n\n\n\n\n                                                                                                            \u22123\n\n\n\n\n                                       1                2            3          4                                   0             1             2       3              4          5\n                                                     Sample Quantiles                                                                           Sample Quantiles\n\n\n                                                     n=150, \u03c3 = 1/5                                                                         n=1000, \u03c3 = 1/5\nTheoretical Quantiles\n\n\n\n\n                                                                                    Theoretical Quantiles\n                                                                                                            1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                            \u22121\n                        \u22122\n\n\n\n\n                                                                                                            \u22123\n\n\n\n\n                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.12",
      "section_title": "contains normal plots of samples of size 150 from t-distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6       0.8          1.0         1.2   1.4                                               0.6       0.8       1.0   1.2        1.4       1.6        1.8\n                                                     Sample Quantiles                                                                           Sample Quantiles\n\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "0.8          1.0         1.2   1.4                                               0.6       0.8       1.0   1.2        1.4       1.6        1.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11. Normal probability plots of random samples of sizes 150 and 1000 from\nlognormal populations with \u03bc = 0 and \u03c3 = 1, 1/2, or 1/5. The reference lines pass\nthrough the \ufb01rst and third quartiles.\n\n   5\n                        See Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "Normal probability plots of random samples of sizes 150 and 1000 from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 for an introduction to the lognormal distribution and the\n                        de\ufb01nition of the log-standard deviation.\n\f58        4 Exploratory Data Analysis\n\n10 degrees of freedom, and the tails of the t-distribution with 30 degrees of\nfreedom are not much di\ufb00erent from the tails of a normal distribution. It\nis a general property of the t-distribution that the tails become heavier as\nthe degrees of freedom parameter decreases and the distribution approaches\nthe normal distribution as the degrees of freedom approaches in\ufb01nity. Any\nt-distribution is symmetric,6 so none of the samples is skewed. Heavy-tailed\ndistributions with little or no skewness are common in \ufb01nance and, as we\nwill see, the t-distribution is a reasonable model for stock returns and other\n\ufb01nancial markets data.\n    Sometimes, a normal plot will not have any of the patterns discussed\nhere but instead will have more complex behavior. An example is shown in\nFig. 4.13, which uses a simulated sample from a trimodal density. The al-\nternation of the QQ plot between concavity and convexity indicates complex\nbehavior which should be investigated by a KDE. Here, the KDE reveals the\ntrimodality. Multimodality is somewhat rare in practice and often indicates a\nmixture of several distinct groups of data.\n    It is often rather di\ufb03cult to decide whether a normal plot is close enough to\nlinear to conclude that the data are normally distributed, especially when the\nsample size is small. For example, even though the plots in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "9.4",
      "section_title": "for an introduction to the lognormal distribution and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10 are close\nto linear, there is some nonlinearity. Is this nonlinearity due to nonnormality\nor just due to random variation? If one did not know that the data were\nsimulated from a normal distribution, then it would be di\ufb03cult to tell, unless\none were very experienced with normal plots. In such situations, a test of\nnormality is very helpful. These tests are discussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "are close",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4.\n\n4.3.3 Half-Normal Plots\n\nThe half-normal plot is a variation of the normal plot used for detecting\noutlying data rather than checking for a normal distribution. For example,\nsuppose one has data Y1 , . . . , Yn and wants to see whether any of the absolute\ndeviations |Y1 \u2212 Y |, . . . , |Yn \u2212 Y | from the mean are unusual. In a half-normal\nplot, these deviation are plotted against the quantiles of |Z|, where Z is N (0, 1)\ndistributed. More precisely, a half-normal plot is a scatterplot of the order\nstatistics of the absolute values of the data against \u03a6\u22121 {(n + i)/(2n + 1)},\ni = 1, . . . , n, where n is the sample size. The function halfnorm() in R\u2019s\nfaraway package creates a half-normal plot and labels the nlab most outlying\nobservations, where nlab is an argument of this function with a default value\nof 2.\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "4.3.3 Half-Normal Plots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1. DM/dollar exchange rate\u2014Half-normal plot\n\n   Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "DM/dollar exchange rate\u2014Half-normal plot",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.14 is a half-normal plot of changes in the DM/dollar exchange\nrate. The plot shows that case #1447 is the most outlying, with case #217\n6\n     However, t-distributions have been generalized in at least two di\ufb00erent ways to\n     the so-called skewed-t-distributions, which need not be symmetric. See Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.14",
      "section_title": "is a half-normal plot of changes in the DM/dollar exchange",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7.\n\f                                        4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                                                   59\nTheoretical Quantiles                              n=150, df=4                                                                       n=150, df=4\n\n\n\n\n                                                                                     Theoretical Quantiles\n\n                                                                                                             1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                             \u22121\n                        \u22122\n\n\n\n\n                                                                                                             \u22123\n                             \u22124          \u22122                0         2           4                                   \u22126        \u22124   \u22122       0       2           4   6\n                                                  Sample Quantiles                                                                  Sample Quantiles\n\n\n                                                  n=150, df=10                                                                      n=150, df=10\nTheoretical Quantiles\n\n\n\n\n                                                                                     Theoretical Quantiles\n\n                                                                                                             1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                             \u22121\n                        \u22122\n\n\n\n\n                                                                                                             \u22123\n                              l\n\n\n                                  \u22124    \u22123        \u22122       \u22121   0        1   2                                            \u22124        \u22122           0           2       4\n                                                  Sample Quantiles                                                                  Sample Quantiles\n\n\n                                                  n=150, df=30                                                                      n=150, df=30\nTheoretical Quantiles\n\n\n\n\n                                                                                     Theoretical Quantiles\n\n                                                                                                             1 2 3\n                        2\n                        1\n                        0\n\n\n\n\n                                                                                                             \u22121\n                        \u22122\n\n\n\n\n                                                                                                             \u22123\n\n\n\n\n                                   \u22122        \u22121        0        1        2       3                                         \u22122            0               2           4\n                                                  Sample Quantiles                                                                  Sample Quantiles\n\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                                                   59",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.12. Normal probability plot of a random sample of size 150 and 1000 from a\nt-distribution with 4, 10, and 30 degrees of freedom. The reference lines pass through\nthe \ufb01rst and third quartiles.\n\n\nthe next most outlying. Only the two most outlying cases are labeled because\nthe default value of nlab was used. The code to produce this \ufb01gure is below.\n   1 data(Garch, package = \"Ecdat\")\n   2 diffdm = diff(dm) # Deutsch mar\n   3 pdf(\"dm_halfnormal.pdf\" ,width = 7, height = 6)  # Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.12",
      "section_title": "Normal probability plot of a random sample of size 150 and 1000 from a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.14\n   4 halfnorm(abs(diffdm), main = \"changes in DM/dollar exchange rate\",\n\n   5    ylab = \"Sorted data\")\n   6 graphics.off()\n\n\n\n                                                                                                                                                                         \u0002\n\n                        Another application of half-normal plotting can be found in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.14",
      "section_title": "4 halfnorm(abs(diffdm), main = \"changes in DM/dollar exchange rate\",",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.3.\n\f60              4 Exploratory Data Analysis\n\n                                       density.default(x = x)                                                   Normal Q\u2212Q Plot\n\n\n\n\n                                                                                                     3\n                                                                             Theoretical Quantiles\n                                                                                                     2\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "10.1",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n\n\n\n\n                                                                                                     1\n      Density\n\n\n\n\n                                                                                                     \u22121 0\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n                 0.00\n\n\n\n\n                                                                                                     \u22123\n                               \u22125          0     5     10     15                                            0         5          10\n                                       N = 650 Bandwidth = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.065                                                 Sample Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.065",
      "section_title": "Sample Quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.13. Kernel density estimate (left) and normal plot (right) of a simulated\nsample from a trimodal density. The reference lines pass through the \ufb01rst and third\nquartiles. Because of the three modes, the normal plot changes convexity three times,\nconcave to convex to concave to convex, going from left to right.\n\n                                                     changes in DM/dollar exchange rate\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.13",
      "section_title": "Kernel density estimate (left) and normal plot (right) of a simulated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n\n\n\n\n                                                                                                                                  1447\n\n\n                                                                                                                                217\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "1447",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n                 Sorted data\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.015",
      "section_title": "Sorted data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.010\n                               0.005\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.010",
      "section_title": "0.005",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                          l\n                                          l\n                                          l\n                                          ll\n\n\n\n                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000",
      "section_title": "l",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0     0.5        1.0        1.5                           2.0        2.5       3.0         3.5\n                                                                  Half\u2212normal quantiles\n\n          Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.5        1.0        1.5                           2.0        2.5       3.0         3.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.14. Half-normal plot of changes in DM/dollar exchange rate.\n\f                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.14",
      "section_title": "Half-normal plot of changes in DM/dollar exchange rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles                61\n\n                                                                                     t\u2212probability plot,\n        a                          Normal probability plot   b                             df = 1\n\n\n\n                           3\n        normal quantiles\n\n\n\n\n                                                                           500\n                                                             t\u2212quantiles\n                           1\n\n\n\n\n                                                                           0\n                           \u22121\n\n\n\n\n                                                                           \u2212500\n                           \u22123\n\n\n\n\n                                   \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "Order Statistics, the Sample CDF, and Sample Quantiles                61",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20   \u22120.10     0.00                         \u22120.20   \u22120.10     0.00\n                                              Data                                           Data\n\n\n\n        c                             t\u2212probability plot,    d                       t\u2212probability plot,\n                                            df = 2                                         df = 4\n                           40\n\n\n\n\n                                                                           10\n                           20\n\n\n\n\n                                                                           5\n        t\u2212quantiles\n\n\n\n\n                                                             t\u2212quantiles\n                           0\n\n\n\n\n                                                                           0\n                                                                           \u22125\n                                                                           \u221210\n                           \u221240\n\n\n\n\n                                   \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.20",
      "section_title": "\u22120.10     0.00                         \u22120.20   \u22120.10     0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20   \u22120.10     0.00                         \u22120.20   \u22120.10     0.00\n                                              Data                                           Data\n\n\n\n                                    t\u2212probability plot,                              t\u2212probability plot,\n        e                                                    f\n                                          df = 8                                          df = 15\n                                                                           4\n                           4\n\n\n\n\n                                                                           2\n        t\u2212quantiles\n\n\n\n\n                                                             t\u2212quantiles\n                           2\n                           0\n\n\n\n\n                                                                           0\n                                                                           \u22122\n                           \u22124\n\n\n\n\n                                                                           \u22124\n\n\n\n\n                                   \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.20",
      "section_title": "\u22120.10     0.00                         \u22120.20   \u22120.10     0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20   \u22120.10     0.00                         \u22120.20   \u22120.10     0.00\n                                              Data                                           Data\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.20",
      "section_title": "\u22120.10     0.00                         \u22120.20   \u22120.10     0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.15. Normal and t probability plots of the daily returns on the S&P 500 index\nfrom January 1981 to April 1991. This data set is the SP500 series in the Ecdat\npackage in R. The reference lines pass through the \ufb01rst and third quartiles.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.15",
      "section_title": "Normal and t probability plots of the daily returns on the S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.4 Quantile\u2013Quantile Plots\n\nNormal probability plots are special cases of quantile-quantile plots, also\nknown as QQ plots. A QQ plot is a plot of the quantiles of one sample or\ndistribution against the quantiles of a second sample or distribution.\n    For example, suppose that we wish to model a sample using the t\u03bd (\u03bc, \u03c3 2 )\ndistribution de\ufb01ned in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "4 Quantile\u2013Quantile Plots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.2. The parameter \u03bd is called the \u201cdegrees\nof freedom,\u201d or simply \u201cdf.\u201d Suppose, initially, that we have a hypothesized\nvalue of \u03bd, say \u03bd = 6 to be concrete. Then we plot the sample quantiles\n\f62        4 Exploratory Data Analysis\n\nagainst the quantiles of the t6 (0, 1) distribution. If the data are from a t6 (\u03bc, \u03c3 2 )\ndistribution, then, apart from random variation, the plot will be linear with\nintercept and slope depending on \u03bc and \u03c3.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "2. The parameter \u03bd is called the \u201cdegrees",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.15 contains a normal plot of the S&P 500 log returns in panel (a)\nand t-plots with 1, 2, 4, 8, and 15 df in panels (b) through (f). None of the\nplots looks exactly linear, but the t-plot with 4 df is rather straight through\nthe bulk of the data. There are approximately nine returns in the left tail and\nfour in the right tail that deviate from a line through the remaining data, but\nthese are small numbers compared to the sample size of 2783. Nonetheless, it is\nworthwhile to keep in mind that the historical data have more extreme outliers\nthan a t-distribution. The t-model with 4 df and mean and standard deviation\nestimated by maximum likelihood7 implies that a daily log return of \u22120.228,\nthe return on Black Monday, or less has probability ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.15",
      "section_title": "contains a normal plot of the S&P 500 log returns in panel (a)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.2 \u00d7 10\u22126 . This means\napproximately 3 such returns every 1,000,000 days or 40,000 years, assuming\n250 trading days per year. Thus, the t-model implies that Black Monday was\nextremely unlikely, and anyone using that model should be mindful that it\ndid happen.\n    There are two reasons why the t-model does not give a credible probability\nof a negative return as extreme as on Black Monday. First, the t-model is\nsymmetric, but the return distribution appears to have some skewness in the\nextreme left tail, which makes extreme negative returns more likely than under\nthe t-model. Second, the t-model assumes constant conditional volatility, but\nvolatility was unusually high in October 1987. GARCH models (Chap. 14)\ncan accommodate this type of volatility clustering and provide more realistic\nestimates of the probability of an extreme event such as Black Monday.\n    Quantile\u2013quantile plots are useful not only for comparing a sample with\na theoretical model, as above, but also for comparing two samples. If the\ntwo samples have the same sizes, then one need only plot their order statistics\nagainst each other. Otherwise, one computes the same sets of sample quantiles\nfor each and plots them. This is done automatically with the R command\nqqplot().\n    The interpretation of convex, concave, convex-concave, and concave-convex\nQQ plots is similar to that with QQ plots of theoretical quantiles versus sam-\nple quantiles. A concave plot implies that the sample on the x-axis is more\nright-skewed, or less left-skewed, than the sample on the y-axis. A convex\nplot implies that the sample on the x-axis is less right-skewed, or more left-\nskewed, than the sample on the y-axis. A convex-concave (concave-convex)\nplot implies that the sample on the x-axis is more (less) heavy-tailed than\nthe sample on the y-axis. As before, a straight line, e.g., through the \ufb01rst and\nthird quartiles, is often added for reference.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.2",
      "section_title": "\u00d7 10\u22126 . This means",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.16 contains sample QQ plots for all three pairs of the three time\nseries, S&P 500 returns, changes in the DM/dollar rate, and changes in the\nrisk-free return, used as examples in this chapter. One sees that the S&P 500\n\n7\n     See Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.16",
      "section_title": "contains sample QQ plots for all three pairs of the three time",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14.\n\f                                         4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                    63\n\nreturns have more extreme outliers than the other two series. The changes\nin DM/dollar and risk-free returns have somewhat similar shapes, but the\nchanges in the risk-free rate have slightly more extreme outliers in the left\ntail. To avoid any possible confusion, it should be mentioned that the plots\nin Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "4.3 Order Statistics, the Sample CDF, and Sample Quantiles                                    63",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.16 only compare the marginal distributions of the three time series.\nThey tell us nothing about dependencies between the series and, in fact, the\nthree series were observed on di\ufb00erent time intervals so correlations between\nthese time series cannot be estimated from these data.\n\n        a                                                                   b\n\n\n\n\n                                                                            change in risk\u2212free return\n         change in DM/dollar rate\n                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.16",
      "section_title": "only compare the marginal distributions of the three time series.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n\n\n\n\n                                                                                                         0.2\n                                                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.015",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                    0.000\n\n\n\n\n                                                                                                         \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                    \u22120.015\n\n\n\n\n                                             \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "\u22120.015",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20    \u22120.10         0.00                                        \u22120.20   \u22120.10   0.00\n                                                       S&P return                                                       S&P return\n        c\n        change in DM/dollar rate\n                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.20",
      "section_title": "\u22120.10         0.00                                        \u22120.20   \u22120.10   0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n                                    0.000\n                                    \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.015",
      "section_title": "0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n\n\n\n\n                                             \u22120.4    \u22120.2     0.0     0.2\n                                               change in risk\u2212free return\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.015",
      "section_title": "\u22120.4    \u22120.2     0.0     0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.16. Sample QQ plots. The straight lines pass through the \ufb01rst and third\nsample quantiles. (a) Change in DM/dollar rate versus S&P return. (b) Change in\nrisk-free rate versus S&P return. (c) Change in DM/dollar rate versus change in\nrisk-free rate.\n\n\nThe code for panel (a) of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.16",
      "section_title": "Sample QQ plots. The straight lines pass through the \ufb01rst and third",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.16 is below. The code for the other panels is\nsimilar and so is omitted.\n1 qqplot(SPreturn, diffdm, xlab = \"S&P return\",\n2    ylab = \"change in DM/dollar rate\", main = \"(a)\")\n3 xx = quantile(SPreturn, c(0.25, 0.75))\n\n4 yy = quantile(diffdm, c(0.25, 0.75))\n\n5 slope = (yy[2] - yy[1]) / (xx[2] - xx[1])\n\n6 inter = yy[1] - slope*xx[1]\n\n7 abline(inter, slope, lwd = 2 )\n\f64     4 Exploratory Data Analysis\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.16",
      "section_title": "is below. The code for the other panels is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4 Tests of Normality\nWhen viewing a normal probability plot, it is often di\ufb03cult to judge whether\nany deviation from linearity is systematic or instead merely due to sampling\nvariation, so a statistical test of normality is useful. The null hypothesis is\nthat the sample comes from a normal distribution and the alternative is that\nthe sample is from a nonnormal distribution.\n    The Shapiro\u2013Wilk test of these hypotheses uses something similar to a\nnormal plot. Speci\ufb01cally, the Shapiro\u2013Wilk test is based on the association\nbetween sample order statistics Y(i) and the expected normal order statistics\nwhich, for large samples, are close to \u03a6\u22121 {i/(n + 1)}, the quantiles of the\nstandard normal distribution. The vector of expected order statistics is mul-\ntiplied by the inverse of its covariance matrix. Then the correlation between\nthis product and the sample order statistics is used as the test statistic. Corre-\nlation and covariance matrices will be discussed in greater detail in Chap. 7.\nFor now, only a few facts will be mentioned. The covariance between two\nrandom variables X and Y is\n                                      \u001a                           \u001b\n             Cov(X, Y ) = \u03c3XY = E {X \u2212 E(X)}{Y \u2212 E(Y )} ,\n\nand the Pearson correlation coe\ufb03cient between X and Y is\n\n                      Corr(X, Y ) = \u03c1XY = \u03c3XY /\u03c3X \u03c3Y .                      (4.4)\n\nA correlation equal to 1 indicates a perfect positive linear relationship, where\nY = \u03b20 + \u03b21 X with \u03b21 > 0. Under normality, the correlation between sample\norder statistics and the expected normal order statistics should be close to\n1 and the null hypothesis of normality is rejected for small values of the\ncorrelation coe\ufb03cient. In R, the Shapiro\u2013Wilk test can be implemented using\nthe shapiro.test() function.\n    The Jarque\u2013Bera test uses the sample skewness and kurtosis coe\ufb03cients\nand is discussed in Sect ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "Tests of Normality",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0. Under normality, the correlation between sample\norder statistics and the expected normal order statistics should be close to\n1 and the null hypothesis of normality is rejected for small values of the\ncorrelation coe\ufb03cient. In R, the Shapiro\u2013Wilk test can be implemented using\nthe shapiro.test() function.\n    The Jarque\u2013Bera test uses the sample skewness and kurtosis coe\ufb03cients\nand is discussed in Sect",
        "start": 1473,
        "end": 1882
      }
    ]
  },
  {
    "content": "5.4 where skewness and kurtosis are introduced. Other\ntests of normality in common use are the Anderson\u2013Darling, Crame\u0301r\u2013von\nMises, and Kolmogorov\u2013Smirnov tests. These tests compare the sample CDF\nto the normal CDF with mean equal to Y and variance equal to s2Y . The\nKolmogorov\u2013Smirnov test statistic is the maximum absolute di\ufb00erence be-\ntween these two functions, while the Anderson\u2013Darling and Crame\u0301r\u2013von Mises\ntests are based on a weighted integral of the squared di\ufb00erence. The p-values of\nthe Shapiro\u2013Wilk, Anderson\u2013Darling, Crame\u0301r\u2013von Mises, and Kolmogorov\u2013\nSmirnov tests are routinely part of the output of statistical software. A small\np-value is interpreted as evidence that the sample is not from a normal\ndistribution.\n    A recent comparison of eight tests of normality (Yap and Sim 2011)\nfound that the Shapiro-Wilk test was as powerful as its competitors for both\nshort- and long-tailed symmetric alternatives and was the most powerful\n\f                                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "where skewness and kurtosis are introduced. Other",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5 Boxplots      65\n\ntest for asymmetric alternatives. The tests in this study were: Shapiro-\nWilk, Kolmogorov-Smirnov, Lilliefors, Crame\u0301r-vo Mises, Anderson-Darling,\nD\u2019Agostino-Pearson, Jarque-Bera, and chi-squared.\n    For the S&P 500 returns, the Shapiro\u2013Wilk test rejects the null hypoth-\nesis of normality with a p-value less than ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.5",
      "section_title": "Boxplots      65",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 \u00d7 10\u221216 . The Shapiro\u2013Wilk\nalso strongly rejects normality for the changes in DM/dollar rate and for the\nchanges in risk-free return. With large sample sizes, e.g., 2783, 1866, and 515,\nfor the S&P 500 returns, changes in DM/dollar rate, and changes in risk-free\nreturn, respectively, it is quite likely that normality will be rejected, since any\nreal population will deviate to some extent from normality and any deviation,\nno matter how small, will be detected with a large enough sample. When the\nsample size is large, it is important to look at normal plots to see whether\nthe deviation from normality is of practical importance. For \ufb01nancial time\nseries, the deviation from normality in the tails is often large enough to be\nimportant.8\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "\u00d7 10\u221216 . The Shapiro\u2013Wilk",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5 Boxplots\n\nThe boxplot is a useful graphical tool for comparing several samples. The\nappearance of a boxplot depends somewhat on the speci\ufb01c software used. In\nthis section, we will describe boxplots produced by the R function boxplot().\nThe three boxplots in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.5",
      "section_title": "Boxplots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.17 were created by boxplot() with default choice\nof tuning parameters. The \u201cbox\u201d in the middle of each plot extends from\nthe \ufb01rst to the third quartile and thus gives the range of the middle half of\nthe data, often called the interquartile range, or IQR. The line in the middle\nof the box is at the median. The \u201cwhiskers\u201d are the vertical dashed lines\nextending from the top and bottom of each box. The whiskers extend to the\nsmallest and largest data points whose distance from the bottom or top of\nthe box is at most ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.17",
      "section_title": "were created by boxplot() with default choice",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 times the IQR.9 The ends of the whiskers are indicated\nby horizontal lines. All observations beyond the whiskers are plotted with an\n\u201co\u201d. The most obvious di\ufb00erences among the three boxplots in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "times the IQR.9 The ends of the whiskers are indicated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.17 are\ndi\ufb00erences in scale, with the monthly risk-free return changes being the most\nvariable and the daily DM/dollar changes being the least variable. It is not\nsurprising that the changes in the risk-free return are most variable, since\nthese are changes over months, not days as with the other series.\n    These scale di\ufb00erences obscure di\ufb00erences in shape. To remedy this prob-\nlem, in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.17",
      "section_title": "are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.18 the three series have been standardized by subtracting the\nmedian and then dividing by the MAD. Now, di\ufb00erences in shape are clearer.\nOne can see that the S&P 500 returns have heavier tails because the \u201co\u201ds\nare farther from the whiskers. The return of the S&P 500 on Black Monday\n\n8\n  See Chap. 19 for a discussion on how tail weight can greatly a\ufb00ect risk measures\n  such as VaR and expected shortfall.\n9\n  The factor ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.18",
      "section_title": "the three series have been standardized by subtracting the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 is the default value of the range parameter and can be changed.\n\f66      4 Exploratory Data Analysis\n\n\n\n\n              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "is the default value of the range parameter and can be changed.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n              0.0\n              \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n              \u22120.4\n\n\n\n\n                       S&P 500         DM/dollar       risk\u2212free\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.17. Boxplots of the S&P 500 daily log returns, daily changes in the DM/dollar\nexchange rate, and monthly changes in the risk-free returns.\n              10\n              0\n              \u221210\n              \u221220\n              \u221230\n\n\n\n\n                       S&P 500         DM/dollar       risk\u2212free\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.17",
      "section_title": "Boxplots of the S&P 500 daily log returns, daily changes in the DM/dollar",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.18. Boxplots of the standardized S&P 500 daily log returns, daily changes in\nthe DM/dollar exchange rate, and monthly changes in the risk-free returns.\n\f                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.18",
      "section_title": "Boxplots of the standardized S&P 500 daily log returns, daily changes in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6 Data Transformation       67\n\nis quite detached from the remaining data. Of course, one should be aware\nof di\ufb00erences in scale, so it is worthwhile to look at boxplots of the variables\nboth without and with standardization.\n    When comparing several samples, boxplots and QQ plots provide di\ufb00erent\nviews of the data. It is best to use both. However, if there are N samples,\nthen the number of QQ plots is N (N \u2212 1)/2 or N (N \u2212 1) if, by interchanging\naxes, one includes two plots for each pair of samples. This number can get out\nof hand quickly, so, for large values of N , one might use boxplots augmented\nwith a few selected QQ plots.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "Data Transformation       67",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6 Data Transformation\n\nThere are a number of reasons why data analysts often work not with the\noriginal variables, but rather with transformations of the variables such as\nlogs, square roots, or other power transformations. Many statistical methods\nwork best when the data are normally distributed or at least symmetrically\ndistributed and have a constant variance, and the transformed data will often\nexhibit less skewness and a more constant variance compared to the original\nvariables, especially if the transformation is selected to induce these features.\n    A transformation is called variance stabilizing if it removes a dependence\nbetween the conditional variance and the conditional mean of a variable.\nFor example, if Y is Poisson distributed with a conditional mean depending\non X, then its conditional variance is equal to the conditional mean. A trans-\nformation h would be variance-stabilizing for Y if the conditional variance of\nh(Y ) did not depend on the conditional mean of h(Y ).\n    The logarithm transformation is probably the most widely used transfor-\nmation in data analysis, though the square root is a close second. The log\nstabilizes the variance of a variable whose conditional standard deviation is\nproportional to its conditional mean. This is illustrated in Fig. 4.19, which\nplots monthly changes in the risk-free return (top row) and changes in the log\nof the return (bottom row) against the lagged risk-free return (left column) or\nyear (right column). Notice that the changes in the return are more variable\nwhen the lagged return is higher. This behavior is called nonconstant condi-\ntional variance or conditional heteroskedasticity. We see in the bottom row\nthat the changes in the log return have relatively constant variability, at least\ncompared to changes in the return.\n    The log transformation is sometimes embedded into the power transfor-\nmation family by using the so-called Box\u2013Cox power transformation\n                                    \u000e y\u03b1 \u22121\n                            y (\u03b1)\n                                  =     \u03b1 ,    \u03b1=0\n                                                                            (4.5)\n                                      log(y), \u03b1 = 0.\n\nIn (4.5), the subtraction of 1 from y \u03b1 and the division by \u03b1 are not essential,\nbut they make the transformation continuous in \u03b1 at 0 since\n\f68                         4 Exploratory Data Analysis\n\n     a                                                       b\n\n\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.6",
      "section_title": "Data Transformation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                  0.2\n      change in rate\n\n\n\n\n                                                             change in rate\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                  0.0\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                  \u22120.4\n                                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2   0.6       1.0                                   1960 1970 1980 1990 2000\n                                         lagged rate                                                year\n\n     c                                                       d\n      change in log rate\n\n\n\n\n                                                             change in log rate\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.6       1.0                                   1960 1970 1980 1990 2000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                  0.2\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                  \u22120.2\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                  \u22120.6\n\n\n\n                                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "\u22120.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2   0.6       1.0                                    0   100 200 300 400 500\n                                         lagged rate                                                year\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.6       1.0                                    0   100 200 300 400 500",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.19. Changes in risk-free rate (top) and changes in the logarithm of the risk-\nfree rate (bottom) plotted against time and against lagged rate. The risk-free returns\nare the variable rf of the Capm data set in R\u2019s Ecdat package. (a) Change in risk-\nfree rate versus change in lagged rate. (b) Change in rate versus year. (c) Change\nin log(rate) versus lagged rate. (d) Change in log(rate) versus year.\n\n                                                      y\u03b1 \u2212 1\n                                                   lim       = log(y).\n                                                  \u03b1\u21920   \u03b1\nNote that division by \u03b1 ensures that the transformation is increasing even\nwhen \u03b1 < 0. This is convenient though not essential. For the purposes of in-\nducing symmetry and a constant variance, y \u03b1 and y (\u03b1) work equally well and\ncan be used interchangeably, especially if, when \u03b1 < 0, y \u03b1 replaced by \u2212y \u03b1\nto ensure that the transformation is monotonically increasing for all values of\n\u03b1. The use of a monotonically decreasing, rather than increasing, transforma-\ntion is inconvenient since decreasing transformations reverse ordering and, for\nexample, transform the pth quantile to the (1 \u2212 p)th quantile.\n    It is commonly the case that the response is right-skewed and the condi-\ntional response variance is an increasing function of the conditional response\nmean. In such cases, a concave transformation, e.g., a Box\u2013Cox transforma-\ntion with \u03b1 < 1, will remove skewness and stabilize the variance. If a Box\u2013Cox\ntransformation with \u03b1 < 1 is used, then the smaller the value of \u03b1, the greater\nthe e\ufb00ect of the transformation. One can go too far\u2014if the transformed re-\nsponse is left-skewed or has a conditional variance that is decreasing as a\nfunction of the conditional mean, then \u03b1 has been chosen too small. Instances\nof this type of overtransformation are given in Examples 4.2, 4.4, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.19",
      "section_title": "Changes in risk-free rate (top) and changes in the logarithm of the risk-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2.\n\f                                                                                               4.6 Data Transformation        69\n\n    Typically, the value of \u03b1 that is best for symmetrizing the data is not the\nsame value of \u03b1 that is best for stabilizing the variance. Then, a compromise\nis needed so that the transformation is somewhat too weak for one purpose\nand somewhat too strong for the other. Often, however, the compromise is\nnot severe, and near symmetry and homoskedasticity can both be achieved.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "13.2",
      "section_title": "4.6 Data Transformation        69",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2. Gas \ufb02ows in pipelines\n\n    In this example, we will use a data set of daily \ufb02ows of natural gas in three\npipelines. These data are part of a larger data set used in an investigation\nof the relationships between \ufb02ows in the pipelines and prices. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Gas \ufb02ows in pipelines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.20\ncontains histograms of the daily \ufb02ows. Notice that all three distributions are\nleft-skewed. For left-skewed data, a Box\u2013Cox transformation should use \u03b1 > 1.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.20",
      "section_title": "contains histograms of the daily \ufb02ows. Notice that all three distributions are",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1.\n    Figure",
        "start": 157,
        "end": 173
      }
    ]
  },
  {
    "content": "4.21 shows KDEs of the \ufb02ows in pipeline 1 after a Box\u2013Cox transfor-\nmation using \u03b1 = 1, 2, 3, 4, 5, 6. One sees that \u03b1 between 3 and 4 removes most\n\n\n     a                                 Histogram of dat$Flow1                                        Histogram of dat$Flow2\n                 20 40 60 80 100\n\n\n\n\n                                                                                          80\n                                                                                          60\n     Frequency\n\n\n\n\n                                                                              Frequency\n                                                                                          40\n                                                                                          20\n                 0\n\n\n\n\n                                                                                          0\n\n\n\n\n                                   40      60      80       100    120                         160      180     200    220\n                                                dat$Flow1                                                  dat$Flow2\n\n\n     b                                  Histogram of dat$Flow3\n                 80\n                 60\n     Frequency\n                 40\n                 20\n                 0\n\n\n\n\n                                   0      10      20    30        40     50\n                                                 dat$Flow3\n\n                                    Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.21",
      "section_title": "shows KDEs of the \ufb02ows in pipeline 1 after a Box\u2013Cox transfor-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.20. Histograms of daily \ufb02ows in three pipelines.\n\f70                    4 Exploratory Data Analysis\n\n                             \u03b1=1                                            \u03b1=2                                             \u03b1=3\n          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.20",
      "section_title": "Histograms of daily \ufb02ows in three pipelines.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.030\n\n\n\n\n                                                                                                         2.0e\u221206\n                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.030",
      "section_title": "2.0e\u221206",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00020\nDensity\n\n\n\n\n                                               Density\n\n\n\n\n                                                                                               Density\n          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00020",
      "section_title": "Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n\n\n\n\n                                                                                                         0.0e+00\n                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.015",
      "section_title": "0.0e+00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00000\n          0.000\n\n\n\n\n                      40      80     120                             0 2000       6000                             0e+00      4e+05\n                    N = 342 Bandwidth = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00000",
      "section_title": "0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11                       N = 342 Bandwidth = 380.6               N = 342 Bandwidth = 3.474e+04\n\n\n                             \u03b1=4                                            \u03b1=5                                             \u03b1=6\n          2.0e\u221208\n\n\n\n\n                                                         2.0e\u221210\n\n\n\n\n                                                                                                         2.0e\u221212\nDensity\n\n\n\n\n                                               Density\n\n\n\n\n                                                                                               Density\n          0.0e+00\n\n\n\n\n                                                         0.0e+00\n\n\n\n\n                                                                                                         0.0e+00\n              \u22121e+07       2e+07   5e+07                     \u22121e+09       2e+09    5e+09                      \u22121e+11       2e+11   5e+11\n           N = 342 Bandwidth = 3.269e+06                    N = 342 Bandwidth = 3.15e+08                   N = 342 Bandwidth = 3.097e+10\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "N = 342 Bandwidth = 380.6               N = 342 Bandwidth = 3.474e+04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.21. Kernel density estimates for gas \ufb02ows in pipeline 1 with Box\u2013Cox trans-\nformations.\n\n\nof the left-skewness and \u03b1 = 5 or greater overtransforms to right-skewness.\nLater, in Example 5.7, we will illustrate an automatic method for selecting \u03b1\nand \ufb01nd that \u03b1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.21",
      "section_title": "Kernel density estimates for gas \ufb02ows in pipeline 1 with Box\u2013Cox trans-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 is chosen.                                             \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "is chosen.                                             \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3. t-Tests and transformations\n\n    This example shows the deleterious e\ufb00ect of skewness and nonconstant\nvariance on hypothesis testing and how a proper data transformation can\nremedy this problem. The boxplots on the panel (a) in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "t-Tests and transformations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.22 are of in-\ndependent samples of size 15 from lognormal(1,4) (left) and lognormal(3,4)\ndistributions. Panel (b) shows boxplots of the log-transformed data.\n    Suppose one wants to test the null hypothesis that the two populations\nhave the same means against a two-sided alternative. The transformed data\nsatisfy the assumptions of the t-test that the two populations are normally\ndistributed with the same variance, but of course the original data do not\nmeet these assumptions. Two-sided independent-samples t-tests have p-values\nof ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.22",
      "section_title": "are of in-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.105 and 0.00467 using the original data and the log-transformed data,\nrespectively. These two p-values lead to rather di\ufb00erent conclusions, for the\n\ufb01rst test that the means are not signi\ufb01cantly di\ufb00erent at the usual \u03b1 = 0.05,\nand not quite signi\ufb01cant even at \u03b1 = 0.1, and for the second test that the\ndi\ufb00erence is highly signi\ufb01cant. The \ufb01rst test reaches an incorrect conclusion\nbecause its assumptions are not met.                                        \u0002\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.105",
      "section_title": "and 0.00467 using the original data and the log-transformed data,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7 The Geometry of Transformations    71\n\n    The previous example illustrates some general principles to keep in mind.\nAll statistical estimators and tests make certain assumptions about the dis-\ntribution of the data. One should check these assumptions, and graphical\nmethods are often the most convenient way to diagnose problems. If the as-\nsumptions are not met, then one needs to know how sensitive the estimator\nor test is to violations of the assumptions. If the estimator or test is likely to\nbe seriously degraded by violations of the assumptions, which is called nonro-\nbustness, then there are two recourses. The \ufb01rst is to \ufb01nd a new estimator or\ntest that is suitable for the data. The second is to transform the data so that\nthe transformed data satisfy the assumptions of the original test or estimator.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.7",
      "section_title": "The Geometry of Transformations    71",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7 The Geometry of Transformations\nResponse transformations induce normality of a distribution and stabilize vari-\nances because they can stretch apart data in one region and push observations\ntogether in other regions. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.7",
      "section_title": "The Geometry of Transformations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.23 illustrates this behavior. On the hor-\nizontal axis is a sample of data from a right-skewed lognormal distribution.\nThe transformation h(y) is the logarithm. The transformed data are plotted\non the vertical axis. The dashed lines show the transformation of y to h(y) as\none moves from a y-value on the x-axis upward to the curve and then to h(y)\non the y-axis. Notice the near symmetry of the transformed data. This sym-\nmetry is achieved because the log transformation stretches apart data with\nsmall values and shrinks together data with large values. This can be seen by\nobserving the derivative of the log function. The derivative of log(y) is 1/y,\nwhich is a decreasing function of y. The derivative is, of course, the slope of\nthe tangent line and the tangent lines at y = 1 and y = 5 are plotted to show\nthe decrease in the derivative as y increases.\n    Consider an arbitrary increasing transformation, h(y). If x and x\u0003 are two\nnearby data points that are transformed to h(x) and h(x\u0003 ), respectively, then\n\n\n        a           no transformation        b      log transformation\n        1000 1500\n\n\n\n\n                                              6\n                                              4\n                                              2\n        500\n\n\n\n\n                                              0\n                                              \u22122\n        0\n\n\n\n\n                      1          2                     1           2\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.23",
      "section_title": "illustrates this behavior. On the hor-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.22. Boxplots of samples from two lognormal distributions without (a) and\nwith (b) log transformation.\n\f72     4 Exploratory Data Analysis\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.22",
      "section_title": "Boxplots of samples from two lognormal distributions without (a) and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                            tangent line at Y=1\n                   2\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "tangent line at Y=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n                   1\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n        log(Y)\n\n\n\n\n                                              tangent line at Y=5\n                   0\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "log(Y)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n                  \u22121\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n                  \u22122\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                        0       1        2        3       4         5   6   7   8\n                                                          Y\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "0       1        2        3       4         5   6   7   8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.23. A symmetrizing transformation. The skewed lognormal data on the\nhorizontal axis are transformed to symmetry by the log transformation.\n\n\nthe distance between transformed values is |h(x) \u2212 h(x\u0003 )| \u2248 h(1) (x)|x \u2212 x\u0003 |.\nTherefore, h(x) and h(x\u0003 ) are stretched apart where h(1) is large and pushed\ntogether where h(1) is small. A function h is called concave if h(1) (y) is a\ndecreasing function of y. As can be seen in Fig. 4.23, concave transformations\ncan remove right skewness.\n    Concave transformations can also stabilize the variance when the untrans-\nformed data are such that small observations are less variable than large ob-\nservations. This is illustrated in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.23",
      "section_title": "A symmetrizing transformation. The skewed lognormal data on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.24. There are two groups of responses,\none with a mean of 1 and a relatively small variance and another with a mean\nof 5 and a relatively large variance. If the expected value of the response Yi ,\nconditional on X i , followed a regression model m(X i ; \u03b2), then two groups like\nthese would occur if there were two possible values of X i , one with a small\nvalue of m(X i ; \u03b2) and the other with a large value. Because of the concavity\nof the transformation h, the variance of the group with a mean of 5 is reduced\nby transformation. After the transformation, the groups have nearly the same\nvariance, as can be seen by observing the scatter of the two groups on the\ny-axis.\n    The strength of a transformation can be measured by how much its deriva-\ntive changes over some interval, say a to b. More precisely, for a < b, the\nstrength of an increasing transformation h is the derivative ratio h\u0003 (b)/h\u0003 (a).\nIf the transformation is concave, then the derivative ratio is less than 1 and the\nsmaller the ratio the stronger the concavity. Conversely, if the transformation\nis convex, then the derivative ratio is greater than 1 and the larger the ratio,\n\f                                                                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.24",
      "section_title": "There are two groups of responses,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7 The Geometry of Transformations        73\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.7",
      "section_title": "The Geometry of Transformations        73",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                                            tangent line at Y=1\n                          2\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "tangent line at Y=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n                          1\n\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n        log(Y)\n\n\n\n\n                                                              tangent line at Y=5\n                          0\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "log(Y)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n                  \u22121\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n                  \u22122\n\n                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                                      0         1        2         3       4        5      6       7     8\n                                                                           Y\n\n                                  Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "0         1        2         3       4        5      6       7     8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.24. A variance-stabilizing transformation.\n\n                                                                  Example: b/a = 2\n                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.24",
      "section_title": "A variance-stabilizing transformation.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n                                      1.5\n                   Derivative ratio\n                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.0",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                      0.5\n\n\n\n\n                                                                        concave           convex\n\n                                             \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0     \u22120.5        0.0     0.5       1.0    1.5     2.0\n                                                                           \u03b1\n\n                 Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "\u22120.5        0.0     0.5       1.0    1.5     2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.25. Derivative ratio for Box\u2013Cox transformations.\n\n\nthe greater the convexity. For a Box\u2013Cox transformation, the derivative ratio\nis (b/a)\u03b1\u22121 and so depends on a and b only through the ratio b/a. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.25",
      "section_title": "Derivative ratio for Box\u2013Cox transformations.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.25\nshows the derivative ratio of Box\u2013Cox transformations when b/a = 2. One\ncan see that the Box\u2013Cox transformation is concave when \u03b1 < 1, with the\nconcavity becoming stronger as \u03b1 decreases. Similarly, the transformation is\nconvex for \u03b1 > 1, with increasing convexity as \u03b1 increases.\n\f74      4 Exploratory Data Analysis\n\n\n\n\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.25",
      "section_title": "shows the derivative ratio of Box\u2013Cox transformations when b/a = 2. One",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1, with increasing convexity as \u03b1 increases.\n\f74      4 Exploratory Data Analysis",
        "start": 239,
        "end": 324
      }
    ]
  },
  {
    "content": "0.4\n                            0.3\n              correlation\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                            0.1\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 0.0\n\n\n\n\n                                                         absolute changes\n                                                         squared changes\n\n                                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2   0.4         0.6      0.8   1.0\n\n                                                         \u03b1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.2   0.4         0.6      0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.26. Correlations between the lagged risk-free returns and absolute (solid) and\nsquared (dashed) changes in the Box\u2013Cox transformed returns. A zero correlation\nindicates a constant conditional variance. Zero correlations are achieved with the\ntransformation parameter \u03b1 equal to ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.26",
      "section_title": "Correlations between the lagged risk-free returns and absolute (solid) and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.036 and 0.076 for the absolute and squared\nchanges, respectively, as indicated by the vertical lines. If \u03b1 \u2248 0, then the data are\nconditionally homoskedastic, or at least nearly so.\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.036",
      "section_title": "and 0.076 for the absolute and squared",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4. Risk-free returns\u2014Strength of the Box\u2013Cox transformation for\nvariance stabilization\n\n     In this example, we return to the changes in the risk-free interest rates. In\nFig. 4.19, it was seen that there is noticeable conditional heteroskedasticity in\nthe changes in the untransformed rate but little or no heteroskedasticity in the\nchanges in the logarithms of the rate. We will see that for a Box\u2013Cox transfor-\nmation intermediate in strength between the identity transformation (\u03b1 = 1)\nand the log transformation (\u03b1 = 0), some but not all of the heteroskedastic-\nity is removed, and that a transformation with \u03b1 < 0 is too strong for this\napplication so that a new type of heteroskedasticity is induced.\n     The strength of a Box\u2013Cox transformation for this example is illustrated in\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.4",
      "section_title": "Risk-free returns\u2014Strength of the Box\u2013Cox transformation for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.26. In that \ufb01gure, the correlations between the lagged risk-free interest\n                                                         (\u03b1)   (\u03b1)          (\u03b1)\nreturns, rt\u22121 , and absolute and squared changes, |rt \u2212 rt\u22121 | and {rt \u2212\n  (\u03b1)\nrt\u22121 }2 , in the transformed rate are plotted against \u03b1. The two correlations are\nsimilar, especially when they are near zero. Any deviations of the correlations\nfrom zero indicate conditional heteroskedasticity where the standard deviation\nof the change in the transformed rate depends on the previous value of the\nrate. We see that the correlations decrease as \u03b1 decreases from 1 so that the\nconcavity of the transformation increases. The correlations are equal to zero\nwhen \u03b1 is very close to 0, that is, the log transformation. If \u03b1 is much below\n0, then the transformation is too strong and the overtransformation induces a\nnegative correlation, which indicates that the conditional standard deviation\nis a decreasing function of the lagged rate.                                    \u0002\n\f                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.26",
      "section_title": "In that \ufb01gure, the correlations between the lagged risk-free interest",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 Transformation Kernel Density Estimation   75\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "Transformation Kernel Density Estimation   75",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 Transformation Kernel Density Estimation\nThe kernel density estimator (KDE) discussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "Transformation Kernel Density Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 is popular because\nof its simplicity and because it is available on most software platforms. How-\never, the KDE has some drawbacks. One disadvantage of the KDE is that\nit undersmooths densities with long tails. For example, the solid curve in\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "is popular because",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.27 is a KDE of annual earnings in 1988\u20131989 for 1109 individuals. The\ndata are in the Earnings data set in R\u2019s Ecdat package. The long right tail of\nthe density estimate exhibits bumps, which seem due solely to random varia-\ntion in the data, not to bumps in the true density. The problem is that there\nis no single bandwidth that works well both in the center of the data and in\nthe right tail. The automatic bandwidth selector chose a bandwidth that is a\ncompromise, undersmoothing in the tails and perhaps oversmoothing in the\ncenter. The latter problem can cause the height of the density at the mode(s)\nto be underestimated.\n    A better density estimate can be obtained by the transformation kernel\ndensity estimator (TKDE). The idea is to transform the data so that the\ndensity of the transformed data is easier to estimate by the KDE. For the\nearnings data, the square roots of the earnings are closer to being symmetric\nand have a shorter right tail than the original data; see Fig. 4.28, which\ncompares histograms of the original data and the data transformed by the\nsquare root. The KDE should work well for the square roots of the earnings.\n    Of course, we are interested in the density of the earnings, not the density\nof their square roots. However, it is easy to convert an estimate of the latter to\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.27",
      "section_title": "is a KDE of annual earnings in 1988\u20131989 for 1109 individuals. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.030\n\n\n\n\n                                                              KDE\n                                                              TKDE\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.030",
      "section_title": "KDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n        Density(y)\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "Density(y)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.010\n                     0.000\n\n\n\n\n                             0   20       40         60         80      100\n                                        y=income (in $1000)\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.010",
      "section_title": "0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.27. Kernel density and transformation kernel density estimates of annual\nearnings in 1988\u20131989 expressed in thousands of 1982 dollars. These data are the\nsame as in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.27",
      "section_title": "Kernel density and transformation kernel density estimates of annual",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.28.\n\f76            4 Exploratory Data Analysis\n\n                            Histogram of y                                        Histogram of sqrt(y)\n\n\n                  150\n\n\n\n\n                                                                     200\n                                                         Frequency\n      Frequency\n                  100\n\n\n\n\n                                                                     50 100\n                  50\n                  0\n\n\n\n\n                                                                     0\n                        0   20    40      60    80                            0     2    4    6    8     10\n                                   y\n                                                                                         sqrt(y)\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.28",
      "section_title": "76            4 Exploratory Data Analysis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.28. Histograms of earnings (y) and the square roots of earnings. The data\nare from the Earnings data set in R\u2019s Ecdat package and use only age group g1.\n\n\none of the former. To do that, one uses the change-of-variables formula (A.4).\nFor convenience, we repeat the result here\u2014if X = g(Y ), where g is monotonic\nand fX and fY are the densities of X and Y , respectively, then\n\n                                        fY (y) = fX {g(y)} |g \u0003 (y)|.                                         (4.6)\n                                         \u221a\nFor example, if x = g(y) =                   y, then g \u0003 (y) = y \u22121/2 /2 and\n                                                     \u221a\n                                       fY (y) = {fX ( y)y \u22121/2 }/2.\n\nPutting y = g \u22121 (x) into Eq. (4.6), we obtain\n\n                                 fY {g \u22121 (x)} = fX (x) |g \u0003 {g \u22121 (x)}|.                                     (4.7)\n\nEquation (4.7) suggests a convenient method for computing the TKDE:\n 1. start with data Y1 , . . . , Yn ;\n 2. transform the data to X1 = g(Y1 ), . . . , Xn = g(Yn );\n 3. let f\u0002X be the usual\n                    \u001a    KDE calculated on a grid x\u001b1 , . . . , xm using X1 , . . . , Xn ;\n                                      \u0015                \u0015\n 4. plot the pairs g (xj ), f\u0002X (xj ) \u0015g \u0003 {g \u22121 (xj )}\u0015 , j = 1, . . . , m.\n                      \u22121\n\n\n    The red dashed curve in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.28",
      "section_title": "Histograms of earnings (y) and the square roots of earnings. The data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.27 is a plot of the TKDE of the earnings\ndata using the square-root transformation. Notice the smoother right tail, the\nfaster decrease to 0 at the left boundary, and the somewhat sharper peak at\nthe mode compared to the KDE (solid curve).\n    When using a TKDE, it is important to choose a good transformation. For\npositive, right-skewed variables such as the earnings data, a concave transfor-\nmation is needed. A power transformation, y \u03b1 , for some \u03b1 < 1 is a common\nchoice. Although there are automatic methods for choosing \u03b1 (see Sect. 4.9),\ntrial-and-error is often good enough.\n\f                                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.27",
      "section_title": "is a plot of the TKDE of the earnings",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10 R Lab      77\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "R Lab      77",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.9 Bibliographic Notes\nExploratory data analysis was popularized by Tukey (1977). Hoaglin,\nMosteller, and Tukey (1983,1985) are collections of early articles on ex-\nploratory data analysis, data transformations, and robust estimation. Kleiber\nand Zeileis (2008) is an introduction to econometric modeling with R and cov-\ners exploratory data analysis as well as material in latter chapters of this book\nincluding regression and time series analysis. The R package AER accompanies\nKleiber and Zeileis\u2019s book.\n    The central limit theorem for sample quantiles is stated precisely and\nproved in textbooks on asymptotic theory such as Ser\ufb02ing (1980); Lehmann\n(1999), and van der Vaart (1998).\n    Silverman (1986) is an early book on nonparametric density estimation\nand is still well worth reading. Scott (1992) covers both univariate and mul-\ntivariate density estimation. Wand and Jones (1995) has an excellent treat-\nment of kernel density estimation as well as nonparametric regression, which\nwe cover in Chap. 21. Wand and Jones cover more recent developments such\nas transformation kernel density estimation. An alternative to the TKDE is\nvariable-bandwidth KDE; see Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.9",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.10 of Wand and Jones (1995) as well as\nAbramson (1982) and Jones (1990).\n    Atkinson (1985) and Carroll and Ruppert (1988) are good sources of in-\nformation about data transformations.\n    Wand, Marron, and Ruppert (1991) is an introduction to the TKDE and\ndiscusses methods for automatic selection of the transformation to minimize\nthe expected squared error of the estimator. Applications of TKDE to losses\ncan be found in Bolance, Guille\u0301n, and Nielsen (2003).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.10",
      "section_title": "of Wand and Jones (1995) as well as",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10 R Lab\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10.1 European Stock Indices\n\nThis lab uses four European stock indices in R\u2019s EuStockMarkets database.\nRun the following code to access the database, learn its mode and class, and\nplot the four time series. The plot() function will produce a plot tailored\nto the class of the object on which it is acting. Here four time series plots\nare produced because the class of EuStockMarkets is mts, multivariate time\nseries.\n   data(EuStockMarkets)\n   mode(EuStockMarkets)\n   class(EuStockMarkets)\n   plot(EuStockMarkets)\n\nIf you right-click on the plot, a menu for printing or saving will open. There\nare alternative methods for printing graphs. For example,\n\f78      4 Exploratory Data Analysis\n\n     pdf(\"EuStocks.pdf\", width = 6, height = 5)\n     plot(EuStockMarkets)\n     graphics.off()\n\nwill send a pdf \ufb01le to the working directory and the width and height pa-\nrameters allow one to control the size and aspect ratio of the plot.\n\nProblem 1 Write a brief description of the time series plots of the four in-\ndices. Do the series look stationary? Do the \ufb02uctuations in the series seem to\nbe of constant size? If not, describe how the volatility \ufb02uctuates.\n\n\nNext, run the following R code to compute and plot the log returns on the\nindices.\n     logR = diff(log(EuStockMarkets))\n     plot(logR)\n\nProblem 2 Write a brief description of the time series plots of the four series\nof log returns. Do the series look stationary? Do the \ufb02uctuations in the series\nseem to be of constant size? If not, describe how the volatility \ufb02uctuates.\n\n\n    In R, data can be stored as a data frame, which does not assume that the\ndata are in time order and would be appropriate, for example, with cross-\nsectional data. To appreciate how plot() works on a data frame rather than\non a multivariate time series, run the following code. You will be plotting the\nsame data as before, but they will be plotted in a di\ufb00erent way.\n     plot(as.data.frame(logR))\n\nRun the code that follows to create normal plots of the four indices and to\ntest each for normality using the Shapiro\u2013Wilk test. You should understand\nwhat each line of code does.\n     par(mfrow=c(2, 2))\n     for(i in colnames(logR))\n     {\n       qqnorm(logR[ ,i], datax = T, main = i)\n       qqline(logR[ ,i], datax = T)\n       print(shapiro.test(logR[ ,i]))\n     }\n\nProblem 3 Brie\ufb02y describe the shape of each of the four normal plots and\nstate whether the marginal distribution of each series is skewed or symmetric\nand whether its tails appear normal. If the tails do not appear normal, do they\nappear heavier or lighter than normal? What conclusions can be made from\nthe Shapiro\u2013Wilk tests? Include the plots with your work.\n\f                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "1 European Stock Indices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10 R Lab      79\n\n    The next set of R code creates t-plots with 1, 4, 6, 10, 20, and 30 degrees of\nfreedom and all four indices. However, for the remainder of this lab, only the\nDAX index will be analyzed. Notice how the reference line is created by the\nabline() function, which adds lines to a plot, and the lm() function, which\n\ufb01ts a line to the quantiles. The lm() function is discussed in Chap. 9.\n1  n=dim(logR)[1]\n2  q_grid = (1:n) / (n + 1)\n 3 df_grid = c(1, 4, 6, 10, 20, 30)\n\n 4 index.names = dimnames(logR)[[2]]\n\n 5 for(i in 1:4)\n\n 6 {\n\n 7   # dev.new()\n 8   par(mfrow = c(3, 2))\n 9   for(df in df_grid)\n10   {\n11     qqplot(logR[,i], qt(q_grid,df),\n12        main = paste(index.names[i], \", df = \", df) )\n13     abline(lm(qt(c(0.25, 0.75), df = df) ~\n14        quantile(logR[,i], c(0.25, 0.75))))\n15   }\n16 }\n\n\n\n   If you are running R from Rstudio, then line 7 should be left as it is. If\nyou are working directly in R, then remove the \u201c#\u201d in this line to open a new\nwindow for each plot.\n\nProblem 4 What does the code q.grid = (1:n) / (n + 1) do? What does\nqt(q.grid, df = df[j]) do? What does paste do?\n\n\nProblem 5 For the DAX index, state which choice of the degrees of freedom\nparameter gives the best-\ufb01tting t-distribution and explain why.\n\n\nRun the next set of code to create a kernel density estimate and two parametric\ndensity estimates, t with df degrees of freedom and normal, for the DAX index.\nHere df equals 5, but you should vary df so that the t density agrees as closely\nas possible with the kernel density estimate.\n    At lines 5\u20136, a robust estimator of the standard deviation of the t-\ndistribution is calculated using the mad() function. The default value of the\nargument constant is 1.4826, which is calibrated to the normal distribution\nsince 1/\u03a6\u22121 (3/4) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "R Lab      79",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4826. To calibrate to the t-distribution, the normal\nquantile is replaced by the corresponding t-quantile and multiplied by df/(df\n- 2) to convert from the scale parameter to the standard deviation.\n1   library(\"fGarch\")\n2   x=seq(-0.1, 0.1,by = 0.001)\n\f80      4 Exploratory Data Analysis\n\n3  par(mfrow = c(1, 1))\n4  df = 5\n 5 mad_t = mad(logR[ , 1],\n\n 6    constant = sqrt(df / (df - 2)) / qt(0.75, df))\n 7 plot(density(logR[ , 1]), lwd = 2, ylim = c(0, 60))\n\n 8 lines(x, dstd(x, mean = mean(logR[,1]), sd = mad_t, nu = df),\n\n 9    lty = 5, lwd = 2, col = \"red\")\n10 lines(x, dnorm(x, mean = mean(logR[ ,1]), sd = sd(logR[ ,1])),\n\n11    lty = 3, lwd = 4, col = \"blue\")\n12 legend(\"topleft\", c(\"KDE\", paste(\"t: df = \",df), \"normal\"),\n\n13    lwd = c(2, 2, 4), lty = c(1, 5, 3),\n14    col = c(\"black\", \"red\", \"blue\"))\nTo examine the left and right tails, plot the density estimate two more times,\nonce zooming in on the left tail and then zooming in on the right tail. You\ncan do this by using the xlim parameter of the plot() function and changing\nylim appropriately. You can also use the adjust parameter in density() to\nsmooth the tail estimate more than is done with the default value of adjust.\n\nProblem 6 Do either of the parametric models provide a reasonably good \ufb01t\nto the \ufb01rst index? Explain.\n\n\nProblem 7 Which bandwidth selector is used as the default by density?\nWhat is the default kernel?\n\n\nProblem 8 For the CAC index, state which choice of the degrees of freedom\nparameter gives the best-\ufb01tting t-distribution and explain why.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.4826",
      "section_title": "To calibrate to the t-distribution, the normal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10.2 McDonald\u2019s Prices and Returns\nThis section analyzes daily stock prices and returns of the McDonald\u2019s Cor-\nporation (MCD) over the period Jan-4-10 to Sep-5-14. The data set is in the\n\ufb01le MCD PriceDail.csv. Run the following commands to load the data and\nplot the adjusted closing prices:\n     data = read.csv(\u2019MCD_PriceDaily.csv\u2019)\n     head(data)\n     adjPrice = data[ , 7]\n     plot(adjPrice, type = \"l\", lwd = 2)\n\nProblem 9 Does the price series appear stationary? Explain your answer.\n\n\nProblem 10 Transform the prices into log returns and call that series LogRet.\nCreate a time series plot of LogRet and discuss whether or not this series ap-\npears stationary.\n\f                                                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "2 McDonald\u2019s Prices and Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11 Exercises     81\n\nThe following code produces a histogram of the McDonald\u2019s log returns. The\nhistogram will have 80 evenly spaced bins, and the argument freq = FALSE\nspeci\ufb01es the density scale.\n   hist(LogRet, 80, freq = FALSE)\n\nAlso, make a QQ plot of LogRet.\n\nProblem 11 Discuss any features you see in the histogram and QQ plot,\nand, speci\ufb01cally, address the following questions: Do the log returns appear\nto be normally distributed? If not, in what ways do they appear non-normal?\nAre the log returns symmetrically distributed? If not, how are they skewed?\nDo the log returns seems heavy tailed compared to a normal distribution? How\ndo the left and right tails compare; is one tail heavier than the other?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "Exercises     81",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11 Exercises\n\n1. This problem uses the data set ford.csv on the book\u2019s web site. The\n   data were taken from the ford.s data set in R\u2019s fEcofin package. This\n   package is no longer on CRAN. This data set contains 2000 daily Ford\n   returns from January 2, 1984, to December 31, 1991.\n   (a) Find the sample mean, sample median, and standard deviation of the\n       Ford returns.\n   (b) Create a normal plot of the Ford returns. Do the returns look nor-\n       mally distributed? If not, how do they di\ufb00er from being normally\n       distributed?\n   (c) Test for normality using the Shapiro\u2013Wilk test? What is the p-value?\n       Can you reject the null hypothesis of a normal distribution at 0.01?\n   (d) Create several t-plots of the Ford returns using a number of choices\n       of the degrees of freedom parameter (df). What value of df gives a\n       plot that is as linear as possible? The returns include the return on\n       Black Monday, October 19, 1987. Discuss whether or not to ignore\n       that return when looking for the best choices of df.\n   (e) Find the standard error of the sample median using formula (4.3) with\n       the sample median as the estimate of F \u22121 (0.5) and a KDE to esti-\n       mate f . Is the standard error of the sample median larger or smaller\n       than the standard error of the sample mean?\n2. Column seven of the data set RecentFord.csv on the book\u2019s web site\n   contains Ford daily closing prices, adjusted for splits and dividends, for\n   the years 2009\u20132013. Repeat Problem 1 using these more recent returns.\n   One of returns is approximately \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.175. For part (d), use that return in\n   place of Black Monday. (Black Monday, of course, is not in this data set.)\n   On what date did this return occur? Search the Internet for news about\n   Ford that day. Why did the Ford price drop so precipitously that day?\n\f82      4 Exploratory Data Analysis\n\n3. This problems uses the Garch data set in R\u2019s Ecdat package.\n   (a) Using a solid curve, plot a kernel density estimate of the \ufb01rst dif-\n       ferences of the variable dy, which is the U.S. dollar/Japanese yen\n       exchange rate. Using a dashed curve, superimpose a normal density\n       with the same mean and standard deviation as the sample. Do the\n       two estimated densities look similar? Describe how they di\ufb00er.\n   (b) Repeat part (a), but with the mean and standard deviation equal to\n       the median and MAD. Do the two densities appear more or less similar\n       compared to the two densities in part (a)?\n4. Suppose in a normal plot that the sample quantiles are plotted on the\n   vertical axis, rather than on the horizontal axis as in this book.\n   (a) What is the interpretation of a convex pattern?\n   (b) What is the interpretation of a concave pattern?\n   (c) What is the interpretation of a convex-concave pattern?\n   (d) What is the interpretation of a concave-convex pattern?\n5. Let diffbp be the changes (that is, di\ufb00erences) in the variable bp, the\n   U.S. dollar to British pound exchange rate, which is in the Garch data set\n   of R\u2019s Ecdat package.\n   (a) Create a 3 \u00d7 2 matrix of normal plots of diffbp and in each plot\n       add a reference line that goes through the p- and (1 \u2212 p)-quantiles,\n       where p = 0.25, 0.1, 0.05, 0.025, 0.01, and 0.0025, respectively, for the\n       six plots. Create a second set of six normal plots using n simulated\n       N (0, 1) random variables, where n is the number of changes in bp\n       plotted in the \ufb01rst \ufb01gure. Discuss how the reference lines change with\n       the value of p and how the set of six di\ufb00erent reference lines can help\n       detect nonnormality.\n   (b) Create a third set of six normal plots using changes in the logarithm\n       of bp. Do the changes in log(bp) look closer to being normally dis-\n       tributed than the changes in bp?\n6. Use the following fact about the standard normal cumulative distribution\n   function \u03a6(\u00b7):\n                               \u03a6\u22121 (0.025) = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.175",
      "section_title": "For part (d), use that return in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.96.\n   (a) What value is \u03a6\u22121 (0.975)? Why?\n   (b) What is the 0.975-quantile of the normal distribution with mean -1\n       and variance 2?\n7. Suppose that Y1 , . . . , Yn are i.i.d. with a uniform distribution on the in-\n   terval (0,1), with density function f and distribution function F de\ufb01ned\n   as                                                   \u23a7\n                   \u000e                                    \u23a8 0 if x \u2264 0,\n                     1 if x \u2208 (0, 1),\n           f (x) =                         and F (x) = x if x \u2208 (0, 1),\n                     0 otherwise,                       \u23a9\n                                                          1 if x \u2265 1.\n     Use Result ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.96",
      "section_title": "(a) What value is \u03a6\u22121 (0.975)? Why?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 to conclude which sample quantile q will have the smallest\n     variance?\n\f                                                              References      83\n\nReferences\nAbramson, I. (1982) On bandwidth variation in kernel estimates\u2014a square\n   root law. Annals of Statistics, 9, 168\u2013176.\nAtkinson, A. C. (1985) Plots, transformations, and regression: An introduc-\n   tion to graphical methods of diagnostic regression analysis, Clarendon Press,\n   Oxford.\nBolance, C., Guille\u0301n, M., and Nielsen, J. P. (2003) Kernel density estimation of\n   actuarial loss functions. Insurance: Mathematics and Economics, 32, 19\u201336.\nCarroll, R. J., and Ruppert, D. (1988) Transformation and Weighting in\n   Regression, Chapman & Hall, New York.\nHoaglin, D. C., Mosteller, F., and Tukey, J. W., Eds. (1983) Understanding\n   Robust and Exploratory Data Analysis, Wiley, New York.\nHoaglin, D. C., Mosteller, F., and Tukey, J. W., Eds. (1985) Exploring Data\n   Tables, Trends, and Shapes, Wiley, New York.\nJones, M. C. (1990) Variable kernel density estimates and variable kernel\n   density estimates. Australian Journal of Statistics, 32, 361\u2013371. (Note: The\n   title is intended to be ironic and is not a misprint.)\nKleiber, C., and Zeileis, A. (2008) Applied Econometrics with R, Springer,\n   New York.\nLehmann, E. L. (1999) Elements of Large-Sample Theory, Springer-Verlag,\n   New York.\nScott, D. W. (1992) Multivariate Density Estimation: Theory, Practice, and\n   Visualization, Wiley-Interscience, New York.\nSer\ufb02ing, R. J. (1980) Approximation Theorems of Mathematical Statistics,\n   Wiley, New York.\nSilverman, B. W. (1986) Density Estimation for Statistics and Data Analysis,\n   Chapman & Hall, London.\nTukey, J. W. (1977) Exploratory Data Analysis, Addison-Wesley, Reading,\n   MA.\nvan der Vaart, A. W. (1998) Asymptotic Statistics, Cambridge University\n   Press, Cambridge.\nWand, M. P., and Jones, M. C. (1995) Kernel Smoothing, Chapman & Hall,\n   London.\nWand, M. P., Marron, J. S., and Ruppert, D. (1991) Transformations in\n   density estimation, Journal of the American Statistical Association, 86,\n   343\u2013366.\nYap, B. W., and Sim, C. H. (2011) Comparisons of various types of normality\n   tests. Journal of Statistical Computation and Simulation, 81, 2141\u20132155.\n\f5\nModeling Univariate Distributions\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.1",
      "section_title": "to conclude which sample quantile q will have the smallest",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1 Introduction\n\nAs seen in Chap. 4, usually the marginal distributions of \ufb01nancial time series\nare not well \ufb01t by normal distributions. Fortunately, there are a number of\nsuitable alternative models, such as t-distributions, generalized error distribu-\ntions, and skewed versions of t- and generalized error distributions. All of these\nwill be introduced in this chapter. Typically, the parameters in these distri-\nbutions are estimated by maximum likelihood. Sections ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9 and 5.14 provide\nan introduction to the maximum likelihood estimator (MLE), and Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.9",
      "section_title": "and 5.14 provide",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.18\nprovides references for further study on this topic.\n    Software for maximum likelihood is readily available for standard models,\nand a reader interested only in data analysis and modeling often need not be\ngreatly concerned with the technical details of maximum likelihood. However,\nwhen performing a statistical analysis, it is always worthwhile to understand\nthe underlying theory, at least at a conceptual level, since doing so can prevent\nmisapplications. Moreover, when using a nonstandard model, often there is\nno software available for automatic computation of the MLE and one needs\nto understand enough theory to write a program to compute the MLE.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.18",
      "section_title": "provides references for further study on this topic.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2 Parametric Models and Parsimony\n\nIn a parametric statistical model, the distribution of the data is completely\nspeci\ufb01ed except for a \ufb01nite number of unknown parameters. For example,\nassume that Y1 , . . . , Yn are i.i.d. from a t-distribution1 with mean \u03bc, variance\n\n1\n    The reader who is unfamiliar with t-distributions should look ahead to Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "Parametric Models and Parsimony",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.2.\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                                     85\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 5\n\f86      5 Modeling Univariate Distributions\n\n\u03c3 2 , and degrees of freedom \u03bd. Then this is a parametric model provided that,\nas is usually the case, one or more of \u03bc, \u03c3 2 , and \u03bd are unknown.\n      A model should have only as many parameters as needed to capture the\nimportant features of the data. Each unknown parameter is another quantity\nto estimate and another source of estimation error. Estimation error, among\nother things, increases the uncertainty when one forecasts future observations.\nOn the other hand, a statistical model must have enough parameters to ade-\nquately describe the behavior of the data. A model with too few parameters\ncan create biases because the model does not \ufb01t the data well.\n      A statistical model with little bias, but without excess parameters, is called\nparsimonious and achieves a good tradeo\ufb00 between bias and variance. Finding\none or a few parsimonious models is an important part of data analysis.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3 Location, Scale, and Shape Parameters\n\nParameters are often classi\ufb01ed as location, scale, or shape parameters de-\npending upon which properties of a distribution they determine. A location\nparameter is a parameter that shifts a distribution to the right or left without\nchanging the distribution\u2019s shape or variability. Scale parameters quantify dis-\npersion. A parameter is a scale parameter for a univariate sample if the param-\neter is increased by the amount |a| when the data are multiplied by a. Thus,\nif \u03c3(X) is a scale parameter for a random variable X, then \u03c3(aX) = |a|\u03c3(X).\nA scale parameter is a constant multiple of the standard deviation provided\nthat the latter is \ufb01nite. Many examples of location and scale parameters can\nbe found in the following sections. If \u03bb is a scale parameter, then \u03bb\u22121 is\ncalled an inverse-scale parameter. Since scale parameters quantify dispersion,\ninverse-scale parameters quantify precision.\n    If f (y) is any \ufb01xed density, then f (y \u2212 \u03bc) is a family of distributions with\nlocation parameter \u03bc; \u03b8\u22121 f (y/\u03b8), \u03b8 > 0, is a family of distributions with a\nscale parameter \u03b8; and \u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)} is a family of distributions with\nlocation parameter \u03bc and scale parameter \u03b8. These facts can be derived by\nnoting that if Y has density f (y) and \u03b8 > 0, then, by Result A.1, Y + \u03bc\nhas density f (y \u2212 \u03bc), \u03b8Y has density \u03b8\u22121 f (\u03b8\u22121 y), and \u03b8Y + \u03bc has density\n\u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)}.\n    A shape parameter is de\ufb01ned as any parameter that is not changed by\nlocation and scale changes. More precisely, for any f (y), \u03bc, and \u03b8 > 0, the\nvalue of a shape parameter for the density f (y) will equal the value of that\nshape parameter for \u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)}. The degrees-of-freedom parameters\nof t-distributions and the log-standard deviations of lognormal distributions\nare shape parameters. Other shape parameters will be encountered later in\nthis chapter. Shape parameters are often used to specify the skewness or tail\nweight of a distribution.\n\f                                                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "Location, Scale, and Shape Parameters",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, is a family of distributions with a\nscale parameter \u03b8; and \u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)} is a family of distributions with\nlocation parameter \u03bc and scale parameter \u03b8. These facts can be derived by\nnoting that if Y has density f (y) and \u03b8 > 0, then, by Result A.1, Y + \u03bc\nhas density f (y \u2212 \u03bc), \u03b8Y has density \u03b8\u22121 f (\u03b8\u22121 y), and \u03b8Y + \u03bc has density\n\u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)}.\n    A shape parameter is de\ufb01ned as any parameter that is not changed by\nlocation and scale changes. More precisely, for any f (y), \u03bc, and \u03b8 > 0, the\nvalue of a shape parameter for the density f (y) will equal the value of that\nshape parameter for \u03b8\u22121 f {\u03b8\u22121 (y \u2212 \u03bc)}. The degrees-of-freedom parameters\nof t-distributions and the log-standard deviations of lognormal distributions\nare shape parameters. Other shape parameters will be encountered later in\nthis chapter. Shape parameters are often used to specify the skewness or tail\nweight of a distribution.",
        "start": 1060,
        "end": 2032
      }
    ]
  },
  {
    "content": "5.4 Skewness, Kurtosis, and Moments                87\n\n         a                         Right\u2212skewed         b                         Left\u2212skewed\n\n\n\n\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "Skewness, Kurtosis, and Moments                87",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                     3.0\n         Density(y)\n\n\n\n\n                                                        Density(y)\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.0",
      "section_title": "3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                     2.0\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                     1.0\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                     0.0\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2       0.2        0.6                      \u22120.6      \u22120.2       0.2\n                                        y                                             y\n\n         c                          Symmetric\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.2        0.6                      \u22120.6      \u22120.2       0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n                      2.0\n         Density(y)\n                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                      0.0\n\n\n\n\n                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4       0.0        0.4\n                                        y\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0        0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1. Skewed and symmetric densities. In each case, the mean is zero and is indi-\ncated by a vertical line. The distributions in panels (a)\u2013(c) are beta(4,10), beta(10,4),\nand beta(7,7), respectively. The R function dbeta() was used to calculate these den-\nsities.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.1",
      "section_title": "Skewed and symmetric densities. In each case, the mean is zero and is indi-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4 Skewness, Kurtosis, and Moments\n\nSkewness and kurtosis help characterize the shape of a probability distribu-\ntion. Skewness measures the degree of asymmetry, with symmetry implying\nzero skewness, positive skewness indicating a relatively long right tail com-\npared to the left tail, and negative skewness indicating the opposite. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "Skewness, Kurtosis, and Moments",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1\nshows three densities, all with an expectation equal to 0. The densities are\nright-skewed, left-skewed, and symmetric about 0, respectively, in panels\n(a)\u2013(c).\n    Kurtosis indicates the extent to which probability is concentrated in the\ncenter and especially the tails of the distribution rather than in the \u201cshoul-\nders,\u201d which are the regions between the center and the tails.\n    In Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.1",
      "section_title": "shows three densities, all with an expectation equal to 0. The densities are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.2, the left tail was de\ufb01ned as the region from \u2212\u221e to \u03bc \u2212 2\u03c3\nand the right tail as the region from \u03bc + 2\u03c3 to +\u221e. Here \u03bc and \u03c3 could\nbe the mean and standard deviation or the median and MAD. Admittedly,\nthese de\ufb01nitions are somewhat arbitrary. Reasonable de\ufb01nitions of center and\nshoulder would be that the center is the region from \u03bc \u2212 \u03c3 to \u03bc + \u03c3, the left\n\f88      5 Modeling Univariate Distributions\n\n                                       left                               right\n                   left tail           shoulder           center          shoulder         right tail\n\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "2, the left tail was de\ufb01ned as the region from \u2212\u221e to \u03bc \u2212 2\u03c3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n         0.3                                                                     t\u2212distribution\n\n                                                       normal\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.3                                                                     t\u2212distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2                                           distribution\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n           0\n            \u22124           \u22123        \u22122             \u22121          0           1          2         3        4\n\n        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n\n\n        0.02\n\n                                                  t\u2212distribution\n        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n\n                 normal\n                    distribution\n           0\n               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.01",
      "section_title": "normal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5                 3                   3.5                4              4.5            5\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "3                   3.5                4              4.5            5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2. Comparison of a normal density and a t-density with 5 degrees of freedom.\nBoth densities have mean 0 and standard deviation 1. The upper plot also indicates\nthe locations of the center, shoulders, and tail regions. The lower plot zooms in on\nthe right tail region.\n\n\nshoulder is from \u03bc \u2212 2\u03c3 to \u03bc \u2212 \u03c3, and the right shoulder is from \u03bc + \u03c3 to\n\u03bc + 2\u03c3. See the upper plot in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "Comparison of a normal density and a t-density with 5 degrees of freedom.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2. Because skewness and kurtosis measure\nshape, they do not depend on the values of location and scale parameters.\n   The skewness of a random variable Y is\n                         \u000e              \u000f3\n                            Y \u2212 E(Y )        E{Y \u2212 E(Y )}3\n                 Sk = E                    =                .\n                                \u03c3                  \u03c33\n\nTo appreciate the meaning of the skewness, it is helpful to look at an example;\nthe binomial distribution is convenient for that purpose. The skewness of the\nBinomial(n, p) distribution is\n                                                     1 \u2212 2p\n                               Sk(n, p) = \u0011                           ,   0 < p < 1.\n                                                    np(1 \u2212 p)\n\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "Because skewness and kurtosis measure",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3 shows the binomial probability distribution and its skewness\nfor n = 10 and four values of p. Notice that\n 1. the skewness is positive if p < 0.5, negative if p > 0.5, and 0 if p = 0.5;\n 2. the absolute skewness becomes larger as p moves closer to either 0 or 1\n    with n \ufb01xed;\n 3. the absolute skewness decreases to 0 as n increases to \u221e with p \ufb01xed;\n\f                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "shows the binomial probability distribution and its skewness",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.5, and 0 if p = 0.5;\n 2. the absolute skewness becomes larger as p moves closer to either 0 or 1\n    with n \ufb01xed;\n 3. the absolute skewness decreases to 0 as n increases to \u221e with p \ufb01xed;",
        "start": 165,
        "end": 407
      }
    ]
  },
  {
    "content": "5.4 Skewness, Kurtosis, and Moments                  89\n\n    Positive skewness is also called right skewness and negative skewness is\ncalled left skewness. A distribution is symmetric about a point \u03b8 if P (Y >\n\u03b8 + y) = P (Y < \u03b8 \u2212 y) for all y > 0. In this case, \u03b8 is a location parameter\nand equals E(Y ), provided that E(Y ) exists. The skewness of any symmetric\ndistribution is 0. Property 3 is not surprising in light of the central limit\ntheorem. We know that the binomial distribution converges to the symmetric\nnormal distribution as n \u2192 \u221e with p \ufb01xed and not equal to 0 or 1.\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "Skewness, Kurtosis, and Moments                  89",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0. In this case, \u03b8 is a location parameter\nand equals E(Y ), provided that E(Y ) exists. The skewness of any symmetric\ndistribution is 0. Property 3 is not surprising in light of the central limit\ntheorem. We know that the binomial distribution converges to the symmetric\nnormal distribution as n \u2192 \u221e with p \ufb01xed and not equal to 0 or 1.",
        "start": 243,
        "end": 584
      }
    ]
  },
  {
    "content": "0.4\n               p = 0.9, Sk = \u22120.84                                   p = 0.5, Sk = 0\n               K = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "p = 0.9, Sk = \u22120.84                                   p = 0.5, Sk = 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5                                         0.3   K = 2.8\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "0.3   K = 2.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\nP(X=x)\n\n\n\n\n                                                      P(X=x)\n                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "P(X=x)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n         0.2\n\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1                                                   0.1\n\n\n          0                                                     0\n               0 1 2 3 4 5 6 7 8 9 10                                0 1 2 3 4 5 6 7 8 9 10\n                         x                                                     x\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4                                                    1\n\n                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n         0.3                K = 3.03\n                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.3                K = 3.03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\nP(X=x)\n\n\n\n\n                                                      P(X=x)\n\n\n\n\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "P(X=x)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2                p = 0.2, Sk = 0.47\n                                                                                p = 0.02, Sk = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "p = 0.2, Sk = 0.47",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.17\n                                                               0.4\n         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.17",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1                                                                      K = 7.50\n                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "K = 7.50",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n          0                                                     0\n               0 1 2 3 4 5 6 7 8 9 10                                0 1 2 3 4 5 6 7 8 9 10\n                         x                                                     x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0                                                     0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3. Several binomial probability distributions with n = 10 and their skewness\ndetermined by the shape parameter p. Sk = skewness coe\ufb03cient and K = kurtosis\ncoe\ufb03cient. The top left plot has left-skewness (Sk = \u22120.84). The top right plot has\nno skewness (Sk = 0). The bottom left plot has moderate right-skewness (Sk = 0.47).\nThe bottom-left plot has strong right skewness (Sk = 2.17).\n\n\n         The kurtosis of a random variable Y is\n                               \u000e           \u000f4\n                                 Y \u2212 E(Y )      E{Y \u2212 E(Y )}4\n                      Kur = E                 =               .\n                                    \u03c3                \u03c34\n\nThe kurtosis of a normal random variable is 3. The smallest possible value of\nthe kurtosis is 1 and is achieved by any random variable taking exactly two\n\f90       5 Modeling Univariate Distributions\n\ndistinct values, each with probability 1/2. The kurtosis of a Binomial(n, p)\ndistribution is\n                                          1 \u2212 6p(1 \u2212 p)\n                      KurBin (n, p) = 3 +               .\n                                            np(1 \u2212 p)\nNotice that KurBin (n, p) \u2192 3, the value at the normal distribution, as\nn \u2192 \u221e with p \ufb01xed, which is another sign of the central limit theorem\nat work. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "Several binomial probability distributions with n = 10 and their skewness",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3 also gives the kurtosis of the distributions in that \ufb01g-\nure. KurBin (n, p) equals 1, the minimum value of kurtosis, when n = 1 and\np = 1/2.\n    It is di\ufb03cult to interpret the kurtosis of an asymmetric distribution be-\ncause, for such distributions, kurtosis may measure both asymmetry and tail\nweight, so the binomial is not a particularly good example for understand-\ning kurtosis. For that purpose we will look instead at t-distributions because\nthey are symmetric. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "also gives the kurtosis of the distributions in that \ufb01g-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2 compares a normal density with the t5 -density\nrescaled to have variance equal to 1. Both have a mean of 0 and a standard\ndeviation of 1. The mean and standard deviation are location and scale pa-\nrameters, respectively, and do not a\ufb00ect kurtosis. The parameter \u03bd of the\nt-distribution is a shape parameter. The kurtosis of a t\u03bd -distribution is \ufb01nite\nif \u03bd > 4 and then the kurtosis is\n                                                6\n                              Kurt (\u03bd) = 3 +       .                         (5.1)\n                                               \u03bd\u22124\nFor example, the kurtosis is 9 for a t5 -distribution. Since the densities in\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "compares a normal density with the t5 -density",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "4 and then the kurtosis is\n                                                6\n                              Kurt (\u03bd) = 3 +       .                         (5.1)\n                                               \u03bd\u22124\nFor example, the kurtosis is 9 for a t5 -distribution. Since the densities in\nFig.",
        "start": 361,
        "end": 657
      }
    ]
  },
  {
    "content": "5.2 have the same mean and standard deviation, they also have the same\ntail, center, and shoulder regions, at least according to our somewhat arbitrary\nde\ufb01nitions of these regions, and these regions are indicated on the top plot.\nThe bottom plot zooms in on the right tail. Notice that the t5 -density has more\nprobability in the tails and center than the N (0, 1) density. This behavior of\nt5 is typical of symmetric distributions with high kurtosis.\n    Every normal distribution has a skewness coe\ufb03cient of 0 and a kurtosis\nof 3. The skewness and kurtosis must be the same for all normal distributions,\nbecause the normal distribution has only location and scale parameters, no\nshape parameters. The kurtosis of 3 agrees with formula (5.1) since a normal\ndistribution is a t-distribution with \u03bd = \u221e. The \u201cexcess kurtosis\u201d of a distri-\nbution is (Kur \u2212 3) and measures the deviation of that distribution\u2019s kurtosis\nfrom the kurtosis of a normal distribution. From (5.1) we see that the excess\nkurtosis of a t\u03bd -distribution is 6/(\u03bd \u2212 4).\n    An exponential distribution2 has a skewness equal to 2 and a kurtosis of 9.\nA double-exponential distribution has skewness 0 and kurtosis 6. Since the ex-\nponential distribution has only a scale parameter and the double-exponential\nhas only a location and a scale parameter, their skewness and kurtosis must\nbe constant since skewness and kurtosis depend only on shape parameters.\n2\n     The exponential   and   double-exponential   distributions   are   de\ufb01ned   in\n     Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "have the same mean and standard deviation, they also have the same",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5.\n\f                                      5.4 Skewness, Kurtosis, and Moments          91\n\n    The Lognormal(\u03bc, \u03c3 2 ) distribution, which is discussed in Appendix A.9.4,\nhas the log-mean \u03bc as a scale parameter and the log-standard deviation \u03c3 as\na shape parameter\u2014even though \u03bc and \u03c3 are location and scale parameters\nfor the normal distribution itself, they are scale and shape parameters for\nthe lognormal. The e\ufb00ects of \u03c3 on lognormal shapes can be seen in Figs. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "9.5",
      "section_title": "5.4 Skewness, Kurtosis, and Moments          91",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.11\nand A.1. The skewness coe\ufb03cient of the lognormal(\u03bc, \u03c3 2 ) distribution is\n                                       \u0011\n                        {exp(\u03c3 2 ) + 2} exp(\u03c3 2 ) \u2212 1.                    (5.2)\nSince \u03bc is a scale parameter, it has no e\ufb00ect on the skewness. The skewness\nis always positive and increases from 0 to \u221e as \u03c3 increases from 0 to \u221e.\n    Estimation of the skewness and kurtosis of a distribution is relatively\nstraightforward if we have a sample, Y1 , . . . , Yn , from that distribution. Let the\nsample mean and standard deviation be Y and s. Then the sample skewness,\n            \u001f is\ndenoted by Sk,\n                                    n \u0007               \b3\n                           \u001f= 1            Yi \u2212 Y\n                           Sk                             ,                      (5.3)\n                                 n i=1          s\n\nand the sample kurtosis, denoted by Kur, is\n                                   n \u0007        \b4\n                                1      Yi \u2212 Y\n                         Kur =                   .                              (5.4)\n                                n i=1     s\n\nOften the factor 1/n in (5.3) and (5.4) is replaced by 1/(n\u22121), in analogy with\nthe sample variance. Both the sample skewness and the excess kurtosis should\nbe near 0 if a sample is from a normal distribution. Deviations of the sample\nskewness and kurtosis from these values are an indication of nonnormality.\n    A word of caution is in order. Skewness and kurtosis are highly sensitive\nto outliers. Sometimes outliers are due to contaminants, that is, bad data not\nfrom the population being sampled. An example would be a data recording\nerror. A sample from a normal distribution with even a single contaminant\nthat is su\ufb03ciently outlying will appear highly nonnormal according to the\nsample skewness and kurtosis. In such a case, a normal plot will look linear,\nexcept that the single contaminant will stick out. See Fig. 5.4, which is a\nnormal plot of a sample of 999 N (0, 1) data points plus a contaminant equal\nto 30. This \ufb01gure shows clearly that the sample is nearly normal but with\nan outlier. The sample skewness and kurtosis, however, are ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.11",
      "section_title": "and A.1. The skewness coe\ufb03cient of the lognormal(\u03bc, \u03c3 2 ) distribution is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.85 and 243.04,\nwhich might give the false impression that the sample is far from normal. Also,\neven if there were no contaminants, a distribution could be extremely close to\na normal distribution except in the extreme tails and yet have a skewness or\nexcess kurtosis that is very di\ufb00erent from 0.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "10.85",
      "section_title": "and 243.04,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4.1 The Jarque\u2013Bera Test\nThe Jarque\u2013Bera test of normality compares the sample skewness and kurtosis\nto 0 and 3, their values under normality. The test statistic is\n\f92                  5 Modeling Univariate Distributions\n\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "1 The Jarque\u2013Bera Test",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.999\n                    0.997\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.999",
      "section_title": "0.997",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99\n                     0.98\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.99",
      "section_title": "0.98",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.95\n                                                               contaminant\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.95",
      "section_title": "contaminant",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.90\n\n\n                     0.75\n      probability\n\n\n\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.90",
      "section_title": "0.75",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.50\n\n\n                     0.25\n\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.50",
      "section_title": "0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n                     0.05\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n                     0.01\n\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "0.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.003\n                    0.001\n\n\n                              0        5          10      15        20       25   30\n\n                                                       Data\n\n     Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.003",
      "section_title": "0.001",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4. Normal plot of a sample of 999 N (0, 1) data plus a contaminant.\n\n                                              2\n                                          \u001f /6 + (Kur \u2212 3)2 /24},\n                                   JB = n{Sk\n\nwhich, of course, is 0 when Sk \u001f and Kur, respectively, have the values 0 and\n3, the values expected under normality, and increases as Sk   \u001f and Kur deviate\nfrom these values. In R, the test statistic and its p-value can be computed with\nthe jarque.bera.test() function.\n    A large-sample approximation is used to compute a p-value. Under the\nnull hypothesis, JB converges to the chi-square distribution with 2 degrees of\nfreedom (\u03c722 ) as the sample size becomes in\ufb01nite, so the approximate p-value\nis 1 \u2212 F\u03c722 (JB), where F\u03c722 is the CDF of the \u03c722 -distribution.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "Normal plot of a sample of 999 N (0, 1) data plus a contaminant.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4.2 Moments\n\nThe expectation, variance, skewness coe\ufb03cient, and kurtosis of a random vari-\nable are all special cases of moments, which will be de\ufb01ned in this section.\n    Let X be a random variable. The kth moment of X is E(X k ), so in par-\nticular the \ufb01rst moment is the expectation of X. The kth absolute moment is\nE|X|k .\n    The kth central moment is\n                                  \u0018               \u0019\n                            \u03bck = E {X \u2212 E(X)}k ,                          (5.5)\n\nso, for example, \u03bc2 is the variance of X. The skewness coe\ufb03cient of X is\n\f                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "2 Moments",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 Heavy-Tailed Distributions     93\n                                                \u03bc3\n                                   Sk(X) =             ,                         (5.6)\n                                              (\u03bc2 )3/2\n\nand the kurtosis of X is\n                                                 \u03bc4\n                                   Kur(X) =            .                         (5.7)\n                                                (\u03bc2 )2\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Heavy-Tailed Distributions     93",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 Heavy-Tailed Distributions\nAs discussed in earlier chapters, distributions with higher tail probabilities\ncompared to a normal distribution are called heavy-tailed. Because kurtosis is\nparticularly sensitive to tail weight, high kurtosis is nearly synonymous with\nhaving a heavy tailed distribution. Heavy-tailed distributions are important\nmodels in \ufb01nance, because equity returns and other changes in market prices\nusually have heavy tails. In \ufb01nance applications, one is especially concerned\nwhen the return distribution has heavy tails because of the possibility of an\nextremely large negative return, which could, for example, entirely deplete the\ncapital reserves of a \ufb01rm. If one sells short,3 then large positive returns are\nalso worrisome.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Heavy-Tailed Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.1 Exponential and Polynomial Tails\n\nDouble-exponential distributions have slightly heavier tails than normal dis-\ntributions. This fact can be appreciated by comparing their densities. The\ndensity of the double-exponential with scale parameter \u03b8 is proportional to\nexp(\u2212|y/\u03b8|) and the density of the N (0, \u03c3 2 ) distribution is proportional to\nexp{\u22120.5(y/\u03c3)2 }. The term \u2212y 2 converges to \u2212\u221e much faster than \u2212|y| as\n|y| \u2192 \u221e. Therefore, the normal density converges to 0 much faster than the\ndouble-exponential density as |y| \u2192 \u221e. The generalized error distributions\ndiscussed soon in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "1 Exponential and Polynomial Tails",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6 have densities proportional to\n                                                  \u03b1\n                                     exp (\u2212 |y/\u03b8| ) ,                            (5.8)\n\nwhere \u03b1 > 0 is a shape parameter and \u03b8 is a scale parameter. The special\ncases of \u03b1 = 1 and 2 are, of course, the double-exponential and normal den-\nsities. If \u03b1 < 2, then a generalized error distribution will have heavier tails\nthan a normal distribution, with smaller values of \u03b1 implying heavier tails.\nIn particular, \u03b1 < 1 implies a tail heavier than that of a double-exponential\ndistribution.\n    However, no density of the form (5.8) will have truly heavy tails, and, in\nparticular, E(|Y |k ) < \u221e for all k, so all moments are \ufb01nite. To achieve a very\nheavy right tail, the density must be such that\n\n                             f (y) \u223c Ay \u2212(a+1) as y \u2192 \u221e                          (5.9)\n3\n    See Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "have densities proportional to",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 is a shape parameter and \u03b8 is a scale parameter. The special\ncases of \u03b1 = 1 and 2 are, of course, the double-exponential and normal den-\nsities. If \u03b1 < 2, then a generalized error distribution will have heavier tails\nthan a normal distribution, with smaller values of \u03b1 implying heavier tails.\nIn particular, \u03b1 < 1 implies a tail heavier than that of a double-exponential\ndistribution.\n    However, no density of the form (5.8) will have truly heavy tails, and, in\nparticular, E(|Y |k ) < \u221e for all k, so all moments are \ufb01nite. To achieve a very\nheavy right tail, the density must be such that",
        "start": 183,
        "end": 782
      }
    ]
  },
  {
    "content": "16.5 for a discussion of short selling.\n\f94        5 Modeling Univariate Distributions\n\nfor some A > 0 and a > 0, which will be called a right polynomial tail, in\ncontrast to\n                    f (y) \u223c A exp(\u2212y/\u03b8) as y \u2192 \u221e                   (5.10)\nfor some A > 0 and \u03b8 > 0, which will be called an exponential right tail.\nPolynomial and exponential left tails are de\ufb01ned analogously.\n    A polynomial tail is also called a Pareto tail after the Pareto distribution\nde\ufb01ned in Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "16.5",
      "section_title": "for a discussion of short selling.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and a > 0, which will be called a right polynomial tail, in\ncontrast to\n                    f (y) \u223c A exp(\u2212y/\u03b8) as y \u2192 \u221e                   (5.10)\nfor some A > 0 and \u03b8 > 0, which will be called an exponential right tail.\nPolynomial and exponential left tails are de\ufb01ned analogously.\n    A polynomial tail is also called a Pareto tail after the Pareto distribution\nde\ufb01ned in Appendix A.",
        "start": 99,
        "end": 487
      }
    ]
  },
  {
    "content": "9.8. The parameter a of a polynomial tail is called the\ntail index. The smaller the value of a, the heavier the tail. The value of a must\nbe greater than 0, because if a \u2264 0, then the density integrates to \u221e, not 1.\nAn exponential tail as in (5.8) is lighter than any polynomial tail, since\n                           exp(\u2212|y/\u03b8|\u03b1 )\n                                         \u2192 0 as |y| \u2192 \u221e\n                             |y|\u2212(a+1)\nfor all \u03b8 > 0, \u03b1 > 0, and a > 0.\n    It is, of course, possible to have left and right tails that behave quite\ndi\ufb00erently from each other. For example, one could be polynomial and the\nother exponential, or they could both be polynomial but with di\ufb00erent indices.\n    A density with both tails polynomial will have a \ufb01nite kth absolute moment\nonly if the smaller of the two tail indices is larger than k. If both tails are\nexponential, then all moments are \ufb01nite.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "9.8",
      "section_title": "The parameter a of a polynomial tail is called the",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, \u03b1 > 0, and a > 0.\n    It is, of course, possible to have left and right tails that behave quite\ndi\ufb00erently from each other. For example, one could be polynomial and the\nother exponential, or they could both be polynomial but with di\ufb00erent indices.\n    A density with both tails polynomial will have a \ufb01nite kth absolute moment\nonly if the smaller of the two tail indices is larger than k. If both tails are\nexponential, then all moments are \ufb01nite.",
        "start": 437,
        "end": 891
      }
    ]
  },
  {
    "content": "5.5.2 t-Distributions\n\nThe t-distributions have played an extremely important role in classical statis-\ntics because of their use in testing and con\ufb01dence intervals when the data are\nmodeled as having normal distributions. More recently, t-distributions have\ngained added importance as models for the distribution of heavy-tailed phe-\nnomena such as \ufb01nancial markets data.\n    We will start with some de\ufb01nitions. If Z is N (0, 1), W is chi-squared4 with\n\u03bd degrees of freedom, and Z and W are independent, then the distribution of\n                                       \u0011\n                                    Z/ W/\u03bd                                (5.11)\n\nis called the t-distribution with \u03bd degrees of freedom and denoted t\u03bd . The \u03b1-\nupper quantile of the t\u03bd -distribution is denoted by t\u03b1,\u03bd and is used in tests\nand con\ufb01dence intervals about population means, regression coe\ufb03cients, and\nparameters in time series models.5 In testing and interval estimation, the\nparameter \u03bd generally assumes only positive integer values, but when the\nt-distribution is used as a model for data, \u03bd is restricted only to be positive.\n    The density of the t\u03bd -distribution is\n                           !                \"\n                              \u0393 {(\u03bd + 1)/2}           1\n                ft,\u03bd (y) =                                       .        (5.12)\n                             (\u03c0\u03bd) \u0393 (\u03bd/2) {1 + (y /\u03bd)}(\u03bd+1)/2\n                                  1/2               2\n\n4\n     Chi-squared distributions are discussed in Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "2 t-Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.\n5\n     See Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "10.1",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1 for con\ufb01dence intervals for the mean.\n\f                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "17.1",
      "section_title": "for con\ufb01dence intervals for the mean.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 Heavy-Tailed Distributions       95\n\nHere \u0393 is the gamma function de\ufb01ned by\n                          \u0013 \u221e\n                  \u0393 (t) =     xt\u22121 exp(\u2212x)dx,           t > 0.                (5.13)\n                               0\n\nThe quantity in large square brackets in (5.12) is just a constant, though a\nsomewhat complicated one.\n    The variance of a t\u03bd is \ufb01nite and equals \u03bd/(\u03bd \u2212 2) if \u03bd > 2. If 0 < \u03bd \u2264 1,\nthen the expected value of the t\u03bd -distribution does not exist and the variance\nis not de\ufb01ned.6 If 1 < \u03bd \u2264 2, then the expected value is 0 and the variance is\nin\ufb01nite. If Y has a t\u03bd -distribution, then\n\n                                      \u03bc + \u03bbY\n\nis said to have a t\u03bd (\u03bc, \u03bb2 ) distribution, and \u03bb will be called the scale parameter.\nWith this notation, the t\u03bd and t\u03bd (0, 1) distributions are the same. If \u03bd > 1,\nthen the t\u03bd (\u03bc, \u03bb2 ) distribution has a mean equal to \u03bc, and if \u03bd > 2, then it\nhas a variance equal to \u03bb2 \u03bd/(\u03bd \u2212 2).\n    The t-distribution will also be called the classical t-distribution to distin-\nguish it from the standardized t-distribution de\ufb01ned in the next section.\n\nStandardized t-Distributions\n\nInstead of the classical t-distribution just discussed, some software uses a\n\u201cstandardized\u201d version of the t-distribution. The di\ufb00erence between the two\nversions is merely notational, but it is important to be aware of this di\ufb00erence.\n    The t\u03bd {0, (\u03bd \u2212 2)/\u03bd} distribution with \u03bd > 2 has a mean equal to 0 and\nvariance equal to 1 and is called a standardized t-distribution, and will be de-\nnoted by tstd                                                std      2\n           \u03bd (0, 1). More generally, for \u03bd > 2, de\ufb01ne the t\u03bd (\u03bc, \u03c3 ) distribution\nto be equal to the t\u03bd [ \u03bc, {(\u03bd \u2212 2)/\u03bd}\u03c3 ] distribution, so that \u03bc and \u03c3 2 are the\n                                           2\n\nmean and variance of the tstd\u03bd (\u03bc, \u03c3 ) distribution. For \u03bd \u2264 2, t\u03bd (\u03bc, \u03c3 ) cannot\n                                     2                             std    2\n\nbe de\ufb01ned since the t-distribution does not have a \ufb01nite variance in this case.\nThe advantage in using the tstd           2                        2\n                                \u03bd (\u03bc, \u03c3 ) distribution is that \u03c3 is the variance,\nwhereas for the t\u03bd (\u03bc, \u03bb ) distribution, \u03bb is not the variance but instead \u03bb2 is\n                         2                    2\n\nthe variance times (\u03bd \u2212 2)/\u03bd.\n    Some software uses the standardized t-distribution while other software\nuses the classical t-distribution. It is, of course, important to understand which\nt-distribution is being used in any speci\ufb01c application. However, estimates\nfrom one model can be translated easily into the estimates one would obtain\nfrom the other model; see Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Heavy-Tailed Distributions       95",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.                (5.13)\n                               0",
        "start": 168,
        "end": 229
      },
      {
        "language": "r",
        "code": "2. If 0 < \u03bd \u2264 1,\nthen the expected value of the t\u03bd -distribution does not exist and the variance\nis not de\ufb01ned.6 If 1 < \u03bd \u2264 2, then the expected value is 0 and the variance is\nin\ufb01nite. If Y has a t\u03bd -distribution, then",
        "start": 392,
        "end": 614
      },
      {
        "language": "r",
        "code": "1,\nthen the t\u03bd (\u03bc, \u03bb2 ) distribution has a mean equal to \u03bc, and if \u03bd > 2, then it\nhas a variance equal to \u03bb2 \u03bd/(\u03bd \u2212 2).\n    The t-distribution will also be called the classical t-distribution to distin-\nguish it from the standardized t-distribution de\ufb01ned in the next section.",
        "start": 820,
        "end": 1100
      },
      {
        "language": "r",
        "code": "2 has a mean equal to 0 and\nvariance equal to 1 and is called a standardized t-distribution, and will be de-\nnoted by tstd                                                std      2\n           \u03bd (0, 1). More generally, for \u03bd > 2, de\ufb01ne the t\u03bd (\u03bc, \u03c3 ) distribution\nto be equal to the t\u03bd [ \u03bc, {(\u03bd \u2212 2)/\u03bd}\u03c3 ] distribution, so that \u03bc and \u03c3 2 are the\n                                           2",
        "start": 1411,
        "end": 1804
      }
    ]
  },
  {
    "content": "5.14 for an example.\n\n\n\n\n6\n    See Appendix A.3 for discussion of when the mean and the variance exist and\n    when they are \ufb01nite.\n\f96     5 Modeling Univariate Distributions\n\nt-Distributions Have Polynomial Tails\n\nThe t-distributions are a class of heavy-tailed distributions and can be used\nto model heavy-tail returns data. For t-distributions, both the kurtosis and\nthe weight of the tails increase as \u03bd gets smaller. When \u03bd \u2264 4, the tail weight\nis so high that the kurtosis is in\ufb01nite. For \u03bd > 4, the kurtosis is given by (5.1).\n    By (5.12), the t-distribution\u2019s density is proportional to\n                                        1\n                               {1 + (y 2 /\u03bd)}(\u03bd+1)/2\n\nwhich for large values of |y| is approximately\n                                   1\n                                           \u221d |y|\u2212(\u03bd+1) .\n                           (y 2 /\u03bd)(\u03bd+1)/2\n\nTherefore, the t-distribution has polynomial tails with tail index a = \u03bd. The\nsmaller the value of \u03bd, the heavier the tails.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "for an example.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "4, the kurtosis is given by (5.1).\n    By (5.12), the t-distribution\u2019s density is proportional to\n                                        1\n                               {1 + (y 2 /\u03bd)}(\u03bd+1)/2",
        "start": 498,
        "end": 694
      }
    ]
  },
  {
    "content": "5.5.3 Mixture Models\n\nDiscrete Mixtures\n\nAnother class of models containing heavy-tailed distributions is the set of mix-\nture models. Consider a distribution that is 90 % N (0, 1) and 10 % N (0, 25).\nA random variable Y with this distribution can be obtained by generating a\nnormal random variable X with mean 0 and variance 1 and a uniform(0,1) ran-\ndom variable U that is independent of X. If U < 0.9, then Y = X. If U \u2265 0.9,\nthen Y = 5X. If an independent sample from this distribution is generated,\nthen the expected percentage of observations from the N (0, 1) component is\n90 %. The actual percentage is random; in fact, it has a Binomial(n, 0.9) dis-\ntribution, where n is a sample size. By the law of large numbers, the actual\npercentage converges to 90 % as n \u2192 \u221e. This distribution could be used to\nmodel a market that has two regimes, the \ufb01rst being \u201cnormal volatility\u201d and\nsecond \u201chigh volatility,\u201d with the \ufb01rst regime occurring 90 % of the time.\n    This is an example of a \ufb01nite or discrete normal mixture distribution,\nsince it is a mixture of a \ufb01nite number, here two, di\ufb00erent normal distribu-\ntions called the components. A random variable with this distribution has a\nvariance equal to 1 with 90 % probability and equal to 25 with 10 % probabil-\nity. Therefore, the variance\n                          \u221a of this distribution is (0.9)(1) + (0.1)(25) = 3.4, so\nits standard deviation is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "3 Mixture Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4 = 1.84. This distribution is much di\ufb00erent from\nan N (0, 3.4) distribution, even though the two distributions have the same\nmean and variance. To appreciate this, look at Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "= 1.84. This distribution is much di\ufb00erent from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.\n    You can see in Fig. 5.5a that the two densities look quite di\ufb00erent. The\nnormal density looks much more dispersed than the normal mixture, but they\nactually have the same variances. What is happening? Look at the detail of\n\f                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "You can see in Fig. 5.5a that the two densities look quite di\ufb00erent. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 Heavy-Tailed Distributions                                    97\n\n      a                                          densities                   b                                          densities\n\n\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Heavy-Tailed Distributions                                    97",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                               normal                                                               normal\n\n\n\n\n                                                                                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "normal                                                               normal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n                                                               mixture                                                              mixture\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "mixture                                                              mixture",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n       density\n\n\n\n\n                                                                              density\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                     0.010\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                                                                                     0.000\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                     \u22125         0      5       10       15                                   4    6      8    10   12        14\n                                                      x                                                                      x\n\n\n      c                                    QQ plot, normal                   d                                   QQ plot, mixture\n                               3\n\n\n\n\n                                                                                                     3\n                                                                                                     2\n                               2\n       theoretical quantiles\n\n\n\n\n                                                                             theoretical quantiles\n                                                                                                     1\n                               1\n\n\n\n\n                                                                                                     0\n                               0\n                               \u22121\n\n\n\n\n                                                                                                     \u22121\n                               \u22122\n\n\n\n\n                                                                                                     \u22122\n                               \u22123\n\n\n\n\n                                                                                                     \u22123\n\n\n\n\n                                          \u22124    \u22122    0    2        4                                            \u22125          0           5\n                                               sample quantiles                                                       sample quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22125         0      5       10       15                                   4    6      8    10   12        14",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5. Comparison of N (0, 3.4) distribution and heavy-tailed normal mixture\ndistributions. These distributions have the same mean and variance. The normal\nmixture distribution is 90 % N (0, 1) and 10 % N (0, 25). In (c) and (d) the sample\nsize is 200. In panel (a), the left tail is not shown fully to provide detail at the center\nand because the left tail is the mirror image of the right tail. (b) Detail of right tail.\n\n\nthe right tails in panel (b). The normal mixture density is much higher than\nthe normal density when x is greater than 6. This is the \u201coutlier\u201d region (along\nwith x < \u22126).7 The normal mixture has far more outliers than the normal\ndistribution and the outliers come from the 10 % of the population with a\nvariance of 25. Remember that \u00b16 is only 6/5 standard deviations from the\nmean, using the standard deviation 5 of the component from which they\ncome. Thus, these observations are not outlying relative to their component\u2019s\nstandard deviation of 5, only relative to the population standard deviation of\n\n7\n    There is nothing special about \u201c6\u201d to de\ufb01ne the boundary of the outlier range,\n    but a speci\ufb01c number was needed to make numerical comparisons. Clearly, |x| > 7\n    or |x| > 8, say, would have been just as appropriate as outlier ranges.\n\f98     5 Modeling Univariate Distributions\n\u221a\n  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Comparison of N (0, 3.4) distribution and heavy-tailed normal mixture",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "7\n    or |x| > 8, say, would have been just as appropriate as outlier ranges.\n\f98     5 Modeling Univariate Distributions\n\u221a",
        "start": 1194,
        "end": 1322
      }
    ]
  },
  {
    "content": "3.4 = 1.84 since 6/1.84 = 3.25 and three or more standard deviations from\nthe mean is generally considered rather outlying.\n    Outliers have a powerful e\ufb00ect on the variance and this small fraction of\noutliers in\ufb02ates the variance from ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "= 1.84 since 6/1.84 = 3.25 and three or more standard deviations from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 (the variance of 90 % of the population)\nto ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "(the variance of 90 % of the population)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4.\n    Let\u2019s see how much more probability the normal mixture distribution has\nin the outlier range |x| > 6 compared to the normal distribution. For an\nN (0, \u03c3 2 ) random variable Y ,\n                        P {|Y | > y} = 2{1 \u2212 \u03a6(y/\u03c3)}.\nTherefore, for the normal distribution with variance 3.4,\n                                           \u221a\n                  P {|Y | > 6} = 2{1 \u2212 \u03a6(6/ 3.4)} = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "Let\u2019s see how much more probability the normal mixture distribution has",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "6 compared to the normal distribution. For an\nN (0, \u03c3 2 ) random variable Y ,\n                        P {|Y | > y} = 2{1 \u2212 \u03a6(y/\u03c3)}.\nTherefore, for the normal distribution with variance 3.4,\n                                           \u221a\n                  P {|Y | > 6} = 2{1 \u2212 \u03a6(6/ 3.4)} =",
        "start": 106,
        "end": 395
      }
    ]
  },
  {
    "content": "0.0011.\nFor the normal mixture population that has variance 1 with probability 0.9\nand variance 25 with probability 0.1, we have that\n                             !                               \"\n             P {|Y | > 6} = 2 0.9{1 \u2212 \u03a6(6)} + 0.1{1 \u2212 \u03a6(6/5)}\n\n                          = 2{(0.9)(0) + (0.1)(0.115)} = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0011",
      "section_title": "For the normal mixture population that has variance 1 with probability 0.9",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "6} = 2 0.9{1 \u2212 \u03a6(6)} + 0.1{1 \u2212 \u03a6(6/5)}",
        "start": 218,
        "end": 260
      }
    ]
  },
  {
    "content": "0.023.\nSince 0.023/0.0011 \u2248 21, the normal mixture distribution is 21 times more\nlikely to be in this outlier range than the N (0, 3.4) population, even though\nboth have a variance of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.023",
      "section_title": "Since 0.023/0.0011 \u2248 21, the normal mixture distribution is 21 times more",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.4. In summary, the normal mixture is much more\nprone to outliers than a normal distribution with the same mean and standard\ndeviation. So, we should be much more concerned about very large negative\nreturns if the return distribution is more like the normal mixture distribution\nthan like a normal distribution. Large positive returns are also likely under\na normal mixture distribution and would be of concern if an asset was sold\nshort.\n    It is not di\ufb03cult to compute the kurtosis of this normal mixture. Because a\nnormal distribution has kurtosis equal to 3, if Z is N (\u03bc, \u03c3 2 ), then E(Z \u2212\u03bc)4 =\n3\u03c3 4 . Therefore, if Y has this normal mixture distribution, then\n                      E(Y 4 ) = 3{",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.4",
      "section_title": "In summary, the normal mixture is much more",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 + (0.1)252 } = 190.2\nand the kurtosis of X is 190.2/",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "+ (0.1)252 } = 190.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.42 = 16.45.\n    Normal probability plots of samples of size 200 from the normal and normal\nmixture distributions are shown in panels (c) and (d) of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.42",
      "section_title": "= 16.45.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5. Notice how\nthe outliers in the normal mixture sample give the probability plot a convex-\nconcave pattern typical of heavy-tailed data. The deviation of the plot of the\nnormal sample from linearity is small and is due entirely to randomness.\n    In this example, the conditional variance of any observation is 1 with\nprobability ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Notice how",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 and 25 with probability 0.1. Because there are only two com-\nponents, the conditional variance is discrete, in fact, with only two possible\nvalues, and the example was easy to analyze. This example is a normal scale\nmixture because only the scale parameter \u03c3 varies between components. It is\nalso a discrete mixture because there are only a \ufb01nite number of components.\n\f                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "and 25 with probability 0.1. Because there are only two com-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6 Generalized Error Distributions      99\n\nContinuous Mixtures\n\nThe marginal distributions of the GARCH processes studied in Chap. 14 are\nalso normal scale mixtures, but with in\ufb01nitely many components and a contin-\nuous distribution of the conditional variance. Although GARCH processes are\nmore complex than the simple mixture model in this section, the same theme\napplies\u2014a nonconstant conditional variance of a mixture distribution induces\nheavy-tailed marginal distributions even though the conditional distributions\nare normal distributions and have relatively light tails.\n    The general de\ufb01nition of a normal scale mixture is that it is the distribution\nof the random variable                 \u221a\n                                  \u03bc + UZ                                    (5.14)\nwhere \u03bc is a constant equal to the mean, Z is N (0, 1), U is a positive random\nvariable giving the variance of each component, and Z and U are independent.\nIf U can assume only a \ufb01nite number of values, then (5.14) is a discrete (or\n\ufb01nite) scale mixture distribution. If U is continuously distributed, then we\nhave a continuous scale mixture distribution. The distribution of U is called\nthe mixing distribution. By (5.11), a t\u03bd -distribution is a continuous normal\nscale mixture with \u03bc = 0 and U = \u03bd/W , where \u03bd and W are as de\ufb01ned above\nEq. (5.11).\n    Despite the apparent heavy tails of a \ufb01nite normal mixture, the tails\nare exponential, not polynomial. A continuous normal mixture can have a\npolynomial tail if the mixture distribution\u2019s tail is heavy enough, e.g., as in\nt-distributions.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "Generalized Error Distributions      99",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6 Generalized Error Distributions\n\nGeneralized error distributions mentioned brie\ufb02y in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "Generalized Error Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.1 have exponen-\ntial tails. This section provides more detailed information about them. The\nstandardized generalized error distribution, or GED, with shape parameter \u03bd\nhas density\n                                   \u000e     \u0015 \u0015\u03bd \u000f\n                                      1 \u0015\u0015 y \u0015\u0015\n              fged (y|\u03bd) = \u03ba(\u03bd) exp \u2212 \u0015 \u0015 , \u2212\u221e < y < \u221e,\n                std\n                                      2 \u03bb\u03bd\n\nwhere \u03ba(\u03bd) and \u03bb\u03bd are constants given by\n                \u000e \u22122/\u03bd           \u000f1/2\n                 2     \u0393 (\u03bd \u22121 )                         \u03bd\n           \u03bb\u03bd =                       and \u03ba(\u03bd) =\n                   \u0393 (3/\u03bd)                       \u03bb\u03bd 21+1/\u03bd \u0393 (\u03bd \u22121 )\n\nand chosen so that the function integrates to 1, as it must to be a density, and\nthe variance is 1. The latter property is not necessary but is often convenient.\n    The shape parameter \u03bd > 0 determines the tail weight, with smaller values\nof \u03bd giving greater tail weight. When \u03bd = 2, a GED is a normal distribution,\n\f100     5 Modeling Univariate Distributions\n\nand when \u03bd = 1, it is a double-exponential distribution. The generalized\nerror distributions can give tail weights intermediate between the normal and\ndouble-exponential distributions by having 1 < \u03bd < 2. They can also give\ntail weights more extreme than the double-exponential distribution by having\n\u03bd < 1.\n                         1e\u221204\n                         1e\u221208\n               density\n                         1e\u221212\n\n\n\n\n                                         ged, \u03bd=",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "1 have exponen-",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 determines the tail weight, with smaller values\nof \u03bd giving greater tail weight. When \u03bd = 2, a GED is a normal distribution,\n\f100     5 Modeling Univariate Distributions",
        "start": 845,
        "end": 1020
      }
    ]
  },
  {
    "content": "1.5\n                                         ged, \u03bd=1\n                         1e\u221216\n\n\n\n\n                                         ged, \u03bd=1/2\n                                         t, \u03bd=4\n                                         t, \u03bd=15\n                         1e\u221220\n\n\n\n\n                                 1   2          5     10   20   50   100\n                                                      x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "ged, \u03bd=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6. A comparison of the tails of several generalized error (solid curves) and\nt-distributions (dashed curves).\n\n\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "A comparison of the tails of several generalized error (solid curves) and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6 shows the right tails of several t- and generalized error den-\nsities with mean 0 and variance ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "shows the right tails of several t- and generalized error den-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.8 Since they are standardized, the argu-\nment y is the number of standard deviations from the median of 0. Because\nt-distributions have polynomial tails, any t-distribution is heavier-tailed than\nany generalized error distribution. However, this is only an asymptotic result\nas y \u2192 \u221e. In the more practical range of y, tail weight depends as much on\nthe tail weight parameter as it does on the choice between a t-distribution or\na generalized error distribution.\n    The t-distributions and generalized error densities also di\ufb00er in their\nshapes at the median. This can be seen in Fig. 5.7, where the generalized\nerror densities have sharp peaks at the median with the sharpness increasing\nas \u03bd decreases. In comparison, a t-density is smooth and rounded near the\nmedian, even with \u03bd small. If a sample is better \ufb01t by a t-distribution than\nby a generalized error distribution, this may be due more to the sharp central\npeaks of generalized error densities than to di\ufb00erences between the tails of the\ntwo types of distributions.\n8\n    This plot and Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.8",
      "section_title": "Since they are standardized, the argu-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7 used the R functions dged() and dstd() in the fGarch\n    package.\n\f                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "used the R functions dged() and dstd() in the fGarch",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7 Creating Skewed from Symmetric Distributions    101\n\n\n\n\n                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "Creating Skewed from Symmetric Distributions    101",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n                                        ged, \u03bd=1.5\n\n\n\n\n                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.0",
      "section_title": "ged, \u03bd=1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                        ged, \u03bd=1\n                                        ged, \u03bd=1/2\n              density                   t, \u03bd=4\n                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "ged, \u03bd=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                        0.5\n                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0             t, \u03bd=15\n\n\n\n\n                               \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "t, \u03bd=15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0       \u22120.5         0.0        0.5          1.0\n                                                        x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "\u22120.5         0.0        0.5          1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7. A comparison of the centers of several generalized error (solid) and t-\ndensities (dashed) with mean 0 and variance 1.\n\n\n          std\n    The fged  (y|\u03bd) density is symmetric about 0, which is its mean, median,\nand mode, and has a variance equal to 1. However, it can be shifted and\nrescaled to create a location-scale family. The GED distribution with mean\n\u03bc, variance \u03c3 2 , and shape parameter \u03bd has density\n                                std\n                              fged  (y|\u03bc, \u03c3 2 , \u03bd) := fged\n                                                        std\n                                                            {(y \u2212 \u03bc)/\u03c3|\u03bd}/\u03c3.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "A comparison of the centers of several generalized error (solid) and t-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7 Creating Skewed from Symmetric Distributions\n\nReturns and other \ufb01nancial markets data typically have no natural lower or\nupper bounds, so one would like to use models with support equal to (\u2212\u221e, \u221e).\nThis is \ufb01ne if the data are symmetric since then one can use, for example,\nnormal, t, or generalized error distributions as models. What if the data are\nskewed? Unfortunately, many of the well-known skewed distributions, such\nas gamma and log-normal distributions, have support [0, \u221e) and so are not\nsuitable for modeling the changes in many types of \ufb01nancial markets data.\nThis section describes a remedy to this problem.\n    Fernandez and Steel (1998) have devised a clever way for inducing skewness\nin symmetric distributions such as normal and t-distributions. The fGarch\npackage in R implements their idea. Let \u03be be a positive constant and f a\ndensity that is symmetric about 0. De\ufb01ne\n                                    \u000e\n                                      f (y\u03be) if y < 0,\n                        f \u2217 (y|\u03be) =                                      (5.15)\n                                      f (y/\u03be) if y \u2265 0.\n\nSince f \u2217 (y|\u03be) integrates to (\u03be \u22121 + \u03be)/2, f \u2217 (y|\u03be) is divided by this constant to\ncreate a probability density. After this normalization, the density is given a\n\f102     5 Modeling Univariate Distributions\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "Creating Skewed from Symmetric Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                               symmetric t\n                               skew t\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "symmetric t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n                         0.4\n               density\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n                         0.2\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                         0.0\n\n\n\n\n                               \u22124      \u22122    0         2        4\n                                             x\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.8. Symmetric (solid) and skewed (dashed) t-densities, both with mean 0,\nstandard deviation 1, and \u03bd = 10. \u03be = 2 in the skewed density. Notice that the mode\nof the skewed density lies to the left of its mean, a typical behavior of right-skewed\ndensities.\n\n\nlocation shift and scale change to induce a mean equal to 0 and variance of 1.\nThe \ufb01nal result is denoted by f (y|\u03be).\n    If \u03be > 1, then the right half of f (y|\u03be) is elongated relative to the left\nhalf, which induces right skewness. Similarly, \u03be < 1 induces left skewness.\nFigure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "Symmetric (solid) and skewed (dashed) t-densities, both with mean 0,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1, then the right half of f (y|\u03be) is elongated relative to the left\nhalf, which induces right skewness. Similarly, \u03be < 1 induces left skewness.\nFigure",
        "start": 385,
        "end": 538
      }
    ]
  },
  {
    "content": "5.8 shows standardized symmetric and skewed t-distributions9 with\n\u03bd = 10 in both cases and \u03be = 2 for the skewed distribution. Similarly, if \u03be < 1,\nthen f (y|\u03be) is left skewed.\n    If f is a t-distribution, then f (y|\u03be) is called a skewed t-distribution. Skewed\nt-distributions include symmetric t-distributions as special cases where \u03be = 1.\nIn the same way, skewed generalized error distributions are created when f is\na generalized error distribution. The skewed distributions just described will\nbe called Fernandez\u2013Steel or F-S skewed distributions.\n    Fernandez and Steel\u2019s technique is not the only method for creating skewed\nversions of the normal and t-distributions. Azzalini and Capitanio (2003) have\ncreated somewhat di\ufb00erent skewed normal and t-distributions.10 These distri-\nbutions have a shape parameter \u03b1 that determines the skewness; the distri-\nbution is left-skewed, symmetric, or right-skewed according to whether \u03b1 is\nnegative, zero, or positive. An example is given in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "shows standardized symmetric and skewed t-distributions9 with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 and multivariate\nversions are discussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "and multivariate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9. We will refer to these as Azzalini\u2013Capitanio\nor A-C skewed distributions.\n\n\n9\n   R\u2019s dstd() (for symmetric t) and dsstd() (for skewed t) functions in the fGarch\n   package were used for to create this plot.\n10\n   Programs for \ufb01tting these distributions, computing their densities, quantile, and\n   distribution functions, and generating random samples are available in R\u2019s sn\n   package.\n\f               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "We will refer to these as Azzalini\u2013Capitanio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.8 Quantile-Based Location, Scale, and Shape Parameters         103\n\n    The A-C skewed normal density is g(y|\u03be, \u03c9, \u03b1) = (2/\u03c9)\u03c6(z)\u03a6(\u03b1z) where\nz = (y \u2212 \u03be)/\u03c9 and \u03c6() and \u03a6() are the N (0, 1) density and CDF, respectively.\nThe parameters \u03be, \u03c9, and \u03b1 determine location, scale, and skewness and are\ncalled the direct parameters or DP. The parameters \u03be and \u03c9 are the mean and\nstandard deviation of \u03c6(z) and \u03b1 determines the amount of skewness induced\nby \u03a6(\u03b1z). The skewness of g(y|\u03be, \u03c9, \u03b1) is positive if \u03b1 > 0 and negative if\n\u03b1 < 0.\n    The direct parameters do not have simple interpretations for the skew\nnormal density g(y|\u03be, \u03c9, \u03b1). Therefore, the so-called centered parameters (CP)\nare de\ufb01ned to be the mean, standard deviation, and skewness of g(y|\u03be, \u03c9, \u03b1).\n    The A-C skew-t distribution has four parameters. The four DP are the\nmean, scale, and degrees of freedom of a t-density and \u03b1 which measures the\namount of skewness induced into that density. The CP are the mean, standard\ndeviation, skewness, and kurtosis of the skew t.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "Quantile-Based Location, Scale, and Shape Parameters         103",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and negative if\n\u03b1 < 0.\n    The direct parameters do not have simple interpretations for the skew\nnormal density g(y|\u03be, \u03c9, \u03b1). Therefore, the so-called centered parameters (CP)\nare de\ufb01ned to be the mean, standard deviation, and skewness of g(y|\u03be, \u03c9, \u03b1).\n    The A-C skew-t distribution has four parameters. The four DP are the\nmean, scale, and degrees of freedom of a t-density and \u03b1 which measures the\namount of skewness induced into that density. The CP are the mean, standard\ndeviation, skewness, and kurtosis of the skew t.",
        "start": 503,
        "end": 1035
      }
    ]
  },
  {
    "content": "5.8 Quantile-Based Location, Scale, and Shape\nParameters\nAs has been seen, the mean, standard deviation, skewness coe\ufb03cient, and\nkurtosis are moments-based location, scale, and shape parameters. Although\nthey are widely used, they have the drawbacks that they are sensitive to\noutliers and may be unde\ufb01ned or in\ufb01nite for distributions with heavy tails.\nAn alternative is to use parameters based on quantiles.\n    Any quantile F \u22121 (p), 0 < p < 1, is a location parameter. A positive\n                                             \u0017L           \u22121\nweighted average of quantiles, that is,          \u0002=1 w\u0002 F    (p\u0002 ), where w\u0002 > 0 for\n           \u0017L\nall \u0007 and        w\n              \u0002=1 \u0002  =  1, is  also  a location    parameter.   A simple example is\n{F \u22121 (1 \u2212 p) + F \u22121 (p)}/2 where 0 < p < 1/2, which equals the mean and\nmedian if F is symmetric.\n    A scale parameter can be obtained from the di\ufb00erence between two quan-\ntiles:\n                                       F \u22121 (p2 ) \u2212 F \u22121 (p1 )\n                        s(p1 , p2 ) =\n                                                  a\nwhere 0 < p1 < p2 < 1 and a is a positive constant. An obvious choice is\np1 < 1/2 and p2 = 1 \u2212 p1 . If a = \u03a6\u22121 (p2 ) \u2212 \u03a6\u22121 (p1 ), then s(p1 , p2 ) is equal\nto the standard deviation when F is a normal distribution. If a = 1, then\ns(1/4, 3/4) is called the interquartile range or IQR.\n    A quantile-based shape parameter that quanti\ufb01es skewness is a ratio with\nthe numerator the di\ufb00erence between two scale parameters and the denomi-\nnator a scale parameter:\n                             s(1/2, p2 ) \u2212 s(1/2, p1 )\n                                                       .                     (5.16)\n                                    s(p3 , p4 )\nwhere p1 < 1/2, p2 > 1/2, and 0 < p3 < p4 < 1. For example, one could use\np2 = 1 \u2212 p1 , p4 = p2 , and p3 = p1 .\n\f104     5 Modeling Univariate Distributions\n\n    A quantile-based shape parameter that quanti\ufb01es tail weight is the ratio\nof two scale parameters:\n                               s(p1 , 1 \u2212 p1 )\n                                               ,                      (5.17)\n                               s(p2 , 1 \u2212 p2 )\nwhere 0 < p1 < p2 < 1/2. For example, one might have p1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "Quantile-Based Location, Scale, and Shape",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 for\n           \u0017L\nall \u0007 and        w\n              \u0002=1 \u0002  =  1, is  also  a location    parameter.   A simple example is\n{F \u22121 (1 \u2212 p) + F \u22121 (p)}/2 where 0 < p < 1/2, which equals the mean and\nmedian if F is symmetric.\n    A scale parameter can be obtained from the di\ufb00erence between two quan-\ntiles:\n                                       F \u22121 (p2 ) \u2212 F \u22121 (p1 )\n                        s(p1 , p2 ) =\n                                                  a\nwhere 0 < p1 < p2 < 1 and a is a positive constant. An obvious choice is\np1 < 1/2 and p2 = 1 \u2212 p1 . If a = \u03a6\u22121 (p2 ) \u2212 \u03a6\u22121 (p1 ), then s(p1 , p2 ) is equal\nto the standard deviation when F is a normal distribution. If a = 1, then\ns(1/4, 3/4) is called the interquartile range or IQR.\n    A quantile-based shape parameter that quanti\ufb01es skewness is a ratio with\nthe numerator the di\ufb00erence between two scale parameters and the denomi-\nnator a scale parameter:\n                             s(1/2, p2 ) \u2212 s(1/2, p1 )\n                                                       .                     (5.16)\n                                    s(p3 , p4 )\nwhere p1 < 1/2, p2 > 1/2, and 0 < p3 < p4 < 1. For example, one could use\np2 = 1 \u2212 p1 , p4 = p2 , and p3 = p1 .\n\f104     5 Modeling Univariate Distributions",
        "start": 621,
        "end": 1884
      }
    ]
  },
  {
    "content": "0.01 or 0.05 and\np2 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.01",
      "section_title": "or 0.05 and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25.\n\n\n5.9 Maximum Likelihood Estimation\n\nMaximum likelihood is the most important and widespread method of esti-\nmation. Many well-known estimators such as the sample mean, and the least-\nsquares estimator in regression are maximum likelihood estimators if the data\nhave a normal distribution. Maximum likelihood estimation generally provides\nmore e\ufb03cient (less variable) estimators than other techniques of estimation.\nAs an example, for a t-distribution, the maximum likelihood estimator of the\nmean is more e\ufb03cient than the sample mean.\n    Let Y = (Y1 , . . . , Yn )T be a vector of data and let \u03b8 = (\u03b81 , . . . , \u03b8p )T be a\nvector of parameters. Let f (Y |\u03b8) be the density of Y , which depends on the\nparameters.\n    The function L(\u03b8) = f (Y |\u03b8) viewed as a function of \u03b8 with Y \ufb01xed at the\nobserved data is called the likelihood function. It tells us the likelihood of the\nsample that was actually observed. The maximum likelihood estimator (MLE)\nis the value of \u03b8 that maximizes the likelihood function. In other words, the\nMLE is the value of \u03b8 at which the likelihood of the observed data is largest.\nWe denote the MLE by \u03b8        \u0002ML . Often it is mathematically easier to maximize\nlog{L(\u03b8)}, which is called the log-likelihood. If the data are independent,\nthen the likelihood is the product of the marginal densities and products\nare cumbersome to di\ufb00erentiate. Taking the logarithm converts the product\ninto an easily di\ufb00erentiated sum. Also, in numerical computations, using the\nlog-likelihood reduces the possibility of under\ufb02ow or over\ufb02ow. Since the log\nfunction is increasing, maximizing log{L(\u03b8)} is equivalent to maximizing L(\u03b8).\n    In examples found in introductory statistics textbooks, it is possible to\n\ufb01nd an explicit formula for the MLE. With more complex models such as\nthe ones we will mostly be using, there is no explicit formula for the MLE.\nInstead, one must write a program that computes log{L(\u03b8)} for any \u03b8 and\nthen use optimization software to maximize this function numerically; see\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "5.9 Maximum Likelihood Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3. The R functions optim() and nlminb() minimize functions and\ncan be applied to \u2212L(\u03b8).\n    For many important models, such as the examples in the Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "The R functions optim() and nlminb() minimize functions and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 and the\nARIMA and GARCH time series models discussed in Chap. 12, R and other\nsoftware packages contain functions to \ufb01nd the MLE for these models.\n\f     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10 Fisher Information and the Central Limit Theorem for the MLE      105\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.10",
      "section_title": "Fisher Information and the Central Limit Theorem for the MLE      105",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10 Fisher Information and the Central Limit Theorem\nfor the MLE\nStandard errors are essential for measuring the accuracy of estimators. We\nhave formulas for the standard errors of simple estimators such as Y , but\nwhat about standard errors for other estimators? Fortunately, there is a simple\nmethod for calculating the standard error of a maximum likelihood estimator.\n   We assume for now that \u03b8 is one-dimensional. The Fisher information is\nde\ufb01ned to be minus the expected second derivative of the log-likelihood, so if\nI(\u03b8) denotes the Fisher information, then\n                                   ! 2             \"\n                                      d\n                        I(\u03b8) = \u2212E         log{L(\u03b8)} .                     (5.18)\n                                     d \u03b82\n\nThe standard error of \u03b8\u0002 is simply the inverse square root of the Fisher infor-\nmation, with the unknown \u03b8 replaced by \u03b8: \u0002\n\n                                           1\n                                 s\u03b8\u0002 = #          .                      (5.19)\n                                             \u0002\n                                           I(\u03b8)\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.10",
      "section_title": "Fisher Information and the Central Limit Theorem",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1. Fisher information for a normal model mean\n\n    Suppose that Y1 , . . . , Yn are i.i.d. N (\u03bc, \u03c3 2 ) with \u03c3 2 known. The log-\nlikelihood for the unknown parameter \u03bc is\n                                                        n\n                       n                        1\n          log{L(\u03bc)} = \u2212 {log(\u03c3 2 ) + log(2\u03c0)} \u2212 2     (Yi \u2212 \u03bc)2 .\n                       2                       2\u03c3 i=1\n\nTherefore,\n                                               n\n                        d             1\n                          log{L(\u03bc)} = 2    (Yi \u2212 \u03bc),\n                       d\u03bc            \u03c3 i=1\n\nso that Y is the MLE of \u03bc and\n                                           \u0017n\n                      d2                    i=1 1      n\n                         log{L(\u03bc)} = \u2212            = \u2212 2.\n                     d\u03bc2                    \u03c32        \u03c3\n                                           \u221a\nIt follows that I(\u0002\u03bc) = n/\u03c3 2 and s\u03bc\u0002 = \u03c3/ n. Since the MLE of  \u221a \u03bc is Y , this\nresult is the familiar fact that\n                              \u221a  when \u03c3 is known,  then s Y = \u03c3/ n and when\n\u03c3 is unknown, then sY = s/ n.                                                \u0002\n\n\n   The theory justifying using these standard errors is the central limit the-\norem for the maximum likelihood estimator. This theorem can be stated\n\f106       5 Modeling Univariate Distributions\n\nin a mathematically precise manner that is di\ufb03cult to understand without\nadvanced probability theory. The following less precise statement is more\neasily understood:\n\nResult ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.1",
      "section_title": "Fisher information for a normal model mean",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1 Under suitable assumptions, for large enough sample sizes, the\nmaximum likelihood estimator is approximately normally distributed with\nmean equal to the true parameter and with variance equal to the inverse of\nthe Fisher information.\n\n\n   The central limit theorem for the maximum likelihood estimator justi\ufb01es\nthe following large-sample con\ufb01dence interval for the MLE of \u03b8:\n\n                                       \u03b8\u0002 \u00b1 s\u03b8\u0002 z\u03b1/2 ,                              (5.20)\n\nwhere z\u03b1/2 is the \u03b1/2-upper quantile of the normal distribution and s\u03b8\u0002 is\nde\ufb01ned in (5.19).\n   The observed Fisher information is\n                                               d2\n                              I obs (\u03b8) = \u2212        log{L(\u03b8)},                       (5.21)\n                                              d \u03b82\nwhich di\ufb00ers from (5.18) in that there is no expectation taken. In many ex-\namples, (5.21) is a sum of many independent terms and, by the law of large\nnumbers, will be close to (5.18). The expectation in (5.18) may be di\ufb03cult to\ncompute and using (5.21) instead is a convenient alternative.\n   The standard error of \u03b8\u0002 based on observed Fisher information is\n                                                  1\n                                    sobs\n                                     \u03b8\u0002\n                                         =#                 .                       (5.22)\n                                                       \u0002\n                                                I obs (\u03b8)\n\nOften sobs\n        \u03b8\u0002\n             is used in place of s\u03b8\u0002 in the con\ufb01dence interval (5.20). There is\ntheory suggesting that using the observed Fisher information will result in a\nmore accurate con\ufb01dence interval, that is, an interval with the true coverage\nprobability closer to the nominal value of 1\u2212\u03b1, so observed Fisher information\ncan be justi\ufb01ed by more than mere convenience; see Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.1",
      "section_title": "Under suitable assumptions, for large enough sample sizes, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.18.\n    So far, it has been assumed that \u03b8 is one-dimensional. In the multivari-\nate case, the second derivative in (5.18) is replaced by the Hessian matrix\nof second derivatives,11 and the result is called the Fisher information ma-\ntrix. Analogously, the observed Fisher information matrix is the multivariate\nanalog of (5.21). The covariance matrix of the MLE can be estimated by the\ninverse of the observed Fisher information matrix. If the negative of the log-\nlikelihood is minimized by the R function optim(), then the observed Fisher\ninformation matrix is computed numerically and returned if hessian = TRUE\n11\n     The Hessian matrix of a function f (x1 , . . . , xm ) of m variables is the m\u00d7m matrix\n     whose i, jth entry is the second partial derivative of f with respect to xi and xj .\n\f                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.18",
      "section_title": "So far, it has been assumed that \u03b8 is one-dimensional. In the multivari-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.11 Likelihood Ratio Tests     107\n\nin the call to this function. See Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "Likelihood Ratio Tests     107",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3 for an example where standard\nerrors of the MLEs are computed numerically. Fisher information matrices\nare discussed in more detail in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "for an example where standard",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.10.\n\nBias and Standard Deviation of the MLE\n\nIn many examples, the MLE has a small bias that decreases to 0 at rate n\u22121\nas the sample size n increases to \u221e. More precisely,\n                                             A\n                 BIAS(\u03b8\u0002ML ) = E(\u03b8\u0002ML ) \u2212 \u03b8 \u223c , as n \u2192 \u221e,                    (5.23)\n                                             n\nfor some constant A. The bias of the MLE of a normal variance is an example\nand A = \u2212\u03c3 2 in this case.\n    Although this bias can be corrected in some special problems, such as\nestimation of a normal variance, usually the bias is ignored. There are two\ngood reasons for this. First, the log-likelihood usually is the sum of n terms\nand so grows at rate n. The same is true of the Fisher information. Therefore,\nthe variance of the MLE decreases at rate n\u22121 , that is,\n                                         B\n                          Var(\u03b8\u0002ML ) \u223c     , as n \u2192 \u221e,                       (5.24)\n                                         n\nfor some B > 0. Variability should be measured by the standard deviation,\nnot the variance, and by (5.24),\n                                   \u221a\n                            \u0002       B\n                        SD(\u03b8ML ) \u223c \u221a , as n \u2192 \u221e.                   (5.25)\n                                     n\n\nThe convergence rate in (5.25) can also be obtained from the CLT for the\nMLE. Comparing (5.23) and (5.25), one sees that as n gets larger, the bias\nof the MLE becomes negligible compared to the standard deviation. This is\nespecially important with \ufb01nancial markets data, where sample sizes tend to\nbe large.\n    Second, even if the MLE of a parameter \u03b8 is unbiased, the same is not true\nfor a nonlinear function of \u03b8. For example, even if \u03c3                          \u0002 is\n                                                      \u00022 is unbiased for \u03c3 2 , \u03c3\nbiased for \u03c3. The reason for this is that for a nonlinear function g, in general,\n                                  \u0002 = g{E(\u03b8)}.\n                              E{g(\u03b8)}     \u0002\n\nTherefore, it is impossible to correct for all biases.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.10",
      "section_title": "Bias and Standard Deviation of the MLE",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0. Variability should be measured by the standard deviation,\nnot the variance, and by (5.24),\n                                   \u221a\n                            \u0002       B\n                        SD(\u03b8ML ) \u223c \u221a , as n \u2192 \u221e.                   (5.25)\n                                     n",
        "start": 1002,
        "end": 1287
      }
    ]
  },
  {
    "content": "5.11 Likelihood Ratio Tests\nSome readers may wish to review hypothesis testing by reading Appendix A.18\nbefore starting this section.\n\f108       5 Modeling Univariate Distributions\n\n    Likelihood ratio tests, like maximum likelihood estimation, are based upon\nthe likelihood function. Both are convenient, all-purpose tools that are widely\nused in practice.\n    Suppose that \u03b8 is a parameter vector and that the null hypothesis puts\nm equality constraints on \u03b8. More precisely, there are m functions g1 , . . . , gm\nand the null hypothesis is that gi (\u03b8) = 0 for i = 1, . . . , m. The models without\nand with the constraints are called the full and reduced models, respectively.\n    It is also assumed that none of these constraints is redundant, that is,\nimplied by the others. To illustrate redundancy, suppose that \u03b8 = (\u03b81 , \u03b82 , \u03b83 )\nand the constraints are \u03b81 = \u03b82 , \u03b82 = \u03b83 , and \u03b81 = \u03b83 . Then the constraints\nhave a redundancy since any two of them imply the third. Thus, m = 2, not 3.\n    Of course, redundancies need not be so easy to detect. One way to check\nis that the m \u00d7 dim(\u03b8) matrix\n                                   \u239b          \u239e\n                                     \u2207g1 (\u03b8)\n                                   \u239c    ..    \u239f\n                                   \u239d     .    \u23a0                              (5.26)\n                                       \u2207gm (\u03b8)\n\nmust have rank m. Here \u2207gi (\u03b8) is the gradient of gi .\n    As an example, one might want to test that a population mean is zero;\nthen \u03b8 = (\u03bc, \u03c3)T and m = 1 since the null hypothesis puts one constraint on\n\u03b8, speci\ufb01cally that \u03bc = 0.\n        \u0002ML be the maximum likelihood estimator without restrictions and\n    Let \u03b8\n    \u00020,ML be the value of \u03b8 that maximizes L(\u03b8) subject to the restrictions of\nlet \u03b8\nthe null hypothesis. If H0 is true, then \u03b8 \u00020,ML and \u03b8 \u0002ML should both be close\nto \u03b8 and therefore L(\u03b8\u00020,ML ) should be similar to L(\u03b8).\n                                                       \u0002 If H0 is false, then the\n                      \u0002               \u0002              \u0002\nconstraints will keep \u03b8 0,ML far from \u03b8 ML and so L(\u03b8 0,ML ) should be noticeably\n                 \u0002\nsmaller that L(\u03b8).\n    The likelihood ratio test rejects H0 if\n                     \u001a                                 \u001b\n                    2 log{L(\u03b8  \u0002ML )} \u2212 log{L(\u03b8\u00020,ML )} \u2265 c,               (5.27)\n\nwhere c is a critical value. The left-hand side of (5.27) is twice the log of\nthe likelihood ratio L(\u03b8\u0002ML )/L(\u03b8\u00020,ML ), hence the name likelihood ratio test.\nOften, an exact critical value can be found. A critical value is exact if it gives\na level that is exactly equal to \u03b1. When an exact critical value is unknown,\nthen the usual choice of the critical value is\n\n                                      c = \u03c72\u03b1,m ,                             (5.28)\n\nwhere, as de\ufb01ned in Appendix A.10.1, \u03c72\u03b1,m is the \u03b1-upper quantile value of the\nchi-squared distribution with m degrees of freedom.12 The critical value (5.28)\n12\n     The reader should now appreciate why it is essential to calculate m correctly by\n     eliminating redundant constraints. The wrong value of m will cause an incorrect\n     critical value to be used.\n\f                                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "Likelihood Ratio Tests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12 AIC and BIC       109\n\nis only approximate and uses the fact that under the null hypothesis, as the\nsample size increases the distribution of twice the log-likelihood ratio con-\nverges to the chi-squared distribution with m degrees of freedom if certain\nassumptions hold. One of these assumptions is that the null hypothesis is not\non the boundary of the parameter space. For example, if the null hypothesis is\nthat a variance parameter is zero, then the null hypothesis is on the boundary\nof the parameter space since a variance must be zero or greater. In this case\n(5.27) should not be used; see Self and Liang (1987). Also, if the sample size\nis small, then the large-sample approximation (5.27) is suspect and should be\nused with caution. An alternative is to use the bootstrap to determine the\nrejection region. The bootstrap is discussed in Chap. 6.\n    Computation of likelihood ratio tests is often very simple. In some cases,\nthe test is computed automatically by statistical software. In other cases,\nsoftware will compute the log-likelihood for each model (full and reduced)\nand these can be plugged into the left-hand side of (5.27).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.12",
      "section_title": "AIC and BIC       109",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12 AIC and BIC\n\nAn important practical problem is choosing between two or more statistical\nmodels that might be appropriate for a data set. The maximized value of the\nlog-likelihood, denoted here by log{L(\u03b8   \u0002ML )}, can be used to measure how\nwell a model \ufb01ts the data or to compare the \ufb01ts of two or more models.\nHowever, log{L(\u03b8  \u0002ML )} can be increased simply by adding parameters to the\nmodel. The additional parameters do not necessarily mean that the model is a\nbetter description of the data-generating mechanism, because the additional\nmodel complexity due to added parameters may simply be \ufb01tting random\nnoise in the data, a problem that is called over\ufb01tting. Therefore, models should\nbe compared both by \ufb01t to the data and by model complexity. To \ufb01nd a\nparsimonious model one needs a good tradeo\ufb00 between maximizing \ufb01t and\nminimizing model complexity.\n    AIC (Akaike\u2019s information criterion) and BIC (Bayesian information crite-\nrion) are two means for achieving a good tradeo\ufb00 between \ufb01t and complexity.\nThey di\ufb00er slightly and BIC seeks a somewhat simpler model than AIC. They\nare de\ufb01ned by\n                                     \u0002ML )} + 2p\n                      AIC = \u22122 log{L(\u03b8                                    (5.29)\n                                     \u0002ML )} + log(n)p,\n                      BIC = \u22122 log{L(\u03b8                                    (5.30)\n\nwhere p equals the number of parameters in the model and n is the sample\nsize. For both criteria, \u201csmaller is better,\u201d since small values tend to maximize\nL(\u03b8\u0002ML ) (minimize \u2212 log{L(\u03b8  \u0002ML )}) and minimize p, which measures model\ncomplexity. The terms 2p and log(n)p are called \u201ccomplexity penalties\u201d since\nthe penalize larger models.\n\f110    5 Modeling Univariate Distributions\n\n     The term deviance is often used for minus twice the log-likelihood, so\nAIC = deviance + 2p and BIC = deviance + log(n)p. Deviance quanti\ufb01es\nmodel \ufb01t, with smaller values implying better \ufb01t.\n     Generally, from a group of candidate models, one selects the model that\nminimizes whichever criterion, AIC or BIC, is being used. However, any model\nthat is within 2 or 3 of the minimum value is a good candidate and might be\nselected instead, for example, because it is simpler or more convenient to use\nthan the model achieving the absolute minimum. Since log(n) > 2 provided,\nas is typical, that n > 8, BIC penalizes model complexity more than AIC does,\nand for this reason BIC tends to select simpler models than AIC. However,\nit is common for both criteria to select the same, or nearly the same, model.\nOf course, if several candidate models all have the same value of p, then AIC,\nBIC, and \u22122 log{L(\u03b8   \u0002ML )} are minimized by the same model.\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.12",
      "section_title": "AIC and BIC",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "2 provided,\nas is typical, that n > 8, BIC penalizes model complexity more than AIC does,\nand for this reason BIC tends to select simpler models than AIC. However,\nit is common for both criteria to select the same, or nearly the same, model.\nOf course, if several candidate models all have the same value of p, then AIC,\nBIC, and \u22122 log{L(\u03b8   \u0002ML )} are minimized by the same model.",
        "start": 2318,
        "end": 2704
      }
    ]
  },
  {
    "content": "5.13 Validation Data and Cross-Validation\n\nWhen the same data are used both to estimate parameters and to assess \ufb01t,\nthere is a strong tendency towards over\ufb01tting. Data contain both a signal and\nnoise. The signal contains characteristics that are present in the population\nand therefore in each sample from the population, but the noise is random\nand varies from sample to sample. Over\ufb01tting means selecting an unneces-\nsarily complex model to \ufb01t the noise. The obvious remedy to over\ufb01tting is\nto diagnose model \ufb01t using data that are independent of the data used for\nparameter estimation. We will call the data used for estimation the training\ndata and the data used to assess \ufb01t the validation data or test data.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.13",
      "section_title": "Validation Data and Cross-Validation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2. Estimating the expected returns of midcap stocks\n\n    This example uses 500 daily returns on 20 midcap stocks in the \ufb01le\nmidcapD.ts.csv on the book\u2019s web site. The data were originally in the\nmidcapD.ts data set in R\u2019s fEcofin package. The data are from 28-Feb-91\nto 29-Dec-95. Suppose we need to estimate the 20 expected returns. Consider\ntwo estimators. The \ufb01rst, called \u201cseparate-means,\u201d is simply the 20 sample\nmeans. The second, \u201ccommon-mean,\u201d uses the average of the 20 sample means\nas the common estimator of all 20 expected returns.\n    The rationale behind the common-mean estimator is that midcap stocks\nshould have similar expected returns. The common-mean estimator pools data\nand greatly reduces the variance of the estimator. The common-mean estima-\ntor has some bias because the true expected returns will not be identical,\nwhich is the requirement for unbiasedness of the common-mean estimator.\nThe separate-means estimator is unbiased but at the expense of a higher vari-\nance. This is a classic example of a bias\u2013variance tradeo\ufb00.\n\f                                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "Estimating the expected returns of midcap stocks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.13 Validation Data and Cross-Validation   111\n\n   Which estimator achieves the best tradeo\ufb00? To address this question, the\ndata were divided into the returns for the \ufb01rst 250 days (training data) and for\nthe last 250 days (validation data). The criterion for assessing goodness-of-\ufb01t\nwas the sum of squared errors, which is\n                             20    &                     '\n                                                      val 2\n                                       \u0002ktrain \u2212 Y k\n                                       \u03bc                      ,\n                             k=1\n\n      \u0002ktrain is the estimator (using the training data) of the kth expected\nwhere \u03bc\n                 val\nreturn and Y k is the validation data sample mean of the returns on the kth\nstock. The sum of squared errors are ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.13",
      "section_title": "Validation Data and Cross-Validation   111",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.262 and 0.898, respectively, for the\nseparate-means and common-mean estimators. The conclusion, of course, is\nthat in this example the common-mean estimator is much more accurate than\nusing separate means.\n    Suppose we had used the training data also for validation? The goodness-\nof-\ufb01t criterion would have been\n                             20   &                    '\n                                                  train 2\n                                      \u0002ktrain \u2212 Y k\n                                      \u03bc                           ,\n                            k=1\n\n         train\nwhere Y k     is the training data sample mean for the kth stock and is also\nthe separate-means estimator for that stock. What would the results have\nbeen? Trivially, the sum of squared errors for the separate-means estimator\nwould have been 0\u2014each mean is estimated by itself with perfect accuracy!\nThe common-mean estimator has a sum of squared errors equal to ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.262",
      "section_title": "and 0.898, respectively, for the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.920. The\ninappropriate use of the training data for validation would have led to the\nerroneous conclusion that the separate-means estimator is more accurate.\n    There are compromises between the two extremes of a common mean\nand separate means. These compromise estimators shrink the separate means\ntoward the common mean. Bayesian estimation, discussed in Chap. 20, is an\ne\ufb00ective method for selecting the amount of shrinkage; see Example 20.12,\nwhere this set of returns is analyzed further.                            \u0002\n\n    A common criterion for judging \ufb01t is the deviance, which is \u22122 times the\nlog-likelihood. The deviance of the validation data is\n                                    &              '\n                                            \u0002 train ,\n                           \u2212 2 log f Y val |\u03b8                         (5.31)\n\n        train\nwhere \u03b8 \u0002    is the MLE of the training data, Y val is the validation data, and\nf (y |\u03b8) is the density of the validation data.\n    val\n\n    When the sample size is small, splitting the data once into training and\nvalidation data is wasteful. A better technique is cross-validation, often called\nsimply CV, where each observation gets to play both roles, training and vali-\ndation. K-fold cross-validation divides the data set into K subsets of roughly\n\f112    5 Modeling Univariate Distributions\n\nequal size. Validation is done K times. In the kth validation, k = 1, . . . , K,\nthe kth subset is the validation data and the other K \u22121 subsets are combined\nto form the training data. The K estimates of goodness-of-\ufb01t are combined,\nfor example, by averaging them. A common choice is n-fold cross-validation,\nalso called leave-one-out cross-validation. With leave-one-out cross-validation,\neach observation takes a turn at being the validation data set, with the other\nn \u2212 1 observations as the training data.\n    An alternative to actually using validation data is to calculate what would\nhappen if new data could be obtained and used for validation. This is how\nAIC was derived. AIC is an approximation to the expected deviance of a hy-\npothetical new sample that is independent of the actual data. More precisely,\nAIC approximates\n                          \u001a         (       \u0015        )\u001b\n                        E \u22122 log f Y new \u0015 \u03b8(Y\u0002 obs ) ,                   (5.32)\n\n                                   \u0002 obs ) is the MLE computed from Y obs ,\nwhere Y obs is the observed data, \u03b8(Y\n       new\nand Y      is a hypothetical new data set such that Y obs and Y new are i.i.d.\nStated di\ufb00erently, Y new is an unobserved independent replicate of Y obs . Since\nY new is not observed but has the same distribution as Y obs , to obtain AIC\none substitutes Y obs for Y new in (5.32) and omits the expectation in (5.32).\nThen one calculates the e\ufb00ect of this substitution. The approximate e\ufb00ect is to\nreduce (5.32) by twice the number of parameters. Therefore, AIC compensates\nby adding 2p to the deviance, so that\n                                   (       \u0015         )\n                   AIC = \u22122 log f Y obs \u0015 \u03b8(Y\u0002 obs ) + 2p,                (5.33)\n\nwhich is a reexpression of (5.29).\n    The approximation used in AIC becomes more accurate when the sample\nsize increases. A small-sample correction to AIC is\n\n                                          2p(p + 1)\n                          AICc = AIC +              .                    (5.34)\n                                          n\u2212p\u22121\nFinancial markets data sets are often large enough that the correction term\n2p(p + 1)/(n\u2212p\u22121) is small, so that AIC is adequate and AICc is not needed.\nFor example, if n = 200, then 2p(p + 1)/(n \u2212 p \u2212 1) is 0.12, 0.21, 0.31, and\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.920",
      "section_title": "The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.44 and for p = 3, 4, 5, and 6, respectively. Since a di\ufb00erence less than 1 in\nAIC values is usually considered inconsequential, the correction would have\nlittle e\ufb00ect when comparing models with 3 to 6 parameters when n is at least\n200. Even more dramatically, when n is 500, then the corrections for 3, 4, 5,\nand 6 parameters are only 0.05, 0.08, 0.12, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.44",
      "section_title": "and for p = 3, 4, 5, and 6, respectively. Since a di\ufb00erence less than 1 in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.17.\n     Traders often develop trading strategies using a set of historical data and\nthen test the strategies on new data. This is called back-testing and is a form\nof validation.\n\f                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.17",
      "section_title": "Traders often develop trading strategies using a set of historical data and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 Fitting Distributions by Maximum Likelihood        113\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "Fitting Distributions by Maximum Likelihood        113",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 Fitting Distributions by Maximum Likelihood\nAs mentioned previously, one can \ufb01nd a formula for the MLE only for a few\n\u201ctextbook\u201d examples. In most cases, the MLE must be found numerically. As\nan example, suppose that Y1 , . . . , Yn is an i.i.d. sample from a t-distribution.\nLet\n                                      std\n                                   ft,\u03bd   (y | \u03bc, \u03c3)                         (5.35)\nbe the density of the standardized t-distribution with \u03bd degrees of freedom\nand with mean \u03bc and standard deviation \u03c3. Then the parameters \u03bd, \u03bc, and \u03c3\nare estimated by maximizing\n                              n       (                 )\n                                        std\n                                   log ft,\u03bd (Yi | \u03bc, \u03c3)                     (5.36)\n                             i=1\n\nusing any convenient optimization software. Estimation of other models is\nsimilar.\n    In the following examples, t-distributions and generalized error distribu-\ntions are \ufb01t.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "Fitting Distributions by Maximum Likelihood",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3. Fitting a t-distribution to changes in risk-free returns\n\n    This example uses one of the time series in Chap. 4, the changes in the\nrisk-free returns that has been called diffrf. This time series will be used\nto illustrate several methods for \ufb01tting a t-distribution. The simplest method\nuses the R function fitdistr().\n\n   data(Capm, package = \"Ecdat\")\n   x = diff(Capm$rf)\n   fitdistr(x,\"t\")\n\nThe output is:\n\n   > fitdistr(x,\"t\")\n          m           s          df\n     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.3",
      "section_title": "Fitting a t-distribution to changes in risk-free returns",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "fitdistr(x,\"t\")\n          m           s          df",
        "start": 421,
        "end": 480
      }
    ]
  },
  {
    "content": "0.0012243   0.0458549   3.3367036\n    (0.0024539) (0.0024580) (0.5000096)\n\nThe parameters, in order, are the mean, the scale parameter, and the degrees\nof freedom. The numbers in parentheses are the standard errors.\n    Next, we \ufb01t the t-distribution by writing a function to return the negative\nlog-likelihood and using R\u2019s optim() function to minimize the log-likelihood.\nWe compute standard errors by using solve() to invert the Hessian and then\ntaking the square roots of the diagonal elements of the inverted Hessian. We\nalso compute AIC and BIC.\n\f114      5 Modeling Univariate Distributions\n\n      library(fGarch)\n      n = length(x)\n      start = c(mean(x), sd(x), 5)\n      loglik_t = function(beta) sum( - dt((x - beta[1]) / beta[2],\n         beta[3], log = TRUE) + log(beta[2]) )\n      fit_t = optim(start, loglik_t, hessian = T,\n         method = \"L-BFGS-B\", lower = c(-1, 0.001, 1))\n      AIC_t = 2 * fit_t$value + 2 * 3\n      BIC_t = 2 * fit_t$value + log(n) * 3\n      sd_t = sqrt(diag(solve(fit_t$hessian)))\n      fit_t$par\n      sd_t\n      AIC_t\n      BIC_t\n\nThe results are below. The estimates and the standard errors agree with those\nproduced by fitdistr(), except for small numerical errors.\n\n      > fit_t$par\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0012243",
      "section_title": "0.0458549   3.3367036",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "fit_t$par\n      [1]",
        "start": 1218,
        "end": 1240
      }
    ]
  },
  {
    "content": "0.00122 0.04586 3.33655\n      > sd_t\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00122",
      "section_title": "0.04586 3.33655",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd_t\n      [1]",
        "start": 30,
        "end": 47
      }
    ]
  },
  {
    "content": "0.00245 0.00246 0.49982\n      > AIC_t\n      [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00245",
      "section_title": "0.00246 0.49982",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "AIC_t\n      [1] -",
        "start": 30,
        "end": 49
      }
    ]
  },
  {
    "content": "1380.4\n      > BIC_t\n      [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1380.4",
      "section_title": "> BIC_t",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "BIC_t\n      [1] -",
        "start": 13,
        "end": 32
      }
    ]
  },
  {
    "content": "1367.6\n\nThe standardized t-distribution can be \ufb01t by changing dt() to dstd(). Then\nthe parameters are the mean, standard deviation, and degrees of freedom.\n\n      loglik_std = function(beta) sum(- dstd(x, mean = beta[1],\n         sd = beta[2], nu = beta[3], log = TRUE))\n      fit_std = optim(start, loglik_std, hessian = T,\n         method = \"L-BFGS-B\", lower = c(-0.1, 0.01, 2.1))\n      AIC_std = 2*fit_std$value + 2 * 3\n      BIC_std = 2*fit_std$value + log(n) * 3\n      sd_std = sqrt(diag(solve(fit_std$hessian)))\n      fit_std$par\n      sd_std\n      AIC_std\n      BIC_std\n\nThe results are below.\n                 \u0011     The estimates agree with those when using dt() since\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1367.6",
      "section_title": "The standardized t-distribution can be \ufb01t by changing dt() to dstd(). Then",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0725 = 0.0459 3.33/(3.33 \u2212 2), aside from numerical error. Notice that\nAIC and BIC are unchanged, as expected since we are \ufb01tting the same model\nas before and only changing the parameterization.\n\f                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0725",
      "section_title": "= 0.0459 3.33/(3.33 \u2212 2), aside from numerical error. Notice that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 Fitting Distributions by Maximum Likelihood      115\n\n   > fit_std$par\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "Fitting Distributions by Maximum Likelihood      115",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "fit_std$par\n   [1]",
        "start": 62,
        "end": 83
      }
    ]
  },
  {
    "content": "0.0012144 0.0725088 3.3316132\n   > sd_std\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0012144",
      "section_title": "0.0725088 3.3316132",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd_std\n   [1]",
        "start": 33,
        "end": 49
      }
    ]
  },
  {
    "content": "0.0024538 0.0065504 0.4986456\n   > AIC_std\n   [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0024538",
      "section_title": "0.0065504 0.4986456",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "AIC_std\n   [1] -",
        "start": 33,
        "end": 51
      }
    ]
  },
  {
    "content": "1380.4\n   > BIC_std\n   [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1380.4",
      "section_title": "> BIC_std",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "BIC_std\n   [1] -",
        "start": 10,
        "end": 28
      }
    ]
  },
  {
    "content": "1367.6\n\n                                                                              \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1367.6",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4. Fitting an F-S skewed t-distribution to changes in risk-free re-\nturns\n\n   Next, we \ufb01t the F-S skewed t-distribution.\n\n   loglik_sstd = function(beta) sum(- dsstd(x, mean = beta[1],\n      sd = beta[2], nu = beta[3], xi = beta[4], log = TRUE))\n   start = c(mean(x), sd(x), 5, 1)\n   fit_sstd = optim(start, loglik_sstd, hessian = T,\n      method = \"L-BFGS-B\", lower = c(-0.1, 0.01, 2.1, -2))\n   AIC_sstd = 2*fit_sstd$value + 2 * 4\n   BIC_sstd = 2*fit_sstd$value + log(n) * 4\n   sd_sstd = sqrt(diag(solve(fit_sstd$hessian)))\n   fit_sstd$par\n   sd_sstd\n   AIC_sstd\n   BIC_sstd\n\nThe results are below. The estimate of \u03be (the fourth parameter) is very close to\n1, which corresponds to the usual t-distribution. Both AIC and BIC increase\nsince the extra skewness parameter does not improve the \ufb01t but adds 1 to the\nnumber of parameters.\n\n   > fit_sstd$par\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.4",
      "section_title": "Fitting an F-S skewed t-distribution to changes in risk-free re-",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "fit_sstd$par\n   [1]",
        "start": 839,
        "end": 861
      }
    ]
  },
  {
    "content": "0.0011811 0.0724833 3.3342759 0.9988491\n   > sd_sstd\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0011811",
      "section_title": "0.0724833 3.3342759 0.9988491",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd_sstd\n   [1]",
        "start": 43,
        "end": 60
      }
    ]
  },
  {
    "content": "0.0029956 0.0065790 0.5057846 0.0643003\n   > AIC_sstd\n   [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0029956",
      "section_title": "0.0065790 0.5057846 0.0643003",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "AIC_sstd\n   [1] -",
        "start": 43,
        "end": 62
      }
    ]
  },
  {
    "content": "1378.4\n   > BIC_sstd\n   [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1378.4",
      "section_title": "> BIC_sstd",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "BIC_sstd\n   [1] -",
        "start": 10,
        "end": 29
      }
    ]
  },
  {
    "content": "1361.4\n\n                                                                              \u0002\n\f116      5 Modeling Univariate Distributions\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1361.4",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5. Fitting a generalized error distribution to changes in risk-free\nreturns\n\n   The \ufb01t of the generalized error distribution to diffrf was obtained using\noptim() similarly to the previous example.\n\n      > fit_ged$par\n      [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "Fitting a generalized error distribution to changes in risk-free",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "fit_ged$par\n      [1] -",
        "start": 206,
        "end": 231
      }
    ]
  },
  {
    "content": "0.00019493 0.06883004 1.00006805\n      > sd_ged\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00019493",
      "section_title": "0.06883004 1.00006805",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd_ged\n      [1]",
        "start": 39,
        "end": 58
      }
    ]
  },
  {
    "content": "0.0011470 0.0033032 0.0761374\n      > AIC_ged\n      [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0011470",
      "section_title": "0.0033032 0.0761374",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "AIC_ged\n      [1] -",
        "start": 36,
        "end": 57
      }
    ]
  },
  {
    "content": "1361.4\n      > BIC_ged\n      [1] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1361.4",
      "section_title": "> BIC_ged",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "BIC_ged\n      [1] -",
        "start": 13,
        "end": 34
      }
    ]
  },
  {
    "content": "1344.4\n\nThe three parameters are the estimates of the mean, standard deviation,\nand the shape parameter \u03bd, respectively. The estimated shape parameter is\nextremely close to 1, implying a double-exponential distribution. Note that\nAIC and BIC are considerably larger than for the t-distribution. Therefore,\nt-distributions appear to be better models for these data compared to gen-\neralized error distributions. A possible reason for this is that, like the t-\ndistributions, the density of the data seems to be rounded near the median; see\nthe kernel density estimate in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1344.4",
      "section_title": "The three parameters are the estimates of the mean, standard deviation,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9. QQ plots in Fig. 5.10 of diffrf versus\nthe quantiles of the \ufb01tted t- and generalized error distributions are similar, in-\ndicating that neither model has a decidedly better \ufb01t than the other. However,\nthe QQ plot of the t-distribution is slightly more linear.                      \u0002\n                         6\n               Density\n                         4\n                         2\n                         0\n\n\n\n\n                             \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.9",
      "section_title": "QQ plots in Fig. 5.10 of diffrf versus",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3   \u22120.2   \u22120.1   0.0     0.1   0.2   0.3\n                                            change in return\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "\u22120.2   \u22120.1   0.0     0.1   0.2   0.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9. Kernel estimate of the probability density of diffrf, the changes in the\nrisk-free returns.\n\f                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.9",
      "section_title": "Kernel estimate of the probability density of diffrf, the changes in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 Fitting Distributions by Maximum Likelihood                                                    117\n\na                           t model             b                             ged model       c                                  Normal mixture\n\n\n\n\n                                                                                              Normal\u2212mixture quantiles\n              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "Fitting Distributions by Maximum Likelihood                                                    117",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                                                                                                                         0.2\n                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                ged\u2212quantiles\nt\u2212quantiles\n              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "ged\u2212quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                                                0.0\n\n\n\n\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n              \u22120.1\n\n\n\n\n                                                                \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22120.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                         \u22120.2\n              \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                     \u22120.4      0.0    0.2                              \u22120.4       0.0   0.2                                     \u22120.4    0.0   0.2\n                             Data                                               Data                                                   Data\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3",
      "section_title": "\u22120.4      0.0    0.2                              \u22120.4       0.0   0.2                                     \u22120.4    0.0   0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10. (a) QQ plot of diffrf versus the quantiles of a tstd          2\n                                                                 \u03bd (\u03bc, s ) distribution\n         2                                                   o\nwith \u03bc, s , and \u03bd estimated by maximum likelihood. A 45 line through the origin\nhas been added for reference. (b) A similar plot for the generalized error distribution.\n(c) A similar plot for a normal mixture model.\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.10",
      "section_title": "(a) QQ plot of diffrf versus the quantiles of a tstd          2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.6. A-C skewed t-distribution \ufb01t to pipeline \ufb02ows\n\n    This example uses the daily \ufb02ows in natural gas pipelines introduced in\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.6",
      "section_title": "A-C skewed t-distribution \ufb01t to pipeline \ufb02ows",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2. Recall that all three distributions are left-skewed. There are\nmany well-known parametric families of right-skewed distributions, such as,\nthe gamma and log-normal distributions, but there are not as many families of\nleft-skewed distributions. The F-S skewed t- and A-C skewed t-distributions,\nwhich contain both right- and left-skewed distributions, are important excep-\ntions. In this example, the A-C skewed normal distributions will be used.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Recall that all three distributions are left-skewed. There are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.11 has one row of plots for each variable. The left plots have two\ndensity estimates, an estimate using the Azzalini\u2013Capitanio skewed normal\ndistribution (solid) and a KDE (dashed). The right plots are QQ plots using\nthe \ufb01tted skewed normal distributions.\n    The \ufb02ows in pipelines 1 and, to a lesser extent, 2 are \ufb01t reasonably well\nby the A-C skewed normal distribution. This can be seen in the agreement\nbetween the parametric density estimates and the KDEs and in the nearly\nstraight patterns in the QQ plots. The \ufb02ows in pipeline 3 have a KDE with\neither a wide, \ufb02at mode or, perhaps, two modes. This pattern cannot be ac-\ncommodated very well by the A-C skewed normal distributions. The result is\nless agreement between the parametric and KDE \ufb01ts and a curved QQ plot.\nNonetheless, a skewed normal distribution might be an adequate approxima-\ntion for some purposes.\n    The following code produced the top row of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "has one row of plots for each variable. The left plots have two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.11. The code for the\nremaining rows is similar. The function sn.mple() at line 7 computed the\nMLEs using the CD parametrization and the function cp2dp() at line 8 con-\nverted the MLEs to the DP parametrization, which is used by the functions\ndsn() and qsn() at lines 9 and 18 that were needed in the plots. The red\nreference line through the quartiles in the QQ plot is created at lines 20\u201322.\n\f118                        5 Modeling Univariate Distributions\n                                                                                                                                   Flow 1\n\n\n\n\n                                                                                                      120\n                                                                                   skew\u2212t quantiles\n                                     t\u2212model\n flow 1 density\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "The code for the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n                                     KDE\n\n\n\n\n                                                                                                      80\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.020",
      "section_title": "KDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                                                                                      40\n                          40          60             80          100         120                            40          60          80          100         120\n                                                     flow                                                                           data\n\n\n                                                                                                                                   Flow 2\n\n\n\n\n                                                                                   skew\u2212t quantiles\n flow 2 density\n\n\n\n\n                                     t\u2212model\n\n\n\n\n                                                                                                      210\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000",
      "section_title": "40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n\n\n\n\n                                     KDE\n\n\n\n\n                                                                                                      180\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "KDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                               170     180     190     200       210   220   230                                 170    180   190     200       210   220   230\n                                                     flow                                                                           data\n\n\n                                                                                                                                   Flow 3\n                                                                                   skew\u2212t quantiles\n flow 3 density\n\n\n\n\n                                     t\u2212model\n                                                                                                      30\n\n\n\n                                     KDE\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "170     180     190     200       210   220   230                                 170    180   190     200       210   220   230",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n\n\n\n\n                                                                                                      10\n                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.03",
      "section_title": "10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                                                                                                      \u221210\n\n\n\n\n                                 10            20           30         40                                          10         20           30         40\n                                                     flow                                                                           data\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "\u221210",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.11. Parametric (solid) and nonparametric (dashed) density estimates for\ndaily \ufb02ows in three pipelines (left) and QQ plots for the parametric \ufb01ts (right). The\nreference lines go through the \ufb01rst and third quartiles.\n\n\n1  library(sn)\n2  dat = read.csv(\"FlowData.csv\")\n 3 dat = dat/10000\n\n 4 par(mfrow = c(3, 2))\n\n 5 x = dat$Flow1\n\n 6 x1 = sort(x)\n\n 7 fit1 = sn.mple(y = x1, x = as.matrix(rep(1, length(x1))))\n\n 8 est1 = cp2dp(fit1$cp, family = \"SN\")\n\n 9 plot(x1, dsn(x1, dp = est1),\n\n10      type = \"l\", lwd = 2, xlab = \"flow\",\n11      ylab = \"flow 1 density\")\n12 d = density(x1)\n\n13 lines(d$x, d$y, lty = 2, lwd = 2)\n\n14 legend(40, 0.034, c(\"t-model\", \"KDE\"), lty = c(1, 2),\n\n15    lwd = c(2, 2))\n16 n = length(x1)\n\n17 u=(1:n) / (n + 1)\n\n18 plot(x1, qsn(u, dp = est1),xlab = \"data\",\n\f                                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "Parametric (solid) and nonparametric (dashed) density estimates for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.15 Pro\ufb01le Likelihood       119\n\n19    ylab = \"skew-t quantiles\", main = \"Flow 1\")\n20 lmfit = lm(qsn(c(0.25, 0.75), dp = est1) ~ quantile(x1,\n21    c(0.25, 0.75)) )\n22 abline(lmfit)\n\n\n                                                                                    \u0002\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.15",
      "section_title": "Pro\ufb01le Likelihood       119",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.15 Pro\ufb01le Likelihood\nPro\ufb01le likelihood is a technique based on the likelihood ratio test introduced\nin Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.15",
      "section_title": "Pro\ufb01le Likelihood",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.11. Pro\ufb01le likelihood is used to create con\ufb01dence intervals and is\noften a convenient way to \ufb01nd a maximum likelihood estimator. Suppose the\nparameter vector is \u03b8 = (\u03b81 , \u03b8 2 ), where \u03b81 is a scalar parameter and the vector\n\u03b8 2 contains the other parameters in the model. The pro\ufb01le log-likelihood for\n\u03b81 is\n                           Lmax (\u03b81 ) = max L(\u03b81 , \u03b8 2 ).                   (5.37)\n                                           \u03b82\n\nThe right-hand side of (5.37) means the L(\u03b81 , \u03b8 2 ) is maximized over \u03b8 2 with\n\u03b81 \ufb01xed to create a function of \u03b81 only. De\ufb01ne \u03b8      \u00022 (\u03b81 ) as the value of \u03b8 2 that\nmaximizes the right-hand side of (5.37).\n     The MLE of \u03b81 is the value, \u03b8\u00021 , that maximizes Lmax (\u03b81 ) and the MLE of\n       \u00022 (\u03b8\u00021 ). Let \u03b80,1 be a hypothesized value of \u03b81 . By the theory of likelihood\n\u03b8 2 is \u03b8\nratio tests in Sect. 5.11, one accepts the null hypothesis H0 : \u03b81 = \u03b80,1 if\n                                                     1\n                         Lmax (\u03b80,1 ) > Lmax (\u03b8\u00021 ) \u2212 \u03c72\u03b1,1 .                   (5.38)\n                                                     2\nHere \u03c72\u03b1,1 is the \u03b1-upper quantile of the chi-squared distribution with one de-\ngree of freedom. The pro\ufb01le likelihood con\ufb01dence interval (or, more properly,\ncon\ufb01dence region since it need not be an interval) for \u03b81 is the set of all null\nvalues that would be accepted, that is,\n                     \u000e                                     \u000f\n                                                     1\n                      \u03b81 : Lmax (\u03b81 ) > Lmax (\u03b8\u00021 ) \u2212 \u03c72\u03b1,1 .            (5.39)\n                                                     2\n   The pro\ufb01le likelihood can be de\ufb01ned for a subset of the parameters, rather\nthan for just a single parameter, but this topic will not be pursued here.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.11",
      "section_title": "Pro\ufb01le likelihood is used to create con\ufb01dence intervals and is",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "Lmax (\u03b8\u00021 ) \u2212 \u03c72\u03b1,1 .                   (5.38)\n                                                     2\nHere \u03c72\u03b1,1 is the \u03b1-upper quantile of the chi-squared distribution with one de-\ngree of freedom. The pro\ufb01le likelihood con\ufb01dence interval (or, more properly,\ncon\ufb01dence region since it need not be an interval) for \u03b81 is the set of all null\nvalues that would be accepted, that is,\n                     \u000e                                     \u000f\n                                                     1\n                      \u03b81 : Lmax (\u03b81 ) > Lmax (\u03b8\u00021 ) \u2212 \u03c72\u03b1,1 .            (5.39)\n                                                     2\n   The pro\ufb01le likelihood can be de\ufb01ned for a subset of the parameters, rather\nthan for just a single parameter, but this topic will not be pursued here.",
        "start": 995,
        "end": 1783
      }
    ]
  },
  {
    "content": "5.7. Estimating a Box\u2013Cox transformation\n\n   An automatic method for estimating the transformation parameter for\na Box\u2013Cox transformation13 assumes that for some values of \u03b1, \u03bc, and \u03c3,\n                       (\u03b1)         (\u03b1)\nthe transformed data Y1 , . . . , Yn are i.i.d. N (\u03bc, \u03c3 2 )-distributed. All three\n13\n     See Eq. (4.5).\n\f120                      5 Modeling Univariate Distributions\n\nparameters can be estimated by maximum likelihood. For a \ufb01xed value of \u03b1,\n                                                (\u03b1)        (\u03b1)\n\u0002 and \u03c3\n\u03bc      \u0002 are the sample mean and variance of Y1 , . . . , Yn and these values\ncan be plugged into the log-likelihood to obtain the pro\ufb01le log-likelihood for\n\u03b1. This can be done with the function boxcox() in R\u2019s MASS package, which\nplots the pro\ufb01le log-likelihood with con\ufb01dence intervals.\n   Estimating \u03b1 by the use of pro\ufb01le likelihood will be illustrated using the\ndata on gas pipeline \ufb02ows. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "Estimating a Box\u2013Cox transformation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12 shows the pro\ufb01le log-likelihoods and\nthe KDEs and normal QQ plots of the \ufb02ows transformed using the MLE of \u03b1.\n\n                                                                            Flow 1                                            Flow 1\n\n\n\n\n                                                                                           Theoretical Quantiles\n                                                                                                                   3\n log\u2212Likelihood\n\n\n\n\n                                                            2.0e\u221207\n                  \u2212346\n\n\n\n\n                                                  Density\n\n\n\n\n                                                                                                                   1\n                          95%\n\n\n\n\n                                                                                                                   \u22121\n                  \u2212348\n\n\n\n\n                                                            0.0e+00\n\n\n\n\n                                                                                                                   \u22123\n                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.12",
      "section_title": "shows the pro\ufb01le log-likelihoods and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0        4.0                            0e+00 4e+06                                   0e+00     3e+06\n                                  \u03b1                                                                                     Sample Quantiles\n                                                      N = 342 Bandwidth = 5.038e+05\n\n\n\n                                                                            Flow 2                                            Flow 2\n                                                                                           Theoretical Quantiles\n                  55\n\n\n\n\n                                                                                                                   3\n                                                            2.0e\u221224\n log\u2212Likelihood\n\n\n\n\n                                                  Density\n\n\n\n\n                                                                                                                   1\n\n\n\n\n                          95%\n                  53\n\n\n\n\n                                                                                                                   \u22121\n                                                            0.0e+00\n                  51\n\n\n\n\n                                                                                                                   \u22123\n\n\n\n\n                         8 9      11         13                       0e+00 4e+23                                   0e+00     3e+23\n                                  \u03b1                   N = 342 Bandwidth = 5.055e+22                                     Sample Quantiles\n\n\n\n                                                                            Flow 3                                            Flow 3\n                                                                                           Theoretical Quantiles\n                                                                                                                   3\n\n\n\n\n                          95%\n log\u2212Likelihood\n\n\n\n\n                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.0",
      "section_title": "4.0                            0e+00 4e+06                                   0e+00     3e+06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.008\n                                                  Density\n                  \u2212652\n\n\n\n\n                                                                                                                   1\n                                                                                                                   \u22121\n                  \u2212658\n\n\n\n\n                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.008",
      "section_title": "Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                                                                                                   \u22123\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000",
      "section_title": "\u22123",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 1.0 1.4 1.8                                0     50     100                                 20       60\n                                 \u03b1                          N = 342 Bandwidth = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "1.0 1.4 1.8                                0     50     100                                 20       60",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.451                                   Sample Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "9.451",
      "section_title": "Sample Quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12. Pro\ufb01le log-likelihoods and 95 % con\ufb01dence intervals for the parameter \u03b1 of\nthe Box\u2013Cox transformation (left), KDEs of the transformed data (middle column),\nand normal plots of the transformed data (right).\n\f                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.12",
      "section_title": "Pro\ufb01le log-likelihoods and 95 % con\ufb01dence intervals for the parameter \u03b1 of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.16 Robust Estimation     121\n\nThe KDE used adjust = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.16",
      "section_title": "Robust Estimation     121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 to smooth out local bumpiness seen with the\ndefault bandwidth. For the \ufb02ows in pipeline 1, the MLE is \u03b1\u0002 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "to smooth out local bumpiness seen with the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5. Recall that\nin Example 4.2, we saw by trial-and-error that \u03b1 between 3 and 4 was best\nfor symmetrizing the data. It is gratifying to see that maximum likelihood\ncorroborates this choice. The QQ plots show that the Box\u2013Cox transformed\n\ufb02ows have light tails. Light tails are not usually considered to be a problem\nand are to be expected here since the pipeline \ufb02ows are bounded, below by 0\nand above by the capacity of the pipeline.\n    The top row of Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "Recall that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12 was produced by the following code. The function\nboxcox() at line 8 created the top-left plot containing the pro\ufb01le likelihood\nof the transformation parameter.\n1  dat = read.csv(\"FlowData.csv\")\n2  dat = dat / 10000\n 3 library(\"MASS\") ####  for boxcox()\n 4 adj = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.12",
      "section_title": "was produced by the following code. The function",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n 5 par(mfrow = c(3, 3))\n\n 6 x = dat$Flow1\n\n 7 x1 = sort(x)\n\n 8 bcfit1 = boxcox(x1 ~ 1, lambda = seq(2.6, 4.5, 1 / 100),\n\n 9    xlab = expression(alpha))\n10 text(3, -1898.75, \"Flow 1\")\n\n11 plot(density((x1^",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "5 par(mfrow = c(3, 3))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 - 1) / 3.5, adjust = adj), main = \"Flow 1\")\n\n12 qqnorm((x1^",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "- 1) / 3.5, adjust = adj), main = \"Flow 1\")",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5 - 1) / 3.5, datax = TRUE, main = \"Flow 1\")\n\n\n                                                                              \u0002\n    It is worth pointing out that we have now seen two distinct methods for\naccommodating the left skewness in the pipeline \ufb02ows, modeling the untrans-\nformed data by a skewed t-distribution (Example 5.6) and Box\u2013Cox trans-\nformation to a normal distribution (Example 5.7). A third method would be\nto forego parametric modeling and use the kernel density estimation. This is\nnot an atypical situation; often data can be analyzed in several di\ufb00erent, but\nequally appropriate, ways.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.5",
      "section_title": "- 1) / 3.5, datax = TRUE, main = \"Flow 1\")",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.16 Robust Estimation\nAlthough maximum likelihood estimators have many attractive properties,\nthey have one serious drawback of which anyone using them should be aware.\nMaximum likelihood estimators can be very sensitive to the assumptions of the\nstatistical model. For example, the MLE of the mean of a normal population\nis the sample mean and the MLE of \u03c3 2 is the sample variance, except with\nthe minor change of a divisor of n rather than n \u2212 1. The sample mean\nand variance are e\ufb03cient estimators when the population is truly normally\ndistributed, but these estimators are very sensitive to outliers, especially the\nsample standard deviation. Because these estimators are averages of the data\n\f122       5 Modeling Univariate Distributions\n\nand the squared deviations from the mean, respectively, a single outlier in\nthe sample can drive the sample mean and variance to wildly absurd values\nif the outlier is far enough removed from the other data. Extreme outliers are\nnearly impossible with exactly normally distributed data, but if the data are\nonly approximately normal with heavier tails than the normal distribution,\nthen outliers are more probable and, when they do occur, more likely to be\nextreme. Therefore, the sample mean and variance can be very ine\ufb03cient\nestimators. Statisticians say that the MLE is not robust to mild deviations\nfrom the assumed model. This is bad news and has led researchers to \ufb01nd\nestimators that are robust.\n    A robust alternative to the sample mean is the trimmed mean. An \u03b1-\ntrimmed mean is computed by ordering the sample from smallest to largest,\nremoving the fraction \u03b1 of the smallest and the same fraction of the largest\nobservations, and then taking the mean of the remaining observations. The\nidea behind trimming is simple and should be obvious: The sample is trimmed\nof extreme values before the mean is calculated. There is a mathematical\nformulation of the \u03b1-trimmed mean. Let k = n\u03b1 rounded14 to an integer; k is\nthe number of observations removed from both ends of the sample. Then the\n\u03b1-trimmed mean is                    \u0017n\u2212k\n                                              Y(i)\n                               X \u03b1 = i=k+1         ,\n                                        n \u2212 2k\nwhere Y(i) is the ith order statistic. Typical values of \u03b1 are 0.1, 0.15, 0.2,\nand ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.16",
      "section_title": "Robust Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25. As \u03b1 approaches 0.5, the \u03b1-trimmed mean approaches the sample\nmedian, which is the 0.5-sample quantile.\n    Dispersion refers to the variation in a distribution or sample. The sample\nstandard deviation is the most common estimate of dispersion, but as stated\nit is nonrobust. In fact, the sample standard deviation is even more nonro-\nbust than the sample mean, because squaring makes outliers more extreme.\nA robust estimator of dispersion is the MAD (median absolute deviation)\nestimator, de\ufb01ned as\n                   \u0002MAD = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "As \u03b1 approaches 0.5, the \u03b1-trimmed mean approaches the sample",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4826 \u00d7 median{|Yi \u2212 median(Yi )|}.\n                   \u03c3                                                            (5.40)\nThis formula should be interpreted as follows. The expression \u201cmedian(Yi )\u201d\nis the sample median, |Yi \u2212 median(Yi )| is the absolute deviation of the ob-\nservations from their median, and median{|Yi \u2212 median(Yi )|} is the median\nof these absolute deviations. For normally distributed data, the median{|Yi \u2212\nmedian(Yi )|} estimates not \u03c3 but rather \u03a6\u22121 (0.75)\u03c3 = \u03c3/1.4826, because\nfor normally distributed data the median{|Yi \u2212 median(Yi )|} will converge to\n\u03c3/",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.4826",
      "section_title": "\u00d7 median{|Yi \u2212 median(Yi )|}.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4826 as the sample size increases. Thus, the factor 1.4826 in Eq. (5.40)\n           \u0002MAD so that it estimates \u03c3 when applied to normally distributed\ncalibrates \u03c3\ndata.\n14\n     De\ufb01nitions vary and the rounding could be either upward or to the nearest integer.\n\f                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.4826",
      "section_title": "as the sample size increases. Thus, the factor 1.4826 in Eq. (5.40)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.17 Transformation Kernel Density Estimation      123\n\n    \u0002MAD does not estimate \u03c3 for a nonnormal population. It does measure\n    \u03c3\ndispersion, but not dispersion as measured by the standard deviation. But\nthis is just the point. For nonnormal populations the standard deviation can\nbe very sensitive to the tails of the distribution and does not tell us much\nabout the dispersion in the central range of the distribution, just in the tails.\n    In R, mad() computes (5.40). Some authors de\ufb01ne MAD to be median{|Yi \u2212\nmedian(Yi )|}, that is, without ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.17",
      "section_title": "Transformation Kernel Density Estimation      123",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4826. Here the notation \u03c3\u0002MAD is used to em-\nphasize the standardization by ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.4826",
      "section_title": "Here the notation \u03c3\u0002MAD is used to em-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4826 in order to estimate a normal standard\ndeviation.\n    An alternative to using robust estimators is to assume a model where out-\nliers are more probable. Then the MLE will automatically downweight out-\nliers. For example, the MLE of the parameters of a t-distribution is much more\nrobust to outliers than the MLE of the parameters of a normal distribution.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.4826",
      "section_title": "in order to estimate a normal standard",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.17 Transformation Kernel Density Estimation\nwith a Parametric Transformation\n\nWe saw in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.17",
      "section_title": "Transformation Kernel Density Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 that the transformation kernel density estimator (TKDE)\ncan avoid the bumps seen when the ordinary KDE is applied to skewed data.\nThe KDE also can exhibit bumps in the tails when both tails are long, as\nis common with \ufb01nancial markets data. An example is the variable diffrf\nwhose KDE is in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "that the transformation kernel density estimator (TKDE)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9. For such data, the TKDE needs a transformation\nthat is convex to the right of the mode and concave to the left of the mode.\nThere are many such transformations, and in this section we will use some\nfacts from probability theory, as well as maximum likelihood estimation, to\nselect a suitable one.\n    The key ideas used here are that (1) normally distributed data have light\ntails and are suitable for estimation with the KDE, (2) it is easy to transform\ndata to normality if one knows the CDF, and (3) the CDF can be estimated\nby assuming a parametric model and using maximum likelihood. If a random\nvariable has a continuous distribution F , then F (X) has a uniform distri-\nbution and \u03a6\u22121 {F (X)} has an N (0, 1) distribution; here \u03a6 is the standard\nnormal CDF. Of course, in practice F is unknown, but one can estimate F\nparametrically, assuming, for example, that F is some t-distribution. It is not\nnecessary that F actually be a t-distribution, only that a t-distribution can\nprovide a reasonable enough \ufb01t to F in the tails so that an appropriate trans-\nformation is selected. If it was known that F was a t-distribution, then, of\ncourse, there would be no need to use a KDE or TKDE to estimate its den-\nsity. The transformation to use in the TKDE is g(y) = \u03a6\u22121 {F (y)}, which has\ninverse g \u22121 (x) = F \u22121 {\u03a6(x)}. The derivative of g is needed to compute the\nTKDE and is\n                                           f (y)\n                             g \u0003 (y) =                .                   (5.41)\n                                       \u03c6[\u03a6\u22121 {F (y)}]\n\f124    5 Modeling Univariate Distributions\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.9",
      "section_title": "For such data, the TKDE needs a transformation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.8. TKDE for risk-free returns\n\n\n                                        KDE\n                                        TKDE\n\n\n                           6\n              Density(y)\n                           4\n                           2\n                           0\n\n\n\n\n                                        \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "TKDE for risk-free returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4      \u22120.2        0.0     0.2    0.4\n                                                          y\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "\u22120.2        0.0     0.2    0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.13. Kernel density and transformation kernel density estimates of monthly\nchanges in the risk-free returns, January 1960 to December 2002. The data are in\nthe Capm series in the Ecdat package in R.\n\n\n    This example uses the changes in the risk-free returns in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.13",
      "section_title": "Kernel density and transformation kernel density estimates of monthly",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3. We saw in\nSect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "We saw in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14 that these data are reasonably well \ufb01t by a t-distribution with mean,\nstandard deviation, and \u03bd equal to 0.00121, 0.0724, and 3.33, respectively. This\ndistribution will be used as F . Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "that these data are reasonably well \ufb01t by a t-distribution with mean,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.13 compares the ordinary KDE to the\nTKDE for this example. Notice that the TKDE is much smoother in the tails;\nthis can be seen better in Fig. 5.14, which gives detail on the left tail.\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.13",
      "section_title": "compares the ordinary KDE to the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                        KDE\n                                        TKDE\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "KDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n              Density(y)\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "Density(y)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                           0.4\n                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                           0.0\n\n\n\n\n                                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5          \u22120.4      \u22120.3       \u22120.2    \u22120.1\n                                                          y\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "\u22120.4      \u22120.3       \u22120.2    \u22120.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.14. Kernel density and transformation kernel density estimates of monthly\nchanges in the risk-free returns, January 1960 to December 2002, zooming in on left\ntail.\n\f                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.14",
      "section_title": "Kernel density and transformation kernel density estimates of monthly",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.17 Transformation Kernel Density Estimation   125\n\n   The transformation used in this example is shown in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.17",
      "section_title": "Transformation Kernel Density Estimation   125",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.15. Notice the\nconcave-convex shape that brings the left and right tails closer to the center\nand results in transformed data without the heavy tails seen in the original\ndata. The removal of the heavy tails can be seen in Fig. 5.16, which is a normal\nplot of the transformed data.\nThe code to create Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.15",
      "section_title": "Notice the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.13 is below:\n1 data(Capm, package = \"Ecdat\")\n2 y = diff(Capm$rf)\n3 diffrf = y\n\n4 library(fGarch)\n\n5 x1 = pstd(y, mean = 0.001, sd = .0725, nu = 3.34)\n\n6 x = qnorm(x1)\n                                      3\n                                      2\n                                      1\n             g(y)\n                                      0\n                                      \u22121\n                                      \u22122\n                                      \u22123\n\n\n\n\n                                           \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.13",
      "section_title": "is below:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4         \u22120.2         0.0       0.2\n                                                                 y\n\n           Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "\u22120.2         0.0       0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.15. Plot of the transformation used in Example 5.8.\n\n                                                          Normal Q\u2212Q Plot\n                                      3\n                                      2\n              Theoretical Quantiles\n                                      1\n                                      0\n                                      \u22121\n                                      \u22122\n                                      \u22123\n\n\n\n\n                                           \u22123      \u22122     \u22121      0        1     2   3\n                                                          Sample Quantiles\n\n      Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.15",
      "section_title": "Plot of the transformation used in Example 5.8.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.16. Normal plot of the transformed data used in Example 5.8.\n\f126    5 Modeling Univariate Distributions\n\n7  par(mfrow = c(1, 1))\n8  d1 = density(diffrf)\n 9 plot(d1$x, d1$y, type = \"l\", xlab = \"y\", ylab = \"Density(y)\",\n\n10    lwd = 2)\n11 d2 = density(x)\n\n12 ginvx = qstd(pnorm(d2$x), mean = 0.001, sd = .0725, nu = 3.34)\n\n13 gprime_num = dstd(ginvx, mean = 0.001, sd = .0725, nu = 3.34)\n\n14 gprime_den = dnorm(qnorm(pstd(ginvx, mean = 0.001,\n\n15    sd = .0725, nu = 3.34)))\n16 gprime = gprime_num / gprime_den\n\n17 lines(ginvx,d2$y * gprime, type = \"l\", lty = 2, col = \"red\", lwd = 2)\n\n18 legend(\"topleft\", c(\"KDE\", \"TKDE\"), lty = c(1,2), lwd = 2,\n\n19    col = c(\"black\", \"red\"))\n\n    Lines 5\u20136 compute the transformation. Line 8 computes the KDE of the\nuntransformed data and line 11 computes the KDE of the transformed data.\nLines 12\u201316 compute g \u0003 in (5.41). At line 17 the KDE of the transformed data\nis multiplied by g \u0003 as in Eq. (4.6) to compute the TKDE.                  \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.16",
      "section_title": "Normal plot of the transformed data used in Example 5.8.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.18 Bibliographic Notes\n\nMaximum likelihood estimation and likelihood ratio tests are discussed in all\ntextbooks on mathematical statistics, including Boos and Stefanski (2013);\nCasella and Berger (2002), and Wasserman (2004).\n    Burnham and Anderson (2002) is a comprehensive introduction to model\nselection and is highly recommended for further reading. They also cover\nmultimodel inference, a more advanced topic that includes model averaging\nwhere estimators or predictions are averaged across several models. Chap-\nter 7 of Burnham and Anderson provides the statistical theory behind AIC as\nan approximate deviance of hypothetical validation data. The small-sample\ncorrected AIC is due to Hurvich and Tsai (1989).\n    Buch-Larsen et al. (2005) and Ruppert and Wand (1992) discuss other\nmethods for choosing the transformation when the TKDE is applied to heavy-\ntailed data.\n    The central limit theorem for the MLE is stated precisely and proved in\ntextbooks on asymptotic theory such as Ser\ufb02ing (1980), van der Vaart (1998),\nand Lehmann (1999).\n    Observed and expected Fisher information are compared by Efron and\nHinkley (1978), who argue that the observed Fisher information gives superior\nstandard errors.\n    Box\u2013Cox transformations were introduced by Box and Dox (1964). See Az-\nzalini (2014); Azzalini and Capitanio (2003), and Arellano-Valle and Azzalini\n(2013) for discussion of the A-C skewed distributions.\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.18",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.19 R Lab     127\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "R Lab     127",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.19 R Lab\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.19.1 Earnings Data\nRun the following R code to \ufb01nd a symmetrizing transformation for 1998\nearnings data from the Current Population Survey. The code looks at the\nuntransformed data and the square-root and log-transformed data. The trans-\nformed data are compared by normal plots, boxplots, and kernel density\nestimates.\n   library(\"Ecdat\")\n   ?CPSch3\n   data(CPSch3)\n   dimnames(CPSch3)[[2]]\n\n   male.earnings = CPSch3[CPSch3[ ,3] == \"male\", 2]\n   sqrt.male.earnings = sqrt(male.earnings)\n   log.male.earnings = log(male.earnings)\n\n   par(mfrow = c(2, 2))\n   qqnorm(male.earnings ,datax = TRUE, main = \"untransformed\")\n   qqnorm(sqrt.male.earnings, datax = TRUE,\n      main = \"square-root transformed\")\n   qqnorm(log.male.earnings, datax = TRUE, main = \"log-transformed\")\n\n   par(mfrow = c(2, 2))\n   boxplot(male.earnings, main = \"untransformed\")\n   boxplot(sqrt.male.earnings, main = \"square-root transformed\")\n   boxplot(log.male.earnings, main = \"log-transformed\")\n\n   par(mfrow = c(2,2))\n   plot(density(male.earnings), main = \"untransformed\")\n   plot(density(sqrt.male.earnings), main = \"square-root transformed\")\n   plot(density(log.male.earnings), main = \"log-transformed\")\n\nProblem 1 Which of the three transformation provides the most symmetric\ndistribution? Try other powers beside the square root. Which power do you\nthink is best for symmetrization? You may include plots with your work if you\n\ufb01nd it helpful to do that.\n\n\n   Next, you will estimate the Box\u2013Cox transformation parameter by max-\nimum likelihood. The model is that the data are N (\u03bc, \u03c3 2 )-distributed after\nbeing transformed by some \u03bb. The unknown parameters are \u03bb, \u03bc, and \u03c3.\n   Run the following R code to plot the pro\ufb01le likelihood for \u03bb on the grid\nseq(-2, 2, 1/10) (this is the default and can be changed). The command\nboxcox takes an R formula as input. The left-hand side of the formula is the\nvariable to be transformed. The right-hand side is a linear model (see Chap. 9).\nIn this application, the model has only an intercept, which is indicated by\n\f128       5 Modeling Univariate Distributions\n\n\u201c1.\u201d \u201cMASS\u201d is an acronym for \u201cModern Applied Statistics with S-PLUS,\u201d a\nhighly-regarded textbook whose fourth edition also covers R. The MASS library\naccompanies this book.\n      library(\"MASS\")\n      par(mfrow = c(1, 1))\n      boxcox(male.earnings ~ 1)\n\nThe default grid of \u03bb values is large, but you can zoom in on the high-likelihood\nregion with the following:\n      boxcox(male.earnings ~ 1, lambda = seq(0.3, 0.45, 1 / 100))\n\nTo \ufb01nd the MLE, run this R code:\n      bc = boxcox(male.earnings ~ 1, lambda = seq(0.3, 0.45, by = 1 / 100),\n         interp = FALSE)\n      ind = (bc$y == max(bc$y))\n      ind2 = (bc$y > max(bc$y) - qchisq(0.95, df = 1) / 2)\n      bc$x[ind]\n      bc$x[ind2]\n\n\nProblem 2 (a) What are ind and ind2 and what purposes do they serve?\n(b) What is the e\ufb00ect of interp on the output from boxcox?\n(c) What is the MLE of \u03bb?\n(d) What is a 95 % con\ufb01dence interval for \u03bb?\n(e) Modify the code to \ufb01nd a 99 % con\ufb01dence interval for \u03bb.\n\n\n    Rather than trying to transform the variable male.earnings to a Gaussian\ndistribution, we could \ufb01t a skewed Gaussian or skewed t-distribution. R code\nthat \ufb01ts a skewed t is listed below:\n      library(\"fGarch\")\n      fit = sstdFit(male.earnings, hessian = TRUE)\n\nProblem 3 What are the estimates of the degrees-of-freedom parameter and\nof \u03be?\n\n\nProblem 4 Produce a plot of a kernel density estimate of the pdf of male.\nearnings. Overlay a plot of the skewed t-density with MLEs of the parameters.\nMake sure that the two curves are clearly labeled, say with a legend, so that it\nis obvious which curve is which. Include your plot with your work. Compare\nthe parametric and nonparametric estimates of the pdf. Do they seem similar?\nBased on the plots, do you believe that the skewed t-model provides an adequate\n\ufb01t to male.earnings?\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "1 Earnings Data",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "max(bc$y) - qchisq(0.95, df = 1) / 2)\n      bc$x[ind]\n      bc$x[ind2]",
        "start": 2702,
        "end": 2776
      }
    ]
  },
  {
    "content": "5.19 R Lab     129\n\nProblem 5 Fit a skewed GED model to male.earnings and repeat Problem 4\nusing the skewed GED model in place of the skewed t. Which parametric model\n\ufb01ts the variable male.earnings best, skewed t or skewed GED?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "R Lab     129",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.19.2 DAX Returns\n\nThis section uses log returns on the DAX index in the data set EuStock-\nMarkets. Your \ufb01rst task is to \ufb01t the standardized t-distribution (std) to the\nlog returns. This is accomplished with the following R code.\n    Here loglik std is an R function that is de\ufb01ned in the code. This function\nreturns minus the log-likelihood for the std model. The std density function\nis computed with the function dstd in the fGarch package. Minus the log-\nlikelihood, which is called the objective function, is minimized by the function\noptim. The L-BFGS-B method is used because it allows us to place lower and\nupper bounds on the parameters. Doing this avoids the errors that would be\nproduced if, for example, a variance parameter were negative. When optim is\ncalled, start is a vector of starting values. Use R\u2019s help to learn more about\noptim. In this example, optim returns an object fit std. The component\nfig std$par contains the MLEs and the component fig std$value contains\nthe minimum value of the objective function.\n   data(Garch, package = \"Ecdat\")\n   library(\"fGarch\")\n   data(EuStockMarkets)\n   Y = diff(log(EuStockMarkets[ ,1]))       #   DAX\n\n   ##### std #####\n   loglik_std = function(x) {\n      f = -sum(dstd(Y, x[1], x[2], x[3], log = TRUE))\n      f}\n   start = c(mean(Y), sd(Y), 4)\n   fit_std = optim(start, loglik_std, method = \"L-BFGS-B\",\n      lower = c(-0.1, 0.001, 2.1),\n      upper = c(0.1, 1, 20), hessian = TRUE)\n   cat(\"MLE =\", round(fit_std$par, digits = 5))\n   minus_logL_std = fit_std$value # minus the log-likelihood\n   AIC_std = 2 * minus_logL_std + 2 * length(fit_std$par)\n\nProblem 6 What are the MLEs of the mean, standard deviation, and the\ndegrees-of-freedom parameter? What is the value of AIC?\n\n\nProblem 7 Modify the code so that the MLEs for the skewed t-distribution\nare found. Include your modi\ufb01ed code with your work. What are the MLEs?\nWhich distribution is selected by AIC, the t or the skewed t-distribution?\n\f130    5 Modeling Univariate Distributions\n\nProblem 8 Compute and plot the TKDE of the density of the log returns\nusing the methodology in Sects. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "2 DAX Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.8 and 5.17. The transformation that you use\nshould be g(y) = \u03a6\u22121 {F (y)}, where F is the t-distribution with parameters\nestimated in Problem 6. Include your code and the plot with your work.\n\n\nProblem 9 Plot the KDE, TKDE, and parametric estimator of the log-return\ndensity, all on the same graph. Zoom in on the right tail, speci\ufb01cally the region\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.8",
      "section_title": "and 5.17. The transformation that you use",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.035 < y < 0.06. Compare the three densities for smoothness. Are the TKDE\nand parametric estimates similar? Include the plot with your work.\n\n\nProblem 10 Fit the F-S skewed t-distribution to the returns on the FTSE\nindex in EuStockMarkets. Find the MLE, the standard errors of the MLE,\nand AIC.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.035",
      "section_title": "< y < 0.06. Compare the three densities for smoothness. Are the TKDE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.19.3 McDonald\u2019s Returns\n\nThis section continues the analysis of McDonald\u2019s stock returns begun in\nSect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.19",
      "section_title": "3 McDonald\u2019s Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.4 and continued in Sect. 4.10.2. Run the code below.\n1  data = read.csv(\u2019MCD_PriceDaily.csv\u2019)\n2  adjPrice = data[ ,7]\n 3 LogRet = diff(log(adjPrice))\n\n 4 library(MASS)\n\n 5 library(fGarch)\n\n 6 fit.T = fitdistr(LogRet, \"t\")\n\n 7 params.T = fit.T$estimate\n\n 8 mean.T = params.T[1]\n\n 9 sd.T = params.T[2] * sqrt(params.T[3] / (params.T[3] - 2))\n\n10 nu.T = params.T[3]\n\n11 x = seq(-0.04, 0.04, by = 0.0001)\n\n12 hist(LogRet, 80, freq = FALSE)\n\n13 lines(x, dstd(x, mean = mean.T, sd = sd.T, nu = nu.T),\n\n14    lwd = 2, lty = 2, col = \u2019red\u2019)\n\nProblem 11 Referring to lines by number, describe in detail what the code\ndoes. Examine the plot and comment on the goodness of \ufb01t.\n\n\nProblem 12 Is the mean signi\ufb01cantly di\ufb00erent than 0?\n\n\nProblem 13 Discuss di\ufb00erences between the histogram and the parametric\n\ufb01t. Do you think that the parametric \ufb01t is adequate or should a nonparametric\nestimate be used instead?\n\f                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.4",
      "section_title": "4 and continued in Sect. 4.10.2. Run the code below.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.20 Exercises    131\n\nProblem 14 How heavy is the tail of the parametric \ufb01t? Does it appear that\nthe \ufb01tted t-distribution has a \ufb01nite kurtosis? How con\ufb01dent are you that the\nkurtosis is \ufb01nite?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.20",
      "section_title": "Exercises    131",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.20 Exercises\n\n1. Load the CRSPday data set in the Ecdat package and get the variable\n   names with the commands\n       library(Ecdat)\n       data(CRSPday)\n       dimnames(CRSPday)[[2]]\n\n   Plot the IBM returns with the commands\n       r = CRSPday[ ,5]\n       plot(r)\n\n   Learn the mode and class of the IBM returns with\n       mode(r)\n       class(r)\n\n   You will see that the class of the variable r is \u201cts,\u201d which means \u201ctime\n   series.\u201d Data of class ts are plotted di\ufb00erently than data not of this class.\n   To appreciate this fact, use the following commands to convert the IBM\n   returns to class numeric before plotting them:\n       r2 = as.numeric(r)\n       class(r2)\n       plot(r2)\n\n   The variable r2 contains the same data as the variable r, but r2 has class\n   numeric.\n   Find the covariance matrix, correlation matrix, and means of GE, IBM,\n   and Mobil with the commands\n       cov(CRSPday[ ,4:6])\n       cor(CRSPday[ ,4:6])\n       apply(CRSPday[ ,4:6], 2, mean)\n\n   Use your R output to answer the following questions:\n   (a) What is the mean of the Mobil returns?\n   (b) What is the variance of the GE returns?\n   (c) What is the covariance between the GE and Mobil returns?\n   (d) What is the correlation between the GE and Mobil returns?\n\f132     5 Modeling Univariate Distributions\n\n 2. Suppose that Y1 , . . . , Yn are i.i.d. N (\u03bc, \u03c3 2 ), where \u03bc is known. Show that\n    the MLE of \u03c3 2 is\n                                            n\n                                      n\u22121         (Yi \u2212 \u03bc)2 .\n                                            i=1\n\n 3. Show that f (y|\u03be) given by Eq. (5.15) integrates to (\u03be + \u03be \u22121 )/2.\n                 \u2217\n\n 4. Let X be a random variable with mean \u03bc and standard deviation \u03c3.\n    (a) Show that the kurtosis of X is equal to 1 plus the variance of {(X \u2212\n         \u03bc)/\u03c3}2 .\n    (b) Show that the kurtosis of any random variable is at least 1.\n    (c) Show that a random variable X has a kurtosis equal to 1 if and only\n         if P (X = a) = P (X = b) = 1/2 for some a = b.\n 5. (a) What is the kurtosis of a normal mixture distribution that is 95 %\n         N (0, 1) and 5 % N (0, 10)?\n    (b) Find a formula for the kurtosis of a normal mixture distribution that\n         is 100p% N (0, 1) and 100(1 \u2212 p)% N (0, \u03c3 2 ), where p and \u03c3 are pa-\n         rameters. Your formula should give the kurtosis as a function of p\n         and \u03c3.\n    (c) Show that the kurtosis of the normal mixtures in part (b) can be made\n         arbitrarily large by choosing p and \u03c3 appropriately. Find values of p\n         and \u03c3 so that the kurtosis is 10,000 or larger.\n    (d) Let M > 0 be arbitrarily large. Show that for any p0 < 1, no matter\n         how close to 1, there is a p > p0 and a \u03c3, such that the normal mixture\n         with these values of p and \u03c3 has a kurtosis at least M . This shows that\n         there is a normal mixture arbitrarily close to a normal distribution but\n         with a kurtosis above any arbitrarily large value of M .\n 6. Fit the F-S skewed t-distribution to the gas \ufb02ow data. The data set is in\n    the \ufb01le GasFlowData.csv, which can be found on the book\u2019s website.\n 7. Suppose that X1 , . . . , Xn are i.i.d. exponential(\u03b8). Show that the MLE of\n    \u03b8 is X.\n 8. For any univariate parameter \u03b8 and estimator \u03b8,     \u0002 we de\ufb01ne the bias to be\n           \u0002        \u0002\n    Bias(\u03b8) = E(\u03b8) \u2212 \u03b8 and the MSE (mean square error) to be MSE(\u03b8)          \u0002 =\n    E(\u03b8\u0002 \u2212 \u03b8) . Show that\n               2\n\n\n                              \u0002 = {Bias(\u03b8)}\n                          MSE(\u03b8)        \u0002 2 + Var(\u03b8).\n                                                  \u0002\n\n                                iid\n 9. Suppose\u0017that   X1 , . . . , Xn \u223c N ormal(\u03bc, \u03c3 2 ), with 0 < \u03c3 2 < \u221e, and de\ufb01ne\n         1   n\n    \u03bc\u0302 = n i=1 Xi . What is Bias(\u03bc\u0302)? What is MSE(\u03bc\u0302)? What if the distribu-\n    tion of the Xi is not Normal, but Student\u2019s t distribution with the same\n    mean \u03bc and variance \u03c3 2 , and tail index (\u03bd, df) of 5?\n10. Assume that you have a sample from a t-distribution and the sample\n    kurtosis is 9. Based on this information alone, what would you use as an\n    estimate of \u03bd, the tail-index parameter?\n11. The number of small businesses in a certain region defaulting on loans was\n    observed for each month over a 4-year period. In the R program below,\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.20",
      "section_title": "Exercises",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 be arbitrarily large. Show that for any p0 < 1, no matter\n         how close to 1, there is a p > p0 and a \u03c3, such that the normal mixture\n         with these values of p and \u03c3 has a kurtosis at least M . This shows that\n         there is a normal mixture arbitrarily close to a normal distribution but\n         with a kurtosis above any arbitrarily large value of M .\n 6. Fit the F-S skewed t-distribution to the gas \ufb02ow data. The data set is in\n    the \ufb01le GasFlowData.csv, which can be found on the book\u2019s website.\n 7. Suppose that X1 , . . . , Xn are i.i.d. exponential(\u03b8). Show that the MLE of\n    \u03b8 is X.\n 8. For any univariate parameter \u03b8 and estimator \u03b8,     \u0002 we de\ufb01ne the bias to be\n           \u0002        \u0002\n    Bias(\u03b8) = E(\u03b8) \u2212 \u03b8 and the MSE (mean square error) to be MSE(\u03b8)          \u0002 =\n    E(\u03b8\u0002 \u2212 \u03b8) . Show that\n               2",
        "start": 2614,
        "end": 3458
      }
    ]
  },
  {
    "content": "5.20 Exercises    133\n\n    the variable y is the number of defaults in a month and x is the value\n    for that month of an economic variable thought to a\ufb00ect the default rate.\n    The function dpois computes the Poisson density.\n       start =c(1,1)\n       loglik = function(theta) {-sum(log(dpois(y,\n          lambda = exp(theta[1] + theta[2] * x))))}\n       mle = optim(start, loglik, hessian = TRUE)\n       invFishInfo = solve(mle$hessian)\n       options(digits = 4)\n       mle$par\n       mle$value\n       mle$convergence\n       sqrt(diag(invFishInfo))\n\n    The output is\n       > mle$par\n       [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.20",
      "section_title": "Exercises    133",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mle$par\n       [1]",
        "start": 582,
        "end": 603
      }
    ]
  },
  {
    "content": "1.0773 0.4529\n       > mle$value\n       [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0773",
      "section_title": "0.4529",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mle$value\n       [1]",
        "start": 21,
        "end": 44
      }
    ]
  },
  {
    "content": "602.4\n       > mle$convergence\n       [1] 0\n       > sqrt(diag(invFishInfo))\n       [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "602.4",
      "section_title": "> mle$convergence",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mle$convergence\n       [1] 0\n       > sqrt(diag(invFishInfo))\n       [1]",
        "start": 13,
        "end": 88
      }
    ]
  },
  {
    "content": "0.08742 0.03912\n\n    (a) Describe the statistical model being used here.\n    (b) What are the parameter estimates?\n    (c) Find 95 % con\ufb01dence intervals for the parameters in the model. Use a\n        normal approximation.\n12. In this problem you will \ufb01t a t-distribution by maximum likelihood to the\n    daily log returns for BMW. The data are in the data set bmw that is part\n    of the evir package. Run the following code:\n       library(evir)\n       library(fGarch)\n       data(bmw)\n       start_bmw = c(mean(bmw), sd(bmw), 4)\n       loglik_bmw = function(theta)\n       {\n       -sum(dstd(bmw, mean = theta[1], sd = theta[2],\n          nu = theta[3], log = TRUE))\n       }\n       mle_bmw = optim(start_bmw, loglik_bmw, hessian = TRUE)\n       CovMLE_bmw = solve(mle_bmw$hessian)\n\n    Note: The R code de\ufb01nes a function loglik bmw that is minus the log-\n    likelihood. See Chap. 10 of An Introduction to R for more information\n    about functions in R. Also, see page 59 of this manual for more about max-\n    imum likelihood estimation in R. optim minimizes this objective function\n\f134      5 Modeling Univariate Distributions\n\n    and returns the MLE (which is mle bmw$par) and other information,\n    including the Hessian of the objective function evaluated at the MLE\n    (because hessian=TRUE\u2014the default is not to return the Hessian).\n    (a) What does the function dstd do, and what package is it in?\n    (b) What does the function solve do?\n    (c) What is the estimate of \u03bd, the degrees-of-freedom parameter?\n    (d) What is the standard error of \u03bd?\n13. In this problem, you will \ufb01t a t-distribution to daily log returns of Siemens.\n    You will estimate the degrees-of-freedom parameter graphically and then\n    by maximum likelihood. Run the following code, which produces a 3 \u00d7\n    2 matrix of probability plots. If you wish, add reference lines as done in\n    Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08742",
      "section_title": "0.03912",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.10.1.\n         library(evir)\n         data(siemens)\n         n = length(siemens)\n         par(mfrow = c(3, 2))\n         qqplot(siemens, qt(((1 : n) - 0.5) / n, 2),\n            ylab = \"t(2) quantiles\",\n            xlab = \"data quantiles\")\n         qqplot(siemens,qt(((1:n)-.5)/n,3),ylab=\"t(3) quantiles\",\n            xlab=\"data quantiles\")\n         qqplot(siemens,qt(((1:n)-.5)/n,4),ylab=\"t(4) quantiles\",\n            xlab=\"data quantiles\")\n         qqplot(siemens,qt(((1:n)-.5)/n,5),ylab=\"t(5) quantiles\",\n            xlab=\"data quantiles\")\n         qqplot(siemens,qt(((1:n)-.5)/n,8),ylab=\"t(8) quantiles\",\n            xlab=\"data quantiles\")\n         qqplot(siemens,qt(((1:n)-.5)/n,12),ylab=\"t(12) quantiles\",\n            xlab=\"data quantiles\")\n\n      R has excellent graphics capabilities\u2014see Chap. 12 of An Introduction to\n      R for more about R graphics and, in particular, pages 67 and 72 for more\n      information about par and mfrow, respectively.\n      (a) Do the returns have lighter or heavier tails than a t-distribution with\n          2 degrees of freedom?\n      (b) Based on the QQ plots, what seems like a reasonable estimate of \u03bd?\n      (c) What is the MLE of \u03bd for the Siemens log returns?\n\n\nReferences\nArellano-Valle, R. B., and Azzalini, A. (2013) The centred parameterization\n  and related quantities of the skew-t distribution. Journal of Multivariate\n  Analysis, 113, 73\u201390.\nAzzalini, A. (2014) The Skew-Normal and Related Families (Institute of Math-\n  ematical Statistics Monographs, Book 3), Cambridge University Press.\n\f                                                           References    135\n\nAzzalini, A., and Capitanio, A. (2003) Distributions generated by pertur-\n  bation of symmetry with emphasis on a multivariate skew t distribution.\n  Journal of the Royal Statistics Society, Series B, 65, 367\u2013389.\nBoos, D. D., and Stefanski, L. A. (2013) Essential Statistical Inference,\n  Springer.\nBox, G. E. P., and Dox, D. R. (1964) An analysis of transformations. Journal\n  of the Royal Statistical Society, Series B, 26 211\u2013246.\nBuch-Larsen, T., Nielsen, J. P., Guille\u0301n, M., and Bolance, C. (2005), Kernel\n  density estimation for heavy-tailed distributions using the champernowne\n  transformation. Statistics, 39, 503\u2013518.\nBurnham, K. P. and Anderson, D. R. (2002) Model Selection and Multimodel\n  Inference, Springer, New York.\nCasella, G. and Berger, R. L. (2002) Statistical Inference, 2nd ed., Duxbury/\n  Thomson Learning, Paci\ufb01c Grove, CA.\nEfron, B., and Hinkley, D. V. (1978) Assessing the accuracy of the maxi-\n  mum likelihood estimator: Observed versus expected Fisher information.\n  Biometrika, 65, 457\u2013487.\nFernandez, C., and Steel, M. F. J. (1998) On Bayesian Modelling of fat tails\n  and skewness, Journal of the American Statistical Association, 93, 359\u2013371.\nHurvich, C. M., and Tsai, C-L. (1989) Regression and time series model se-\n  lection in small samples. Biometrika, 76, 297\u2013307.\nLehmann, E. L. (1999) Elements of Large-Sample Theory, Springer-Verlag,\n  New York.\nRuppert, D., and Wand, M. P. (1992) Correction for kurtosis in density esti-\n  mation. Australian Journal of Statistics, 34, 19\u201329.\nSelf, S. G., and Liang, K. Y. (1987) Asymptotic properties of maximum like-\n  lihood estimators and likelihood ratio tests under non-standard conditions.\n  Journal of the American Statistical Association, 82, 605\u2013610.\nSer\ufb02ing, R. J. (1980) Approximation Theorems of Mathematical Statistics,\n  Wiley, New York.\nvan der Vaart, A. W. (1998) Asymptotic Statistics, Cambridge University\n  Press, Cambridge.\nWasserman, L. (2004) All of Statistics, Springer, New York.\n\f6\nResampling\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.10",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1 Introduction\n\nFinding a single set of estimates for the parameters in a statistical model is not\nenough. An assessment of the uncertainty in these estimates is also needed.\nStandard errors and con\ufb01dence intervals are common methods for expressing\nuncertainty.1 In the past, it was sometimes di\ufb03cult, if not impossible, to\nassess uncertainty, especially for complex models. Fortunately, the speed of\nmodern computers, and the innovations in statistical methodology inspired\nby this speed, have largely overcome this problem. In this chapter we apply\na computer simulation technique called the \u201cbootstrap\u201d or \u201cresampling\u201d to\n\ufb01nd standard errors and con\ufb01dence intervals. The bootstrap method is very\nwidely applicable and will be used extensively in the remainder of this book.\nThe bootstrap is one way that modern computing has revolutionized statistics.\nMarkov chain Monte Carlo (MCMC) is another; see Chap. 20.\n    The term \u201cbootstrap\u201d was coined by Bradley Efron (1979) and comes from\nthe phrase \u201cpulling oneself up by one\u2019s bootstraps.\u201d\n    When statistics are computed from a randomly chosen sample, then these\nstatistics are random variables. Students often do not appreciate this fact.\nAfter all, what could be random about Y ? We just averaged the data, so\nwhat is random? The point is that the sample is only one of many possible\nsamples. Each possible sample gives a di\ufb00erent value of Y . Thus, although we\nonly see one value of Y , it was selected at random from the many possible\nvalues and therefore Y is a random variable.\n    Methods of statistical inference such as con\ufb01dence intervals and hypoth-\nesis tests are predicated on the randomness of statistics. For example, the\n1\n    See Appendices A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2 and A.17 for introductions to standard errors and con\ufb01-\n    dence intervals.\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                               137\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 6\n\f138    6 Resampling\n\ncon\ufb01dence coe\ufb03cient of a con\ufb01dence interval tells us the probability, before\na random sample is taken, that an interval constructed from the sample will\ncontain the parameter. Therefore, by the law of large numbers, the con\ufb01dence\ncoe\ufb03cient is also the long-run frequency of intervals that cover their parame-\nter. Con\ufb01dence intervals are usually derived using probability theory. Often,\nhowever, the necessary probability calculations are intractable, and in such\ncases we can replace theoretical calculations by Monte Carlo simulation.\n    But how do we simulate sampling from an unknown population? The an-\nswer, of course, is that we cannot do this exactly. However, a sample is a\ngood representative of the population, and we can simulate sampling from\nthe population by sampling from the sample, which is called resampling.\n    Each resample has the same sample size n as the original sample. The rea-\nson for this is that we are trying to simulate the original sampling, so we want\nthe resampling to be as similar as possible to the original sampling. By boot-\nstrap approximation, we mean the approximation of the sampling process by\nresampling.\n    There are two basic resampling methods, model-free and model-based,\nwhich are also known, respectively, as nonparametric and parametric. In this\nchapter, we assume that we have an i.i.d. sample from some population.\nFor dependent data, resampling requires di\ufb00erent techniques, which will be\ndiscussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "16.2",
      "section_title": "and A.17 for introductions to standard errors and con\ufb01-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.6.\n    In model-free resampling, the resamples are drawn with replacement from\nthe original sample. Why with replacement? The reason is that only sampling\nwith replacement gives independent observations, and we want the resamples\nto be i.i.d. just as the original sample. In fact, if the resamples were drawn\nwithout replacement, then every resample would be exactly the same as the\noriginal sample, so the resamples would show no random variation. This would\nnot be very satisfactory, of course.\n    Model-based resampling does not take a sample from the original sample.\nInstead, one assumes that the original sample was drawn i.i.d. from a density\nin the parametric family, {f (y|\u03b8) : \u03b8 \u2208 \u0398}, so, for an unknown value of\n\u03b8, f (y|\u03b8) is the population density. The resamples are drawn i.i.d. from the\n              \u0002 where \u03b8\ndensity f (y|\u03b8),         \u0002 is some estimate of the parameter vector \u03b8.\n    The number of resamples taken should, in general, be large. Just how large\ndepends on the context and is discussed more fully later. Sometimes thousands\nor even tens of thousands of resamples are used. We let B denote the number\nof resamples.\n    When reading the following section, keep in mind that with resampling,\nthe original sample plays the role of the population, because the resamples\nare taken from the original sample. Therefore, estimates from the sample play\nthe role of true population parameters.\n\f            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "13.6",
      "section_title": "In model-free resampling, the resamples are drawn with replacement from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2 Bootstrap Estimates of Bias, Standard Deviation, and MSE          139\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "Bootstrap Estimates of Bias, Standard Deviation, and MSE          139",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2 Bootstrap Estimates of Bias, Standard Deviation,\nand MSE\nLet \u03b8 be a one-dimensional parameter, let \u03b8\u0002 be its estimate from the sam-\nple, and let \u03b8\u00021\u2217 , . . . , \u03b8\u0002B\n                              \u2217\n                                  be estimates from B resamples. Also, de\ufb01ne \u03b8\u0002\u2217 to be\nthe mean of \u03b8\u00021 , . . . , \u03b8\u0002B . An asterisk indicates a statistic calculated from a\n                  \u2217             \u2217\n\nresample.\n    The bias of \u03b8\u0002 is de\ufb01ned as BIAS(\u03b8)        \u0002 = E(\u03b8)\n                                                      \u0002 \u2212 \u03b8. Since expectations, which\nare population averages, are estimated by averaging over resamples, the boot-\nstrap estimate of bias is\n\n                                        \u0002 = \u03b8\u0002\u2217 \u2212 \u03b8.\n                              BIASboot (\u03b8)        \u0002                             (6.1)\n\nNotice that, as discussed in the last paragraph of the previous section, in the\nbootstrap estimate of bias, the unknown population parameter \u03b8 is replaced\nby the estimate \u03b8\u0002 from the sample. The bootstrap standard error for \u03b8\u0002 is the\nsample standard deviation of \u03b8\u00021\u2217 , . . . , \u03b8\u0002B\n                                              \u2217\n                                                , that is,\n                                   *\n                                   +\n                                   + 1             B\n                      sboot (\u03b8) = ,\n                             \u0002                       (\u03b8\u0002b\u2217 \u2212 \u03b8\u0002\u2217 )2 .     (6.2)\n                                        B\u22121\n                                              b=1\n\n       \u0002 estimates the standard deviation of \u03b8.\nsboot (\u03b8)                                    \u0002\n   The mean-squared error (MSE) of \u03b8 is E(\u03b8\u0002 \u2212 \u03b8)2 and is estimated by\n                                      \u0002\n\n                                              B\n                                   \u0002 =    1\n                          MSEboot (\u03b8)               (\u03b8\u0002b\u2217 \u2212 \u03b8)\n                                                            \u0002 2.\n                                          B\n                                              b=1\n\nAs in the estimation of bias, when estimating MSE, the unknown \u03b8 is replaced\n   \u0002 The MSE re\ufb02ects both bias and variability and, in fact,\nby \u03b8.\n                               \u0002 \u2248 BIAS2 (\u03b8)\n                      MSEboot (\u03b8)           \u0002 + s2 (\u03b8).\n                                                      \u0002                         (6.3)\n                                       boot      boot\n\nWe would have equality in (6.3), rather than an approximation, if in the\ndenominator of (6.1) we used B rather than B \u2212 1. Since B is usually large,\nthe error of the approximation is typically very small.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "Bootstrap Estimates of Bias, Standard Deviation,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2.1 Bootstrapping the MLE of the t-Distribution\n\nFunctions that compute the MLE, such as, fitdistr() in R, usually compute\nstandard errors for the MLE along with the estimates themselves. The stan-\ndard errors are justi\ufb01ed theoretically by an \u201casymptotic\u201d or \u201clarge-sample\u201d\napproximation, called the CLT (central limit theorem) for the maximum like-\nlihood estimator.2 This approximation becomes exact only as the sample size\n2\n    See Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "1 Bootstrapping the MLE of the t-Distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10.\n\f140    6 Resampling\n\nincreases to \u221e. Since a sample size is always \ufb01nite, one cannot be sure of the\naccuracy of the standard errors. Computing standard errors by the bootstrap\ncan serve as a check on the accuracy of the large-sample approximation, as\nillustrated in the following example.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.10",
      "section_title": "140    6 Resampling",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. Bootstrapping GE Daily Returns\n\n    This example uses the GE daily returns from January 3, 1969, to Decem-\nber 31, 1998, in the data set CRSPday in R\u2019s Ecdat package. The sample size\nis 2,528 and the number of resamples is B = 1,000. The t-distribution was\n\ufb01t using fitdistr() in R and the model-free bootstrap was used. The \ufb01rst\nand third lines in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "Bootstrapping GE Daily Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1 are the estimates and standard errors returned\nby fitdistr(), which uses observed Fisher information to calculate standard\nerrors. The second and fourth lines have the results from bootstrapping. The\ndi\ufb00erences between \u201cEstimate\u201d and \u201cBootstrap mean\u201d are the bootstrap es-\ntimates of bias. We can see that the biases are small relative to the standard\nerrors in the row labeled \u201cSE.\u201d Small, and even negligible, bias is common\nwhen the sample size is in the thousands, as in this example.\n\n\nTable ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "are the estimates and standard errors returned",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. Estimates from \ufb01tting a t-distribution to the 2,528 GE daily returns.\n\u201cEstimate\u201d = MLE. \u201cSE\u201d is standard error from observed Fisher information re-\nturned by the R function fitdistr(). \u201cBootstrap mean\u201d and \u201cBootstrap SE\u201d are the\nsample mean and standard deviation of the maximum likelihood estimates from 1,000\nbootstrap samples. \u03bd is the degrees-of-freedom parameter. The model-free bootstrap\nwas used.\n                                         \u03bc          \u03c3       \u03bd\n                      Estimate       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "Estimates from \ufb01tting a t-distribution to the 2,528 GE daily returns.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000879    0.0113    6.34\n                   Bootstrap mean    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000879",
      "section_title": "0.0113    6.34",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000874    0.0113    6.30\n                         SE          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000874",
      "section_title": "0.0113    6.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000253   0.000264   0.73\n                    Bootstrap SE     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000253",
      "section_title": "0.000264   0.73",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000252   0.000266   0.82\n\n\n\n   It is reassuring that \u201cSE\u201d and \u201cBootstrap SE\u201d agree as closely as they\ndo in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000252",
      "section_title": "0.000266   0.82",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. This is an indication that both are reliable estimates of the\nuncertainty in the parameter estimates. Such close agreement is more likely\nwith samples as large as this one.                                          \u0002\n1 library(bootstrap)\n2 library(MASS)\n3 set.seed(\"3857\")\n\n4 data(CRSPday, package = \"Ecdat\")\n\n5 ge = CRSPday[ ,4]\n\n6 nboot = 1000\n\n7 t_mle = function(x){as.vector(fitdistr(x, \"t\")$estimate)}\n\n8 results = bootstrap(ge, nboot, t_mle)\n\f          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "This is an indication that both are reliable estimates of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2 Bootstrap Estimates of Bias, Standard Deviation, and MSE         141\n\n9  rowMeans(results$thetastar[ , ])\n10 apply(results$thetastar[,], 1, sd)\n11 fitdistr(ge, \"t\")\n\n\n    The code above computes the results reported in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "Bootstrap Estimates of Bias, Standard Deviation, and MSE         141",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. The boot-\nstrap was performed at line 8 by the function bootstrap() in the bootstrap\npackage which is loaded at line 1. The function bootstrap() has three argu-\nments, the data, the value of B, and the function that computes the statistic\nto be bootstrapped; in this example that function is t mle() which is de\ufb01ned\nat line 7. The function fitdistr() is in the package MASS that is loaded at\nline 2. Lines 9, 10, and 11 compute, respectively, the bootstrap mean, the\nbootstrap SEs, and the MLE and its standard errors.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "The boot-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2. Bootstrapping GE daily returns, continued\n\n    To illustrate the bootstrap for a smaller sample size, we now use only the\n\ufb01rst 250 daily GE returns, approximately the \ufb01rst year of data. The number\nof bootstrap samples is 1,000. The results are in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "Bootstrapping GE daily returns, continued",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2. For \u03bc and s, the\nresults in Tables ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "For \u03bc and s, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1 and 6.2 are comparable though the standard errors in\nTable ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "and 6.2 are comparable though the standard errors in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2 are, of course, larger because of the smaller sample size. For the\nparameter \u03bd, the results in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "are, of course, larger because of the smaller sample size. For the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2 are di\ufb00erent in two respects from those\nin Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "are di\ufb00erent in two respects from those",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. First, the estimate and the bootstrap mean di\ufb00er by more than 1,\na sign that there is some bias. Second, the bootstrap standard deviation is\n2.99, considerably larger than the SE, which is only ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "First, the estimate and the bootstrap mean di\ufb00er by more than 1,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.97. This suggests that\nthe SE, which is based on large-sample theory, speci\ufb01cally the CLT for the\nMLE, is not an accurate measure of uncertainty in the parameter \u03bd, at least\nnot for the smaller sample.\n\n\nTable ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.97",
      "section_title": "This suggests that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2. Estimates from \ufb01tting a t-distribution to the \ufb01rst 250 GE daily returns.\nNotation as in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "Estimates from \ufb01tting a t-distribution to the \ufb01rst 250 GE daily returns.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. The nonparametric bootstrap was used.\n                                        \u03bc        \u03c3     \u03bd\n                       Estimate     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "The nonparametric bootstrap was used.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00142 0.01055 5.52\n                     Bootstrap mean ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00142",
      "section_title": "0.01055 5.52",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00145 0.01064 6.77\n                           SE       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00145",
      "section_title": "0.01064 6.77",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000764 0.000817 1.98\n                      Bootstrap SE ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000764",
      "section_title": "0.000817 1.98",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000777 0.000849 2.99\n\n\n\n    To gain some insight about why the results of \u03bd in these two tables dis-\nagree, kernel density estimates of the two bootstrap samples were plotted in\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.000777",
      "section_title": "0.000849 2.99",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. We see that with the smaller sample size in panel (a), the density is\nbimodal and has noticeable right skewness. The density with the full sample\nis unimodal and has much less skewness.\n    Tail-weight parameters such as \u03bd are di\ufb03cult to estimate unless the sample\nsize is in the thousands. With smaller sample sizes, such as 250, there will\n\f142    6 Resampling\n\nnot be enough extreme observations to obtain a precise estimate of the tail-\nweight parameters. This problem has been nicely illustrated by the bootstrap.\nThe number of extreme observations will vary between bootstrap samples.\nThe bootstrap samples with fewer extreme values will have larger estimates\nof \u03bd, since larger values of \u03bd correspond to thinner tails.\n\n        a                     n = 250          b                   n = 2528\n\n\n\n\n                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "We see that with the smaller sample size in panel (a), the density is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                   0.20\n\n\n\n\n                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "0.20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n         Density\n\n\n\n\n                                               Density\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n\n\n\n\n                                                         0.2\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                                                         0.0\n                          5   10     15   20                   5    10     15   20\n                                df                                    df\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.1. Kernel density estimates of 1,000 bootstrap estimates of df using (a) the\n\ufb01rst 250 daily GE returns and (b) all 2,528 GE returns. The default bandwidth was\nused in R\u2019s density function to create the estimates.\n\n\n    However, even with only 250 observations, \u03bd can be estimated accurately\nenough to show, for example, that for the GE daily returns \u03bd is very likely\nless than 13, the 98th percentile of the bootstrap distribution of \u03bd. Therefore,\nthe bootstrap provides strong evidence that the normal model corresponding\nto \u03bd = \u221e is not as satisfactory as a t-model.\n    By the CLT for the MLE, we know that the MLE is nearly normally\ndistributed for large enough values of n. But this theorem does not tell us\nhow large is large enough. To answer that question, we can use the bootstrap.\nWe have seen here that n = 250 is not large enough for near normality of \u03bd\u0002,\nand, though n = 2,528 is su\ufb03ciently large so that the bootstrap distribution\nis unimodal, there is still some right skewness when n = 2,528.               \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.1",
      "section_title": "Kernel density estimates of 1,000 bootstrap estimates of df using (a) the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3 Bootstrap Con\ufb01dence Intervals\nBesides its use in estimating bias and \ufb01nding standard errors, the bootstrap\nis widely used to construct con\ufb01dence intervals. There are many bootstrap\ncon\ufb01dence intervals and some are quite sophisticated. We can only describe\na few and the reader is pointed to the references in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4 for additional\ninformation.\n    Except in certain simple cases, con\ufb01dence intervals are based on approxi-\nmations such as the CLT for the MLE. The bootstrap is based on the approx-\nimation of the population\u2019s probability distribution using the sample. When\na con\ufb01dence interval uses an approximation, there are two coverage probabil-\nities, the nominal one that is stated and the actual one that is unknown. Only\nfor exact con\ufb01dence intervals making no use of approximations will the two\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.4",
      "section_title": "for additional",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3 Bootstrap Con\ufb01dence Intervals      143\n\nprobabilities be equal. By the \u201caccuracy\u201d of a con\ufb01dence interval, we mean the\ndegree of agreement between the nominal and actual coverage probabilities.\nEven exact con\ufb01dence intervals such as (A.44) for a normal mean and (A.45)\nfor a normal variance are exact only when the data meet the assumptions\nexactly, e.g., are exactly normally distributed.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals      143",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.1 Normal Approximation Interval\n\nLet \u03b8\u0002 be an estimate of \u03b8 and let sboot (\u03b8)\n                                          \u0002 be the estimate of standard error\ngiven by (6.2). Then the normal theory con\ufb01dence interval for \u03b8 is\n\n                                \u03b8\u0002 \u00b1 sboot (\u03b8)\n                                            \u0002 z\u03b1/2 ,                          (6.4)\n\nwhere z\u03b1/2 is the \u03b1/2-upper quantile of the normal distribution. When \u03b8\u0002 is\nan MLE, this interval is essentially the same as (5.20) except that bootstrap,\nrather than the Fisher information, is used to \ufb01nd the standard error.\n    To avoid confusion, it should be emphasized that the normal approxima-\ntion does not assume that the population is normally distributed but only\nthat \u03b8\u0002 is normally distributed by a CLT.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "1 Normal Approximation Interval",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.2 Bootstrap-t Intervals\n                                             \u0002 for example, from Fisher infor-\nOften one has available a standard error for \u03b8,\nmation. In this case, the bootstrap-t method can be used and, compared to\nnormal approximation con\ufb01dence intervals, o\ufb00ers the possibility of more ac-\ncurate con\ufb01dence intervals, that is, with nominal coverage probability closer\nto the actual coverage probability. We start by showing how the bootstrap-t\nmethod is related to the usual t-based con\ufb01dence interval for a normal popu-\nlation mean, and then discuss the general theory.\n\nCon\ufb01dence Intervals for a Population Mean\n\nSuppose we wish to construct a con\ufb01dence interval for the population mean\nbased on a random sample. One starts with the so-called \u201ct-statistic,\u201d3\nwhich is\n                                    \u03bc\u2212Y\n                                t= \u221a .                                (6.5)\n                                    s/ n\n                        \u221a\nThe denominator of t, s/ n, is just the standard error of the mean, so that\nthe denominator estimates the standard deviation of the numerator.\n\n3\n    Actually, t is not quite a statistic since it depends on the unknown \u03bc, whereas\n    a statistic, by de\ufb01nition, is something that depends only on the sample, not on\n    unknown parameters. However, the term \u201ct-statistic\u201d is so widespread that we\n    will use it here.\n\f144    6 Resampling\n\n    If we are sampling from a normally distributed population, then the prob-\nability distribution of t is known to be the t-distribution with n \u2212 1 degrees\nof freedom. Using the notation of Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "2 Bootstrap-t Intervals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.2, we denote by t\u03b1/2,n\u22121 the \u03b1/2\nupper t-value, that is, the \u03b1/2-upper quantile of this distribution. Thus, t in\n(6.5) has probability \u03b1/2 of exceeding t\u03b1/2,n\u22121 . Because of the symmetry of\nthe t-distribution, the probability is also \u03b1/2 that t is less than \u2212t\u03b1/2,n\u22121 .\n    Therefore, for normally distributed data, the probability is 1 \u2212 \u03b1 that\n\n                           \u2212 t\u03b1/2,n\u22121 \u2264 t \u2264 t\u03b1/2,n\u22121 .                      (6.6)\n\nSubstituting (6.5) into (6.6), after a bit of algebra we \ufb01nd that\n                      \u000e                                           \u000f\n                                       s                       s\n          1 \u2212 \u03b1 = P Y \u2212 t\u03b1/2,n\u22121 \u221a \u2264 \u03bc \u2264 Y + t\u03b1/2,n\u22121 \u221a             ,       (6.7)\n                                        n                       n\nwhich shows that\n                                    s\n                               Y \u00b1 \u221a t\u03b1/2,n\u22121\n                                     n\nis a 1 \u2212 \u03b1 con\ufb01dence interval for \u03bc, assuming normally distributed data. This\nis the con\ufb01dence interval given by Eq. (A.44). Note that in (6.7) the random\nvariables are Y and s, and \u03bc is \ufb01xed.\n    What if we are not sampling from a normal distribution? In that case, the\ndistribution of t de\ufb01ned by (6.5) is not the t-distribution, but rather some\nother distribution that is not known to us. There are two problems. First, we\ndo not know the distribution of the population. Second, even if the popula-\ntion distribution were known, it is a di\ufb03cult, usually intractable, probability\ncalculation to get the distribution of the t-statistic from the distribution of\nthe population. This calculation has only been done for normal populations.\nConsidering the di\ufb03culty of these two problems, can we still get a con\ufb01dence\ninterval? The answer is \u201cyes, by resampling.\u201d\n    We start with a large number, say B, of resamples from the original sample.\nLet Y boot,b and sboot,b be the sample mean and standard deviation of the bth\nresample, b = 1, . . . , B, and let Y be the mean of the original sample. De\ufb01ne\n\n                                         Y \u2212 Y boot,b\n                             tboot,b =            \u221a .                       (6.8)\n                                         sboot,b / n\nNotice that tboot,b is de\ufb01ned in the same way as t except for two changes.\nFirst, Y and s in t are replaced by Y boot,b and sboot,b in tboot,b . Second, \u03bc\nin t is replaced by Y in tboot,b . The last point is a bit subtle, and uses the\nprinciple stated at the end of Sect. 6.1\u2014a resample is taken using the original\nsample as the population. Thus, for the resample, the population mean is Y !\n    Because the resamples are independent of each other, the collection tboot,1 ,\ntboot,2 , . . . can be treated as a random sample from the distribution of the\nt-statistic. After B values of tboot,b have been calculated, one from each re-\nsample, we \ufb01nd the \u03b1/2-lower and -upper quantiles of these tboot,b values. Call\nthese percentiles tL and tU .\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "2, we denote by t\u03b1/2,n\u22121 the \u03b1/2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3 Bootstrap Con\ufb01dence Intervals     145\n\n    If the original population is skewed, then there is no reason to suspect\nthat the \u03b1/2-lower quantile is minus the \u03b1/2-upper quantile as happens for\nsymmetric populations such as the t-distribution. In other words, we do not\nnecessarily expect that tL = \u2212tU , but this causes us no problem since the\nbootstrap allows us to estimate tL and tU without assuming any relationship\nbetween them. Now we replace \u2212t\u03b1/2,n\u22121 and t\u03b1/2,n\u22121 in the con\ufb01dence inter-\nval (6.7) by tL and tU , respectively. Finally, the bootstrap con\ufb01dence interval\nfor \u03bc is                   \u0007                          \b\n                                     s             s\n                             Y + tL \u221a , Y + tU \u221a        .                  (6.9)\n                                      n             n\nIn (6.9), Y and s are the mean and standard deviation of the original sample,\nand only tL and tU are calculated from the B bootstrap resamples.\n    The bootstrap has solved both problems mentioned above. One does not\nneed to know the population distribution since we can estimate it by the sam-\nple. A sample isn\u2019t a probability distribution. What is being done is creating\na probability distribution, called the empirical distribution, from the sam-\nple by giving each observation in the sample probability 1/n where n is the\nsample size. Moreover, one doesn\u2019t need to calculate the distribution of the\nt-statistic using probability theory. Instead we can simulate from the empirical\ndistribution.\n\nCon\ufb01dence Interval for a General Parameter\n\nThe method of constructing a t-con\ufb01dence interval for \u03bc can be generalized to\nother parameters. Let \u03b8\u0002 and s(\u03b8)\n                               \u0002 be the estimate of \u03b8 and its standard error\n                                             \u0002 be the same quantities from the\ncalculated from the sample. Let \u03b8\u0002b\u2217 and sb (\u03b8)\nbth bootstrap sample. Then the bth bootstrap t-statistic is\n\n                                             \u03b8\u0002 \u2212 \u03b8\u0002b\u2217\n                                 tboot,b =             .                    (6.10)\n                                                  \u0002\n                                              sb (\u03b8)\n\nAs when estimating a population mean, let tL and tU be the \u03b1/2-lower and\n\u03b1/2-upper sample quantiles of these t-statistics. Then the con\ufb01dence interval\nfor \u03b8 is                &                          '\n                                    \u0002 \u03b8\u0002 + tU s(\u03b8)\n                          \u03b8\u0002 + tL s(\u03b8),          \u0002\n\nsince\n                             \u0012                  \u0014\n                                 \u03b8\u0002 \u2212 \u03b8\u0002b\u2217\n                 1 \u2212 \u03b1 \u2248 P tl \u2264            \u2264 tU                             (6.11)\n                                      \u0002\n                                  sb (\u03b8)\n                          \u0012                    \u0014\n                                 \u03b8 \u2212 \u03b8\u0002\n                       \u2248 P tl \u2264           \u2264 tU                              (6.12)\n                                     \u0002\n                                  s(\u03b8)\n                          (                                )\n                                      \u0002 \u2264 \u03b8 \u2264 \u03b8\u0002 + tU s(\u03b8)\n                       = P \u03b8\u0002 + tL s(\u03b8)                 \u0002 .\n\f146    6 Resampling\n\nThe approximation in (6.11) is due to Monte Carlo error and can be made\nsmall by choosing B large. The approximation in (6.12) is from the bootstrap\napproximation of the population\u2019s distribution by the empirical distribution.\nThe error of the second approximation is independent of B and becomes small\nonly as the sample size n becomes large. Though one generally has no control\nover the sample size, fortunately, sample sizes are often large in \ufb01nancial\nengineering.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals     145",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.3 Basic Bootstrap Interval\n\nLet qL and qU be the \u03b1/2-lower and -upper sample quantiles of \u03b8\u00021\u2217 , . . . , \u03b8\u0002B\n                                                                               \u2217\n                                                                                 .\nThe fraction of bootstrap estimates that satisfy\n\n                                  qL \u2264 \u03b8\u0002b\u2217 \u2264 qU                           (6.13)\n\nis 1 \u2212 \u03b1. But (6.13) is algebraically equivalent to\n\n                           \u03b8\u0002 \u2212 qU \u2264 \u03b8\u0002 \u2212 \u03b8\u0002b\u2217 \u2264 \u03b8\u0002 \u2212 qL ,                 (6.14)\n\nso that \u03b8\u0002 \u2212 qU and \u03b8\u0002 \u2212 qL are lower and upper quantiles for the distribution of\n\u03b8\u0002 \u2212 \u03b8\u0002b\u2217 . The basic bootstrap interval uses them as lower and upper quantiles\n                             \u0002 Using the bootstrap approximation, it is assumed\nfor the distribution of \u03b8 \u2212 \u03b8.\nthat\n                              \u03b8\u0002 \u2212 qU \u2264 \u03b8 \u2212 \u03b8\u0002 \u2264 \u03b8\u0002 \u2212 qL                   (6.15)\nwill occur in a fraction 1 \u2212 \u03b1 of samples. Adding \u03b8\u0002 to each term in (6.15) gives\n2\u03b8\u0002 \u2212 qU \u2264 \u03b8 \u2264 2\u03b8\u0002 \u2212 qL , so that\n\n                               (2\u03b8\u0002 \u2212 qU , 2\u03b8\u0002 \u2212 qL )                      (6.16)\n\nis a con\ufb01dence interval for \u03b8. Interval (6.16) is sometimes called the basic\nbootstrap interval.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "3 Basic Bootstrap Interval",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.4 Percentile Con\ufb01dence Intervals\n\nThere are several bootstrap con\ufb01dence intervals based on the so-called per-\ncentile method. Only one, the basic percentile interval, in discussed here in\ndetail.\n    As in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "4 Percentile Con\ufb01dence Intervals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.3, let qL and qU be the \u03b1/2-lower and -upper sample quan-\ntiles of \u03b8\u00021\u2217 , . . . , \u03b8\u0002B\n                          \u2217\n                            . The basic percentile con\ufb01dence interval is simply\n\n                                     (qL , qU ).                           (6.17)\n\nBy (6.13), the proportion of \u03b8\u0002b\u2217 -values in this interval is 1 \u2212 \u03b1. This interval\ncan be justi\ufb01ed by assuming that \u03b8\u0002\u2217 is distributed symmetrically about \u03b8.      \u0002\n                                                         \u0002\nThis assumption implies that for some C > 0, qL = \u03b8 \u2212 C and qU = \u03b8 + C.    \u0002\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "3, let qL and qU be the \u03b1/2-lower and -upper sample quan-",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, qL = \u03b8 \u2212 C and qU = \u03b8 + C.    \u0002",
        "start": 546,
        "end": 624
      }
    ]
  },
  {
    "content": "6.3 Bootstrap Con\ufb01dence Intervals        147\n\nThen 2\u03b8\u0002 \u2212 qU = qL and 2\u03b8\u0002 \u2212 qL = qU , so the basic bootstrap interval (6.16)\ncoincides with the basic percentile interval (6.17).\n    What if \u03b8\u0002\u2217 is not distributed symmetrically about \u03b8?      \u0002 Fortunately, not all\nis lost. As discussed in Sect. 4.6, often random variables can be transformed\nto have a symmetric distribution. So, now assume only that for some mono-\ntonically increasing function g, g(\u03b8\u0002\u2217 ) is symmetrically distributed about g(\u03b8).  \u0002\nAs we will now see, this weaker assumption is all that is needed to justify\nthe basic percentile interval. Because g is monotonically strictly increasing\nand quantiles are transformation-respecting,4 g(qL ) and g(qU ) are lower- and\nupper-\u03b1/2 quantiles of g(\u03b8\u00021\u2217 ), . . . , g(\u03b8\u0002B\n                                             \u2217\n                                               ), and the basic percentile con\ufb01dence\ninterval for g(\u03b8) is\n                                  {g(qL ), g(qU )}.                            (6.18)\nNow, if (6.18) has coverage probability (1 \u2212 \u03b1) for g(\u03b8), then, since g is mono-\ntonically increasing, (6.17) has coverage probability (1\u2212\u03b1) for \u03b8. This justi\ufb01es\nthe percentile interval, at least if one is willing to assume the existence of a\ntransformation to symmetry. Note that it is only assumed that such a g exists,\nnot that it is known. No knowledge of g is necessary, since g is not used to\nconstruct the percentile interval.\n    The basic percentile method is simple, but it is not considered very accu-\nrate, except for large sample sizes. There are two problems with the percentile\nmethod. The \ufb01rst is an assumption of unbiasedness. The basic percentile in-\nterval assumes not only that g(\u03b8\u0002\u2217 ) is distributed symmetrically, but also that\nit is symmetric about g(\u03b8) \u0002 rather than g(\u03b8)  \u0002 plus some bias. Most estima-\ntors satisfy a CLT, e.g., the CLTs for sample quantiles and for the MLE in\nSects. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals        147",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.1 and 5.10, respectively. Therefore, bias becomes negligible in large\nenough samples, but in practice the sample size might not be su\ufb03ciently large\nand bias can cause the nominal and actual coverage probabilities to di\ufb00er.\n    The second problem is that \u03b8\u0002 may have a nonconstant variance, a problem\ncalled heteroskedasticity. If \u03b8\u0002 is the MLE, then the variance of \u03b8\u0002 is, at least\napproximately, the inverse of Fisher information and the Fisher information\nneed not be constant\u2014it often depends on \u03b8.\n    More sophisticated percentile methods can correct for bias and het-\neroskedasticity. The BCa and ABC (approximate bootstrap con\ufb01dence) per-\ncentile intervals are improved percentile intervals in common use. In the name\n\u201cBCa ,\u201d \u201cBC\u201d means \u201cbias-corrected\u201d and \u201ca\u201d means \u201caccelerated,\u201d which\nrefers to the rate at which the variance changes with \u03b8. The BCa method\nautomatically estimates both the bias and the rate of change of the vari-\nance and then makes suitable adjustments. The theory behind the BCa and\nABC intervals is beyond the scope of this book, but is discussed in refer-\nences found in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.3",
      "section_title": "1 and 5.10, respectively. Therefore, bias becomes negligible in large",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4. Both the BCa and ABC methods have been im-\nplemented in statistical software such as R. In R\u2019s bootstrap package, the\nfunctions bcanon(), abcpar(), and abcnon() implement the nonparametric\nBCa , parametric ABC, and nonparametric ABC intervals, respectively.\n4\n    See Appendix A.",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.4",
      "section_title": "Both the BCa and ABC methods have been im-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2.\n\f148      6 Resampling\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.2",
      "section_title": "148      6 Resampling",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3. Con\ufb01dence interval for a quantile-based tail-weight parameter\n\n    It was mentioned in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Con\ufb01dence interval for a quantile-based tail-weight parameter",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.8 that a quantile-based parameter quantifying\ntail weight can be de\ufb01ned as the ratio of two scale parameters:\n                                   s(p1 , 1 \u2212 p1 )\n                                                   ,                    (6.19)\n                                   s(p2 , 1 \u2212 p2 )\nwhere\n                                   F \u22121 (p2 ) \u2212 F \u22121 (p1 )\n                        s(p1 , p2 ) =                      ,\n                                              a\na is a positive constant that does not a\ufb00ect the ratio (6.19) and so can be\nignored, and 0 < p1 < p2 < 1/2. We will call (6.19) quKurt. Finding a\ncon\ufb01dence interval for quKurt can be a daunting task without the bootstrap,\nbut with the bootstrap it is simple. In this example, BCa con\ufb01dence intervals\nwill be found for quKurt. The parameter is computed from a sample y by this\nR function, which has default values p1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.8",
      "section_title": "that a quantile-based parameter quantifying",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025 and p2 = 0.25:\n      quKurt = function(y, p1 = 0.025, p2 = 0.25)\n      {\n      Q = quantile(y, c(p1, p2, 1 - p2, 1 - p1))\n      (Q[4] - Q[1]) / (Q[3] - Q[2])\n      }\nThe BCa intervals are found with the bcanon() function in the bootstrap\npackage using B = 5,000. The seed of the random number generator was \ufb01xed\nso that these results can be reproduced.\n      bmw = read.csv(\"bmw.csv\")\n      library(\"bootstrap\")\n      set.seed(\"5640\")\n      bca_kurt = bcanon(bmwRet[ ,2], 5000, quKurt)\n      bca_kurt$confpoints\nBy default, the output gives four pairs of con\ufb01dence limits.\n      > bca_kurt$confpoints\n           alpha bca point\n      [1,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.025",
      "section_title": "and p2 = 0.25:",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "bca_kurt$confpoints\n           alpha bca point\n      [1,]",
        "start": 585,
        "end": 645
      }
    ]
  },
  {
    "content": "0.025      4.07\n      [2,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.025",
      "section_title": "4.07",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.050      4.10\n      [3,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.050",
      "section_title": "4.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.100      4.14\n      [4,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.100",
      "section_title": "4.14",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.160      4.18\n      [5,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.160",
      "section_title": "4.18",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.840      4.41\n      [6,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.840",
      "section_title": "4.41",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.900      4.45\n      [7,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.900",
      "section_title": "4.45",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.950      4.50\n      [8,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.950",
      "section_title": "4.50",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.975      4.54\nThe results above show, for example, that the 90 % BCa con\ufb01dence interval is\n(4.10, 4.50). For reference, any normal distribution has quKurt equal 2.91, so\nthese data have heavier than Gaussian tails, at least as measured by quKurt.\n\u0002\n\f                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.975",
      "section_title": "4.54",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3 Bootstrap Con\ufb01dence Intervals   149\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals   149",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4. Con\ufb01dence interval for the ratio of two quantile-based tail-weight\nparameters\n\n\n\n\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.4",
      "section_title": "Con\ufb01dence interval for the ratio of two quantile-based tail-weight",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                     0.4\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n              CSGS\n                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "CSGS",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                     \u22120.4 \u22120.2\n\n\n\n\n                                 \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22120.4 \u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 \u22120.10 \u22120.05 0.00   0.05   0.10   0.15\n                                                   LSCC\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.15",
      "section_title": "\u22120.10 \u22120.05 0.00   0.05   0.10   0.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.2. QQ plot of returns on two stocks in the midcapD.ts data set. The reference\nline goes through the \ufb01rst and third quartiles.\n\n\n    This example uses the data set midcapD.ts.csv of returns on midcap\nstocks. Two of the stocks in this data set are LSCC and CSGS. From Fig. 6.2,\nwhich is a QQ plot comparing the returns from these two companies, it appears\nthat LSCC returns have lighter tails than CSGS returns. The values of quKurt\nare ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.2",
      "section_title": "QQ plot of returns on two stocks in the midcapD.ts data set. The reference",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.91 and 4.13 for LSCC and GSGS, respectively, and the ratio of the two\nvalues is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.91",
      "section_title": "and 4.13 for LSCC and GSGS, respectively, and the ratio of the two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.704. This is further evidence that LSCC returns have the lesser\ntail weight. A BCa con\ufb01dence interval for the ratio of quKurt for LSCC and\nCSGS is found with the following R program.\n1  midcapD.ts = read.csv(\"midcapD.ts.csv\")\n2  attach(midcapD.ts)\n 3 quKurt = function(y, p1 = 0.025, p2 = 0.25)\n\n 4 {\n\n 5    Q = quantile(y, c(p1, p2, 1 - p2, 1 - p1))\n 6    as.numeric((Q[4] - Q[1]) / (Q[3] - Q[2]))\n 7 }\n\n 8 compareQuKurt = function(x, p1 = 0.025, p2 = 0.25, xdata)\n\n 9 {\n\n10    quKurt(xdata[x,1], p1, p2) / quKurt(xdata[x,2], p1, p2)\n11 }\n\n12 quKurt(LSCC)\n\n13 quKurt(CSGS)\n\n14 xdata = cbind(LSCC, CSGS)\n\n15 compareQuKurt(1:n, xdata = xdata)\n\n16 library(\"bootstrap\")\n\f150      6 Resampling\n\n17 set.seed(\"5640\")\n18 bca_kurt = bcanon((1:n), 5000, compareQuKurt, xdata = xdata)\n19 bca_kurt$confpoints\n\n    The function compareQuKurt() (lines 8\u201311) computes a quKurt ratio. The\nfunction bcanon() is designed to bootstrap a vector, but this example has\nbivariate data in a matrix with two columns. To bootstrap multivariate data,\nthere is a trick given in R\u2019s help for bcanon()\u2014bootstrap the integers 1 to n\nwhere n is the sample size. This is done at line 18. The resamples of 1, . . . , n\nallow one to resample the rows of the data vector. Thus, in this example\nbcanon() draws a random sample with replacement from 1, . . . , n and selects\nthe rows of xdata corresponding to these indices to create a resample.\n    The 95 % con\ufb01dence interval for the quKurt ratio is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.704",
      "section_title": "This is further evidence that LSCC returns have the lesser",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.587 to 0.897, so with\n95 % con\ufb01dence it can be concluded that LSCC has a smaller value of quKurt.\n      > bca_kurt$confpoints\n           alpha bca point\n      [1,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.587",
      "section_title": "to 0.897, so with",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "bca_kurt$confpoints\n           alpha bca point\n      [1,]",
        "start": 106,
        "end": 166
      }
    ]
  },
  {
    "content": "0.025     0.587\n      [2,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.025",
      "section_title": "0.587",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.050     0.607\n      [3,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.050",
      "section_title": "0.607",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.100     0.634\n      [4,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.100",
      "section_title": "0.634",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.160     0.653\n      [5,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.160",
      "section_title": "0.653",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.840     0.811\n      [6,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.840",
      "section_title": "0.811",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.900     0.833\n      [7,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.900",
      "section_title": "0.833",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.950     0.864\n      [8,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.950",
      "section_title": "0.864",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.975     0.897\n                                                                                \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.975",
      "section_title": "0.897",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4 Bibliographic Notes\nEfron (1979) introduced the name \u201cbootstrap\u201d and did much to popularize re-\nsampling methods. Efron and Tibshirani (1993), Davison and Hinkley (1997),\nGood (2005), and Chernick (2007) are introductions to the bootstrap that\ndiscuss many topics not treated here, including the theory behind the BCa\nand ABC methods for con\ufb01dence intervals. The R package bootstrap is de-\nscribed by its authors as \u201cfunctions for Efron and Tibshirani (1993)\u201d and the\npackage contains the data sets used in that book. The R package boot is a\nmore recent set of resampling functions and data sets to accompany Davison\nand Hinkley (1997).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.4",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5 R Lab\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5.1 BMW Returns\nThis lab uses a data set containing 6146 daily returns on BMW stock from\nJanuary 3, 1973 to July 23, 1996. Run the following code to \ufb01t a skewed\nt-distribution to the returns and check the \ufb01t with a QQ plot.\n\f                                                                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "1 BMW Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5 R Lab   151\n\n1 library(\"fGarch\")\n2 bmwRet = read.csv(\"bmwRet.csv\")\n3 n = dim(bmwRet)[1]\n\n4\n\n5 kurt = kurtosis(bmwRet[ ,2], method = \"moment\")\n6 skew = skewness(bmwRet[,2], method = \"moment\")\n7 fit_skewt = sstdFit(bmwRet[ ,2])\n\n8\n\n9  q.grid = (1:n) / (n+1)\n10 qqplot(bmwRet[ ,2], qsstd(q.grid, fit_skewt$estimate[1],\n11    fit_skewt$estimate[2],\n12    fit_skewt$estimate[3], fit_skewt$estimate[4]),\n13    ylab = \"skewed-t quantiles\" )\n\nThe function qsstd() is in the fGarch package loaded at line 1. The required\npackage timeDate is also loaded and the function kurtosis() is in timeDate.\n\nProblem 1 What is the MLE of \u03bd? Does the t-distribution with this value of\n\u03bd have a \ufb01nite skewness and kurtosis?\n\n\n    Since the kurtosis coe\ufb03cient based on the fourth central moment is in\ufb01nite\nfor some distributions, as in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "R Lab   151",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4 we will de\ufb01ne a quantile-based kurtosis:\n\n                                   F \u22121 (1 \u2212 p1 ) \u2212 F \u22121 (p1 )\n                    quKurt(F) =                                ,\n                                   F \u22121 (1 \u2212 p2 ) \u2212 F \u22121 (p2 )\n\nwhere F is a CDF and 0 < p1 < p2 < 1/2. Typically, p1 is close to zero\nso that the numerator is sensitive to tail weight and p2 is much larger and\nmeasures dispersion in the center of the distribution. Because the numerator\nand denominator of quKurt are each the di\ufb00erence between two quantiles, they\nare location-free and therefore scale parameters. Moreover, because quKurt is\na ratio of two scale parameters, it is scale-free and therefore a shape parameter.\nA typical example would be p1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.4",
      "section_title": "we will de\ufb01ne a quantile-based kurtosis:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025 and p2 = 0.25. quKurt is estimated\nby replacing the population quantiles by sample quantiles.\n\nProblem 2 Write an R program to plot quKurt for the t-distribution as a\nfunction of \u03bd. Use p1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.025",
      "section_title": "and p2 = 0.25. quKurt is estimated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025 and p2 = 0.25. Let \u03bd take values from 1 to\n10, incremented by ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.025",
      "section_title": "and p2 = 0.25. Let \u03bd take values from 1 to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25. If you want to get fancy while labeling the axes,\nxlab=expression(nu) in the call to plot will put a \u201c\u03bd\u201d on the x-axis.\n\n\n   Run the following code, which de\ufb01nes a function to compute quKurt and\nbootstraps this function on the BMW returns. Note that p1 and p2 are given\ndefault values that are used in the bootstrap and that both model-free and\nmodel-based bootstrap samples are taken.\n\f152      6 Resampling\n\n      quKurt = function(y, p1 = 0.025, p2 = 0.25)\n      {\n          Q = quantile(y, c(p1, p2, 1 - p2, 1 - p1))\n          k = (Q[4] - Q[1]) / (Q[3] - Q[2])\n          k\n      }\n      nboot = 5000\n      ModelFree_kurt = rep(0, nboot)\n      ModelBased_kurt = rep(0, nboot)\n      set.seed(\"5640\")\n      for (i in 1:nboot)\n      {\n          samp_ModelFree = sample(bmwRet[,2], n, replace = TRUE)\n          samp_ModelBased = rsstd(n, fit_skewt$estimate[1],\n            fit_skewt$estimate[2],\n            fit_skewt$estimate[3], fit_skewt$estimate[4])\n          ModelFree_kurt[i] = quKurt(samp_ModelFree)\n          ModelBased_kurt[i] = quKurt(samp_ModelBased)\n      }\n\nProblem 3 Plot KDEs of ModelFree kurt and ModelBased kurt. Also, plot\nside-by-side boxplots of the two samples. Describe any major di\ufb00erences be-\ntween the model-based and model-free results. Include the plots with your work.\n\n\nProblem 4 Find 90 % percentile method bootstrap con\ufb01dence intervals for\nquKurt using the model-based and model-free bootstraps.\n\n\nProblem 5 BCa con\ufb01dence intervals can be constructed using the function\nbcanon() in R\u2019s bootstrap package. Find a 90 % BCa con\ufb01dence interval\nfor quKurt. Use 5,000 resamples. Compare the BCa interval to the model-free\npercentile interval from Problem 4.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "If you want to get fancy while labeling the axes,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5.2 Simulation Study: Bootstrapping the Kurtosis\n\nThe sample kurtosis is highly variable because it is based on the 4th moment.\nAs a result, it is challenging to construct an accurate con\ufb01dence interval for\nthe kurtosis. In this section, \ufb01ve bootstrap con\ufb01dence intervals for the kurtosis\nwill be compared. The comparisons will be on widths of the intervals, where\nsmaller is better, and actual coverage probabilities, where closer to nominal\nis better.\n    Run the following code. Warning: this simulation experiment takes a while\nto run, e.g., 5 to 10 minutes, and will have only moderate accuracy. To in-\ncrease the accuracy, you might wish to increase niter and nboot and run the\nexperiment over a longer period, even overnight.\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "2 Simulation Study: Bootstrapping the Kurtosis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5 R Lab   153\n\n   library(bootstrap)\n   Kurtosis = function(x) mean(((x - mean(x)) / sd(x))^4)\n   set.seed(3751)\n   niter = 500\n   nboot = 400\n   n = 50\n   nu = 10\n   trueKurtosis = 3 + 6 / (nu - 4)\n   correct = matrix(nrow = niter, ncol = 5)\n   width    = matrix(nrow = niter, ncol = 5)\n   error = matrix(nrow = niter, ncol = 1)\n   t1 = proc.time()\n   for (i in 1:niter){\n      y = rt(n,nu)\n      int1 = boott(y, Kurtosis, nboott = nboot,\n          nbootsd = 50)$confpoints[c(3, 9)]\n      width[i,1] = int1[2] - int1[1]\n      correct[i,1] = as.numeric((int1[1] < trueKurtosis) &\n          (trueKurtosis < int1[2]))\n      int2 = bcanon(y, nboot, Kurtosis)$confpoints[c(1, 8), 2]\n      width[i,2] = int2[2] - int2[1]\n      correct[i,2] = as.numeric((int2[1] < trueKurtosis) &\n          (trueKurtosis < int2[2]))\n      boot = bootstrap(y, nboot, Kurtosis)$thetastar\n      int3 = Kurtosis(y) + ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "R Lab   153",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.96 * c(-1, 1) * sd(boot)\n      width[i,3] = int3[2] - int3[1]\n      correct[i,3] = as.numeric((int3[1] < trueKurtosis) &\n          (trueKurtosis < int3[2]))\n      int4 = quantile(boot, c(0.025, 0.975))\n      width[i,4] = int4[2] - int4[1]\n      correct[i,4] = as.numeric((int4[1] < trueKurtosis) &\n          (trueKurtosis < int4[2]))\n      int5 = 2*Kurtosis(y) - quantile(boot, c(0.975, 0.025))\n      width[i,5] = int5[2] - int5[1]\n      correct[i,5] = as.numeric((int5[1] < trueKurtosis) &\n          (trueKurtosis < int5[2]))\n      error[i] = mean(boot) - Kurtosis(y)\n   }\n   t2 = proc.time()\n   (t2 - t1)/60\n   colMeans(width)\n   colMeans(correct)\n   options(digits = 3)\n   mean(error)\n   mean(error^2)\n\nProblem 6 Which \ufb01ve bootstrap intervals are being used here?\n\f154    6 Resampling\n\nProblem 7 What is the value of B here?\n\n\nProblem 8 How many simulations are used?\n\n\nProblem 9 What are the estimates of bias?\n\n\nProblem 10 What is the estimated MSE?\n\n\nProblem 11 Estimate the actual coverage probability of the BCa and\nbootstrap-t intervals. (Because this is a simulation experiment, it is subject\nto Monte Carlo errors, so the coverage probability is only estimated.)\n\n\nProblem 12 Find a 95 % con\ufb01dence interval for the actual coverage proba-\nbility of the BCa interval?\n\n\nProblem 13 Which interval is most accurate? Would you consider any of\nthe intervals as highly accurate?\n\n\nProblem 14 How much clock time did the entire simulation take?\n\n\n    As mentioned, kurtosis is di\ufb03cult to estimate because it is based on the\n4th moment and a quantile-based measure of tailweight might be a better\nalternative. The next problem investigates this conjecture.\n\nProblem 15 Repeat the simulation experiment with kurtosis replaced by\nquKurt() de\ufb01ned is Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.96",
      "section_title": "* c(-1, 1) * sd(boot)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5.1 Which interval is most accurate now? Would\nyou consider any of the intervals as highly accurate?\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.5",
      "section_title": "1 Which interval is most accurate now? Would",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.6 Exercises\n1. To estimate the risk of a stock, a sample of 50 log returns was taken\n   and s was ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.6",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.31. To get a con\ufb01dence interval for \u03c3, 10,000 resamples were\n   taken. Let sb,boot be the sample standard deviation of the bth resample.\n   The 10,000 values of sb,boot /s were sorted and the table below contains\n   selected values of sb,boot /s ranked from smallest to largest (so rank 1 is\n   the smallest and so forth).\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.31",
      "section_title": "To get a con\ufb01dence interval for \u03c3, 10,000 resamples were",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.6 Exercises   155\n\n                            Rank Value of sb,boot /s\n                              250      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "6.6",
      "section_title": "Exercises   155",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.52\n                              500      0.71\n                            1,000      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.52",
      "section_title": "500      0.71",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.85\n                            9,000      1.34\n                            9,500      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.85",
      "section_title": "9,000      1.34",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.67\n                            9,750      2.19\n   Find a 90 % con\ufb01dence interval for \u03c3.\n2. In the following R program, resampling was used to estimate the bias and\n   variance of the sample correlation between the variables in the vectors x\n   and y.\n      samplecor = cor(x, y)\n      n = length(x)\n      nboot = 5000\n      resamplecor = rep(0, nboot)\n      for (b in (1:nboot))\n      {\n         ind = sample(1:n, replace = TRUE)\n         resamplecor[b] = cor(x[ind], y[ind])\n      }\n      samplecor\n      mean(resamplecor)\n      sd(resamplecor)\n\n  The output is\n      > n\n      [1] 20\n      > samplecor\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.67",
      "section_title": "9,750      2.19",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "n\n      [1] 20\n      > samplecor\n      [1]",
        "start": 571,
        "end": 616
      }
    ]
  },
  {
    "content": "0.69119\n      > mean(resamplecor)\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.69119",
      "section_title": "> mean(resamplecor)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mean(resamplecor)\n      [1]",
        "start": 14,
        "end": 44
      }
    ]
  },
  {
    "content": "0.68431\n      > sd(resamplecor)\n      [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.68431",
      "section_title": "> sd(resamplecor)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd(resamplecor)\n      [1]",
        "start": 14,
        "end": 42
      }
    ]
  },
  {
    "content": "0.11293\n\n   (a) Estimate the bias of the sample correlation coe\ufb03cient.\n   (b) Estimate the standard deviation of the sample correlation coe\ufb03cient.\n   (c) Estimate the MSE of the sample correlation coe\ufb03cient.\n   (d) What fraction of the MSE is due to bias? How serious is the bias?\n       Should something be done to reduce the bias? Explain your answer.\n3. The following R code was used to bootstrap the sample standard deviation.\n      ( code to read the variable x )\n      sampleSD = sd(x)\n      n = length(x)\n      nboot = 15000\n      resampleSD = rep(0, nboot)\n\f156      6 Resampling\n\n         for (b in (1:nboot))\n         {\n            resampleSD[b] = sd(sample(x, replace = TRUE))\n         }\n         options(digits = 4)\n         sampleSD\n         mean(resampleSD)\n         sd(resampleSD)\n\n      The output is\n         > sampleSD\n         [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.11293",
      "section_title": "(a) Estimate the bias of the sample correlation coe\ufb03cient.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sampleSD\n         [1]",
        "start": 826,
        "end": 850
      }
    ]
  },
  {
    "content": "1.323\n         > mean(resampleSD)\n         [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.323",
      "section_title": "> mean(resampleSD)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mean(resampleSD)\n         [1]",
        "start": 15,
        "end": 47
      }
    ]
  },
  {
    "content": "1.283\n         > sd(resampleSD)\n         [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.283",
      "section_title": "> sd(resampleSD)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sd(resampleSD)\n         [1]",
        "start": 15,
        "end": 45
      }
    ]
  },
  {
    "content": "0.2386\n\n      (a) Estimate the bias of the sample standard deviation of x.\n      (b) Estimate the mean squared error of the sample standard deviation\n          of x.\n\n\nReferences\nChernick, M. R. (2007) Bootstrap Methods: A Guide for Practitioners and\n  Researchers, 2nd ed., Wiley-Interscience, Hoboken, NJ.\nDavison, A. C., and Hinkley, D. V. (1997) Bootstrap Methods and Their Ap-\n  plications, Cambridge University Press, Cambridge.\nEfron, B. (1979) Bootstrap methods: Another look at the jackknife. Annals\n  of Statistics, 7, 1\u201326.\nEfron, B., and Tibshirani, R. (1993) An Introduction to the Bootstrap, Chap-\n  man & Hall, New York.\nGood, P. I. (2005) Resampling Methods: A Practical Guide to Data Analysis,\n  3rd ed., Birkhauser, Boston.\n\f7\nMultivariate Statistical Models\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2386",
      "section_title": "(a) Estimate the bias of the sample standard deviation of x.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1 Introduction\nOften we are not interested merely in a single random variable but rather in\nthe joint behavior of several random variables, for example, returns on sev-\neral assets and a market index. Multivariate distributions describe such joint\nbehavior. This chapter is an introduction to the use of multivariate distribu-\ntions for modeling \ufb01nancial markets data. Readers with little prior knowl-\nedge of multivariate distributions may bene\ufb01t from reviewing Appendices\nA.12\u2013A.14 before reading this chapter.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.2 Covariance and Correlation Matrices\nLet Y = (Y1 , . . . , Yd )T be a random vector. We de\ufb01ne the expectation vector\nof Y to be                               \u239b        \u239e\n                                           E(Y1 )\n                                         \u239c        \u239f\n                                 E(Y ) = \u239d ... \u23a0 .\n                                           E(Yd )\nThe covariance matrix of Y is the matrix whose (i, j)th entry is Cov(Yi , Yj )\nfor i, j = 1, . . . , N . Since Cov(Yi , Yi ) = Var(Yi ), the covariance matrix is\n                          \u239b Var(Y )          Cov(Y , Y ) \u00b7 \u00b7 \u00b7 Cov(Y , Y ) \u239e\n                                1             1   2                 1   d\n                  \u239c Cov(Y2 , Y1 )        Var(Y2 )      \u00b7\u00b7\u00b7    Cov(Y2 , Yd ) \u239f\n        COV(Y ) = \u239c\n                  \u239d     ..                  ..         ..         ..        \u239f.\n                                                                            \u23a0\n                         .                   .            .        .\n                    Cov(Yd , Y1 )      Cov(Yd , Y2 )   \u00b7\u00b7\u00b7      Var(Yd )\nSimilarly, the correlation matrix of Y , denoted CORR(Y ), has i, jth element\n\u03c1Yi Yj . Because Corr(Yi , Yi ) = 1 for all i, the diagonal elements of a correlation\n\n\n\u00a9 Springer Science+Business Media New York 2015                                  157\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 7\n\f158      7 Multivariate Statistical Models\n\nmatrix are all equal to 1. Note the use of \u201cCOV\u201d and \u201cCORR\u201d to denote\nmatrices and \u201cCov\u201d and \u201cCorr\u201d to denote scalars.\n  The covariance matrix can be written as\n                             \u001a                         \u001b\n                                                     T\n                COV(Y ) = E {Y \u2212 E(Y )} {Y \u2212 E(Y )} .            (7.1)\n\nThere are simple relationships between the covariance and correlation matri-\nces. Let S = diag(\u03c3Y1 , . . . , \u03c3Yd ), where \u03c3Yi is the standard deviation of Yi .\nThen\n                       CORR(Y ) = S \u22121 COV(Y )S \u22121                          (7.2)\nand, equivalently,\n                           COV(Y ) = S CORR(Y ) S.                          (7.3)\n    The sample covariance and correlation matrices replace Cov(Yi , Yj ) and\n\u03c1Yi Yj by their estimates given by (A.29) and (A.30).\n    A standardized variable is obtained by subtracting the variable\u2019s mean and\ndividing the di\ufb00erence by the variable\u2019s standard deviation. After standard-\nization, a variable has a mean equal to 0 and a standard deviation equal to 1.\nThe covariance matrix of standardized variables equals the correlation matrix\nof original variables, which is also the correlation matrix of the standardized\nvariables.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.2",
      "section_title": "Covariance and Correlation Matrices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1. CRSPday covariances and correlations\n\n    This example uses the CRSPday data set in R\u2019s Ecdat package. There are\nfour variables, daily returns from January 3, 1969, to December 31, 1998, on\nthree stocks, GE, IBM, and Mobil, and on the CRSP value-weighted index,\nincluding dividends. CRSP is the Center for Research in Security Prices at the\nUniversity of Chicago. The sample covariance matrix for these four series is\n\n                  ge      ibm    mobil     crsp\n      ge    1.88e-04 8.01e-05 5.27e-05 7.61e-05\n      ibm   8.01e-05 3.06e-04 3.59e-05 6.60e-05\n      mobil 5.27e-05 3.59e-05 1.67e-04 4.31e-05\n      crsp 7.61e-05 6.60e-05 4.31e-05 6.02e-05\n\nIt is di\ufb03cult to get much information just by inspecting the covariance ma-\ntrix. The covariance between two random variables depends on their variances\nas well as the strength of the linear relationship between them. Covariance\nmatrices are extremely important as input to, for example, a portfolio anal-\nysis, but to understand the relationship between variables, it is much better\nto examine their sample correlation matrix. The sample correlation matrix in\nthis example is\n\f                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "CRSPday covariances and correlations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3 Linear Functions of Random Variables      159\n\n\n            ge   ibm mobil crsp\n   ge    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "Linear Functions of Random Variables      159",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.000 0.334 0.297 0.715\n   ibm   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.000",
      "section_title": "0.334 0.297 0.715",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.334 1.000 0.159 0.486\n   mobil ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.334",
      "section_title": "1.000 0.159 0.486",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.297 0.159 1.000 0.429\n   crsp ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.297",
      "section_title": "0.159 1.000 0.429",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.715 0.486 0.429 1.000\n\nWe can see that all sample correlations are positive and the largest correlations\nare between crsp and the individual stocks. GE is the stock most highly\ncorrelated with crsp. The correlations between individual stocks and a market\nindex such as crsp are a key component of \ufb01nance theory, especially the\nCapital Asset Pricing Model (CAPM) introduced in Chap. 17.                     \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.715",
      "section_title": "0.486 0.429 1.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3 Linear Functions of Random Variables\nOften we are interested in \ufb01nding the expectation and variance of a linear\ncombination (weighted average) of random variables. For example, consider\nreturns on a set of assets. A portfolio is simply a weighted average of the assets\nwith weights that sum to one. The weights specify what fractions of the total\ninvestment are allocated to the assets. For example, if a portfolio consists of\n200 shares of Stock 1 selling at $88/share and 150 shares of Stock 2 selling at\n$67/share, then the weights are\n\n                 (200)(88)\n   w1 =                          = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "Linear Functions of Random Variables",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.637    and   w2 = 1 \u2212 w1 = 0.363.      (7.4)\n           (200)(88) + (150)(67)\n   Because the return on a portfolio is a linear combination of the returns\non the individual assets in the portfolio, the material in this section is used\nextensively in the portfolio theory of Chaps. 16 and 17.\n   First, we look at a linear function of a single random variable. If Y is a\nrandom variable and a and b are constants, then\n\n                            E(aY + b) = aE(Y ) + b.\n\nAlso,\n                 Var(aY + b) = a2 Var(Y ) and \u03c3aY +b = |a|\u03c3Y .\n   Next, we consider linear combinations of two random variables. If X and\nY are random variables and w1 and w2 are constants, then\n\n                     E(w1 X + w2 Y ) = w1 E(X) + w2 E(Y ),\n\nand\n\n      Var(w1 X + w2 Y ) = w12 Var(X) + 2w1 w2 Cov(X, Y ) + w22 Var(Y ).     (7.5)\n\f160     7 Multivariate Statistical Models\n\nCheck that (7.5) can be reexpressed as\n                                 \u0007                                         \b\u0007        \b\n                                     Var(X)                   Cov(X, Y )        w1\n  Var(w1 X + w2 Y ) = ( w1 w2 )                                                          .   (7.6)\n                                   Cov(X, Y )                  Var(Y )          w2\nAlthough formula (7.6) may seem unnecessarily complicated, we will show\nthat this equation generalizes in an elegant way to more than two random\nvariables; see (7.7) below. Notice that the matrix in (7.6) is the covariance\nmatrix of the random vector ( X Y )T .\n    Let w = (w1 , . . . , wd )T be a vector of weights and let Y = (Y1 , . . . , Yd ) be\na random vector. Then                        N\n                                  wT Y =             wi Y i\n                                               i=1\nis a weighted average of the components of Y . One can easily show that\n                           E(wT Y ) = wT {E(Y )}\nand                              N N\n                     Var(wT Y ) =               wi wj Cov(Yi , Yj ).\n                                     i=1 j=1\nThis last result can be expressed more succinctly using vector/matrix nota-\ntion:\n                        Var(wT Y ) = wT COV(Y )w.                     (7.7)\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.637",
      "section_title": "and   w2 = 1 \u2212 w1 = 0.363.      (7.4)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.2. The variance of a linear combination of correlated random\nvariables\n\n     Suppose that Y = (Y1 Y2 Y3 )T , Var(Y1 ) = 2, Var(Y2 ) = 3, Var(Y3 ) = 5,\n\u03c1Y1 ,Y2 = 0.6, and that Y1 and Y2 are independent of Y3 . Find Var(Y1 + Y2 +\n1/2 Y3 ).\n\nAnswer: The covariance between Y1 and Y3 is 0 by independence,       \u0011 and the\nsame is true of Y2 and Y3 . The covariance between Y1 and Y2 is (0.6) (2)(3) =\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.2",
      "section_title": "The variance of a linear combination of correlated random",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47. Therefore,\n                                    \u239b                \u239e\n                                        2    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "Therefore,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47 0\n                        COV(Y ) = \u239d ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47       3   0\u23a0,\n                                        0      0   5\nand by (7.7),\n                                                      \u239b                  \u239e\u239b \u239e\n                                                        2   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "3   0\u23a0,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47       0   1\n                                                1 \u239d\n         Var(Y1 + Y2 + Y3 /2) = ( 1        1    2 )   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "0   1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47    3        0\u23a0\u239d 1 \u23a0\n                                                                           1\n                                                        0     0        5\n                                                    \u239b      \u239e               2\n                                                      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "3        0\u23a0\u239d 1 \u23a0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.47\n                                                1 \u239d\n                                 = (1      1    2 )   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "3.47",
      "section_title": "1 \u239d",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.47 \u23a0\n                                                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.47",
      "section_title": "\u23a0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                                 = 9.19.\n                                                                                                \u0002\n\f                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.5",
      "section_title": "= 9.19.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3 Linear Functions of Random Variables    161\n\n    An important property of covariance and correlation matrices is that they\nare symmetric and positive semide\ufb01nite. A matrix A is said to be positive\nsemide\ufb01nite (de\ufb01nite) if xT Ax \u2265 0 (> 0) for all vectors x = 0. By (7.7),\nany covariance matrix must be positive semide\ufb01nite, because otherwise there\nwould exist a random variable with a negative variance, a contradiction. A\nnonsingular covariance matrix is positive de\ufb01nite. A covariance matrix must\nbe symmetric because Cov(Yi , Yj ) = Cov(Yj , Yi ) for every i and j. Since a\ncorrelation matrix is the covariance matrix of standardized variables, it is\nalso symmetric and positive semide\ufb01nite.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "Linear Functions of Random Variables    161",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0) for all vectors x = 0. By (7.7),\nany covariance matrix must be positive semide\ufb01nite, because otherwise there\nwould exist a random variable with a negative variance, a contradiction. A\nnonsingular covariance matrix is positive de\ufb01nite. A covariance matrix must\nbe symmetric because Cov(Yi , Yj ) = Cov(Yj , Yi ) for every i and j. Since a\ncorrelation matrix is the covariance matrix of standardized variables, it is\nalso symmetric and positive semide\ufb01nite.",
        "start": 237,
        "end": 699
      }
    ]
  },
  {
    "content": "7.3.1 Two or More Linear Combinations of Random Variables\nMore generally, suppose that wT            T\n                                 1 Y and w 2 Y are two weighted averages of\nthe components of Y , e.g., returns on two di\ufb00erent portfolios. Then\n           Cov(wT       T         T                T\n                1 Y , w 2 Y ) = w 1 COV(Y )w 2 = w 2 COV(Y )w 1 .        (7.8)\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "1 Two or More Linear Combinations of Random Variables",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3. (Example 7.2 continued)\n                                                        T\n   Suppose that the random vector Y = (Y1 , Y2 , Y3 ) has the mean vector\nand covariance matrix used in the previous example and contains the returns\non three assets. Find the covariance between a portfolio that allocates 1/3 to\neach of the three assets and a second portfolio that allocates 1/2 to each of\nthe \ufb01rst two assets. That is, \ufb01nd the covariance between (Y1 + Y2 + Y3 )/3 and\n(Y1 + Y2 )/2.\n\nAnswer: Let\n                                             1 T\n                             w1 = ( 13   1\n                                         3   3 )\nand\n                                               T\n                             w2 = ( 12   1\n                                         2   0) .\nThen\n    \u000e                      \u000f\n      Y1 + Y2 Y1 + Y2 + Y3\nCov          ,               = wT\n                                1 COV(Y )w 2\n         2         3\n                                                    \u239b             \u239e\u239b     \u239e\n                                                    2     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "(Example 7.2 continued)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47 0     1/2\n                               = ( 1/3 1/3 1/3 ) \u239d",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "0     1/2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.47      3   0 \u23a0 \u239d1/2 \u23a0\n                                                    0       0   5     0\n                                                       \u239b      \u239e\n                                                         1/2\n                               = ( ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.47",
      "section_title": "3   0 \u23a0 \u239d1/2 \u23a0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.157 1.490 1.667 ) \u239d 1/2 \u23a0\n                                                          0\n                               = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.157",
      "section_title": "1.490 1.667 ) \u239d 1/2 \u23a0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.323.\n                                                                            \u0002\n\f162     7 Multivariate Statistical Models\n\n    Let W be a nonrandom d \u00d7 q matrix so that W T Y is a random vector of\nq linear combinations of Y . Then (7.7) can be generalized to\n\n                        COV(W T Y ) = W T COV(Y )W .                                (7.9)\n\n     Let Y 1 and Y 2 be two random vectors of dimensions n1 and n2 , respec-\ntively. Then \u03a3 Y1 ,Y2 = COV(Y 1 , Y 2 ) is de\ufb01ned as the n1 \u00d7 n2 matrix whose\ni, jth element is the covariance between the ith component of Y 1 and the jth\ncomponent of Y 2 , that is, \u03a3 Y1 ,Y2 is the matrix of covariances between the\nrandom vectors Y 1 and Y 2 .\n     It is not di\ufb03cult to show that\n\n                 Cov(wT         T           T\n                      1 Y 1 , w 2 Y 2 ) = w 1 COV(Y 1 , Y 2 )w 2 ,                 (7.10)\n\nfor constant vectors w1 and w2 of lengths n1 and n2 .\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.323",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3.2 Independence and Variances of Sums\n\nIf Y1 , . . . , Yd are independent, or at least uncorrelated, then\n\n                  -    .                n                    n\n               Var wT Y = Var               wi Y i       =         wi2 Var(Yi ).   (7.11)\n                                      i=1                    i=1\n\nWhen wT = (1/n, . . . , 1/n) so that wT Y = Y , then we obtain that\n                                              n\n                                        1\n                            Var(Y ) =          Var(Yi ).                           (7.12)\n                                        n2 i=1\n\nIn particular, if Var(Yi ) = \u03c3 2 for all i, then we obtain the well-known result\nthat if Y1 , . . . , Yd are uncorrelated and have a constant variance \u03c3 2 , then\n\n                                                  \u03c32\n                                  Var(Y ) =          .                             (7.13)\n                                                  n\nAnother useful fact that follows from (7.11) is that if Y1 and Y2 are uncorre-\nlated, then\n                     Var(Y1 \u2212 Y2 ) = Var(Y1 ) + Var(Y2 ).               (7.14)\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "2 Independence and Variances of Sums",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4 Scatterplot Matrices\n\nA correlation coe\ufb03cient is only a summary of the linear relationship between\nvariables. Interesting features, such as nonlinearity or the joint behavior of\nextreme values, remain hidden when only correlations are examined. A so-\nlution to this problem is the so-called scatterplot matrix, which is a matrix\n\f                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "Scatterplot Matrices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4 Scatterplot Matrices                 163\n\nof scatterplots, one for each pair of variables. A scatterplot matrix can be\ncreated easily with modern statistical software such as R. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "Scatterplot Matrices                 163",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1 shows a\nscatterplot matrix for the CRSPday data set.\n\n                               \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "shows a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10   0.00   0.10                                \u22120.06   \u22120.02   0.02\n\n\n\n\n                                                                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "0.00   0.10                                \u22120.06   \u22120.02   0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n                ge\n\n\n\n\n                                                                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "ge",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n                                                                                                          \u22120.05\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n\n\n\n\n                                       ibm\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "ibm",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\u22120.10\n\n\n\n\n                                                                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "\u22120.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n                                                                                                          0.05\n                                                             mobil\n\n\n\n\n                                                                                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n                                                                                                          \u22120.05\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.00",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n\n\n\n\n                                                                                          crsp\n\u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "crsp",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n\u22120.06\n\n\n\n\n        \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "\u22120.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05   0.00    0.05                         \u22120.05   0.00   0.05   0.10\n\n                       Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "0.00    0.05                         \u22120.05   0.00   0.05   0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1. Scatterplot matrix for the CRSPday data set.\n\n\n    One sees little evidence of nonlinear relationships in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "Scatterplot matrix for the CRSPday data set.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1. This lack\nof nonlinearities is typical of returns on equities, but it should not be taken\nfor granted\u2014instead, one should always look at the scatterplot matrix. The\nstrong linear association between GE and crsp, which was suggested before\nby their high correlation coe\ufb03cient, can be seen also in their scatterplot.\n    A portfolio is riskier if large negative returns on its assets tend to occur\ntogether on the same days. To investigate whether extreme values tend to\ncluster in this way, one should look at the scatterplots. In the scatterplot for\nIBM and Mobil, extreme returns for one stock do not tend to occur on the same\ndays as extreme returns on the other stock; this can be seen by noticing that\nthe outliers tend to fall along the x- and y-axes. The extreme-value behavior\n\f164        7 Multivariate Statistical Models\n\nis di\ufb00erent with GE and crsp, where extreme values are more likely to occur\ntogether; note that the outliers have a tendency to occur together, that is, in\nthe upper-right and lower-left corners, rather than being concentrated along\nthe axes. The IBM and Mobil scatterplot is said to show tail independence.\nIn contrast, the GE and crsp scatterplot is said to show tail dependence. Tail\ndependence is explored further in Chap. 8.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "This lack",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.5 The Multivariate Normal Distribution\nIn Chap. 5 we saw the importance of having parametric families of univariate\ndistributions as statistical models. Parametric families of multivariate distri-\nbutions are equally useful, and the multivariate normal family is the best\nknown of them.\n\n      a                 corr = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.5",
      "section_title": "The Multivariate Normal Distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5                      b              corr = \u22120.95\n\n                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "b              corr = \u22120.95",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n           2\n\n\n\n\n                                                             2\n\n\n                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n           1\n\n\n\n\n                                                             1\n\n\n\n                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                14\n                              0.\n      Y2\n\n\n\n\n                                                        Y2\n           0\n\n\n\n\n                                                             0\n                                         6\n                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "14",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                     2\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n           \u22121\n\n\n\n\n                                                             \u22121\n\n\n\n\n                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n                      0.04\n           \u22122\n\n\n\n\n                                                             \u22122\n\n\n\n\n                 \u22122   \u22121           0         1      2             \u22122   \u22121   0    1    2\n                                   Y1                                       Y1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.2. Contour plots of a bivariate normal densities with N (0, 1) marginal\ndistributions and correlations of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.2",
      "section_title": "Contour plots of a bivariate normal densities with N (0, 1) marginal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 or \u22120.95.\n\n\n    The random vector Y = (Y1 , . . . , Yd )T has a d-dimensional multivariate\nnormal distribution with mean vector \u03bc = (\u03bc1 , . . . , \u03bcd )T and covariance ma-\ntrix \u03a3 if its probability density function is\n                  !                \"     \u000e                          \u000f\n                           1                1         T \u22121\n    \u03c6d (y|\u03bc, \u03a3) =                    exp  \u2212   (y \u2212 \u03bc)  \u03a3    (y \u2212 \u03bc)   ,   (7.15)\n                    (2\u03c0)d/2 |\u03a3|1/2          2\n\nwhere |\u03a3| is the determinant of \u03a3. The quantity in square brackets is a\nconstant that normalizes the density so that it integrates to 1. The density\ndepends on y only through (y \u2212\u03bc)T \u03a3 \u22121 (y \u2212\u03bc), and so the density is constant\non each ellipse {y : (y \u2212 \u03bc)T \u03a3 \u22121 (y \u2212 \u03bc) = c}. Here c > 0 is a \ufb01xed constant\nthat determines the size of the ellipse, with larger values of c giving larger\nellipses, each centered at \u03bc. Such densities are called elliptically contoured.\n\f                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "or \u22120.95.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 is a \ufb01xed constant\nthat determines the size of the ellipse, with larger values of c giving larger\nellipses, each centered at \u03bc. Such densities are called elliptically contoured.",
        "start": 761,
        "end": 984
      }
    ]
  },
  {
    "content": "7.6 The Multivariate t-Distribution      165\n\nFigure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "The Multivariate t-Distribution      165",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.2 has contour plots of bivariate normal densities. Both Y1 and Y2 are\nN (0, 1) and the correlation between Y1 and Y2 is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.2",
      "section_title": "has contour plots of bivariate normal densities. Both Y1 and Y2 are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 in panel (a) or \u22120.95 in\npanel (b). Notice how the orientations of the contours depend on the sign and\nmagnitude of the correlation. In panel (a) we can see that the height of the\ndensity is constant on ellipses and decreases with the distance from the mean,\nwhich is (0, 0). The same behavior occurs in panel (b), but, because of the\nhigh correlation, the contours are so close together that it was not possible to\nlabel them.\n    If Y = (Y1 , . . . , Yd )T has a multivariate normal distribution, then for every\nset of constants c = (c1 , . . . , cd )T , the weighted average (linear combination)\ncT Y = c1 Y1 + \u00b7 \u00b7 \u00b7 + cd Yd has a normal distribution with mean cT \u03bc and\nvariance cT \u03a3c. In particular, the marginal distribution of Yi is N (\u03bci , \u03c3i2 ),\nwhere \u03c3i2 is the ith diagonal element of \u03a3\u2014to see this, take ci = 1 and cj = 0\nfor j = i.\n    The assumption of multivariate normality facilitates many useful proba-\nbility calculations. If the returns on a set of assets have a multivariate normal\ndistribution, then the return on any portfolio formed from these assets will\nbe normally distributed. This is because the return on the portfolio is the\nweighted average of the returns on the assets. Therefore, the normal distri-\nbution could be used, for example, to \ufb01nd the probability of a loss of some\nsize of interest, say, 10 % or more, on the portfolio. Such calculations have\nimportant applications in \ufb01nding a value-at-risk; see Chap. 19.\n    Unfortunately, we saw in Chap. 5 that often individual returns are not\nnormally distributed, which implies that a vector of returns will not have a\nmultivariate normal distribution. In Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.5",
      "section_title": "in panel (a) or \u22120.95 in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6 we will look at an important\nclass of heavy-tailed multivariate distributions.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "we will look at an important",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6 The Multivariate t-Distribution\nWe have seen that the univariate t-distribution is a good model for the returns\nof individual assets. Therefore, it is desirable to have a model for vectors of\nreturns such that the univariate marginals are t-distributed. The multivariate\nt-distribution has this property. The random vector Y has a multivariate\nt\u03bd (\u03bc, \u039b) distribution if\n                                         \u0010\n                                             \u03bd\n                              Y =\u03bc+            Z,                        (7.16)\n                                            W\n\nwhere W is chi-squared distributed with \u03bd degrees of freedom, Z is Nd (0, \u039b)\ndistributed, and W and Z are independent. Thus, the multivariate t-distribu-\ntion is a continuous scale mixture of multivariate normal distributions. Ex-\ntreme values of Y tend to occur when W is near zero. Since W \u22121/2 multiplies\nall components of Z, outliers in one component tend to occur with outliers in\nother components, that is, there is tail dependence.\n    For \u03bd > 1, \u03bc is the mean vector of Y . For 0 < \u03bd \u2264 1, the expectation of Y\ndoes not exist, but \u03bc can still be regarded as the \u201ccenter\u201d of the distribution\n\f166       7 Multivariate Statistical Models\n\nof Y because, for any value of \u03bd, the vector \u03bc contains the medians of the\ncomponents of Y and the contours of the density of Y are ellipses centered\nat \u03bc. Also, \u03bc is the mode of the distribution, that is, the location where the\ndensity is maximized.\n\n      a                        Multivariate\u2212t        b                Independent t\n            5 10 15 20\n\n\n\n\n                                                          10\n                                                          5\n      Y2\n\n\n\n\n                                                     Y2\n                                                          0\n            0\n\n\n\n\n                                                          \u22125\n            \u221210\n\n\n\n\n                                                          \u221210\n                         \u221210      0        10   20              \u221215   \u22125 0    5 10 15 20\n                                      Y1                                     Y1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "The Multivariate t-Distribution",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1, \u03bc is the mean vector of Y . For 0 < \u03bd \u2264 1, the expectation of Y\ndoes not exist, but \u03bc can still be regarded as the \u201ccenter\u201d of the distribution\n\f166       7 Multivariate Statistical Models",
        "start": 1040,
        "end": 1235
      }
    ]
  },
  {
    "content": "7.3. (a) Plot of a random sample from a bivariate t-distribution with \u03bd = 3,\n\u03bc = (0 0)T and identity covariate matrix. (b) Plot of a random sample of pairs of\nindependent t3 (0, 1) random variables. Both sample sizes are 2,500.\n\n\n    For \u03bd > 2, the covariance matrix of Y exists and is\n                                           \u03bd\n                                  \u03a3=          \u039b.                           (7.17)\n                                        \u03bd\u22122\nWe will call \u039b the scale matrix. The scale matrix exists for all values of\n\u03bd. Since the covariance matrix \u03a3 of Y is just a multiple of the covariance\nmatrix \u039b of Z, Y and Z have the same correlation matrices, assuming \u03bd >\n2 so that the correlation matrix of Y exists. If \u03a3i,j = 0, then Yi and Yj\nare uncorrelated, but they are dependent, nonetheless, because of the tail\ndependence. Tail dependence is illustrated in Fig. 7.3, where panel (a) is a\nplot of 2500 observations from an uncorrelated bivariate t-distribution with\nmarginal distributions that are t3 (0, 1). For comparison, panel (b) is a plot\nof 2500 observations of pairs of independent t3 (0, 1) random variables\u2014these\npairs do not have a bivariate t-distribution. Notice that in (b), outliers in Y1\nare not associated with outliers in Y2 , since the outliers are concentrated near\nthe x- and y-axes. In contrast, outliers in (a) are distributed uniformly in all\ndirections. The univariate marginal distributions are the same in (a) and (b).\n    Tail dependence can be expected in equity returns. For example, on Black\nMonday, almost all equities had extremely large negative returns. Of course,\nBlack Monday was an extreme, even among extreme events. We would not\nwant to reach any general conclusions based upon Black Monday alone. How-\never, in Fig. 7.1, we see little evidence that outliers are concentrated along\nthe axes, with the possible exception of the scatterplot for IBM and Mobil.\nAs another example of dependencies among stock returns, Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.3",
      "section_title": "(a) Plot of a random sample from a bivariate t-distribution with \u03bd = 3,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "2, the covariance matrix of Y exists and is\n                                           \u03bd\n                                  \u03a3=          \u039b.                           (7.17)\n                                        \u03bd\u22122\nWe will call \u039b the scale matrix. The scale matrix exists for all values of\n\u03bd. Since the covariance matrix \u03a3 of Y is just a multiple of the covariance\nmatrix \u039b of Z, Y and Z have the same correlation matrices, assuming \u03bd >\n2 so that the correlation matrix of Y exists. If \u03a3i,j = 0, then Yi and Yj\nare uncorrelated, but they are dependent, nonetheless, because of the tail\ndependence. Tail dependence is illustrated in Fig. 7.3, where panel (a) is a\nplot of 2500 observations from an uncorrelated bivariate t-distribution with\nmarginal distributions that are t3 (0, 1). For comparison, panel (b) is a plot\nof 2500 observations of pairs of independent t3 (0, 1) random variables\u2014these\npairs do not have a bivariate t-distribution. Notice that in (b), outliers in Y1\nare not associated with outliers in Y2 , since the outliers are concentrated near\nthe x- and y-axes. In contrast, outliers in (a) are distributed uniformly in all\ndirections. The univariate marginal distributions are the same in (a) and (b).\n    Tail dependence can be expected in equity returns. For example, on Black\nMonday, almost all equities had extremely large negative returns. Of course,\nBlack Monday was an extreme, even among extreme events. We would not\nwant to reach any general conclusions based upon Black Monday alone. How-\never, in Fig. 7.1, we see little evidence that outliers are concentrated along\nthe axes, with the possible exception of the scatterplot for IBM and Mobil.\nAs another example of dependencies among stock returns, Fig.",
        "start": 240,
        "end": 1975
      }
    ]
  },
  {
    "content": "7.4 contains a\n\f                                              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "contains a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6 The Multivariate t-Distribution           167\n\n                       \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "The Multivariate t-Distribution           167",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4   0.2                  \u22120.10   0.05                \u22120.1   0.1\n\n\n\n\n                                                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.2                  \u22120.10   0.05                \u22120.1   0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n          LSCC\n\n\n\n\n                                                                                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "LSCC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n0.2\n\n\n\n\n                         CSGS\n\u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.15",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                            0.10\n                                       EC\n\n\n\n\n                                                                                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n0.05\n\n\n\n\n                                                     NYB\n\u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n\n\n\n\n                                                                                            0.1\n                                                                   ALTR\n\n\n\n\n                                                                                            \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.10",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n0.1\n\n\n\n\n                                                                                APH\n\u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n        \u22120.15   0.05                \u22120.05   0.10                  \u22120.2   0.1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "\u22120.15   0.05                \u22120.05   0.10                  \u22120.2   0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4. Scatterplot matrix of 500 daily returns on six midcap stocks in R\u2019s\nmidcapD.ts data set.\n\nscatterplot matrix of returns on six midcap stocks in the midcapD.ts data set.\nAgain, tail dependence can be seen. This suggests that tail dependence is com-\nmon among equity returns and the multivariate t-distribution is a promising\nmodel for them.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "Scatterplot matrix of 500 daily returns on six midcap stocks in R\u2019s",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6.1 Using the t-Distribution in Portfolio Analysis\n\nIf Y has a t\u03bd (\u03bc, \u039b) distribution, which we recall has covariance matrix \u03a3 =\n{\u03bd/(\u03bd \u2212 2)}\u039b, and w is a vector of weights, then wT Y has a univariate\nt-distribution with mean wT \u03bc and variance {\u03bd/(\u03bd \u2212 2)}wT \u039bw = wT \u03a3w.\nThis fact can be useful when computing risk measures for a portfolio. If the\nreturns on the assets have a multivariate t-distribution, then the return on\nthe portfolio will have a univariate t-distribution. We will make use of this\nresult in Chap. 19.\n\f168      7 Multivariate Statistical Models\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "1 Using the t-Distribution in Portfolio Analysis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.7 Fitting the Multivariate t-Distribution\nby Maximum Likelihood\nTo estimate the parameters of a multivariate t-distribution, one can use the\nfunction cov.trob in R\u2019s MASS package. This function computes the maximum\nlikelihood estimates of \u03bc and \u039b with \u03bd \ufb01xed. To estimate \u03bd, one computes\nthe pro\ufb01le log-likelihood for \u03bd and \ufb01nds the value, \u03bd\u0002 of \u03bd that maximizes\nthe pro\ufb01le log-likelihood. Then the MLEs of \u03bc and \u039b are the estimates from\ncov.trob with \u03bd \ufb01xed at \u03bd\u0002.\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.7",
      "section_title": "Fitting the Multivariate t-Distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4. Fitting the CRSPday data\n\n    This example uses the data set CRSPday analyzed earlier in Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "Fitting the CRSPday data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1.\nRecall that there are four variables, returns on GE, IBM, Mobil, and the\nCRSP index. The pro\ufb01le log-likelihood is plotted in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.1",
      "section_title": "Recall that there are four variables, returns on GE, IBM, Mobil, and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.5. In that \ufb01gure,\none can see that the MLE of \u03bd is 5.94, and there is relatively little uncertainty\nabout this parameter\u2019s value\u2014the 95 % pro\ufb01le likelihood con\ufb01dence interval\nis (5.41, 6.55). The code to create this \ufb01gure is below.\n      library(mnormt)\n      library(MASS)\n      data(CRSPday, package = \"Ecdat\")\n                                          14\n                                          13\n               2*loglikelihood \u2212 64,000\n                                          12\n                                          11\n                                          10\n                                          9\n                                          8\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.5",
      "section_title": "In that \ufb01gure,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5   6.0      6.5\n                                                     df\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.5",
      "section_title": "6.0      6.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.5. CRSPday data. A pro\ufb01le likelihood con\ufb01dence interval for \u03bd. The solid curve\nis 2Lmax (\u03bd), where Lmax (\u03bd) is the pro\ufb01le likelihood minus 32,000. 32,000 was sub-\ntracted from the pro\ufb01le likelihood to simplify the labeling of the y-axis. The horizontal\n                                     \u03bd ) \u2212 \u03c72\u03b1,1 , where \u03bd\u0002 is the MLE and \u03b1 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.5",
      "section_title": "CRSPday data. A pro\ufb01le likelihood con\ufb01dence interval for \u03bd. The solid curve",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05. All\nline intersects the y-axis at 2Lmax (\u0002\nvalues of \u03bd such that 2Lmax (\u03bd) is above the horizontal line are in the pro\ufb01le likeli-\nhood 95 % con\ufb01dence interval. The two vertical lines intersect the x-axis at ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "All",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.41 and\n6.55, the endpoints of the con\ufb01dence interval.\n\f      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.41",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.7 Fitting the Multivariate t-Distribution by Maximum Likelihood    169\n\n   dat =CRSPday[ , 4:7]\n   df = seq(5.25, 6.75, 0.01)\n   n = length(df)\n   loglik = rep(0,n)\n   for(i in 1:n){\n     fit = cov.trob(dat,nu=df)\n     loglik[i] = sum(log(dmt(dat, mean=fit$center,\n     S = fit$cov, df = df[i])))\n   }\n   aic_t = -max(2 * loglik) + 2 * (4 + 10 + 1) + 64000\n   z1 = (2 * loglik > 2 * max(loglik) - qchisq(0.95, 1))\n   plot(df, 2 * loglik - 64000, type = \"l\", cex.axis = 1.5,\n      cex.lab = 1.5, ylab = \"2 * loglikelihood - 64,000\", lwd = 2)\n   abline(h = 2 * max(loglik) - qchisq(0.95, 1 ) - 64000)\n   abline(h = 2 * max(loglik) - 64000)\n   abline(v = (df[16] + df[17]) / 2)\n   abline(v = (df[130] + df[131]) / 2)\n\n    AIC for this model is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.7",
      "section_title": "Fitting the Multivariate t-Distribution by Maximum Likelihood    169",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "2 * max(loglik) - qchisq(0.95, 1))\n   plot(df, 2 * loglik - 64000, type = \"l\", cex.axis = 1.5,\n      cex.lab = 1.5, ylab = \"2 * loglikelihood - 64,000\", lwd = 2)\n   abline(h = 2 * max(loglik) - qchisq(0.95, 1 ) - 64000)\n   abline(h = 2 * max(loglik) - 64000)\n   abline(v = (df[16] + df[17]) / 2)\n   abline(v = (df[130] + df[131]) / 2)",
        "start": 379,
        "end": 717
      }
    ]
  },
  {
    "content": "15.45 plus 64,000. Here AIC values are expressed\nas deviations from 64,000 to keep these values small. This is helpful when\ncomparing two or more models via AIC. Subtracting the same constant from\nall AIC values, of course, has no e\ufb00ect on model comparisons.\n    The maximum likelihood estimates of the mean vector and the correlation\nmatrix are called $center and $cor, respectively, in the following output:\n   $center\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "15.45",
      "section_title": "plus 64,000. Here AIC values are expressed",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0009424 0.0004481 0.0006883 0.0007693\n\n   $cor\n          [,1]   [,2]   [,3]   [,4]\n   [1,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0009424",
      "section_title": "0.0004481 0.0006883 0.0007693",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0000 0.3192 0.2845 0.6765\n   [2,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0000",
      "section_title": "0.3192 0.2845 0.6765",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3192 1.0000 0.1584 0.4698\n   [3,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3192",
      "section_title": "1.0000 0.1584 0.4698",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2845 0.1584 1.0000 0.4301\n   [4,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2845",
      "section_title": "0.1584 1.0000 0.4301",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6765 0.4698 0.4301 1.0000\n\nThese estimates were computed using cov.trob with \u03bd \ufb01xed at 6.\n   When the data are t-distributed, the maximum likelihood estimates are\nsuperior to the sample mean and covariance matrix in several respects\u2014the\nMLE is less variable and it is less sensitive to outliers. However, in this ex-\nample, the maximum likelihood estimates are similar to the sample mean and\ncorrelation matrix. For example, the sample correlation matrix is\n             ge    ibm mobil    crsp\n   ge    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6765",
      "section_title": "0.4698 0.4301 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0000 0.3336 0.2972 0.7148\n   ibm   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0000",
      "section_title": "0.3336 0.2972 0.7148",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3336 1.0000 0.1587 0.4864\n   mobil ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.3336",
      "section_title": "1.0000 0.1587 0.4864",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2972 0.1587 1.0000 0.4294\n   crsp ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2972",
      "section_title": "0.1587 1.0000 0.4294",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7148 0.4864 0.4294 1.0000\n\n                                                                             \u0002\n\f170     7 Multivariate Statistical Models\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.7148",
      "section_title": "0.4864 0.4294 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.8 Elliptically Contoured Densities\nThe multivariate normal and t-distributions have elliptically contoured densi-\nties, a property that will be discussed in this section. A d-variate multivariate\ndensity f is elliptically contoured if can be expressed as\n\n                   f (y) = |\u039b|\u22121/2 g (y \u2212 \u03bc)T \u039b\u22121 (y \u2212 \u03bc) ,               (7.18)\n                                                         \u0016      -     .\nwhere g is a nonnegative-valued function such that 1 = \u0006d g \u0012y\u00122 dy, \u03bc is\na d \u00d7 1 vector, and \u039b is a d \u00d7 d symmetric, positive de\ufb01nite matrix. Usually,\ng(x) is a decreasing function of x \u2265 0, and we will assume this is true. We will\nalso assume the \ufb01niteness of second moments, in which case \u03bc is the mean\nvector and the covariance matrix \u03a3 is a scalar multiple of \u039b.\n    For each \ufb01xed c > 0,\n\n                         E(c) = {y : (y \u2212 \u03bc)T \u03a3 \u22121 (y \u2212 \u03bc) = c}\n\nis an ellipse centered at \u03bc, and if c1 > c2 , then E(c1 ) is inside E(c2 ) because g is\ndecreasing. The contours of f are concentric ellipses as can be seen in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.8",
      "section_title": "Elliptically Contoured Densities",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0,",
        "start": 799,
        "end": 805
      },
      {
        "language": "r",
        "code": "c2 , then E(c1 ) is inside E(c2 ) because g is\ndecreasing. The contours of f are concentric ellipses as can be seen in Fig.",
        "start": 909,
        "end": 1035
      }
    ]
  },
  {
    "content": "7.6.\n\n\n                                           multivariate t\n                    3\n\n\n\n\n                                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "multivariate t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n                    2\n\n\n\n\n                                                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.02",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n                    1\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                                           0.14\n               X2\n                    0\n\n\n\n\n                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.14",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n                    \u22121\n\n\n\n\n                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.12",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n                                  0.04\n                    \u22122\n                    \u22123\n\n\n\n\n                          \u22123     \u22122        \u22121          0           1        2      3\n                                                       X1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.08",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6. Contour plot of a multivariate t4 -density with \u03bc = (0, 0)T , \u03c312 = 2, \u03c322 = 1,\nand \u03c312 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "Contour plot of a multivariate t4 -density with \u03bc = (0, 0)T , \u03c312 = 2, \u03c322 = 1,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1.\n\f                                     7.8 Elliptically Contoured Densities   171\n\nThat \ufb01gure shows the contours of the bivariate t4 -density with \u03bc = (0, 0)T and\n                                   \u0007        \b\n                                      2 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "7.8 Elliptically Contoured Densities   171",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1\n                             \u03a3=                .\n                                     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "\u03a3=                .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1 1\nThe major axis of the ellipses is a solid line and the minor axis is a dashed\nline.\n    How can the axes be found? From Appendix A.20, we know that \u03a3 has\nan eigenvalue-eigenvector decomposition\n                             \u03a3 = O diag(\u03bbi ) O T ,\nwhere O is an orthogonal matrix whose columns are the eigenvectors of \u03a3\nand \u03bb1 , . . . , \u03bbd are the eigenvalues of \u03a3.\n    The columns of O determine the axes of the ellipse E(c). The decomposi-\ntion can be found in R using the function eigen() and, for the matrix \u03a3 in\nthe example, the decomposition is\n   $values\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.708 0.292\nwhich gives the eigenvalues, and\n   $vectors\n          [,1]   [,2]\n   [1,] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.708",
      "section_title": "0.292",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.841 0.541\n   [2,] -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.841",
      "section_title": "0.541",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.541 -0.841\nwhich has the corresponding eigenvectors as columns; e.g., (\u22120.841, \u22120.541)\nis an eigenvector with eigenvalue ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.541",
      "section_title": "-0.841",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.708. The eigenvectors are normalized so\nhave norm equal to 1. Nonetheless, the eigenvectors are only determined up\nto a sign change, so the \ufb01rst eigenvector could be taken as (\u22120.841, \u22120.541),\nas in the R output, or (0.841, 0.541).\n    If oi is the ith column of O, the ith axis of E(c) goes through the points\n\u03bc and \u03bc + oi . Therefore, this axis is the line\n                          {\u03bc + k oi : \u2212\u221e < k < \u221e}.\nBecause O is an orthogonal matrix, the axes are mutually perpendicular. The\naxes can be ordered according to the size of the corresponding eigenvalues. In\nthe bivariate case the axis associated with the largest (smallest) eigenvalue is\nthe major (minor) axis. We are assuming that there are no ties among the\neigenvalues.\n    Since \u03bc = 0, in our example the major axis is k (0.841, 0.541), \u2212\u221e < k <\n\u221e, and the minor axis is k (0.541, \u22120.841), \u2212\u221e < k < \u221e.\n    When there are ties among the eigenvalues, the eigenvectors are not unique\nand the analysis is somewhat more complicated and will not be discussed in\ndetail. Instead two examples will be given. In the bivariate case if \u03a3 = I,\nthe contours are circles and there is no unique choice of the axes\u2014any pair\nof perpendicular vectors will do. As a trivariate example, if \u03a3 = diag(1,1,3),\nthen the \ufb01rst principle axis is (0,0,1) with eigenvalue 3. The second and third\n\f172     7 Multivariate Statistical Models\n\nprincipal axis can be any perpendicular pair of vectors with third coordinates\nequal to 0. The eigen() function in R returns (0,1,0) and (1,0,0) as the second\nand third axes.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.708",
      "section_title": "The eigenvectors are normalized so",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9 The Multivariate Skewed t-Distributions\nAzzalini and Capitanio (2003) have proposed a skewed extension of the mul-\ntivariate t-distribution. The univariate special case was discussed in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "The Multivariate Skewed t-Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7.\nIn the multivariate case, in addition to the shape parameter \u03bd determining\ntail weight, the skewed t-distribution has a vector \u03b1 = (\u03b11 , . . . , \u03b1d )T of shape\nparameters determining the amounts of skewness in the components of the dis-\ntribution. If Y has a skewed t-distribution, then Yi is left-skewed, symmetric,\nor right-skewed depending on whether \u03b1i < 0, \u03b1i = 0, or \u03b1i > 0.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.7",
      "section_title": "In the multivariate case, in addition to the shape parameter \u03bd determining",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.\n    Figure",
        "start": 381,
        "end": 397
      }
    ]
  },
  {
    "content": "7.7 is a contour plot of a bivariate skewed t-distribution with\n\u03b1 = (\u22121, 0.25)T and df = 4. Notice that, because \u03b11 is reasonably large\nand negative, Y1 has a considerable amount of left skewness, as can be seen in\nthe contours, which are more widely spaced on the left side of the plot com-\npared to the right. Also, Y2 shows a lesser amount of right skewness, since the\ncontours on top are slightly more widely spaced than on the bottom. This fea-\nture is to be expected since \u03b12 is positive but with a relatively small absolute\nvalue.\n\n                         multivariate skewed t: \u03b11 = \u22121, \u03b12 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.7",
      "section_title": "is a contour plot of a bivariate skewed t-distribution with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25\n\n\n                                               0.02\n                   2\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.25",
      "section_title": "0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n                                               0.08\n                   1\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.04",
      "section_title": "0.08",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14\n                                                  6\n              X2\n\n                   0\n\n\n\n\n                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.14",
      "section_title": "6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                      0.1\n                                           2\n                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                   \u22121\n\n\n\n\n                                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n                   \u22122\n\n\n\n\n                            \u22122       \u22121               0   1    2\n\n                                                   X1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.06",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.7. Contours of a bivariate skewed t-density. The contours are more widely\nspaced on the left compared to the right because X1 is left-skewed. Similarly, the\ncontours are more widely spaced on the top compared to the bottom because X2 is\nright-skewed, but the skewness of X2 is relatively small and less easy to see.\n\f                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.7",
      "section_title": "Contours of a bivariate skewed t-density. The contours are more widely",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9 The Multivariate Skewed t-Distributions     173\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "The Multivariate Skewed t-Distributions     173",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.5. Fitting the skewed t-distribution to CRSPday\n\n    We now \ufb01t the skewed t-model to the CRSPday data set using the func-\ntion mst.mple() in R\u2019s sn package. This function maximizes the likelihood\nover all parameters, so there is no need to use the pro\ufb01le likelihood as with\ncov.trob(). The code is below.\n\n   library(sn)\n   data(CRSPday, package = \"Ecdat\")\n   dat = CRSPday[ , 4:7]\n   fit = mst.mple(y = dat, penalty = NULL)\n   aic_skewt = -2 * fit$logL + 64000 + 2 * (4 + 10 + 4 + 1)\n   dp2cp(fit$dp,\"st\")\n   aic_skewt\n\nThe CP estimates are as follows.\n   > dp2cp(fit$dp,\"st\")\n   $beta\n                  ge          ibm        mobil         crsp\n   [1,] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.5",
      "section_title": "Fitting the skewed t-distribution to CRSPday",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "dp2cp(fit$dp,\"st\")\n   $beta\n                  ge          ibm        mobil         crsp\n   [1,]",
        "start": 559,
        "end": 657
      }
    ]
  },
  {
    "content": "0.0009459182 0.0004521179 0.0006917701 0.0007722816\n\n   $var.cov\n                   ge          ibm        mobil         crsp\n   ge    1.899520e-04 7.252242e-05 5.185778e-05 6.957078e-05\n   ibm   7.252242e-05 2.743354e-04 3.492763e-05 5.771567e-05\n   mobil 5.185778e-05 3.492763e-05 1.749708e-04 4.238468e-05\n   crsp 6.957078e-05 5.771567e-05 4.238468e-05 5.565159e-05\n\n   $gamma1\n             ge          ibm        mobil         crsp\n   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0009459182",
      "section_title": "0.0004521179 0.0006917701 0.0007722816",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0010609438 0.0012389968 0.0007125122 0.0009920253\n\n   $gamma2M\n   [1] ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0010609438",
      "section_title": "0.0012389968 0.0007125122 0.0009920253",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "25.24996\n\nHere $beta is the estimate of the means, $var.cov is the estimate of covari-\nance matrix, $gamma1 is the estimate of skewnesses, and $gamma2M estimates\nthe common kurtosis of the four marginal distributions. The DP estimates are\nin fit$dp but are of less interest so are not included here.\n    AIC for the skewed t-model is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "25.24996",
      "section_title": "Here $beta is the estimate of the means, $var.cov is the estimate of covari-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "23.47885 (plus 64,000), larger than 15.45,\nthe AIC found in Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "23.47885",
      "section_title": "(plus 64,000), larger than 15.45,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4 for the symmetric t-model. This result suggests\nthat the symmetric t-model is adequate for this data set.\n    In summary, the CRSPday data are well \ufb01t by a symmetric t-distribution\nand no need was found for using a skewed t-distribution. Also, in the normal\nplots of the four variables in in Fig. 7.8, heavy tails are evident but there are\nno signs of serious skewness. Although this might be viewed as a negative\nresult, since we have not found an improvement in \ufb01t by going to the more\n\f174                            7 Multivariate Statistical Models\n\n                                               GE                                                              IBM\n\n       Theoretical Quantiles\n\n\n\n\n                                                                     Theoretical Quantiles\n                                2\n\n\n\n\n                                                                                             2\n                                0\n\n\n\n\n                                                                                             0\n                                \u22123\n\n\n\n\n                                                                                             \u22123\n                                     \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.4",
      "section_title": "for the symmetric t-model. This result suggests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05    0.00     0.05                                       \u22120.10       0.00           0.10\n                                         Sample Quantiles                                                 Sample Quantiles\n\n\n\n                                             CRSP                                                             CRSP\n       Theoretical Quantiles\n\n\n\n\n                                                                     Theoretical Quantiles\n                                2\n\n\n\n\n                                                                                             2\n                                0\n\n\n\n\n                                                                                             0\n                                \u22123\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "0.00     0.05                                       \u22120.10       0.00           0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05   0.00    0.05     0.10                           \u22123   \u22120.06     \u22120.02     0.02\n                                         Sample Quantiles                                                 Sample Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.05",
      "section_title": "0.00    0.05     0.10                           \u22123   \u22120.06     \u22120.02     0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.8. Normal plots of the four returns series in the CRSPday data set. The\nreference lines go through the \ufb01rst and third quartiles.\n\n\n\ufb02exible skewed t-distribution, the result does give us more con\ufb01dence that the\nsymmetric t-distribution is suitable for modeling this data set.            \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.8",
      "section_title": "Normal plots of the four returns series in the CRSPday data set. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.10 The Fisher Information Matrix\nIn the discussion of Fisher information in Sect. 5.10, \u03b8 was assumed to be\none-dimensional. If \u03b8 is an m-dimensional parameter vector, then the Fisher\ninformation matrix is an m \u00d7 m square matrix, I, and is equal the matrix\nof expected second-order partial derivatives of \u2212 log{L(\u03b8)}.1 In other words,\nthe i, jth entry of the Fisher information matrix is\n                                    !                  \"\n                                        \u22022\n                       Iij (\u03b8) = \u2212E           log{L(\u03b8)} .              (7.19)\n                                      \u2202\u03b8i \u2202\u03b8j\n\nThe standard errors are the square roots of the diagonal entries of the inverse\nof the Fisher information matrix. Thus, the standard error for \u03b8i is\n                                   #\n                                        \u0002 \u22121 }ii .\n                             s\u03b8\u0002i = {I(\u03b8)                                (7.20)\n\n1\n    The matrix of second partial derivatives of a function is called its Hessian matrix,\n    so the Fisher information matrix is the expectation of the Hessian of the negative\n    log-likelihood.\n\f                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.10",
      "section_title": "The Fisher Information Matrix",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.11 Bootstrapping Multivariate Data       175\n\nIn the case of a single parameter, (7.20) reduces to (5.19). The central limit\ntheorem for the MLE in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.11",
      "section_title": "Bootstrapping Multivariate Data       175",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10 generalizes to the following multivariate\nversion.\n\nResult ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.10",
      "section_title": "generalizes to the following multivariate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6. Under suitable assumptions, for large enough sample sizes, the\nmaximum likelihood estimator is approximately normally distributed with\nmean equal to the true parameter vector and with covariance matrix equal\nto the inverse of the Fisher information matrix.\n\n   Computation of the expectation in I(\u03b8) can be challenging. Programming\nthe second derivatives can be di\ufb03cult as well, especially for complex models.\nIn practice, the observed Fisher information matrix, whose i, jth element is\n\n                                          \u22022\n                         Iij\n                          obs\n                              (\u03b8) = \u2212           log{L(\u03b8)}                   (7.21)\n                                        \u2202\u03b8i \u2202\u03b8j\n\nis often used. The observed Fisher information matrix is, of course, the mul-\ntivariate analog of (5.21). Using observed information obviates the need to\ncompute the expectation. Moreover, the Hessian matrix can be computed nu-\nmerically by \ufb01nite di\ufb00erences, for example, using R\u2019s fdHess() function in\nthe nlme package. Also, as demonstrated in several examples in Chap. 5, if\nhessian=TRUE in the call to optim(), then the Hessian matrix is returned\nwhen the negative log-likelihood is minimized by that function.\n    Inverting the observed Fisher information matrix computed by \ufb01nite dif-\nferences is the most commonly used method for obtaining standard errors. The\nadvantage of this approach is that only the computation of the log-likelihood\nis necessary, and of course this computation is necessary simply to compute\nthe MLE.\n    The key point is that there is an explicit method of calculating standard er-\nrors for maximum likelihood estimators. The calculation of standard errors of\nmaximum likelihood estimators by computing and then inverting the observed\nFisher information matrix is routinely programmed into statistical software,\ne.g., by the R function fitdistr() used to \ufb01t univariate distributions.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.6",
      "section_title": "Under suitable assumptions, for large enough sample sizes, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.11 Bootstrapping Multivariate Data\nWhen resampling multivariate data, the dependencies within the observation\nvectors need to be preserved. Let the vectors Y 1 , . . . , Y n be an i.i.d. sample\nof multivariate data. In model-free resampling, the vectors Y 1 , . . . , Y n are\nsampled with replacement. There is no resampling of the components within\na vector. Resampling within vectors would make their components mutually\nindependent and would not mimic the actual data where the components are\ndependent. Stated di\ufb00erently, if the data are in a spreadsheet (or matrix)\nwith rows corresponding to observations and columns to variables, then one\nsamples entire rows.\n\f176    7 Multivariate Statistical Models\n\n    Model-based resampling simulates vectors from the multivariate distribu-\ntion of the Y i , for example, from a multivariate t-distribution with the mean\nvector, covariance matrix, and degrees of freedom equal to the MLEs.\n\n                                          GE                                                  IBM\n\n\n\n\n                                                                 Frequency\n                                                                             80\n        Frequency\n                    80\n\n\n\n\n                                                                             40\n                    40\n                    0\n\n\n\n\n                                                                             0\n                           \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.11",
      "section_title": "Bootstrapping Multivariate Data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2         0.2         0.6    1.0                      \u22120.2    0.2        0.6\n                                               \u03b1                                                  \u03b1\n\n                                         Mobil                                               CRSP\n        Frequency\n\n\n\n\n                                                                 Frequency\n                    80\n\n\n\n\n                                                                             80\n                    40\n\n\n\n\n                                                                             40\n                    0\n\n\n\n\n                                                                             0\n\n\n\n\n                         \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.2         0.6    1.0                      \u22120.2    0.2        0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2     0.0         0.2    0.4   0.6                    \u22121.0     \u22120.6       \u22120.2\n                                               \u03b1                                               \u03b1\n\n                                                   \u0002 for each of the returns series\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.2",
      "section_title": "0.0         0.2    0.4   0.6                    \u22121.0     \u22120.6       \u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9. Histograms of 200 bootstrapped values of \u03b1\nin the CRSPday data set.\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "Histograms of 200 bootstrapped values of \u03b1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.7. Bootstrapping the skewed t \ufb01t to CRSPday\n\n    In Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.7",
      "section_title": "Bootstrapping the skewed t \ufb01t to CRSPday",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.5 the skewed t-model was \ufb01t to the CRSPday data. This\nexample continues that analysis by bootstrapping the estimator of \u03b1 for each\nof the four returns series. Histograms of 200 bootstrap values of \u03b1 \u0002 are found\nin Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.5",
      "section_title": "the skewed t-model was \ufb01t to the CRSPday data. This",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9. Bootstrap percentile 95 % con\ufb01dence intervals include 0 for all four\nstocks, so there is no strong evidence of skewness in any of the returns series.\n    Despite the large sample size of 2,528, the estimators of \u03b1 do not appear to\nbe normally distributed. We can see in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "Bootstrap percentile 95 % con\ufb01dence intervals include 0 for all four",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.9 that they are right-skewed for\nthe three stocks and left-skewed for the CRSP returns. The distribution of \u03b1    \u0002\nalso appears heavy-tailed. The excess kurtosis coe\ufb03cient of the 200 bootstrap\nvalues of \u03b1\u0002 is 2.38, 1.33, 3.18, and ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.9",
      "section_title": "that they are right-skewed for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.38 for the four series.\n    The central limit theorem for the MLE guarantees that \u03b1     \u0002 is nearly nor-\nmally distributed for su\ufb03ciently large samples, but this theorem does not tell\nus how large the sample size must be. Fortunately, the sample size needed for\nnear normality is often small, but there are exceptions. We see in this example\nthat in such cases the sample size must be very large indeed since 2,528 is not\nlarge enough. This is a major reason for preferring to construct con\ufb01dence\nintervals using the bootstrap rather than a normal approximation.\n\f                                                             ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.38",
      "section_title": "for the four series.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13 R Lab     177\n\n   A bootstrap sample of the returns was drawn with the following R code.\nThe returns are in the matrix dat and yboot is a bootstrap sample chosen by\ntaking a random sample of the rows of dat, with replacement of course.\n   yboot = dat[sample((1:n), n, replace = TRUE), ]\n\n                                                                              \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "R Lab     177",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.12 Bibliographic Notes\n\nThe multivariate central limit theorem for the MLE is stated precisely and\nproved in textbooks on asymptotic theory such as Lehmann (1999) and van\nder Vaart (1998). The multivariate skewed t-distribution is in Azzalini and\nCapitanio (2003) and Azzalini (2014).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.12",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13 R Lab\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13.1 Equity Returns\n\nThis section uses the data set berndtInvest on the book\u2019s web site and taken\noriginally from R\u2019s fEcofin package. This data set contains monthly returns\nfrom January 1, 1987, to December 1, 1987, on 16 equities. There are 18\ncolumns. The \ufb01rst column is the date and the last is the risk-free rate.\n    In the lab we will only use the \ufb01rst four equities. The following code\ncomputes the sample covariance and correlation matrices for these returns.\n   berndtInvest = read.csv(\"berndtInvest.csv\")\n   Berndt = as.matrix(berndtInvest[, 2:5])\n   cov(Berndt)\n   cor(Berndt)\n\nIf you wish, you can also plot a scatterplot matrix with the following R code.\n   pairs(Berndt)\n\nProblem 1 Suppose the four variables being used are denoted by X1 , . . . , X4 .\nUse the sample covariance matrix to estimate the variance of 0.5X1 + 0.3X2 +\n0.2X3 . (Useful R facts: \u201ct(a)\u201d is the transpose of a vector or matrix a and\n\u201ca %*% b\u201d is the matrix product of a and b.)\n\n   Fit a multivariate-t model to the data using the function cov.trob in the\nMASS package. This function computes the MLE of the mean and covariance\nmatrix with a \ufb01xed value of \u03bd. To \ufb01nd the MLE of \u03bd, the following code\ncomputes the pro\ufb01le log-likelihood for \u03bd.\n\f178      7 Multivariate Statistical Models\n\n      library(MASS) # needed for cov.trob\n      library(mnormt) # needed for dmt\n      df = seq(2.5, 8, 0.01)\n      n = length(df)\n      loglik_profile = rep(0, n)\n      for(i in 1:n)\n      {\n        fit = cov.trob(Berndt, nu = df[i])\n        mu = as.vector(fit$center)\n        sigma = matrix(fit$cov, nrow = 4)\n        loglik_profile[i] = sum(log(dmt(Berndt, mean = fit$center,\n           S= f it$cov, df = df[i])))\n      }\n\nProblem 2 Using the results produced by the code above, \ufb01nd the MLE of\n\u03bd and a 90 % pro\ufb01le likelihood con\ufb01dence interval for \u03bd. Include your R code\nwith your work. Also, plot the pro\ufb01le log-likelihood and indicate the MLE and\nthe con\ufb01dence interval on the plot.\n\n\n   Section ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "1 Equity Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13.3 demonstrates how the MLE for a multivariate t-model can\nbe \ufb01t directly with the optim function, rather than pro\ufb01le likelihood.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "3 demonstrates how the MLE for a multivariate t-model can",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13.2 Simulating Multivariate t-Distributions\n\nThe following code generates and plots four bivariate samples. Each sam-\nple has univariate marginals that are standard t3 -distributions. However,\nthe dependencies are di\ufb00erent.\n      library(MASS) # need for mvrnorm\n      par(mfrow=c(1,4))\n      N = 2500\n      nu = 3\n\n      set.seed(5640)\n      cov=matrix(c(1, 0.8, 0.8, 1), nrow = 2)\n      x= mvrnorm(N, mu = c(0, 0), Sigma = cov)\n      w = sqrt(nu / rchisq(N, df = nu))\n      x = x * cbind(w, w)\n      plot(x, main = \"(a)\")\n\n      set.seed(5640)\n      cov=matrix(c(1, 0.8, 0.8, 1),nrow = 2)\n      x= mvrnorm(N, mu = c(0, 0), Sigma = cov)\n      w1 = sqrt(nu / rchisq(N, df = nu))\n      w2 = sqrt(nu / rchisq(N, df = nu))\n      x = x * cbind(w1, w2)\n      plot(x, main = \"(b)\")\n\f                                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "2 Simulating Multivariate t-Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13 R Lab     179\n\n   set.seed(5640)\n   cov=matrix(c(1, 0, 0, 1), nrow = 2)\n   x= mvrnorm(N, mu = c(0, 0), Sigma = cov)\n   w1 = sqrt(nu / rchisq(N, df = nu))\n   w2 = sqrt(nu / rchisq(N, df = nu))\n   x = x * cbind(w1, w2)\n   plot(x, main = \"(c)\")\n\n   set.seed(5640)\n   cov=matrix(c(1, 0, 0, 1), nrow = 2)\n   x= mvrnorm(N, mu = c(0, 0), Sigma = cov)\n   w = sqrt(nu / rchisq(N, df = nu))\n   x = x * cbind(w, w)\n   plot(x, main = \"(d)\")\n    Note the use of these R commands: set.seed to set the seed of the ran-\ndom number generator, mvrnorm to generate multivariate normally distributed\nvectors, rchisq to generate \u03c72 -distributed random numbers, cbind to bind\ntogether vectors as the columns of a matrix, and matrix to create a matrix\nfrom a vector. In R, \u201ca * b\u201d is elementwise multiplication of same-size matri-\nces a and b, and \u201ca %*% b\u201d is matrix multiplication of conforming matrices\na and b.\n\nProblem 3 Which sample has independent variates? Explain your answer.\n\n\nProblem 4 Which sample has variates that are correlated but do not have\ntail dependence? Explain your answer.\n\n\nProblem 5 Which sample has variates that are uncorrelated but with tail\ndependence? Explain your answer.\n\n\n\nProblem 6* Suppose that (X, Y ) are the returns on two assets and have a\nmultivariate t-distribution with degrees of freedom, mean vector, and covari-\nance matrix\n                             \u0007       \b          \u0007           \b\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "R Lab     179",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001              0.10 0.03\n               \u03bd = 5, \u03bc =              , \u03a3=                   .\n                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.001",
      "section_title": "0.10 0.03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.002              0.03 0.15\n\nThen R = (X + Y )/2 is the return on an equally weighted portfolio of the two\nassets.\n(a) What is the distribution of R?\n(b) Write an R program to generate a random sample of size 10,000 from\n    the distribution of R. Your program should also compute the ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.002",
      "section_title": "0.03 0.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01 upper\n    quantile of this sample and the sample average of all returns that exceed\n    this quantile. This quantile and average will be useful later when we study\n    risk analysis.\n\f180      7 Multivariate Statistical Models\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.01",
      "section_title": "upper",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13.3 Fitting a Bivariate t-Distribution\n\nWhen you run the R code that follows this paragraph, you will compute the\nMLE for a bivariate t-distribution \ufb01t to CRSP returns data. A challenge\nwhen \ufb01tting a multivariate distribution is enforcing the constraint that the\nscale matrix (or the covariance matrix) must be positive de\ufb01nite. One way to\nmeet this challenge is to let the scale matrix be AT A, where A is an upper\ntriangular matrix. (It is easy to show that AT A is positive semide\ufb01nite if A\nis any square matrix. Because a scale or covariance matrix is symmetric, only\nthe entries on and above the main diagonal are free parameters. In order for\nA to have the same number of free parameters as the covariance matrix, we\nrestrict A to be upper triangular.)\n      library(mnormt)\n      data(CRSPday, package = \"Ecdat\")\n      Y = CRSPday[ , c(5, 7)]\n      loglik = function(par)\n      {\n      mu = par[1:2]\n      A = matrix(c(par[3], par[4], 0, par[5]), nrow = 2, byrow = T)\n      scale_matrix = t(A) %*% A\n      df = par[6]\n      -sum(log(dmt(Y, mean = mu, S = scale_matrix, df = df)))\n      }\n      A = chol(cov(Y))\n      start = as.vector(c(apply(Y, 2, mean),\n         A[1, 1], A[1, 2], A[2, 2], 4))\n      fit_mvt = optim(start, loglik, method = \"L-BFGS-B\",\n         lower = c(-0.02, -0.02, -0.1, -0.1, -0.1, 2),\n         upper = c(0.02, 0.02, 0.1, 0.1, 0.1, 15), hessian = T)\n\n\nProblem 7* Let \u03b8 = (\u03bc1 , \u03bc2 , A1,1 , A1,2 , A2,2 , \u03bd), where \u03bcj is the mean of the\njth variable, A1,1 , A1,2 , and A2,2 are the nonzero elements of A, and \u03bd is the\ndegrees-of-freedom parameter.\n\n(a) What does the code A = chol(cov(Y)) do?\n(b) Find \u03b8 \u0002ML , the MLE of \u03b8.\n(c) Find the Fisher information matrix for \u03b8. (Hint: The Hessian is part of\n     the object fit mvt. Also, the R function solve will invert a matrix.)\n(d) Find the standard errors of the components of \u03b8    \u0002ML using the Fisher\n     information matrix.\n(e) Find the MLE of the covariance matrix of the returns.\n(f ) Find the MLE of \u03c1, the correlation between the two returns (Y1 and Y2 ).\n\f                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.13",
      "section_title": "3 Fitting a Bivariate t-Distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.14 Exercises   181\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.14",
      "section_title": "Exercises   181",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.14 Exercises\n1. Suppose that E(X) = 1, E(Y ) = 1.5, Var(X) = 2, Var(Y ) = 2.7, and\n   Cov(X, Y ) = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "7.14",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8.\n   (a) What are E(0.2X + 0.8Y ) and Var(0.2X + 0.8Y )?\n   (b) For what value of w is Var{wX + (1 \u2212 w)Y } minimized? Suppose that\n       X is the return on one asset and Y is the return on a second asset.\n       Why would it be useful to minimize Var{wX + (1 \u2212 w)Y }?\n2. Let X1 , X2 , Y1 , and Y2 be random variables.\n   (a) Show that Cov(X1 + X2 , Y1 + Y2 ) = Cov(X1 , Y1 ) + Cov(X1 , Y2 ) +\n       Cov(X2 , Y1 ) + Cov(X2 , Y2 ).\n   (b) Generalize part (a) to an arbitrary number of Xi s and Yi s.\n3. Verify formulas (A.24)\u2013(A.27).\n4. (a) Show that\n                                  E{X \u2212 E(X)} = 0\n       for any random variable X.\n   (b) Use the result in part (a) and Eq. (A.31) to show that if two random\n       variables are independent then they are uncorrelated.\n5. Show that if X is uniformly distributed on [\u2212a, a] for any a > 0 and if\n   Y = X 2 , then X and Y are uncorrelated but they are not independent.\n6. Verify the following results that were stated in Sect. 7.3:\n\n                            E(wT X) = wT {E(X)}\n\n   and\n                                   N   N\n                    Var(wT X) =             wi wj Cov(Xi , Xj )\n                                  i=1 j=1\n\n                                = Var(wT X)wT COV(X)w.\n\n7. Suppose Y = (Y1 , Y2 , Y3 ) has covariance matrix\n                                        \u239b             \u239e\n                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "(a) What are E(0.2X + 0.8Y ) and Var(0.2X + 0.8Y )?",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and if\n   Y = X 2 , then X and Y are uncorrelated but they are not independent.\n6. Verify the following results that were stated in Sect. 7.3:",
        "start": 839,
        "end": 987
      }
    ]
  },
  {
    "content": "1.0 0.9 a\n                           COV (Y) = \u239d ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "0.9 a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 1.0 0.9 \u23a0\n                                            a ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "1.0 0.9 \u23a0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 1.0\n\n   for some unknown value a. Use Eq. (7.7) and the fact that the variance\n   of a random variable is always \u2265 0 to show that a cannot equal 0.\n\f182    7 Multivariate Statistical Models\n\nReferences\nAzzalini, A. (2014) The Skew-Normal and Related Families (Institute of Math-\n  ematical Statistics Monographs, Book 3), Cambridge University Press.\nAzzalini, A., and Capitanio, A. (2003) Distributions generated by pertur-\n  bation of symmetry with emphasis on a multivariate skew t distribution.\n  Journal of the Royal Statistics Society, Series B, 65, 367\u2013389.\nLehmann, E. L. (1999) Elements of Large-Sample Theory, Springer-Verlag,\n  New York.\nvan der Vaart, A. W. (1998) Asymptotic Statistics, Cambridge University\n  Press, Cambridge.\n\f8\nCopulas\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.9",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1 Introduction\n\nCopulas are a popular framework for both de\ufb01ning multivariate distributions\nand modeling multivariate data. A copula characterizes the dependence\u2014and\nonly the dependence\u2014between the components of a multivariate distribution;\nthey can be combined with any set of univariate marginal distributions to\nform a full joint distribution. Consequently, the use of copulas allows us to\ntake advantage of the wide variety of univariate models that are available.\n    The primary \ufb01nancial application of copula models is risk assessment and\nmanagement of portfolios that contain assets which exhibit co-movements in\nextreme behavior. For example, a pair of assets may have weakly correlated\nreturns, but their largest losses may tend to occur in the same periods. They\nare commonly applied to portfolios of loans, bonds, and collateralized debt\nobligations (CDOs). Their misapplication in \ufb01nance is also well-documented,\nas referenced in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8.\n    A copula is a multivariate CDF whose univariate marginal distributions are\nall Uniform(0,1). Suppose that Y = (Y1 , . . . , Yd ) has a multivariate CDF FY\nwith continuous marginal univariate CDFs FY1 , . . . , FYd . Then, by Eq. (A.9)\nin Appendix A.9.2, each of FY1 (Y1 ), . . . , FYd (Yd ) is distributed Uniform(0,1).\nTherefore, the CDF of {FY1 (Y1 ), . . . , FYd (Yd )} is a copula. This CDF is called\nthe copula of Y and denoted by CY . CY contains all information about de-\npendencies among the components of Y but has no information about the\nmarginal CDFs of Y .\n    It is easy to \ufb01nd a formula for CY . To avoid technical issues, in this section\nwe will assume that all random variables have continuous, strictly increasing\nCDFs. More precisely, the CDFs are assumed to be increasing on their sup-\nport. For example, the standard exponential CDF\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                                 183\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 8\n\f184     8 Copulas\n                                           \u000e\n                                               1 \u2212 e\u2212y , y \u2265 0,\n                                 F (y) =\n                                               0,        y < 0,\n\nhas support [0, \u221e) and is strictly increasing on that set. The assumption that\nthe CDF is continuous and strictly increasing is reasonable in many \ufb01nancial\napplications, but it is avoided in more mathematically advanced texts; see\nSect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "A copula is a multivariate CDF whose univariate marginal distributions are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8.\n   Since CY is the CDF of {FY1 (Y1 ), . . . , FYd (Yd )}, by the de\ufb01nition of a CDF\nwe have\n\n            CY (u1 , . . . , ud ) = P {FY1 (Y1 ) \u2264 u1 , . . . , FYd (Yd ) \u2264 ud }\n                                  = P Y1 \u2264 FY\u22121  1\n                                                   (u1 ), . . . , Yd \u2264 FY\u22121\n                                                                          d\n                                                                            (ud )\n                                  = FY FY\u22121\n                                          1\n                                            (u1 ), . . . , FY\u22121\n                                                              d\n                                                                (ud ) .                     (8.1)\n\nNext, letting uj = FYj (yj ), for j = 1, . . . , d, in (8.1) we see that\n\n                    FY (y1 , . . . , yd ) = CY {FY1 (y1 ), . . . , FYd (yd )} .             (8.2)\n\nEquation (8.2) is part of a famous theorem due to Sklar which states that\nthe joint CDF FY can be decomposed into the copula CY , which contains all\ninformation about the dependencies among (Y1 , . . . , Yd ), and the univariate\nmarginal CDFs FY1 , . . . , FYd , which contain all information about the univari-\nate marginal distributions.\n    Let\n                                             \u2202d\n                 cY (u1 , . . . , ud ) =               CY (u1 , . . . , ud ) (8.3)\n                                         \u2202u1 \u00b7 \u00b7 \u00b7 \u2202ud\nbe the density associated with CY . By di\ufb00erentiating (8.2), we \ufb01nd that the\ndensity of Y is equal to\n\n      fY (y1 , . . . , yd ) = cY {FY1 (y1 ), . . . , FYd (yd )}fY1 (y1 ) \u00b7 \u00b7 \u00b7 fYd (yd ),   (8.4)\n\nin which fY1 , . . . , fYd are the univariate marginal densities of Y1 , . . . , Yd ,\nrespectively.\n    One important property of copulas is that they are invariant to strictly\nincreasing transformations of the component variables. More precisely, sup-\npose that gj is strictly increasing and Xj = gj (Yj ) for j = 1, . . . , d. Then\nX = (X1 , . . . , Xd ) and Y have the same copulas. To see this, \ufb01rst note that\nthe CDF of X is\n\n             FX (x1 , . . . , xd ) = P {g1 (Y1 ) \u2264 x1 , . . . , gd (Yd ) \u2264 xd }\n                                  = P Y1 \u2264 g1\u22121 (x1 ), . . . , Yd \u2264 gd\u22121 (xd )\n                                  = FY g1\u22121 (x1 ), . . . , gd\u22121 (xd ) ,                     (8.5)\n\nand therefore the CDF of Xj is\n\n                                 FXj (xj ) = FYj gj\u22121 (xj ) .\n\f                                                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "Since CY is the CDF of {FY1 (Y1 ), . . . , FYd (Yd )}, by the de\ufb01nition of a CDF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.2 Special Copulas    185\n\nConsequently,                                    (              )\n                                \u22121                   \u22121\n                               FX j\n                                    (u j ) = g j   F Y j\n                                                         (u j )\n\nand                              (            )\n                             gj\u22121 FX\n                                   \u22121\n                                     j\n                                       (u j )   = FY\u22121\n                                                     j\n                                                       (uj ),                             (8.6)\n\nand by applying (8.1) to X, followed by (8.5), (8.6), and then applying (8.1)\nto Y , we conclude that the copula of X is\n                                       \u22121                 \u22121\n          CX (u1 , . . . , ud ) = FX FX   (u1 ), . . . , FX   (ud )\n                                    \u0018 \u221211 \u22121                d\n                                                                             \u0019\n                                = FY g1 FX1 (u1 ) , . . . , gd\u22121 FX \u22121\n                                                                     d\n                                                                       (ud )\n                             = FY FY\u22121 1\n                                          (u1 ), . . . , FY\u22121\n                                                            d\n                                                              (ud )\n                             = CY (u1 , . . . , ud ).\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.2",
      "section_title": "Special Copulas    185",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.2 Special Copulas\nAll d-dimensional copula functions C have domain [0, 1]d and range [0, 1].\nThere are three copulas of special interest because they represent indepen-\ndence and two extremes of dependence.\n   The d-dimensional independence copula C0 is the CDF of d mutually in-\ndependent Uniform(0,1) random variables. It equals\n\n                              C0 (u1 , . . . , ud ) = u1 \u00b7 \u00b7 \u00b7 ud ,                       (8.7)\n\nand the associated density is uniform on [0, 1]d ; that is, c0 (u1 , . . . , ud ) = 1 on\n[0, 1]d , and zero elsewhere.\n     The d-dimensional co-monotonicity copula C+ characterizes perfect posi-\ntive dependence. Let U be Uniform(0,1). Then, the co-monotonicity copula is\nthe CDF of U = (U, . . . , U ); that is, U contains d copies of U so that all of\nthe components of U are equal. Thus,\n\n        C+ (u1 , . . . , ud ) = P (U \u2264 u1 , . . . , U \u2264 ud )\n                           = P {U \u2264 min(u1 , . . . , ud )} = min(u1 , . . . , ud ).\n\nThe co-monotonicity copula is also an upper bound for all copula functions:\nC(u1 , . . . , ud ) \u2264 C+ (u1 , . . . , ud ), for all (u1 , . . . , ud ) \u2208 [0, 1]d .\n   The two-dimensional counter-monotonicity copula C\u2212 is de\ufb01ned as the\nCDF of (U, 1 \u2212 U ), which has perfect negative dependence. Therefore,\n\n           C\u2212 (u1 , u2 ) = P (U \u2264 u1 , 1 \u2212 U \u2264 u2 )\n                         = P (1 \u2212 u2 \u2264 U \u2264 u1 ) = max(u1 + u2 \u2212 1, 0).                    (8.8)\n\nIt is easy to derive the last equality in (8.8). If 1 \u2212 u2 > u1 , then the event\n{1 \u2212 u2 \u2264 U \u2264 u1 } is impossible, so the probability is 0. Otherwise, the\n\f186     8 Copulas\n\nprobability is the length of the interval (1 \u2212 u2 , u1 ), which is u1 + u2 \u2212 1.\nAll two-dimensional copula functions are bounded below by (8.8). It is not\npossible to have a counter-monotonicity copula with d > 2. If, for example,\nU1 is counter-monotonic to U2 and U2 is counter-monotonic to U3 , then U1\nand U3 will be co-monotonic, not counter-monotonic. However, a lower bound\nfor all copula functions is: max(u1 + \u00b7 \u00b7 \u00b7 + ud + 1 \u2212 d, 0) \u2264 C(u1 , . . . , ud ), for\nall (u1 , . . . , ud ) \u2208 [0, 1]d . This lower bound is obtainable only point-wise, but\nit is not itself a copula function for d > 2.\n     To use copulas to model multivariate dependencies, we next consider para-\nmetric families of copulas.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.2",
      "section_title": "Special Copulas",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "u1 , then the event\n{1 \u2212 u2 \u2264 U \u2264 u1 } is impossible, so the probability is 0. Otherwise, the\n\f186     8 Copulas",
        "start": 1478,
        "end": 1594
      },
      {
        "language": "r",
        "code": "2. If, for example,\nU1 is counter-monotonic to U2 and U2 is counter-monotonic to U3 , then U1\nand U3 will be co-monotonic, not counter-monotonic. However, a lower bound\nfor all copula functions is: max(u1 + \u00b7 \u00b7 \u00b7 + ud + 1 \u2212 d, 0) \u2264 C(u1 , . . . , ud ), for\nall (u1 , . . . , ud ) \u2208 [0, 1]d . This lower bound is obtainable only point-wise, but\nit is not itself a copula function for d > 2.\n     To use copulas to model multivariate dependencies, we next consider para-\nmetric families of copulas.",
        "start": 1803,
        "end": 2303
      }
    ]
  },
  {
    "content": "8.3 Gaussian and t-Copulas\n\nMultivariate normal and multivariate t-distributions o\ufb00er a convenient way to\ngenerate families of copulas. Let Y = (Y1 , . . . , Yd ) have a multivariate normal\ndistribution. Since CY depends only on the dependencies within Y , not the\nunivariate marginal distributions, CY depends only on the d \u00d7 d correlation\nmatrix of Y , which will be denoted by \u03a9. Therefore, there is a one-to-one cor-\nrespondence between correlation matrices and Gaussian copulas. The Gaus-\nsian copula1 with correlation matrix \u03a9 will be denoted C Gauss (u1 . . . , ud |\u03a9).\n    If a random vector Y has a Gaussian copula, then Y is said to have\na meta-Gaussian distribution. This does not, of course, mean that Y has a\nmultivariate Gaussian distribution, since the univariate marginal distributions\nof Y could be any distributions at all. A d-dimensional Gaussian copula whose\ncorrelation matrix is the identity matrix, so that all correlations are zero,\nis the d-dimensional independence copula. A Gaussian copula will converge\nto the co-monotonicity copula C+ if all correlations in \u03a9 converge to 1. In\nthe bivariate case, as the pair-wise correlation converges to \u22121, the copula\nconverges to the counter-monotonicity copula C\u2212 .\n    Similarly, let C t (u1 . . . , ud |\u03a9, \u03bd) denote the copula of a random vector that\nhas a multivariate t-distribution with tail index2 \u03bd and correlation matrix \u03a9.3\nFor multivariate t random vectors the tail index \u03bd a\ufb00ects both the univariate\nmarginal distributions and the tail dependence between components, so \u03bd\nis a parameter of the t-copula C t . We will see in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.3",
      "section_title": "Gaussian and t-Copulas",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.6 that \u03bd similarly\ndetermines the amount of tail dependence of random vectors that have a t-\ncopula. Such a vector is said to have a meta-t-distribution.\n1\n  Gaussian and normal distributions are synonymous and the Gaussian copula may\n  also be referred to as the normal copula, especially in R functions.\n2\n  The tail index parameter for the t-distribution is also commonly referred to as\n  the degrees-of-freedom parameter by its association with the theory of linear\n  regression, and some R functions use the abbreviations df or nu.\n3\n  There is a minor technical issue here if \u03bd \u2264 2. In this case, the t-distribution does\n  not have covariance and correlation matrices. However, it still has a scale matrix\n  and we will assume that the scale matrix is equal to some correlation matrix \u03a9.\n\f                                                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.6",
      "section_title": "that \u03bd similarly",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4 Archimedean Copulas     187\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "Archimedean Copulas     187",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4 Archimedean Copulas\nAn Archimedean copula with a strict generator has the form\n\n                  C(u1 , . . . , ud ) = \u03d5\u22121 {\u03d5(u1 ) + \u00b7 \u00b7 \u00b7 + \u03d5(ud )},        (8.9)\n\nwhere the generator function \u03d5 satis\ufb01es the following conditions\n 1. \u03d5 is a continuous, strictly decreasing, and convex function mapping [0, 1]\n    onto [0, \u221e],\n 2. \u03d5(0) = \u221e, and\n 3. \u03d5(1) = 0.\n    A plot of a generator function is shown in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "Archimedean Copulas",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1 to illustrate these prop-\nerties. It was generated using the iPsi() function from R\u2019s copula package\nwith the following commands.\n1 library(copula)\n2 u = seq(0.000001, 1, length=500)\n3 frank = iPsi(copula=archmCopula(family=\"frank\", param=1), u)\n\n4 plot(u, frank, type=\"l\", lwd=3, ylab=expression(phi(u)))\n\n5 abline(h=0) ; abline(v=0)\n\n\nIt is possible to relax assumption 2, but then the generator is not called strict\nand construction of the copula is more complex. The generator function \u03d5 is\nnot unique; for example, a\u03d5, in which a is any positive constant, generates the\nsame copula as \u03d5. The independence copula C0 is an Archimedean copula with\ngenerator function \u03d5(u) = \u2212 log(u). There are many families of Archimedean\ncopulas, but we will only look at four, the Frank, Clayton, Gumbel, and Joe\ncopulas.\n    Notice that in (8.9), the value of C(u1 , . . . , ud ) is unchanged if we per-\nmute u1 , . . . , ud . A distribution with this property is called exchangeable. One\nconsequence of exchangeability is that both Kendall\u2019s and Spearman\u2019s rank\ncorrelation introduced later in Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "to illustrate these prop-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5 are the same for all pairs of variables.\nArchimedean copulas are most useful in the bivariate case or in applications\nwhere we expect all pairs to have similar dependencies.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "are the same for all pairs of variables.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4.1 Frank Copula\n\nThe Frank copula has generator\n                               \u0007 \u2212\u03b8u     \b\n                                e    \u22121\n             \u03d5Fr (u|\u03b8) = \u2212 log             ,           \u2212\u221e < \u03b8 < \u221e.\n                                 e\u2212\u03b8 \u2212 1\nThe inverse generator is\n                                 1\n                    \u03d5\u22121\n                     Fr (y|\u03b8) = \u2212 log e\n                                        \u2212y \u2212\u03b8\n                                          (e \u2212 1) + 1 .\n                                 \u03b8\n\f188    8 Copulas\n\nTherefore, by (8.9), the bivariate Frank copula is\n\n\n\n\n                     12\n                     10\n                     8\n              \u03c6(u)\n                     6\n                     4\n                     2\n                     0\n\n\n\n\n                          ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "1 Frank Copula",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0    0.2     0.4       0.6    0.8        1.0\n                                               u\n\n          Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.2     0.4       0.6    0.8        1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1. Generator function for the Frank copula with \u03b8 = 1.\n\n\n                                 \u000e                           \u000f\n                             1        (e\u2212\u03b8u1 \u2212 1)(e\u2212\u03b8u2 \u2212 1)\n         CFr (u1 , u2 |\u03b8) = \u2212 log 1 +                          .            (8.10)\n                             \u03b8               e\u2212\u03b8 \u2212 1\n\nThe case \u03b8 = 0 requires some care, since plugging this value into (8.10) gives\n0/0. Instead, one must evaluate the limit of (8.10) as \u03b8 \u2192 0. Using the approx-\nimations ex \u2212 1 \u2248 x and log(1 + x) \u2248 x as x \u2192 0, one can show that as \u03b8 \u2192 0,\nCFr (u1 , u2 |\u03b8) \u2192 u1 u2 , the bivariate independence copula C0 . Therefore, for\n\u03b8 = 0 we de\ufb01ne the Frank copula to be the independence copula.\n   It is interesting to study the limits of CFr (u1 , u2 |\u03b8) as \u03b8 \u2192 \u00b1\u221e. As \u03b8 \u2192\n\u2212\u221e, the bivariate Frank copula converges to the counter-monotonicity copula\nC\u2212 . To see this, \ufb01rst note that as \u03b8 \u2192 \u2212\u221e,\n                                         1   (                  )\n                     CFr (u1 , u2 |\u03b8) \u223c \u2212 log 1 + e\u2212\u03b8(u1 +u2 \u22121) .          (8.11)\n                                         \u03b8\nIf u1 + u2 \u2212 1 > 0, then as \u03b8 \u2192 \u2212\u221e, the exponent \u2212\u03b8(u1 + u2 \u2212 1) in (8.11)\nconverges to \u221e and\n                     (                 )\n                 log 1 + e\u2212\u03b8(u1 +u2 \u22121) \u223c \u2212\u03b8(u1 + u2 \u2212 1),\n\nso that CFr (u1 , u2 |\u03b8) \u2192 u1 + u2 \u2212 1. Similarly, if u1 + u2 \u2212 1 < 0, then \u2212\u03b8(u1 +\nu2 \u2212 1) \u2192 \u2212\u221e, and CFr (u1 , u2 |\u03b8) \u2192 0. Putting these results together, we see\nthat CFr (u1 , u2 |\u03b8) converges to max(0, u1 + u2 \u2212 1), the counter-monotonicity\ncopula C\u2212 , as \u03b8 \u2192 \u2212\u221e.\n    As \u03b8 \u2192 \u221e, CFr (u1 , u2 |\u03b8) \u2192 min(u1 , u2 ), the co-monotonicity copula C+ .\nVeri\ufb01cation of this is left as an exercise for the reader.\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Generator function for the Frank copula with \u03b8 = 1.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, then as \u03b8 \u2192 \u2212\u221e, the exponent \u2212\u03b8(u1 + u2 \u2212 1) in (8.11)\nconverges to \u221e and\n                     (                 )\n                 log 1 + e\u2212\u03b8(u1 +u2 \u22121) \u223c \u2212\u03b8(u1 + u2 \u2212 1),",
        "start": 1106,
        "end": 1286
      }
    ]
  },
  {
    "content": "8.4 Archimedean Copulas            189\n\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "Archimedean Copulas            189",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.2 contains scatterplots of nine bivariate random samples from\nvarious Frank copulas, all with a sample size of 200 and with values of \u03b8 that\ngive dependencies ranging from strongly negative to strongly positive. Pseudo-\nrandom samples may be generated from the copula distributions discussed in\nthis chapter using the rCopula() function from R\u2019s copula package. The\nconvergence to the counter-monotonicity (co-monotonicity) copula as \u03b8 \u2192\n\u2212\u221e (+\u221e) can be seen in the scatterplots.\n6  set.seed(5640)\n7  theta = c(-100, -50, -10, -1, 0, 5, 20, 50, 500)\n 8 par(mfrow=c(3,3), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n\n 9 for(i in 1:9){\n\n10   U = rCopula(n=200,\n11                copula=archmCopula(family=\"frank\", param=theta[i]))\n12   plot(U, xlab=expression(u[1]), ylab=expression(u[2]),\n13        main=eval(substitute(expression(paste(theta,\" = \",j)),\n14        list(j = as.character(theta[i])))))\n15 }\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.2",
      "section_title": "contains scatterplots of nine bivariate random samples from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4.2 Clayton Copula\nThe Clayton copula, with generator function \u03d5Cl (u|\u03b8) = \u03b81 (u\u2212\u03b8 \u2212 1), \u03b8 > 0, is\n\n               CCl (u1 , . . . , ud |\u03b8) = (u\u2212\u03b8           \u2212\u03b8\n                                            1 + \u00b7 \u00b7 \u00b7 + ud + 1 \u2212 d)\n                                                                    \u22121/\u03b8\n                                                                         .\nWe de\ufb01ne the Clayton copula for \u03b8 = 0 as\n                          lim CCl (u1 , . . . , ud |\u03b8) = u1 \u00b7 \u00b7 \u00b7 ud\n                           \u03b8\u21930\n\nwhich is the independence copula C0 . There is another way to derive this\nresult. As \u03b8 \u2193 0, l\u2019Ho\u0302pital\u2019s rule shows that the generator \u03b81 (u\u2212\u03b8 \u22121) converges\nto \u03d5Cl (u|\u03b8 \u2193 0) = \u2212 log(u) with inverse \u03d5\u22121 Cl (y|\u03b8 \u2193 0) = exp(\u2212y). Therefore,\n\n    lim CCl (u1 , . . . , ud |\u03b8) = \u03d5\u22121\n                                    Cl {\u03d5Cl (u1 |\u03b8 \u2193 0) + \u00b7 \u00b7 \u00b7 + \u03d5Cl (ud |\u03b8 \u2193 0)|\u03b8 \u2193 0}\n    \u03b8\u21930\n                             = exp{\u2212(\u2212 log u1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 log ud )} = u1 \u00b7 \u00b7 \u00b7 ud .\n    It is possible to extend the range of \u03b8 to include \u22121 \u2264 \u03b8 < 0, but then\nthe generator (u\u2212\u03b8 \u2212 1)/\u03b8 is \ufb01nite at u = 0 in violation of assumption 2, of\nstrict generators. Thus, the generator is not strict if \u03b8 < 0. As a result, it is\nnecessary to de\ufb01ne CCl (u1 , . . . , ud |\u03b8) to equal 0 for small values of ui in this\ncase. To appreciate this, consider the bivariate Clayton copula. If \u22121 \u2264 \u03b8 < 0,\nthen u\u2212\u03b8        \u2212\u03b8\n        1 + u2 \u2212 1 < 0 occurs when u1 and u2 are both small. In these cases,\nCCl (u1 , u2 |\u03b8) is set equal to 0. Therefore, there is no probability in the region\nu\u2212\u03b8       \u2212\u03b8\n 1 + u2 \u2212 1 < 0. In the limit, as \u03b8 \u2192 \u22121, there is no probability in the\nregion u1 + u2 < 1.\n    As \u03b8 \u2192 \u22121, the bivariate Clayton copula converges to the counter-\nmonotonicity copula C\u2212 , and as \u03b8 \u2192 \u221e, the Clayton copula converges to\nthe co-monotonicity copula C+ .\n\f190          8 Copulas\n\n                  \u03b8 = \u2212100                          \u03b8 = \u221250                          \u03b8 = \u221210\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "2 Clayton Copula",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, is",
        "start": 93,
        "end": 102
      }
    ]
  },
  {
    "content": "0.8\n\n\n\n\n                                                                         0.8\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                    u2\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                         0.4\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n      0.0\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                        0.0\n            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8\n                     u1                               u1                               u1\n\n                   \u03b8 = \u22121                            \u03b8=0                              \u03b8=5\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                        0.8\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                    u2\n\n                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                        0.4\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                         0.0\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                        0.0\n\n\n\n\n            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8\n                     u1                               u1                               u1\n\n                   \u03b8 = 20                           \u03b8 = 50                           \u03b8 = 500\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                         0.8\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                    u2\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n      0.4\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                        0.0\n      ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                         0.0\n\n\n\n\n            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8\n                     u1                               u1                               u1\n\n     Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4        0.8              0.0   0.4       0.8              0.0   0.4       0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.2. Bivariate random samples of size 200 from various Frank copulas.\n\n\n   Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.2",
      "section_title": "Bivariate random samples of size 200 from various Frank copulas.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.3 contains scatterplots of nine bivariate random samples from\nvarious Clayton copulas, all with a sample size of 200 and with values of \u03b8\nthat give dependencies ranging from counter-monotonicity to co-monotonicity.\n16 set.seed(5640)\n17 theta = c(-0.98, -0.7, -0.3, -0.1, 0.1, 1, 5, 15, 100)\n18 par(mfrow=c(3,3), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n\n19 for(i in 1:9){\n\n20   U = rCopula(n=200,\n21                copula=archmCopula(family=\"clayton\", param=theta[i]))\n22   plot(U, xlab=expression(u[1]), ylab=expression(u[2]),\n23        main=eval(substitute(expression(paste(theta,\" = \",j)),\n24        list(j = as.character(theta[i])))))\n25 }\n\n\nComparing Figs. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.3",
      "section_title": "contains scatterplots of nine bivariate random samples from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.2 and 8.3, we see that the Frank and Clayton copulas are\nrather di\ufb00erent when the amount of dependence is somewhere between these\ntwo extremes. In particular, the Clayton copula\u2019s exclusion of the region\nu\u2212\u03b8     \u2212\u03b8\n 1 + u2 \u2212 1 < 0 when \u03b8 < 0 is evident, especially in the example with\n\f                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.2",
      "section_title": "and 8.3, we see that the Frank and Clayton copulas are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4 Archimedean Copulas                  191\n\n                 \u03b8 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "Archimedean Copulas                  191",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98                          \u03b8 = \u22120.7                           \u03b8 = \u22120.3\n\n     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.98",
      "section_title": "\u03b8 = \u22120.7                           \u03b8 = \u22120.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                           0.8\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                      u2\n     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                        0.4\n\n\n\n\n                                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n     0.0\n\n\n\n\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                           0.0\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8\n                    u1                                u1                                 u1\n\n\n                 \u03b8 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1                           \u03b8 = 0.1                             \u03b8=1\n\n\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.1",
      "section_title": "\u03b8 = 0.1                             \u03b8=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                           0.8\n     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                      u2\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n     0.4\n\n\n\n\n                                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n     0.0\n\n\n\n\n                                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                        0.0\n\n\n\n\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8\n                    u1                                u1                                 u1\n\n                   \u03b8=5                              \u03b8 = 15                             \u03b8 = 100\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                           0.8\n     ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nu2\n\n\n\n\n                                   u2\n\n\n\n\n                                                                      u2\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n     0.4\n\n\n\n\n                                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n     0.0\n\n\n\n\n                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                           0.0\n\n\n\n\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8\n                    u1                                u1                                 u1\n\n  Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4         0.8              0.0   0.4        0.8               0.0   0.4        0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.3. Bivariate random samples of size 200 from various Clayton copulas.\n\n\n\u03b8 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.3",
      "section_title": "Bivariate random samples of size 200 from various Clayton copulas.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7. In contrast, the Frank copula has positive probability on the entire\nunit square. The Frank copula is symmetric about the diagonal from (0, 1) to\n(1, 0), but the Clayton copula does not have this symmetry.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.7",
      "section_title": "In contrast, the Frank copula has positive probability on the entire",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4.3 Gumbel Copula\n\nThe Gumbel copula has the generator \u03d5Gu (u|\u03b8) = (\u2212 log u)\u03b8 , \u03b8 \u2265 1, and\nconsequently is equal to\n                                    \u001a                                      1/\u03b8\n                                                                               \u001b\n      CGu (u1 , . . . , ud |\u03b8) = exp \u2212 (\u2212 log u1 )\u03b8 + \u00b7 \u00b7 \u00b7 + (\u2212 log ud )\u03b8       .\n\nThe Gumbel copula is the independence copula C0 when \u03b8 = 1, and converges\nto the co-monotonicity copula C+ as \u03b8 \u2192 \u221e, but the Gumbel copula cannot\nhave negative dependence.\n\f192          8 Copulas\n\n                          \u03b8 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "3 Gumbel Copula",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1                          \u03b8 = 1.5                          \u03b8=2\n\n\n\n\n                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "\u03b8 = 1.5                          \u03b8=2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                               0.8\n              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n        u2\n\n\n\n\n                                          u2\n\n\n\n\n                                                                           u2\n\n                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n              0.4\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n              0.0\n\n\n\n\n                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                               0.0\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4       0.8              0.0   0.4       0.8              0.0   0.4      0.8\n                            u1                               u1                               u1\n\n                          \u03b8=4                              \u03b8=8                              \u03b8 = 50\n\n\n\n\n                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4       0.8              0.0   0.4       0.8              0.0   0.4      0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n              0.8\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n        u2\n\n\n\n\n                                          u2\n\n\n\n\n                                                                           u2\n\n                                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n              0.4\n\n\n\n\n                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                0.0\n              ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                               0.0\n\n\n\n\n                    ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4       0.8              0.0   0.4       0.8              0.0   0.4      0.8\n                            u1                               u1                               u1\n\n     Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4       0.8              0.0   0.4       0.8              0.0   0.4      0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4. Bivariate random samples of size 200 from various Gumbel copulas.\n\n\n   Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "Bivariate random samples of size 200 from various Gumbel copulas.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4 contains scatterplots of six bivariate random samples from var-\nious Gumbel copulas, with a sample size of 200 and with values of \u03b8 that give\ndependencies ranging from near independence to strong positive dependence.\n26 set.seed(5640)\n27 theta = c(1.1, 1.5, 2, 4, 8, 50)\n28 par(mfrow=c(2,3), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n\n29 for(i in 1:6){\n\n30   U = rCopula(n=200,\n31                copula=archmCopula(family=\"gumbel\", param=theta[i]))\n32   plot(U, xlab=expression(u[1]), ylab=expression(u[2]),\n33        main=eval(substitute(expression(paste(theta,\" = \",j)),\n34        list(j = as.character(theta[i])))))\n35 }\n\n\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "contains scatterplots of six bivariate random samples from var-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.4.4 Joe Copula\nThe Joe copula is similar to the Gumbel copula; it cannot have negative\ndependence, but it allows even stronger upper tail dependence and is closer\nto being the reverse of the Clayton copula in the positive dependence case.\nThe Joe copula has the generator \u03d5Joe (u|\u03b8) = \u2212 log 1 \u2212 (1 \u2212 u)\u03b8 , \u03b8 \u2265 1. In\nthe bivariate case, the Joe copula is equal to\n                           \u0018                                                \u00191/\u03b8\n    CJoe (u1 , u2 |\u03b8) = 1 \u2212 (1 \u2212 u1 )\u03b8 + (1 \u2212 u2 )\u03b8 \u2212 (1 \u2212 u1 )\u03b8 (1 \u2212 u2 )\u03b8      .\nThe Joe copula is the independence copula C0 when \u03b8 = 1, and converges to\nthe co-monotonicity copula C+ as \u03b8 \u2192 \u221e.\n\f                                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.4",
      "section_title": "4 Joe Copula",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5 Rank Correlation              193\n\n                       \u03b8 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "Rank Correlation              193",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1                          \u03b8 = 1.5                            \u03b8=2\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.1",
      "section_title": "\u03b8 = 1.5                            \u03b8=2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                            0.8\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      u2\n\n\n\n\n                                       u2\n\n\n\n\n                                                                          u2\n\n                                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                            0.4\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                               0.0\n                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n           0.0\n\n\n\n\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4       0.8              0.0   0.4       0.8                0.0   0.4      0.8\n                         u1                               u1                                 u1\n\n                       \u03b8=4                              \u03b8=8                                \u03b8 = 50\n\n\n                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4       0.8              0.0   0.4       0.8                0.0   0.4      0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n           0.8\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      u2\n\n\n\n\n                                       u2\n\n\n\n\n                                                                          u2\n                                            ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "u2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n           0.4\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                            0.0\n           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                               0.0\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4       0.8              0.0   0.4       0.8                0.0   0.4      0.8\n                         u1                               u1                                u1\n\n     Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.4       0.8              0.0   0.4       0.8                0.0   0.4      0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5. Bivariate random samples of size 200 from various Joe copulas.\n\n   Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "Bivariate random samples of size 200 from various Joe copulas.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5 contains scatterplots of six bivariate random samples from var-\nious Joe copulas, with a sample size of 200 and with values of \u03b8 that give\ndependencies ranging from near independence to strong positive dependence.\n36 set.seed(5640)\n37 theta = c(1.1, 1.5, 2, 4, 8, 50)\n38 par(mfrow=c(2,3), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n\n39 for(i in 1:6){\n\n40   U = rCopula(n=200,\n41                copula=archmCopula(family=\"joe\", param=theta[i]))\n42   plot(U, xlab=expression(u[1]), ylab=expression(u[2]),\n43        main=eval(substitute(expression(paste(theta,\" = \",j)),\n44        list(j = as.character(theta[i])))))\n45 }\n\n\n   In applications, it is useful that the di\ufb00erent copula families have di\ufb00erent\nproperties, since this increases our ability to \ufb01nd a copula that \ufb01ts the data\nadequately.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "contains scatterplots of six bivariate random samples from var-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5 Rank Correlation\n\nThe Pearson correlation coe\ufb03cient de\ufb01ned by (4.4) is not convenient for \ufb01tting\ncopulas to data, since it depends on the univariate marginal distributions as\nwell as the copula. Rank correlation coe\ufb03cients remedy this problem, since\nthey depend only on the copula.\n\f194     8 Copulas\n\n    For each variable, the ranks of that variable are determined by ordering\nthe observations from smallest to largest and giving the smallest rank 1, the\nnext-smallest rank 2, and so forth. In other words, if Y1 , . . . , Yn is a sample,\nthen the rank of Yi in the sample is equal to 1 if Yi is the smallest observation,\n2 if Yi is the second smallest, and so forth. More mathematically, the rank of\nYi can also be de\ufb01ned by the formula\n                                          n\n                           rank(Yi ) =         I(Yj \u2264 Yi ),                   (8.12)\n                                         j=1\n\nwhich counts the number of observations (including Yi itself) that are less\nthan or equal to Yi . A rank statistic is a statistic that depends on the data\nonly through the ranks. A key property of ranks is that they are unchanged by\nstrictly monotonic transformations of the variables. In particular, the ranks\nare unchanged by transforming each variable by its CDF, so the distribution\nof any rank statistic depends only on the copula of the observations, not on\nthe univariate marginal distributions.\n    We will be concerned with rank statistics that measure statistical associ-\nation between pairs of variables. These statistics are called rank correlations.\nThere are two rank correlation coe\ufb03cients in widespread usage, Kendall\u2019s tau\nand Spearman\u2019s rho.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "Rank Correlation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5.1 Kendall\u2019s Tau\n\nLet (Y1 , Y2 ) be a bivariate random vector and let (Y1\u2217 , Y2\u2217 ) be an independent\ncopy of (Y1 , Y2 ). Then (Y1 , Y2 ) and (Y1\u2217 , Y2\u2217 ) are called a concordant pair if\nthe ranking of Y1 relative to Y1\u2217 is the same as the ranking of Y2 relative to\nY2\u2217 , that is, either Y1 > Y1\u2217 and Y2 > Y2\u2217 or Y1 < Y1\u2217 and Y2 < Y2\u2217 . In either\ncase, (Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) > 0. Similarly, (Y1 , Y2 ) and (Y1\u2217 , Y2\u2217 ) are called a\ndiscordant pair if (Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) < 0. Kendall\u2019s tau is the probability\nof a concordant pair minus the probability of a discordant pair. Therefore,\nKendall\u2019s tau for (Y1 , Y2 ) is\n\n  \u03c1\u03c4 (Y1 , Y2 ) = P {(Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) > 0} \u2212 P {(Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) < 0}\n                = E [sign{(Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 )}] ,                          (8.13)\n\nwhere the sign function is\n                                     \u0012 1,          x > 0,\n                            sign(x) = \u22121,          x < 0,\n                                       0,          x = 0.\n\nIt is clear from (8.13) that \u03c1\u03c4 is symmetric in its arguments and takes values\nin [\u22121, 1]. It is easy to check that if g and h are increasing functions, then\n\n                         \u03c1\u03c4 {g(Y1 ), h(Y2 )} = \u03c1\u03c4 (Y1 , Y2 ).                 (8.14)\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "1 Kendall\u2019s Tau",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "Y1\u2217 and Y2 > Y2\u2217 or Y1 < Y1\u2217 and Y2 < Y2\u2217 . In either\ncase, (Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) > 0. Similarly, (Y1 , Y2 ) and (Y1\u2217 , Y2\u2217 ) are called a\ndiscordant pair if (Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) < 0. Kendall\u2019s tau is the probability\nof a concordant pair minus the probability of a discordant pair. Therefore,\nKendall\u2019s tau for (Y1 , Y2 ) is",
        "start": 293,
        "end": 624
      },
      {
        "language": "r",
        "code": "0} \u2212 P {(Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 ) < 0}\n                = E [sign{(Y1 \u2212 Y1\u2217 )(Y2 \u2212 Y2\u2217 )}] ,                          (8.13)",
        "start": 668,
        "end": 792
      },
      {
        "language": "r",
        "code": "0,\n                            sign(x) = \u22121,          x < 0,\n                                       0,          x = 0.",
        "start": 872,
        "end": 994
      }
    ]
  },
  {
    "content": "8.5 Rank Correlation       195\n\nStated di\ufb00erently, Kendall\u2019s tau is invariant to monotonically increasing trans-\nformations. If g and h are the marginal CDFs of Y1 and Y2 , then the left-hand\nside of (8.14) is the value of Kendall\u2019s tau for a pair of random variables dis-\ntributed according to the copula of (Y1 , Y2 ). This shows that Kendall\u2019s tau\ndepends only on the copula of a bivariate random vector. For a random vector\nY , we de\ufb01ne the Kendall\u2019s tau correlation matrix \u03a9 \u03c4 to be the matrix whose\n(j, k) entry is Kendall\u2019s tau for the jth and kth components of Y , that is\n[\u03a9 \u03c4 (Y )]jk = \u03c1\u03c4 (Yj , Yk ).\n    If we have a bivariate sample Y 1:n = {(Yi,1 , Yi,2 ) : i = 1, . . . , n}, then the\nsample Kendall\u2019s tau is\n                      \u0007 \b\u22121\n                        n\n       \u03c1\u0002\u03c4 (Y 1:n ) =              sign {(Yi,1 \u2212 Yj,1 )(Yi,2 \u2212 Yj,2 )} .         (8.15)\n                        2\n                             1\u2264i<j\u2264n\n           \u0007 \b\n              n\nNote that         is the number of summands in (8.15), so \u03c1\u0002\u03c4 is the average\n              2\nof sign{(Yi,1 \u2212 Yj,1 ) (Yi,2 \u2212 Yj,2 )} across all distinct pairs of observations and\nis a sample version of (8.13). The sample Kendall\u2019s tau correlation matrix is\nde\ufb01ned analogously to \u03a9 \u03c4 .\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "Rank Correlation       195",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5.2 Spearman\u2019s Rank Correlation Coe\ufb03cient\n\nFor a sample, Spearman\u2019s correlation coe\ufb03cient is simply the usual Pearson\ncorrelation calculated from the marginal ranks of the data. For a distribution\n(that is, an in\ufb01nite population rather than a \ufb01nite sample), both variables\nare transformed by their univariate marginal CDFs and then the Pearson\ncorrelation is computed for the transformed variables. Transforming a random\nvariable by its CDF is analogous to computing the ranks of a variable in a\n\ufb01nite sample.\n   Stated di\ufb00erently, Spearman\u2019s rank correlation coe\ufb03cient, also called\nSpearman\u2019s rho, for a bivariate random vector (Y1 , Y2 ) will be denoted\nas \u03c1S (Y1 , Y2 ) and is de\ufb01ned to be the Pearson correlation coe\ufb03cient of\n{FY1 (Y1 ), FY2 (Y2 )}:\n\n                      \u03c1S (Y1 , Y2 ) = Corr{FY1 (Y1 ), FY2 (Y2 )}.\n\nSince the joint CDF of {FY1 (Y1 ), FY2 (Y2 )} is the copula of (Y1 , Y2 ), Spearman\u2019s\nrho, like Kendall\u2019s tau, depends only on the copula function.\n   The sample version of Spearman\u2019s correlation coe\ufb03cient can be computed\nfrom the ranks of the data and for a bivariate sample Y 1:n = {(Yi,1 , Yi,2 ) :\ni = 1, . . . , n}, is\n                             n \u000e                  \u000f\u000e                  \u000f\n                     12                       n+1                 n+1\n   \u03c1\u0002S (Y 1:n ) =               rank(Yi,1 ) \u2212       rank(Yi,2 ) \u2212       .\n                  n(n2 \u2212 1) i=1                2                   2\n                                                                      (8.16)\n\f196      8 Copulas\n\nThe set of ranks for any variable is, of course, the integers 1 to n, and hence\n(n+1)/2 is the mean of its ranks. It can be shown that \u03c1\u0002S (Y 1:n ) is the sample\nPearson correlation between the ranks of {Yi,1 } and the ranks of {Yi,2 }.4\n    If Y = (Y1 , . . . , Yd ) is a random vector, then the Spearman\u2019s correlation\nmatrix \u03a9 S of Y is the correlation matrix of {FY1 (Y1 ), . . . , FYd (Yd )} and con-\ntains the Spearman\u2019s correlation coe\ufb03cients for all pairs of components of\nY , such that [\u03a9 S (Y )]jk = \u03c1S (Yj , Yk ), for all j, k = 1, . . . , d. The sample\nSpearman\u2019s correlation matrix is de\ufb01ned analogously.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.5",
      "section_title": "2 Spearman\u2019s Rank Correlation Coe\ufb03cient",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.6 Tail Dependence\nTail dependence measures association between the extreme values of two ran-\ndom variables and depends only on their copula. We will start with lower tail\ndependence, which uses extremes in the lower tail. Suppose that Y = (Y1 , Y2 )\nis a bivariate random vector with copula CY . Then the coe\ufb03cient of lower\ntail dependence is denoted by \u03bb\u0002 and de\ufb01ned as\n\n                     \u03bb\u0002 := lim P Y2 \u2264 FY\u22121\n                                         2\n                                           (q) | Y1 \u2264 FY\u22121\n                                                         1\n                                                           (q)                 (8.17)\n                           q\u21930\n\n                                 P Y1 \u2264 FY\u22121\n                                           1\n                                             (q), Y2 \u2264 FY\u22121\n                                                          2\n                                                            (q)\n                        = lim                                                  (8.18)\n                           q\u21930         P Y1 \u2264 FY\u22121\n                                                 1\n                                                   (q)\n                              P {FY1 (Y1 ) \u2264 q, FY2 (Y2 ) \u2264 q}\n                        = lim                                                  (8.19)\n                           q\u21930       P {FY1 (Y1 ) \u2264 q}\n                              CY (q, q)\n                        = lim           .                                      (8.20)\n                          q\u21930    q\nIt is helpful to look at these equations individually. As elsewhere in this chap-\nter, for simplicity we are assuming that FY1 and FY2 are strictly increasing on\ntheir supports and therefore have inverses.\n    First, (8.17) de\ufb01nes \u03bb\u0002 as the limit as q \u2193 0 of the conditional probability\nthat Y2 is less than or equal to its qth quantile, given that Y1 is less than or\nequal to its qth quantile. Since we are taking a limit as q \u2193 0, we are looking\nat the extreme left tail. What happens if Y1 and Y2 are independent? Then\nP (Y2 \u2264 y2 | Y1 \u2264 y1 ) = P (Y2 \u2264 y2 ) for all y1 and y2 . Therefore, the conditional\nprobability in (8.17) equals the unconditional probability P (Y2 \u2264 FY\u22121   2\n                                                                            (q)) and\nthis probability converges to 0 as q \u2193 0. Therefore, \u03bb\u0002 = 0 implies that in the\nextreme left tail, Y1 and Y2 behave as if they were independent.\n    Equation (8.18) is just the de\ufb01nition of conditional probability. Equa-\ntion (8.19) is simply (8.18) after applying the probability transformation\nto each variable. The numerator in (8.19) is the copula by de\ufb01nition, and the\n4\n    If there are ties, then ranks are averaged among tied observations. For example,\n    if there are two observations tied for smallest, then they each get a rank of ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.6",
      "section_title": "Tail Dependence",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5.\n    When there are ties, these results must be modi\ufb01ed.\n\f                                                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.5",
      "section_title": "When there are ties, these results must be modi\ufb01ed.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.6 Tail Dependence     197\n\ndenominator in (8.20) is the result of FY1 (Y1 ) being distributed Uniform(0,1);\nsee (A.9).\n    Deriving formulas for \u03bb\u0002 for Gaussian and t-copulas is a topic best left for\nmore advanced books. Here we give only the results; see Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.6",
      "section_title": "Tail Dependence     197",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8 for further\nreading. For any bivariate Gaussian copula C Gauss with \u03c1 = 1, \u03bb\u0002 = 0, that\nis, Gaussian copulas do not have tail dependence except in the extreme case\nof perfect positive correlation. For a bivariate t-copula C t with tail index \u03bd\nand correlation \u03c1,\n                                  \u0012 /                    \u0014\n                                        (\u03bd + 1)(1 \u2212 \u03c1)\n                     \u03bb\u0002 = 2Ft,\u03bd+1 \u2212                        ,              (8.21)\n                                              1+\u03c1\n\nwhere Ft,\u03bd+1 is the CDF of the t-distribution with tail index \u03bd + 1.\n    Since Ft,\u03bd+1 (\u2212\u221e) = 0, we see that \u03bb\u0002 \u2192 0 as \u03bd \u2192 \u221e, which makes sense\nsince the t-copula converges to a Gaussian copula as \u03bd \u2192 \u221e. Also, \u03bb\u0002 \u2192 0\nas \u03c1 \u2192 \u22121, which is also not too surprising, since \u03c1 = \u22121 is perfect negative\ndependence and \u03bb\u0002 measures positive tail dependence.\n    The coe\ufb03cient of upper tail dependence \u03bbu is\n\n                  \u03bbu := lim P Y2 \u2265 FY\u22121\n                                      2\n                                        (q) | Y1 \u2265 FY\u22121\n                                                      1\n                                                        (q)                  (8.22)\n                         q\u21911\n                                     1 \u2212 CY (q, q)\n                      = 2 \u2212 lim                    .                         (8.23)\n                               q\u21911      1\u2212q\nWe see that \u03bbu is de\ufb01ned analogously to \u03bb\u0002 ; \u03bbu is the limit as q \u2191 1 of the\nconditional probability that Y2 is greater than or equal to its qth quantile,\ngiven that Y1 is greater than or equal to its qth quantile. Deriving (8.23) is\nleft as an exercise for the interested reader.\n    For Gaussian and t-copula, \u03bbu = \u03bb\u0002 , so that \u03bbu = 0 for any Gaussian cop-\nula and for a t-copula, \u03bb\u0002 is given by the right-hand side of (8.21). Coe\ufb03cients\nof tail dependence for t-copulas are plotted in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "for further",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.6. One can see \u03bb\u0002 = \u03bbu\ndepends strongly on both \u03c1 and \u03bd. For the independence copula C0 , \u03bb\u0002 and\n\u03bbu are both equal to 0, and for the co-monotonicity copula C+ , both are equal\nto 1.\n46 rho = seq(-1,1, by=0.01)\n47 df = c(1, 4, 25, 240)\n48 x1 = -sqrt((df[1]+1)*(1-rho)/(1+rho))\n\n49 lambda1 = 2*pt(x1,df[1]+1)\n\n50 x4 = -sqrt((df[2]+1)*(1-rho)/(1+rho))\n\n51 lambda4 = 2*pt(x4,df[2]+1)\n\n52 x25 = -sqrt((df[3]+1)*(1-rho)/(1+rho))\n\n53 lambda25 = 2*pt(x25,df[3]+1)\n\n54 x250 = -sqrt((df[4]+1)*(1-rho)/(1+rho))\n\n55 lambda250 = 2*pt(x250,df[4]+1)\n\n56 par(mfrow=c(1,1), lwd=2, cex.axis=1.2, cex.lab=1.2)\n\n57 plot(rho, lambda1, type=\"l\", lty=1, xlab=expression(rho),\n\f198     8 Copulas\n\n58      ylab=expression(lambda[l]==lambda[u]))\n59 lines(rho, lambda4, lty=2)\n60 lines(rho, lambda25, lty=3)\n\n61 lines(rho, lambda250, lty=4)\n\n62 legend(\"topleft\", c(expression(nu==1), expression(nu==4),\n\n63        expression(nu==25), expression(nu==250)), lty=1:4)\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.6",
      "section_title": "One can see \u03bb\u0002 = \u03bbu",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n                                \u03bd=1\n                                \u03bd=4\n                                \u03bd = 25\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.0",
      "section_title": "\u03bd=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                \u03bd = 250\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "\u03bd = 250",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n         \u03bbl = \u03bbu\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.6",
      "section_title": "\u03bbl = \u03bbu",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                   0.2\n                   ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                         \u22121.0             \u22120.5   0.0        0.5           1.0\n                                                  \u03c1\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "\u22121.0             \u22120.5   0.0        0.5           1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.6. Coe\ufb03cients of tail dependence for bivariate t-copulas as functions of \u03c1 for\n\u03bd = 1, 4, 25, and 250.\n\n\n\n\n    Knowing whether or not there is tail dependence is important for risk\nmanagement. If there are no tail dependencies among the returns on the assets\nin a portfolio, then there is little risk of simultaneous very negative returns,\nand the risk of an extreme negative return on the portfolio is low. Conversely,\nif there are tail dependencies, then the likelihood of extreme negative returns\noccurring simultaneously on several assets in the portfolio can be high. As\nsuch, tail dependencies should be considered when assessing the diversi\ufb01cation\nand risk of any portfolio.\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.6",
      "section_title": "Coe\ufb03cients of tail dependence for bivariate t-copulas as functions of \u03c1 for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 Calibrating Copulas\nAssume that we have an i.i.d. sample Y 1:n = {(Yi,1 , . . . , Yi,d ) : i = 1, . . . , n},\nand we wish to estimate the copula of Y and perhaps its univariate marginal\ndistributions as well.\n    An important task is choosing a copula model. The various copula mod-\nels di\ufb00er notably from each other. For example, some have tail dependence\n\f                                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Calibrating Copulas",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 Calibrating Copulas                199\n\nand others do not. The Gumbel copula and Joe copula allow only positive\ndependence or independence. The Clayton copula with negative dependence\nexcludes the region where both u1 and u2 are small. As will be seen in this\nsection, an appropriate copula model can be selected via AIC, and by using\ngraphical techniques.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Calibrating Copulas                199",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7.1 Maximum Likelihood\n\nSuppose we have parametric models FY1 (\u00b7 | \u03b8 1 ), . . . , FYd (\u00b7 | \u03b8 d ) for the marginal\nCDFs as well as a parametric model cY (\u00b7|\u03b8 C ) for the copula density. By taking\nlogs of (8.4), we \ufb01nd that the log-likelihood is\n                                     n &\nlog{L(\u03b8 1 , . . . , \u03b8 d , \u03b8 C )} =         log[cY {FY1 (Yi,1 |\u03b8 1 ), . . . , FYd (Yi,d |\u03b8 d )|\u03b8 C }]\n                                     i=1\n                                                                                      '\n                               + log{fY1 (Yi,1 |\u03b8 1 )} + \u00b7 \u00b7 \u00b7 + log{fYd (Yi,d |\u03b8 d )} . (8.24)\n\nMaximum likelihood estimation \ufb01nds the maximum of log{L(\u03b8 1 , . . . , \u03b8 d , \u03b8 C )}\nover the entire set of parameters (\u03b8 1 , . . . , \u03b8 d , \u03b8 C ).\n    There are two potential problems with maximum likelihood estimation.\nFirst, because of the large number of parameters, especially for large values\nof d, numerically maximizing log{L(\u03b8 1 , . . . , \u03b8 d , \u03b8 C )} can be challenging. This\ndi\ufb03culty can be ameliorated by the use of starting values that are close to\nthe MLEs. The pseudo-maximum likelihood estimates discussed in the next\nsection are easier to compute than the MLE and can be used either as an\nalternative to the MLE or as starting values for the MLE.\n    Second, maximum likelihood estimation requires parametric models for\nboth the copula and the univariate marginal distributions. If any of the uni-\nvariate marginal distributions are not well \ufb01t by a convenient parametric\nfamily, this may cause biases in the estimated parameters of both the univari-\nate marginal distributions and the copula. The semiparametric approach to\npseudo-maximum likelihood estimation, where the univariate marginal distri-\nbutions are estimated nonparametrically, provides a remedy to this problem.\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "1 Maximum Likelihood",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7.2 Pseudo-Maximum Likelihood\n\nPseudo-maximum likelihood estimation is a two-step procedure. In the \ufb01rst\nstep, each of the d univariate marginal distribution functions is estimated,\none at a time. Let F\u0002Yj be the estimate of the jth univariate marginal CDF,\nj = 1, . . . , d. In the second step,\n                        n       \u001a (                                        )\u001b\n                             log cY F\u0002Y1 (Yi,1 ), . . . , F\u0002Yd (Yi,d )|\u03b8 C                       (8.25)\n                       i=1\n\nis maximized over \u03b8 C . Note that (8.25) is obtained from (8.24) by deleting\nterms that do not depend on \u03b8 C and replacing the univariate marginal CDFs\n\f200     8 Copulas\n\nby estimates. By estimating parameters in the univariate marginal distribu-\ntions and in the copula separately, the pseudo-maximum likelihood approach\navoids a high-dimensional optimization.\n    There are two approaches to the \ufb01rst step, parametric and nonparametric.\nIn the parametric approach, parametric models FY1 (\u00b7 | \u03b8 1 ), . . . , FYd (\u00b7 | \u03b8 d ) for\nthe univariate marginal CDFs are assumed as in maximum likelihood estima-\ntion. The data Y1,j , . . . , Yn,j for the jth variate are used to estimate \u03b8 j , usually\nby maximum likelihood as discussed in Chap. 5. Then, F\u0002Yj (\u00b7) = FYj (\u00b7|\u03b8             \u0002j ).\n                                          \u0002\nIn the nonparametric approach, FYj is estimated by the empirical CDF of\nY1,j , . . . , Yn,j , except that the divisor n in (4.2) is replaced by n + 1 so that\n                                          \u0017n\n                                \u0002                I{Yi,j \u2264 y}\n                                FYj (y) = i=1                 .                    (8.26)\n                                                 n+1\n\nWith this modi\ufb01ed divisor, the maximum value of F\u0002Yj (Yi,j ) is n/(n + 1)\nrather than 1. Avoiding a value of 1 is essential when, as is often the case,\ncY (u1 , . . . , ud |\u03b8 C ) = \u221e if some of u1 , . . . , ud are equal to 1.\n    When both steps are parametric, the estimation method is called para-\nmetric pseudo-maximum likelihood. The combination of a nonparametric \ufb01rst\nstep and a parametric second step is called semiparametric pseudo-maximum\nlikelihood.\n    In the second step of pseudo-maximum likelihood, the maximization can\nbe di\ufb03cult when \u03b8 C is high-dimensional. For example, if one uses a Gaussian\nor t-copula, then there are d(d\u22121)/2 correlation parameters. One way to solve\nthis problem is to assume some structure among the correlations. An extreme\ncase of this is the equi-correlation model where all non-diagonal elements of\nthe correlation matrix have a common value, call it \u03c1. If one is reluctant to\nassume some type of structured correlation matrix, then it is essential to have\ngood starting values for the correlation matrix when maximizing (8.25). For\nGaussian and t-copulas, starting values can be obtained via rank correlations\nas discussed in the next section.\n    The values F\u0002Yj (Yi,j ), i = 1, . . . , n and j = 1, . . . , d, will be called the\nuniform-transformed variables, since they should be distributed approximately\nUniform(0,1). The multivariate empirical CDF [see Eq. (A.38)] of the uniform-\ntransformed variables is called the empirical copula and is a nonparametric\nestimate of the copula function. The empirical copula is useful for checking\nthe goodness of \ufb01t of parametric copula models; see Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "2 Pseudo-Maximum Likelihood",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1.\n\n8.7.3 Calibrating Meta-Gaussian and Meta-t-Distributions\n\nGaussian Copulas\n\nRank correlation can be useful for estimating the parameters of a copula.\nSuppose Y 1:n = {(Yi,1 , . . . , Yi,d ) : i = 1, . . . , n}, is an i.i.d. sample from a\nmeta-Gaussian distribution. Then its copula is C Gauss ( \u00b7 |\u03a9) for some corre-\nlation matrix \u03a9. To estimate the distribution of Y , we need to estimate the\n\f                                                        ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "8.7.3 Calibrating Meta-Gaussian and Meta-t-Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 Calibrating Copulas         201\n\nunivariate marginal distributions and \u03a9. The marginal distribution can be\nestimated by the methods discussed in Chap. 5. Result (8.28) in the following\ntheorem shows that \u03a9 can be estimated by the sample Spearman\u2019s correlation\nmatrix.\n\nResult ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Calibrating Copulas         201",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1 Let Y = (Y1 , . . . , Yd ) have a meta-Gaussian distribution with\ncontinuous univariate marginal distributions and copula C Gauss ( \u00b7 |\u03a9), and\nlet \u03a9ij = [\u03a9]ij . Then\n\n                                       2\n                        \u03c1\u03c4 (Yi , Yj ) =  arcsin(\u03a9ij ), and                          (8.27)\n                                       \u03c0\n                                       6\n                        \u03c1S (Yi , Yj ) = arcsin(\u03a9ij /2) \u2248 \u03a9ij .                       (8.28)\n                                       \u03c0\n   Suppose, instead, that Y has a meta-t-distribution with continuous uni-\nvariate marginal distributions and copula C t ( \u00b7 |\u03a9, \u03bd). Then (8.27) still holds,\nbut (8.28) does not hold.\n\n\n    The approximation in (8.28) uses the result that\n                            6\n                              arcsin(x/2) \u2248 x for |x| \u2264 1.                          (8.29)\n                            \u03c0\nThe left- and right-hand sides of (8.29) are equal when x = \u22121, 0, 1, and their\nmaximum di\ufb00erence over the range \u22121 \u2264 x \u2264 1 is ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Let Y = (Y1 , . . . , Yd ) have a meta-Gaussian distribution with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.018. However, the relative\nerror \u03c06 arcsin(x/2) \u2212 x / \u03c06 arcsin(x/2) can be larger, as much as 0.047, and\nis largest near x = 0.\n                                                               \u0002\n    By (8.28), the sample Spearman\u2019s rank correlation matrix \u03a9(Y    1:n ) can be\nused as an estimate of the correlation matrix \u03a9 associated with C Gauss ( \u00b7 |\u03a9).\nThis estimate could be the \ufb01nal one or could be used as a starting value for\nnumeric maximum likelihood or pseudo-maximum likelihood estimation.\n\nt-Copulas\n\nIf Y 1:n = {(Yi,1 , . . . , Yi,d ) : i = 1, . . . , n} is a sample from a distribution with\na t-copula C t ( \u00b7 |\u03a9, \u03bd) then we can use (8.27) and the sample Kendall\u2019s tau\ncorrelations to estimate \u03a9. Let \u03a9           \u0002\u03c4,jk be the sample Kendall\u2019s tau correlation\nof {Y1,j , . . . , Yn,j } and {Y1,k , . . . , Yn,k }, the jth and kth components, and let\n 0 \u2217\u2217 be de\ufb01ned such that [\u03a9         0 \u2217\u2217 ]jk = sin{ \u03c0 \u03a9  \u0002            0 \u2217\u2217 will have two of\n\u03a9                                                       2 \u03c4,jk } Then \u03a9\nthe three properties of a correlation matrix; it will be symmetric, with all\ndiagonal entries equal to 1. However, it may not be positive de\ufb01nite, or even\nsemide\ufb01nite, because some of its eigenvalues may be negative.\n    If all of the eigenvalues of \u03a9           0 \u2217\u2217 are positive, then we will use \u03a9   0 \u2217\u2217 to\n                                                  \u2217\u2217\nestimate \u03a9. Otherwise, we alter \u03a9               0     slightly to make it positive de\ufb01nite.\nBy (A.50),\n\f202     8 Copulas\n\n                                0 \u2217\u2217 = O diag(\u03bbi ) O T ,\n                                \u03a9\n                                                                      0 \u2217\u2217\nwhere O is an orthogonal matrix whose columns are the eigenvectors of \u03a9\nand \u03bb1 , . . . , \u03bbd are the corresponding eigenvalues. We then de\ufb01ne\n\n                           0 \u2217 = O diag{max(\u0017, \u03bbi )} O T ,\n                           \u03a9\n\nwhere \u0017 is some small positive quantity, for example, \u0017 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.018",
      "section_title": "However, the relative",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001. Now, \u03a9          0 \u2217 is\nsymmetric and positive de\ufb01nite, but its diagonal elements, \u03a9       0 \u2217 , i = 1, . . . , d,\n                                                                    ii\nmay not be equal to 1. This problem is easily \ufb01xed; multiply the ith row and\n                           -     .\nthe ith column of \u03a9 0 \u2217 by \u03a9 0 \u2217 \u22121/2 , for i = 1, . . . , d. The \ufb01nal result, which\n                              ii\nwe denote as \u03a9,0 is a bona \ufb01de correlation matrix; that is, it is symmetric,\npositive de\ufb01nite, and it has all diagonal entries equal to 1.\n   After \u03a9 has been estimated by \u03a9,    0 an estimate of the tail index \u03bd is still\nneeded. One can be obtained by plugging \u03a9    0 into the log-likelihood (8.25) and\nthen maximizing over \u03bd.\n\n\nExample ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.001",
      "section_title": "Now, \u03a9          0 \u2217 is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1. Flows in pipelines\n\n     In this example, we will continue the analysis of the pipeline \ufb02ows data\nintroduced in Example ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Flows in pipelines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2. Only the \ufb02ows in the \ufb01rst two pipelines will be\nused.\n     In a fully parametric pseudo-likelihood analysis, the univariate skewed\n                                           \u00021,j , . . . , U\nt-model will be used for \ufb02ows 1 and 2. Let U              \u0002n,j be the \ufb02ows in pipeline\nj, j = 1, 2, transformed by their estimated skewed-t CDFs. We will call the\nU\u0002i,j \u201cuniform-transformed \ufb02ows.\u201d De\ufb01ne Z\u0002i,j = \u03a6\u22121 (U        \u0002i,j ), where \u03a6\u22121 is the\nstandard normal quantile function. The Z\u0002i,j should each be approximately\nN (0, 1)-distributed, and we will call them \u201cnormal-transformed \ufb02ows.\u201d\n64 library(copula)\n65 library(sn)\n66 dat = read.csv(\"FlowData.csv\")\n\n67 dat = dat/10000\n\n68 n = nrow(dat)\n\n69 x1 = dat$Flow1\n\n70 fit1 = st.mple(matrix(1,n,1), y=x1, dp=c(mean(x1), sd(x1), 0, 10))\n\n71 est1 = fit1$dp\n\n72 u1 = pst(x1, dp=est1)\n\n73 x2 = dat$Flow2\n\n74 fit2 = st.mple(matrix(1,n,1), y=x2, dp=c(mean(x2), sd(x2), 0, 10))\n\n75 est2 = fit2$dp\n\n76 u2 = pst(x2, dp=est2)\n\n77 U.hat = cbind(u1, u2)\n\n78 z1 = qnorm(u1)\n\n79 z2 = qnorm(u2)\n\n80 Z.hat = cbind(z1, z2)\n\f                                                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "4.2",
      "section_title": "Only the \ufb02ows in the \ufb01rst two pipelines will be",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 Calibrating Copulas                  203\n\n    Both sets of uniform-transformed \ufb02ows should be approximately\nUniform(0,1). Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Calibrating Copulas                  203",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 shows density histograms of both samples of uniform-\ntransformed \ufb02ows as well as their scatterplot and two-dimensional KDE den-\nsity contours. The histograms show some deviations from uniform distribu-\ntions, which suggests that the skewed-t model may not provide adequate \ufb01ts\nand that a semiparametric pseudo-maximum likelihood approach might be\ntried\u2014this is considered below. However, the deviations may be due to ran-\ndom variation.\n81 library(ks)\n82 fhatU = kde(x=U.hat, H=Hscv(x=U.hat))\n83 par(mfrow=c(2,2), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n\n84 hist(u1, main=\"(a)\", xlab=expression(hat(U)[1]), freq = FALSE)\n\n85 hist(u2, main=\"(b)\", xlab=expression(hat(U)[2]), freq = FALSE)\n\n86 plot(u1, u2, main=\"(c)\", xlab = expression(hat(U)[1]),\n\n87      ylab = expression(hat(U)[2]), mgp = c(2.5, 1, 0))\n88 plot(fhatU, drawpoints=FALSE, drawlabels=FALSE,\n\n89      cont=seq(10, 80, 10), main=\"(d)\", xlab=expression(hat(U)[1]),\n90      ylab=expression(hat(U)[2]), mgp = c(2.5, 1, 0))\n\n\n       a                                                        b\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "shows density histograms of both samples of uniform-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2\n\n\n\n\n                                                                           1.2\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "1.2",
      "section_title": "1.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                 Density\n                                                                           ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n       Density\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                           0.4\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                           0.0\n\n\n\n\n                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2   0.4        0.6   0.8   1.0                    0.0   0.2   0.4        0.6   0.8   1.0\n                                         ^                                                         ^\n                                         U1                                                        U2\n\n       c                                                        d\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.2   0.4        0.6   0.8   1.0                    0.0   0.2   0.4        0.6   0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                           0.8\n      U2\n\n\n\n\n                                                                U2\n      ^\n\n\n\n\n                                                                ^\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                           0.4\n                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                           0.0\n\n\n\n\n                       ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2   0.4        0.6   0.8   1.0                    0.0   0.2   0.4        0.6   0.8   1.0\n                                         ^                                                         ^\n                                         U1                                                        U1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.0",
      "section_title": "0.2   0.4        0.6   0.8   1.0                    0.0   0.2   0.4        0.6   0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7. Pipeline data. Density histograms (a) and (b) and a scatterplot (c) of the\nuniform-transformed \ufb02ows. The empirical copula C \u0002 is the empirical CDF of the data\nin (c). Contours (d) from an estimated copula density \u0002c via a two-dimensional KDE\nof (c).\n\f204            8 Copulas\n\n          a                                                                  b\n\n\n                                  3\n\n\n\n\n                                                                                                     3\n          Theoretical Quantiles\n\n\n\n\n                                                                             Theoretical Quantiles\n                                  2\n\n\n\n\n                                                                                                     2\n                                  1\n\n\n\n\n                                                                                                     1\n                                  \u22121 0\n\n\n\n\n                                                                                                     \u22121 0\n                                  \u22123\n\n\n\n\n                                                                                                     \u22123\n                                          \u22122   \u22121   0        1   2   3                                       \u22122    \u22121       0        1       2       3\n                                               Sample Quantiles                                                   Sample Quantiles\n\n\n      c                                                                  d\n                                  3\n\n\n\n\n                                                                                                     3\n                                  2\n\n\n\n\n                                                                                                     2\n                                  1\n\n\n\n\n                                                                                                     1\n      Z2\n\n\n\n\n                                                                         Z2\n      ^\n\n\n\n\n                                                                         ^\n                                  0\n\n\n\n\n                                                                                                     0\n                                  \u22122 \u22121\n\n\n\n\n                                                                                                     \u22122 \u22121\n\n\n\n\n                                          \u22122   \u22121   0        1   2   3                                       \u22122   \u22121    0        1       2       3\n                                                        ^                                                                   ^\n                                                        Z1                                                                  Z1\n\nFig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Pipeline data. Density histograms (a) and (b) and a scatterplot (c) of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8. Pipeline data. Normal quantile plots (a) and (b), a scatterplot (c) and\nKDE density contours for the normal-transformed \ufb02ows.\n\n\n    The scatterplot in Fig. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "Pipeline data. Normal quantile plots (a) and (b), a scatterplot (c) and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 shows some negative association as the data are\nsomewhat concentrated along the diagonal from top left to bottom right. Thus,\nthe Gumbel copula and Joe copula, which cannot have negative dependence,\nare not appropriate. Also, the Clayton copula may not \ufb01t well either, since the\nscatterplot shows data in the region where both U        \u00022 have small values,\n                                                  \u00021 and U\nbut this region is excluded by a Clayton copula with negative dependence.\nWe will soon see that AIC agrees with these conclusions from a graphical\nanalysis, since the Clayton model has higher (worse) AIC values compared to\nthe Gaussian, t, and Frank copula models.\n    Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "shows some negative association as the data are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8 shows that the normal-transformed \ufb02ows have approximately\nlinear normal quantile plots, which would be expected if the estimated uni-\nvariate marginal CDFs were adequate \ufb01ts. Their scatterplot and KDE density\ncontours again show negative association.\n91 fhatZ = kde(x=Z.hat, H=Hscv(x=Z.hat))\n92 par(mfrow=c(2,2), cex.axis=1.2, cex.lab=1.2, cex.main=1.2)\n93 qqnorm(z1, datax=T, main=\"(a)\") ; qqline(z1)\n\n94 qqnorm(z2, datax=T, main=\"(b)\") ; qqline(z2)\n\n95 plot(z1, z2, main=\"(c)\", xlab = expression(hat(Z)[1]),\n\n96      ylab = expression(hat(Z)[2]), mgp = c(2.5, 1, 0))\n97 plot(fhatZ, drawpoints=FALSE, drawlabels=FALSE,\n\f                                                ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "shows that the normal-transformed \ufb02ows have approximately",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7 Calibrating Copulas     205\n\n 98       cont=seq(10, 90, 10), main=\"(d)\", xlab=expression(hat(Z)[1]),\n 99       ylab=expression(hat(Z)[2]), mgp = c(2.5, 1, 0))\n    We will assume for now that the two \ufb02ows have a meta-Gaussian dis-\ntribution. There are three ways to estimate the correlation in their Gaus-\nsian copula. The \ufb01rst, Spearman\u2019s rank correlation, is estimated \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "Calibrating Copulas     205",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.357. The\nsecond, which uses (8.27) is sin(\u0002\u03c1\u03c4 \u03c0/2), where \u03c1\u0002\u03c4 is the sample Kendall\u2019s\ntau rank correlation; its value is \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.357",
      "section_title": "The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.371. The third, Pearson correlation of\nthe normal-transformed \ufb02ows, is \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.371",
      "section_title": "The third, Pearson correlation of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.335. There is reasonably close agreement\namong the three values, especially relative to their uncertainties; for example,\nthe approximate 95 % con\ufb01dence interval for the Pearson correlation of the\nnormal-transformed \ufb02ows is (\u22120.426, \u22120.238), and the other two estimate are\nwell within this interval.\n100 cor.test(u1, u2, method=\"spearman\")\n101 cor.test(u1, u2, method=\"kendall\")\n102 sin(-0.242*pi/2)\n\n103 cor.test(u1, u2, method=\"pearson\")\n\n104 cor.test(z1, z2, method=\"pearson\")\n\n\n      Pearson\u2019s product-moment correlation\n\n      data: z1 and z2\n      t = -6.56, df = 340, p-value = 2.003e-10\n      alternative hypothesis: true correlation is not equal to 0\n      95 percent confidence interval:\n       -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.335",
      "section_title": "There is reasonably close agreement",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.426 -0.238\n      sample estimates:\n         cor\n      -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.426",
      "section_title": "-0.238",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.335\n\n    Four parametric copulas were \ufb01t to the uniform-transformed \ufb02ows: t, Gaus-\nsian, Frank and Clayton. Estimation of the copula distributions discussed\nin this chapter may be performed using the fitCopula() function from R\u2019s\ncopula package. The Gumbel and Joe copulas are not considered since they\nonly allow positive dependence and these data show negative dependence;\nattempting to \ufb01t these models results in numerical failures. Since we used\nparametric estimates to transform the \ufb02ows, we are \ufb01tting the copulas by\nparametric pseudo-maximum likelihood.\n105 omega = -",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.335",
      "section_title": "Four parametric copulas were \ufb01t to the uniform-transformed \ufb02ows: t, Gaus-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.371\n106 Ct = fitCopula(copula=tCopula(dim = 2), data=U.hat,\n107                 method=\"ml\", start=c(omega, 10))\n108 Ct@estimate\n\n109 loglikCopula(param=Ct@estimate, x=U.hat, copula=tCopula(dim = 2))\n\n110 -2*.Last.value + 2*length(Ct@estimate)\n\n111 #\n\n112 Cgauss = fitCopula(copula=normalCopula(dim = 2), data=U.hat,\n\n113                     method=\"ml\", start=c(omega))\n114 Cgauss@estimate\n\f206    8 Copulas\n\n115 loglikCopula(param=Cgauss@estimate, x=U.hat,\n116              copula=normalCopula(dim = 2))\n117 -2*.Last.value + 2*length(Cgauss@estimate)\n\n118 #\n\n119 Cfr = fitCopula(copula=frankCopula(1, dim=2), data=U.hat,\n\n120                 method=\"ml\")\n121 Cfr@estimate\n\n122 loglikCopula(param=Cfr@estimate, x=U.hat,\n\n123              copula=frankCopula(dim = 2))\n124 -2*.Last.value + 2*length(Cfr@estimate)\n\n125 #\n\n126 Ccl = fitCopula(copula=claytonCopula(1, dim=2), data=U.hat,\n\n127                 method=\"ml\")\n128 Ccl@estimate\n\n129 loglikCopula(param=Ccl@estimate, x=U.hat,\n\n130              copula=claytonCopula(dim = 2))\n131 -2*.Last.value + 2*length(Ccl@estimate)\n\n\n     The results are summarized in Table ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.371",
      "section_title": "106 Ct = fitCopula(copula=tCopula(dim = 2), data=U.hat,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1. Looking at the maximized log-\nlikelihood values, we see that the Frank copula \ufb01ts best since it minimizes\nAIC, but the t and Gaussian \ufb01t reasonably well. Figure ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Looking at the maximized log-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9 shows the uniform-\ntransformed \ufb02ows scatterplot and contours of the distribution functions of \ufb01ve\ncopulas: the independence copula and the four estimated parametric copulas;\nthe empirical copula contours have been overlaid for comparison. The t-copula\nis similar to the Gaussian since \u03bd\u0002 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.9",
      "section_title": "shows the uniform-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "22.247 is large. The Frank copula \ufb01ts best\nin the sense that its contours are closest to those of the empirical copula. This\nis in agreement with the AIC values.\n\n\nTable ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "22.247",
      "section_title": "is large. The Frank copula \ufb01ts best",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.1. Estimates of copula parameters, maximized log-likelihood, and AIC using\nthe uniform-transformed pipeline \ufb02ow data.\n              Copula family      Estimates    Maximized       AIC\n                                              log-likelihood\n                     t          \u03c1\u0002 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.1",
      "section_title": "Estimates of copula parameters, maximized log-likelihood, and AIC using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.340      20.98       \u221237.96\n                               \u03bd\u0002 = ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.340",
      "section_title": "20.98       \u221237.96",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "22.247\n                 Gaussian       \u03c1\u0002 = \u22120.331     20.36       \u221238.71\n                  Frank         \u03b8\u0002 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "22.247",
      "section_title": "Gaussian       \u03c1\u0002 = \u22120.331     20.36       \u221238.71",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.249     23.07       \u221244.13\n                 Clayton        \u03b8\u0002 = \u2212",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "2.249",
      "section_title": "23.07       \u221244.13",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.166      9.86       \u221217.72\n\n\n\n132 par(mfrow=c(2,3), mgp = c(2.5, 1, 0))\n133 plot(u1, u2, main=\"Uniform-Transformed Data\",\n134      xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n135 Udex = (1:n)/(n+1)\n\n136 Cn = C.n(u = cbind(rep(Udex, n), rep(Udex, each=n)) , U = U.hat,\n\n137          offset=0, method=\"C\")\n138 EmpCop = expression(contour(Udex,Udex,matrix(Cn,n,n), col=2, add=T))\n\n139 #\n\f                                                 ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "0.166",
      "section_title": "9.86       \u221217.72",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8 Bibliographic Notes    207\n\n140 contour(normalCopula(param=0,dim=2), pCopula, main=expression(C[0]),\n141         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n142 eval(EmpCop)\n\n143 #\n\n144 contour(tCopula(param=Ct@estimate[1], dim=2,\n\n145                 df=round(Ct@estimate[2])),\n146         pCopula, main = expression(hat(C)[t]),\n147         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n148 eval(EmpCop)\n\n149 #\n\n150 contour(normalCopula(param=Cgauss@estimate[1], dim = 2),\n\n151         pCopula, main = expression(hat(C)[Gauss]),\n152         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n153 eval(EmpCop)\n\n154 #\n\n155 contour(frankCopula(param=Cfr@estimate[1], dim = 2),\n\n156         pCopula, main = expression(hat(C)[Fr]),\n157         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n158 eval(EmpCop)\n\n159 #\n\n160 contour(claytonCopula(param=Ccl@estimate[1], dim = 2),\n\n161         pCopula, main = expression(hat(C)[Cl]),\n162         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]))\n163 eval(EmpCop)\n\n\n\n    The analysis in the previous paragraph was repeated with the \ufb02ows\ntransformed by their empirical CDFs. Doing this yielded the semiparamet-\nric pseudo-maximum likelihood estimates. Since the results were very similar\nto those for parametric pseudo-maximum likelihood estimates, they are not\npresented here.                                                           \u0002\n\n\n",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "Bibliographic Notes    207",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.8 Bibliographic Notes\n\nFor discussion of Archimedean copula with non-strict generators, see McNeil,\nFrey, and Embrechts (2005). These authors discuss a number of other topics in\nmore detail than is done here. They discuss methods de\ufb01ning nonexchangeable\nArchimedean copulas. The coe\ufb03cients of tail dependence for Gaussian and t-\ncopulas are derived in their Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.8",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2. The theorem and calibration methods\nin Sect. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "5.2",
      "section_title": "The theorem and calibration methods",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.7.3 are discussed in their Sect. 5.5.\n    Cherubini et al. (2004) treat the application of copulas to \ufb01nance. Joe\n(1997) and Nelsen (2007) are standard references on copulas. ",
    "metadata": {
      "chapter_number": "18",
      "chapter_title": "introduces",
      "section_number": "8.7",
      "section_title": "3 are discussed in their Sect. 5.5.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n      1.0\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                                                                                            0.                                                                 0.\n                                                                                                              7                                                                     7\n\n\n\n\n                                                                                                                     0. .8\n\n\n\n\n                                                                                                                                                                                         0.\n                                                                                                                       9\n\n\n\n\n                                                                                                                                                                                              80 0\n                                                                                                                         0 7\n\n\n\n\n                                                                                                                                                                                                .8 .7\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.                                                                 0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      0.8\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                                                                                                          0.\n                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 0                                                              0.5 0\n                                                                                                               .                                                                  .6\n                                                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0                                                              0.5 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4  0.6                                                           0.4\n\n\n\n\n                                                                                                                                                                                0.\n                                                                                                                 6\n\n\n\n\n                                                                                                                                                                                    5\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.6                                                           0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                                                                                                                0.\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      0.6\n\n\n\n\n                                                                                                                                                                                                  6\n                                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5                                                           0.\n                                                                                                                                                                                          4\nU2\n\n\n\n\n                                                                    U2\n\n\n\n\n                                                                                                                                       U2\n^\n\n\n\n\n                                                                    ^\n\n\n\n\n                                                                                                                                       ^\n                                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                         0.4\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n      0.4\n\n\n\n\n                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2          0.3                                                   0.2            00.3\n                                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.3                                                   0.2            00.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3                                                                    .3\n                                                                                                                                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                         0.2\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                            0.2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                 0.1                                                                0.10.1\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.1                                                                0.10.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                                                         0.0\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n      0.0\n\n\n\n\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2   0.4        0.6         0.8            1.0              0.0   0.2   0.4        0.6        0.8             1.0              0.0   0.2   0.4        0.6        0.8               1.0\n                              ^                                                                  ^                                                                  ^\n                              U1                                                                 U1                                                                 U1\n\n\n\n                         ^                                                                       ^                                                                  ^\n                        CGauss                                                               CFr                                                                CCI\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2   0.4        0.6         0.8            1.0              0.0   0.2   0.4        0.6        0.8             1.0              0.0   0.2   0.4        0.6        0.8               1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n      1.0\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                          0.                                                                0.                                                                 0.\n                                            7                                                                    7                                                               7\n                                                 0.\n\n\n\n\n                                                                                                                                                                               0.\n                                                   80 0\n\n\n\n\n                                                                                                                                                                                    7\n                                                                                                                         0.\n\n\n\n\n                                                                                                                                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.                                                                0.                                                                 0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                                                                                                          08.\n                                                     .8 .7\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "08.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      0.8\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                                                                                                                                                 8.8\n                                                                                                                             8\n                                                                                                                 0.\n                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "8.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 0                                                          0.5 0                                                              0.5 0\n                                                .6                                                             .6                                                                 .6\n\n\n\n\n                                                                                                                     6\n\n\n\n\n                                                                                                                                                                                        0.\n                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0                                                          0.5 0                                                              0.5 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4                                                            0.4                                                                0.4\n\n\n\n\n                                                                                                                                                                                          6\n                                           0.\n                                               5\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4                                                                0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                                       0.\n\n\n\n\n                                                                                                                     0.\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                                                            0.6\n                                                                                                                                                                                0.\n                                                         6\n\n\n\n\n                                                                                                                       5\n                                                0.                                                                                                                                 5\n                                                  4                                                                   0.\nU2\n\n\n\n\n                                                                    U2\n\n\n\n\n                                                                                                                                       U2\n                                                                                                                        4\n^\n\n\n\n\n                                                                    ^\n\n\n\n\n                                                                                                                                       ^\n                                                                                                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                         0.4\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                                                                            0.4\n                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2         00.3                                                  0.2          00.3                                                  0.2 00.3.3\n                                                         .3                                                                 .3\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "00.3                                                  0.2          00.3                                                  0.2 00.3.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2                                                                 0.2\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                                                                                0.2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                            0.2\n                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10.1                                                            0.1                                                                0.1\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "1                                                            0.1                                                                0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1                                                                  0.1\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n      0.0\n\n\n\n\n                                                                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n            0.0   0.2   0.4        0.6         0.8            1.0              0.0   0.2   0.4        0.6        0.8             1.0              0.0   0.2   0.4        0.6        0.8               1.0\n                              ^                                                                  ^                                                                  ^\n                              U1                                                                 U1                                                                 U1\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0   0.2   0.4        0.6         0.8            1.0              0.0   0.2   0.4        0.6        0.8             1.0              0.0   0.2   0.4        0.6        0.8               1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9. Uniform-transformed \ufb02ows for pipeline data. Scatterplot; independence\ncopula contours and four \ufb01tted copula contours via parametric models, versus the\nempirical copula contours.\n\n\nfatally \ufb02awed\u2014way to assess risk\u201d (Salmon 2009); in particular, the model\ndoes not include tail dependence. Du\ufb03e and Singleton\u2019s (2003, Section 10.4)\nalso discusses copula-based methods for modeling dependent default times.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "Uniform-transformed \ufb02ows for pipeline data. Scatterplot; independence",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9.1 Simulating from Copula Models\n\nRun the R code that appears below to generate data from a copula. Line 1\nloads the copula package. Lines 2\u20133 de\ufb01nes a copula object. At this point,\nnothing is done with the copula object\u2014it is simply de\ufb01ned. However, the\ncopula object is used in line 5 to generate a random sample from the speci\ufb01ed\ncopula model. The remaining lines create a scatterplot matrix of the sample\nand print its sample Pearson correlation matrix.\n1 library(copula)\n2 cop_t_dim3 = tCopula(dim = 3, param = c(-0.6,0.75,0),\n3                      dispstr = \"un\", df = 1)\n4 set.seed(5640)\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "1 Simulating from Copula Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9 R Lab    209\n\n 5 rand_t_cop = rCopula(n = 500, copula = cop_t_dim3)\n 6 pairs(rand_t_cop)\n 7 cor(rand_t_cop)\n\n\nYou can use R\u2019s help to learn more about the functions tCopula() and\nrCopula().\n\nProblem 1 Consider the R code above.\n(a) What type of copula model has been sampled? Give the copula family, the\n    correlation matrix, and any other parameters that specify the copula.\n(b) What is the sample size?\n\n\nProblem 2 Examine the scatterplot matrix (generated by line 6) and answer\nthe questions below. Include the scatterplot matrix with your answer.\n(a) Components 2 and 3 are uncorrelated. Do they appear independent? Why\n    or why not?\n(b) Do you see signs of tail dependence? If so, where?\n(c) What are the e\ufb00ects of dependence upon the plots?\n(d) The nonzero correlations in the copula do not have the same values as the\n    corresponding sample correlations. Do you think this is just due to random\n    variation or is something else going on? If there is another cause besides\n    random variation, what might that be? To help answer this question, you\n    can get con\ufb01dence intervals for the Pearson correlation: For example,\n     8   cor.test(rand_t_cop[,1],rand_t_cop[,3])\n     will give a con\ufb01dence interval (95 percent by default) for the correlation\n     (Pearson by default) between components 1 and 3. Does this con\ufb01dence\n     interval include 0.75?\n\n\nLines 9\u201310 in the R code below de\ufb01nes a normal (Gaussian) copula. Lines 11\u2013\n13 de\ufb01ne a multivariate distribution by specifying its copula and its marginal\ndistributions\u2014the copula is the one just de\ufb01ned. Line 15 generates a ran-\ndom sample of size 1,000 from this distribution, which has three components.\nThe remaining lines create a scatterplot matrix and kernel estimates of the\nmarginal densities for each component.\n 9 cop_normal_dim3 = normalCopula(dim = 3, param = c(-0.6,0.75,0),\n10                                dispstr = \"un\")\n11 mvdc_normal = mvdc(copula = cop_normal_dim3, margins = rep(\"exp\",3),\n\n12                    paramMargins = list(list(rate=2), list(rate=3),\n13                                        list(rate=4)))\n14 set.seed(5640)\n\n15 rand_mvdc = rMvdc(n = 1000, mvdc = mvdc_normal)\n\n16 pairs(rand_mvdc)\n\n17 par(mfrow = c(2,2))\n\n18 for(i in 1:3) plot(density(rand_mvdc[,i]))\n\f210      8 Copulas\n\nProblem 3 Run the R code above to generate a random sample.\n\n(a) What are the marginal distributions of the three components in rand mvdc?\n    What are their expected values?\n(b) Are the second and third components independent? Why or why not?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "R Lab    209",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9.2 Fitting Copula Models to Bivariate Return Data\n\nIn this section, you will \ufb01t copula models to a bivariate data set of daily\nreturns on IBM stock and the S&P 500 index.\n    First, you will \ufb01t a model with univariate marginal t-distributions and a\nt-copula. The model has three degrees-of-freedom (tail index) parameters, one\nfor each of the two univariate models and a third for the copula. This means\nthat the univariate distributions can have di\ufb00erent tail indices and that their\ntail indices are independent of the tail dependence from the copula.\n    Run the following R code to load the data and necessary libraries, \ufb01t\nunivariate t-distributions to the two components, and convert estimated scale\nparameters to estimated standard deviations:\n 1 library(MASS)     # for fitdistr() and kde2d() functions\n 2 library(copula)   # for copula functions\n 3 library(fGarch)   # for standardized t density\n 4 netRtns = read.csv(\"IBM_SP500_04_14_daily_netRtns.csv\", header = T)\n\n 5 ibm = netRtns[,2]\n\n 6 sp500 = netRtns[,3]\n\n 7 est.ibm = as.numeric( fitdistr(ibm,\"t\")$estimate )\n\n 8 est.sp500 = as.numeric( fitdistr(sp500,\"t\")$estimate )\n\n 9 est.ibm[2] = est.ibm[2] * sqrt( est.ibm[3] / (est.ibm[3]-2) )\n\n10 est.sp500[2] = est.sp500[2] * sqrt(est.sp500[3] / (est.sp500[3]-2) )\n\n\n\nThe univariate estimates will be used as starting values when the meta-t-\ndistribution is \ufb01t by maximum likelihood. You also need an estimate of the\ncorrelation coe\ufb03cient in the t-copula. This can be obtained using Kendall\u2019s\ntau. Run the following code and complete line 12 so that omega is the estimate\nof the Pearson correlation based on Kendall\u2019s tau.\n11   cor_tau = cor(ibm, sp500, method = \"kendall\")\n12   omega =\n\nProblem 4 How did you complete line 12 of the code? What was the com-\nputed value of omega?\n\n\nNext, de\ufb01ne the t-copula using omega as the correlation parameter and 4 as\nthe degrees-of-freedom (tail index) parameter.\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "2 Fitting Copula Models to Bivariate Return Data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.9 R Lab    211\n\n13   cop_t_dim2 = tCopula(omega, dim = 2, dispstr = \"un\", df = 4)\n\nNow \ufb01t copulas to the uniform-transformed data.\n14 data1 = cbind(pstd(ibm, est.ibm[1], est.ibm[2], est.ibm[3]),\n15               pstd(sp500, est.sp500[1], est.sp500[2], est.sp500[3]))\n16 n = nrow(netRtns) ; n\n\n17 data2 = cbind(rank(ibm)/(n+1), rank(sp500)/(n+1))\n\n18 ft1 = fitCopula(cop_t_dim2, data1, method=\"ml\", start=c(omega,4) )\n\n19 ft2 = fitCopula(cop_t_dim2, data2, method=\"ml\", start=c(omega,4) )\n\n\n\nProblem 5\n(a) Explain the di\ufb00erence between methods used to obtain the two estimates\n    ft1 and ft2.\n(b) Do the two estimates seem signi\ufb01cantly di\ufb00erent (in a practical sense)?\n\n\nThe next step de\ufb01nes a meta-t-distribution by specifying its t-copula and its\nunivariate marginal distributions. Values for the parameters in the univariate\nmargins are also speci\ufb01ed. The values of the copula parameter were already\nde\ufb01ned in the previous step.\n20 mvdc_t_t = mvdc( cop_t_dim2, c(\"std\",\"std\"), list(\n21            list(mean=est.ibm[1],sd=est.ibm[2],nu=est.ibm[3]),\n22            list(mean=est.sp500[1],sd=est.sp500[2],nu=est.sp500[3])))\n\nNow \ufb01t the meta t-distribution. Be patient. This takes awhile; for instance, it\ntook one minute on my laptop. The elapsed time in minutes will be printed.\n23 start = c(est.ibm, est.sp500, ft1@estimate)\n24 objFn = function(param) -loglikMvdc(param,cbind(ibm,sp500),mvdc_t_t)\n25 tic = proc.time()\n\n26 ft = optim(start, objFn, method=\"L-BFGS-B\",\n\n27            lower = c(-.1,0.001,2.2, -0.1,0.001,2.2, 0.2,2.5),\n28            upper = c( .1,   10, 15, 0.1,    10, 15, 0.9, 15) )\n29 toc = proc.time()\n\n30 total_time = toc - tic ; total_time[3]/60\n\n\n\nLower and upper bounds are used to constrain the algorithm to stay inside a\nregion where the log-likelihood is de\ufb01ned and \ufb01nite. The function fitMvdc()\nin the copula package does not allow setting lower and upper bounds and did\nnot converge on this problem.\n\nProblem 6\n(a) What are the estimates of the copula parameters in fit cop?\n(b) What are the estimates of the parameters in the univariate marginal\n    distributions?\n\f212    8 Copulas\n\n(c) Was the estimation method maximum likelihood, semiparametric pseudo-\n    maximum likelihood, or parametric pseudo-maximum likelihood?\n(d) Estimate the coe\ufb03cient of lower tail dependence for this copula.\n\n\nNow \ufb01t normal (Gaussian), Frank, Clayton, Gumbel and Joe copulas to the\ndata.\n31 fnorm = fitCopula(copula=normalCopula(dim=2),data=data1,method=\"ml\")\n32 ffrank = fitCopula(copula = frankCopula(3, dim = 2),\n33                      data = data1, method = \"ml\")\n34 fclayton = fitCopula(copula = claytonCopula(1, dim=2),\n\n35                      data = data1, method = \"ml\")\n36 fgumbel = fitCopula(copula = gumbelCopula(3, dim=2),\n\n37                     data = data1, method = \"ml\")\n38 fjoe = fitCopula(copula=joeCopula(2,dim=2),data=data1,method=\"ml\")\n\n\n\nThe estimated copulas (CDFs) will be compared with the empirical copula.\n39 Udex = (1:n)/(n+1)\n40 Cn = C.n(u=cbind(rep(Udex,n),rep(Udex,each=n)), U=data1, method=\"C\")\n41 EmpCop = expression(contour(Udex, Udex, matrix(Cn, n, n),\n\n42                             col = 2, add = TRUE))\n43 par(mfrow=c(2,3),  mgp = c(2.5,1,0))\n44 contour(tCopula(param=ft$par[7],dim=2,df=round(ft$par[8])),\n\n45         pCopula, main = expression(hat(C)[t]),\n46         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n47 eval(EmpCop)\n\n48 contour(normalCopula(param=fnorm@estimate[1], dim = 2),\n\n49         pCopula, main = expression(hat(C)[Gauss]),\n50         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n51 eval(EmpCop)\n\n52 contour(frankCopula(param=ffrank@estimate[1], dim = 2),\n\n53         pCopula, main = expression(hat(C)[Fr]),\n54         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n55 eval(EmpCop)\n\n56 contour(claytonCopula(param=fclayton@estimate[1], dim = 2),\n\n57         pCopula, main = expression(hat(C)[Cl]),\n58         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n59 eval(EmpCop)\n\n60 contour(gumbelCopula(param=fgumbel@estimate[1], dim = 2),\n\n61         pCopula, main = expression(hat(C)[Gu]),\n62         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n63 eval(EmpCop)\n\n64 contour(joeCopula(param=fjoe@estimate[1], dim = 2),\n\n65         pCopula, main = expression(hat(C)[Joe]),\n66         xlab = expression(hat(U)[1]), ylab = expression(hat(U)[2]) )\n67 eval(EmpCop)\n\f                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.9",
      "section_title": "R Lab    211",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.10 Exercises    213\n\nProblem 7 Do you see any di\ufb00erence between the parametric estimates of\nthe copula? If so, which seem closest to the empirical copula? Include the plot\nwith your work.\n\n\nA two-dimensional KDE of the copula\u2019s density will be compared with the\nparametric density estimates (PDFs).\n68 par(mfrow=c(2,3), mgp = c(2.5,1,0))\n69 contour(tCopula(param=ft$par[7],dim=2,df=round(ft$par[8])),\n70         dCopula, main = expression(hat(c)[t]),\n71   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n72 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n73 contour(normalCopula(param=fnorm@estimate[1], dim = 2),\n\n74         dCopula, main = expression(hat(c)[Gauss]),\n75   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n76 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n77 contour(frankCopula(param=ffrank@estimate[1], dim = 2),\n\n78         dCopula, main = expression(hat(c)[Fr]),\n79   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n80 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n81 contour(claytonCopula(param=fclayton@estimate[1], dim = 2),\n\n82         dCopula, main = expression(hat(c)[Cl]),\n83   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n84 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n85 contour(gumbelCopula(param=fgumbel@estimate[1], dim = 2),\n\n86         dCopula, main = expression(hat(c)[Gu]),\n87   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n88 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n89 contour(joeCopula(param=fjoe@estimate[1], dim = 2),\n\n90         dCopula, main = expression(hat(c)[Joe]),\n91   nlevels=25, xlab=expression(hat(U)[1]),ylab=expression(hat(U)[2]))\n92 contour(kde2d(data1[,1],data1[,2]), col = 2, add = TRUE)\n\n\n\nProblem 8 Do you see any di\ufb00erence between the parametric estimates of\nthe copula density? If so, which seem closest to the KDE? Include the plot\nwith your work.\n\n\nProblem 9 Find AIC for the t, (Gaussian), Frank, Clayton, Gumbel and\nJoe copulas. Which copula model \ufb01ts best by AIC? (Hint: The fitCopula()\nfunction returns the log-likelihood.)\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.10",
      "section_title": "Exercises    213",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.10 Exercises\n 1. Kendall\u2019s tau rank correlation between X and Y is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.10",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.55. Both X and Y are\n    positive. What is Kendall\u2019s tau between X and 1/Y ? What is Kendall\u2019s\n    tau between 1/X and 1/Y ?\n\f214    8 Copulas\n\n 2. Suppose that X is Uniform(0,1) and Y = X 2 . Then the Spearman rank\n    correlation and the Kendall\u2019s tau between X and Y will both equal 1,\n    but the Pearson correlation between X and Y will be less than 1. Explain\n    why.\n 3. Show that an Archimedean copula with generator function \u03d5(u) = \u2212 log(u)\n    is equal to the independence copula C0 . Does the same hold when the natu-\n    ral logarithm is replaced by the common logarithm, i.e.,\n    \u03d5(u) = \u2212 log10 (u)?\n 4. The co-monotonicity copula C+ is not an Archimedean copula; however, in\n    the two-dimensional case, the counter-monotonicity copula C\u2212 (u1 , u2 ) =\n    max(u1 + u2 \u2212 1, 0) is. What is its generator function?\n 5. Show that the generator of a Frank copula\n                                \u000e \u2212\u03b8u     \u000f\n                                 e    \u22121\n              \u03d5Fr (u|\u03b8) = \u2212 log             , \u03b8 \u2208 {(\u2212\u221e, 0) \u222a (0, \u221e)},\n                                  e\u2212\u03b8 \u2212 1\n    satis\ufb01es assumptions 1\u20133 of a strict generator.\n 6. Show that as \u03b8 \u2192 \u221e, CFr (u1 , u2 |\u03b8) \u2192 min(u1 , u2 ), the co-monotonicity\n    copula C+ .\n 7. Suppose that \u03d51 , . . . , \u03d5k are k strict generator functions and de\ufb01ne a new\n    generator \u03d5 as a convex combination of these k generators, that is\n\n                         \u03d5(u) = a1 \u03d51 (u) + \u00b7 \u00b7 \u00b7 + ak \u03d5k (u),\n\n    in which a1 , . . . , ak are any non-negative constants which sum to 1. Show\n    that \u03d5(u) is a strict generator function. For the case in which k = 2, what\n    is the corresponding copula function for \u03d5(u)?\n 8. Let \u03d5(u|\u03b8) = (1 \u2212 u)\u03b8 , for some \u03b8 \u2265 1, and show that for the two-\n    dimensional case this generates the copula\n\n              C(u1 , u2 |\u03b8) = max[0, 1 \u2212 {(1 \u2212 u1 )\u03b8 + (1 \u2212 u2 )\u03b8 }1/\u03b8 ].\n\n    Further, show that as \u03b8 \u2192 \u221e, C(u1 , u2 |\u03b8) \u2192 min(u1 , u2 ), the co-\n    monotonicity copula C+ , and that as \u03b8 \u2192 1, C(u1 , u2 |\u03b8) \u2192 max(u1 +\n    u2 \u2212 1, 0), the counter-monotonicity copula C\u2212 .\n 9. A convex combination of k joint CDFs is itself a joint CDF (\ufb01nite mix-\n    ture), but is a convex combination of k copula functions a copula function\n    itself?\n10. Suppose Y = (Y1 , . . . , Yd ) has a meta-Gaussian distribution with con-\n    tinuous marginal distributions and copula C Gauss (\u00b7|\u03a9). Show that if\n    \u03c1\u03c4 (Yi , Yj ) = 0 then Yi and Yj are independent.\n\n\nReferences\nCherubini, U., Luciano, E., and Vecchiato, W. (2004) Copula Methods in\n  Finance, John Wiley, New York.\n\f                                                           References    215\n\nDu\ufb03e, D. and Singleton, K. J. (2003) Credit Risk, Princeton University Press,\n  Princeton and Oxford.\nJoe, H. (1997) Multivariate Models and Dependence Concepts, Chapman &\n  Hall, London.\nLi, D (2000) On default correlation: A copula function approach, Journal of\n  Fixed Income, 9, 43\u201354.\nMari, D. D. and Kotz, S. (2001) Correlation and Dependence, World Scienti\ufb01c,\n  London.\nMcNeil, A., Frey, R., and Embrechts, P. (2005) Quantitative Risk Manage-\n  ment, Princeton University Press, Princeton and Oxford.\nNelsen, R. B. (2007) An Introduction to Copulas, 2nd ed., Springer, New York.\nSalmon, F. (2009) Recipe for Disaster: The Formula That Killed Wall Street,\n  Wired http://www.wired.com/techbiz/it/magazine/17-03/wp_quant?\n  currentPage=all\n\f9\nRegression: Basics\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.55",
      "section_title": "Both X and Y are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1 Introduction\nRegression is one of the most widely used of all statistical methods.\nFor univariate regression, the available data are one response variable and\np predictor variables, all measured on each of n observations. We let Y de-\nnote the response variable and X1 , . . . , Xp be the predictor or explanatory\nvariables. Also, Yi and Xi,1 , . . . , Xi,p are the values of these variables for the\nith observation. The goals of regression modeling include the investigation of\nhow Y is related to X1 , . . . , Xp , estimation of the conditional expectation of Y\ngiven X1 , . . . , Xp , and prediction of future Y values when the corresponding\nvalues of X1 , . . . , Xp are already available. These goals are closely connected.\n    The multiple linear regression model relating Y to the predictor or regres-\nsor variables is\n                         Yi = \u03b20 + \u03b21 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xi,p + \u0017i ,             (9.1)\nwhere \u0017i is called the noise, disturbances, or errors. The adjective \u201cmultiple\u201d\nrefers to the predictor variables. Multivariate regression, which has more than\none response variable, is covered in Chap. 18. The \u0017i are often called \u201cerrors\u201d\nbecause they are the prediction errors when Yi is predicted by \u03b20 + \u03b21 Xi,1 +\n\u00b7 \u00b7 \u00b7 + \u03b2p Xi,p . It is assumed that\n                              E(\u0017i |Xi,1 , . . . , Xi,p ) = 0,                  (9.2)\nwhich, with (9.1), implies that\n               E(Yi |Xi,1 , . . . , Xi,p ) = \u03b20 + \u03b21 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xi,p .\n   The parameter \u03b20 is the intercept. The regression coe\ufb03cients \u03b21 , . . . , \u03b2p\nare the slopes. More precisely, \u03b2j is the partial derivative of the expected\nresponse with respect to the jth predictor:\n\n\n\u00a9 Springer Science+Business Media New York 2015                                  217\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 9\n\f218    9 Regression: Basics\n\n                                  \u2202 E(Yi |Xi,1 , . . . , Xi,p )\n                          \u03b2j =                                  .\n                                          \u2202 Xi,j\n\nTherefore, \u03b2j is the change in the expected value of Yi when Xi,j changes one\nunit. It is assumed that the noise is i.i.d. white so that\n\n              \u00171 , . . . , \u0017n are i.i.d. with mean 0 and variance \u03c3 2 .      (9.3)\n\nOften the \u0017i s are assumed to be normally distributed, which with (9.3) implies\nGaussian white noise.\n   For the reader\u2019s convenience, the assumptions of the linear regression\nmodel are summarized:\n1. linearity of the conditional expectation: E(Yi |Xi,1 , . . . , Xi,p ) = \u03b20 + \u03b21\n   Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xi,p ;\n2. independent noise: \u00171 , . . . , \u0017n are independent;\n3. constant variance: Var(\u0017i ) = \u03c3 2 for all i;\n4. Gaussian noise: \u0017i is normally distributed for all i.\nThis chapter and, especially, the next two chapters discuss methods for check-\ning these assumptions, the consequences of their violations, and possible reme-\ndies when they do not hold.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2 Straight-Line Regression\nStraight-line regression is linear regression with only one predictor variable.\nThe model is\n                              Y i = \u03b20 + \u03b2 1 Xi + \u0017 i ,                   (9.4)\nwhere \u03b20 and \u03b21 are the unknown intercept and slope of the line and \u0017i is\ncalled the noise or error.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Straight-Line Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2.1 Least-Squares Estimation\n\nThe regression coe\ufb03cients can be estimated by the method of least squares.\nThe least-squares estimates are the values of \u03b2\u00020 and \u03b2\u00021 that minimize\n                              n   (                          )2\n                                      Yi \u2212 (\u03b2\u00020 + \u03b2\u00021 Xi )        .          (9.5)\n                            i=1\n\nGeometrically, we are minimizing the sum of the squared lengths of the vertical\nlines in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "1 Least-Squares Estimation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1. The data points are shown as asterisks. The vertical lines\nconnect the data points and the predictions using the linear equation. The\npredictions themselves are called the \ufb01tted values or \u201cy-hats\u201d and shown as\nopen circles. The di\ufb00erences between the Y -values and the \ufb01tted values are\ncalled the residuals. Using calculus to minimize (9.5), one can show that\n\f                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "The data points are shown as asterisks. The vertical lines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2 Straight-Line Regression      219\n\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Straight-Line Regression      219",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4\n\n\n          1.2\n\n\n           1\n\n\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.4",
      "section_title": "1.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8                                    Y\n                                    residual\n\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "Y",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n          0.4\n                                         Fitted value\n\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n           0\n           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1      0.2      0.3   0.4   0.5   0.6      0.7   0.8   0.9   1\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.2      0.3   0.4   0.5   0.6      0.7   0.8   0.9   1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1. Least-squares estimation. The vertical lines connect the data (*) and the\n\ufb01tted values (o) represent the residuals. The least-squares line is de\ufb01ned as the line\nmaking the sum of the squared residuals as small as possible.\n\n                        \u0017n                           \u0017n\n                          i=1 (Yi \u2212 Y )(Xi \u2212 X)           Yi (Xi \u2212 X)\n                \u03b2\u00021 =      \u0017n                   =    \u0017i=1\n                                                       n              .         (9.6)\n                               i=1 (Xi \u2212 X)            i=1 (Xi \u2212 X)\n                                            2                       2\n\n\nand\n                                    \u03b2\u00020 = Y \u2212 \u03b2\u00021 X.                            (9.7)\nThe least-squares line is\n\n                  Y\u0002 = \u03b2\u00020 + \u03b2\u00021 X = Y + \u03b2\u00021 (X \u2212 X)\n                            \u000e \u0017n                        \u000f\n                                  i=1 (Yi \u2212 Y )(Xi \u2212 X)\n                     =Y +          \u0017n                     (X \u2212 X)\n                                       i=1 (Xi \u2212 X)\n                                                    2\n\n                             sXY\n                     = Y + 2 (X \u2212 X),\n                              sX\n                     \u0017n\nwhere sXY = (n\u22121)\u22121 i=1 (Yi \u2212Y )(Xi \u2212X) is the sample covariance between\nX and Y and s2X is the sample variance of X.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "Least-squares estimation. The vertical lines connect the data (*) and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1. Weekly interest rates \u2014 least-squares estimates\n\n\n\n   Weekly interest rates from February 16, 1977, to December 31, 1993, were\nobtained from the Federal Reserve Bank of Chicago. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "Weekly interest rates \u2014 least-squares estimates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2 is a plot of\n\f220      9 Regression: Basics\n\n\n\n\n                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "is a plot of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                0.4\n                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n           change in AAA rate\n                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "change in AAA rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                \u22120.6 \u22120.4 \u22120.2\n\n\n\n\n                                                 \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.6 \u22120.4 \u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0   \u22120.5             0.0           0.5\n                                                               change in 10YR T rate\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5             0.0           0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2. Changes in Moody\u2019s seasoned corporate AAA bond yields plotted against\nchanges in 10-year Treasury constant maturity rate. Data from Federal Reserve Sta-\ntistical Release H.15 and were taken from the Chicago Federal Bank\u2019s website.\n\n\nchanges in the 10-year Treasury constant maturity rate and changes in the\nMoody\u2019s seasoned corporate AAA bond yield. The plot looks linear, so we try\nlinear regression using R\u2019s lm() function. The code is:\n      options(digits = 3)\n      summary(lm(aaa_dif ~ cm10_dif))\n\nThe code aaa_dif ~ cm10_dif is an example of a formula in R with the\noutcome variable to the left of \u201c~\u201d and the explanatory variables to the right\nof \u201c~.\u201d In this example, there is only one explanatory variable. In cases where\nthere are multiple explanatory variables, they are separated by \u201c+\u201d. Here is\nthe output.\n      Call:\n      lm(formula = aaa_dif ~ cm10_dif)\n\n      Coefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n      (Intercept) -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Changes in Moody\u2019s seasoned corporate AAA bond yields plotted against",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000109   0.002221   -0.05     0.96\n      cm10_dif     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000109",
      "section_title": "0.002221   -0.05     0.96",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.615762   0.012117   50.82   <2e-16 ***\n      ---\n      Signif. codes: 0 *** ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.615762",
      "section_title": "0.012117   50.82   <2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 ** 0.01 * 0.05 . 0.1 1\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "** 0.01 * 0.05 . 0.1 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.066 on 878 degrees of freedom\n      Multiple R-Squared: 0.746,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.066",
      "section_title": "on 878 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.746\n      F-statistic: 2.58e+03 on 1 and 878 DF, p-value: <2e-16\n\f                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.746",
      "section_title": "F-statistic: 2.58e+03 on 1 and 878 DF, p-value: <2e-16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2 Straight-Line Regression   221\n\nFrom the output we see that the least-squares estimates of the intercept and\nslope are \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Straight-Line Regression   221",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000109 and 0.616. The Residual standard error is 0.066; this\n                \u0002 or s, the estimate of \u03c3 ; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000109",
      "section_title": "and 0.616. The Residual standard error is 0.066; this",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3. The remaining items\nis what we call \u03c3\nof the output are explained shortly.                                         \u0002\n                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "The remaining items",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n         Food industry excess return\n\n                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "Food industry excess return",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                       0.0\n                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                                       \u22120.2\n\n\n\n\n                                              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2   \u22120.1          0.0         0.1\n                                                      Market excess return\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.1          0.0         0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3. Plot of excess returns on the food industry versus excess returns on the\nmarket. Data from the data set Capm in R\u2019s Ecdat package.\n\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Plot of excess returns on the food industry versus excess returns on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2. Excess returns on the food sector and the market portfolio\n\n    The excess return on a security or market index is the return minus the\nrisk-free interest rate. An important application of linear regression in \ufb01nance\nis the regression of the excess return of an asset or market sector on the excess\nreturn of the entire market. This type of application will be discussed much\nmore fully in Chap. 17. In this example, we will regress the excess monthly\nreturn of the food sector (rfood) on the excess monthly return of the market\nportfolio (rmrf). The data are in R\u2019s Capm data set in the Ecdat package and\nare plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Excess returns on the food sector and the market portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3. The returns are expressed as percentages in the data\nset but have been converted to fractions in this example. The output from\nlm is\n   Call:\n   lm(formula = rfood ~ rmrf)\n\n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)\n\f222        9 Regression: Basics\n\n      (Intercept) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "The returns are expressed as percentages in the data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00339    0.00128    2.66   0.0081 **\n      rmrf         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00339",
      "section_title": "0.00128    2.66   0.0081 **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.78342   0.02835   27.63   <2e-16 ***\n      ---\n      Signif. codes: 0 *** ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.78342",
      "section_title": "0.02835   27.63   <2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 ** 0.01 * 0.05 . 0.1 1\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "** 0.01 * 0.05 . 0.1 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0289 on 514 degrees of freedom\n      Multiple R-Squared: 0.598,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0289",
      "section_title": "on 514 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.597\n      F-statistic: 763 on 1 and 514 DF, p-value: <2e-16\n\nThus, the \ufb01tted regression equation is\n\n                           rfood = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.597",
      "section_title": "F-statistic: 763 on 1 and 514 DF, p-value: <2e-16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00339 + 0.78342 rmrf + \u0017,\n\n    \u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00339",
      "section_title": "+ 0.78342 rmrf + \u0017,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0289.\nand \u03c3                                                                                         \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0289",
      "section_title": "and \u03c3                                                                                         \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2.2 Variance of \u03b2\u00021\n\nIt is useful to have a formula for the variance of an estimator to show how\nthe estimator\u2019s precision depends on various aspects of the data such as the\nsample size and the values of the predictor variables. Fortunately, it is easy to\nderive a formula for the variance of \u03b2\u00021 . By (9.6), we can write \u03b2\u00021 as a weighted\naverage of the responses\n                                                     n\n                                             \u03b2\u00021 =         wi Y i ,\n                                                     i=1\n\nwhere wi is the weight given by\n\n                                                   Xi \u2212 X\n                                    w i = \u0017n                    .\n                                                 i=1 (Xi \u2212 X)\n                                                              2\n\n\nWe consider X1 , . . . , Xn as \ufb01xed, so if they are random we are conditioning\nupon their values. From the assumptions of the regression model, it follows\nthat Var(Yi |X1 , . . . , Xn ) = \u03c3 2 and Y1 , . . . , Yn are conditionally uncorrelated.\nTherefore,\n                                         n\n                                                                \u03c32              \u03c32\n      Var(\u03b2\u00021 |X1 , . . . , Xn ) = \u03c3 2         wi2 = \u0017n                   =            .   (9.8)\n                                         i=1               i=1 (Xi \u2212 X)\n                                                                        2   (n \u2212 1)s2X\n\nIt is worth taking some time to examine this formula. First, the numerator\n\u03c3 2 is simply the variance of the \u0017i . This is not surprising. More variability in\nthe noise means more variable estimators. The denominator shows us that the\nvariance of \u03b2\u00021 is inversely proportional to (n \u2212 1) and to s2X . So the precision\nof \u03b2\u00021 increases as \u03c3 2 is reduced, n is increased, or s2X is increased. Why does\nincreasing s2X decrease Var(\u03b2\u00021 |X1 , . . . , Xn )? The reason is that increasing s2X\nmeans that the Xi are spread farther apart, which makes the slope of the line\neasier to estimate.\n\f                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "2 Variance of \u03b2\u00021",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3 Multiple Linear Regression     223\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Multiple Linear Regression     223",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3. Optimal sampling frequencies for regression\n\n    Here is an important application of (9.8). Suppose that we have two sta-\ntionary time series, Xt and Yt , and we wish to regress Yt on Xt . We have just\nseen examples of this. A signi\ufb01cant practical question is whether one should\nuse daily or weekly data, or perhaps even monthly or quarterly data. Does it\nmatter which sampling frequency we use? The answer is \u201cyes\u201d and the highest\npossible sampling frequency gives the most precise estimate of the slope. To\nunderstand why this is so, we compare daily and weekly data. Assume that\nthe Xt and Yt are white noise sequences. Since a weekly log return is sim-\nply the sum of the \ufb01ve daily log returns within a week, \u03c3 2 and s2X will each\nincrease by a factor of \ufb01ve if we change from daily to weekly log returns, so\nthe ratio \u03c3 2 /s2X will not change. However, by changing from daily to weekly\nlog returns, (n \u2212 1) is reduced by approximately a factor of \ufb01ve. The result\nis that Var(\u03b2\u00021 |X1 , . . . , Xn ) is approximately \ufb01ve times smaller using daily\nrather than weekly log returns. Similarly, Var(\u03b2\u00021 |X1 , . . . , Xn ) is about four\ntimes larger using monthly rather than weekly returns.\n    The obvious conclusion is that one should use the highest sampling fre-\nquency available, which is often daily returns. We have assumed that the Xt\nand Yt are white noise in order to simplify the calculations, but this con-\nclusion still holds if they are stationary but autocorrelated. (Autocorrelation\nis discussed in Chap. 12.) However, the noise series, that is \u0017i , i = 1, . . ., in\nEq. (9.4) needs to be uncorrelated. If the noise is autocorrelated and becomes\nmore highly correlated as the sampling frequency increases, then this conclu-\nsion need not hold. There may be a point of diminishing returns where more\nfrequent sampling does not improve estimation accuracy.                          \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Optimal sampling frequencies for regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3 Multiple Linear Regression\n\nThe multiple linear regression model is\n\n                         Yi = \u03b20 + \u03b21 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xi,p + \u0017i .\n\nThe least-squares estimates are the values \u03b2\u00020 , \u03b2\u00021 , . . . , \u03b2\u0002p that minimize\n                     n    (                                               )2\n                              Yi \u2212 (\u03b2\u00020 + \u03b2\u00021 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2\u0002p Xi,p )        .      (9.9)\n                    i=1\n\nCalculation of the least-squares estimates is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Multiple Linear Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1. For appli-\ncations, the technical details are not important, since software for least-squares\nestimation is readily available.\n    The ith \ufb01tted value is\n\n                           Y\u0002i = \u03b2\u00020 + \u03b2\u00021 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2\u0002p Xi,p                   (9.10)\n\f224      9 Regression: Basics\n\nand estimates E(Yi |Xi,1 , . . . , Xi,p ). The ith residual is\n\n                \u0017i = Yi \u2212 Y\u0002i = Yi \u2212 (\u03b2\u00020 + \u03b2\u00021 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2\u0002p Xi,p )\n                \u0002                                                           (9.11)\n\nand estimates \u0017i . It is worth noting that (9.11) can be re-expressed as\n\n                                     Yi = Y\u0002i + \u0002\n                                                \u0017i .                        (9.12)\n\nAn unbiased estimate of \u03c3 2 is\n                                          \u0017n\n                                             i=1 \u0002\n                                                 \u00172i\n                                   \u0002 =\n                                   \u03c32\n                                                       .                    (9.13)\n                                         n\u22121\u2212p\nThe denominator in (9.13) is the sample size minus the number of regression\ncoe\ufb03cients that are estimated.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "For appli-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4. Multiple linear regression with interest rates\n\n    As an example, we continue the analysis of the weekly interest-rate data\nbut now with changes in the 30-year Treasury rate (cm30 dif) and changes\nin the Federal funds rate (ff dif) as additional predictors. Thus p = 3.\nFigure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "Multiple linear regression with interest rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 is a scatterplot matrix of the four time series. There is a strong\nlinear relationship between all pairs of aaa dif, cm10 dif, and cm30 dif, but\nff dif is not strongly related to the other series. The code is\n      summary(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))\n\nThe lm() output for this regression is\n      Call:\n      lm(formula = aaa_dif ~ cm10_dif + cm30_dif + ff_dif)\n\n      Coefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n      (Intercept) -9.07e-05   2.18e-03   -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "is a scatterplot matrix of the four time series. There is a strong",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04     0.97\n      cm10_dif     3.55e-01   4.51e-02    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "0.97",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.86 1.1e-14 ***\n      cm30_dif     3.00e-01   5.00e-02    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.86",
      "section_title": "1.1e-14 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.00 2.9e-09 ***\n      ff_dif       4.12e-03   5.28e-03    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.00",
      "section_title": "2.9e-09 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.78     0.44\n      ---\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.78",
      "section_title": "0.44",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0646 on 876 degrees of freedom\n      Multiple R-Squared: 0.756,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0646",
      "section_title": "on 876 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.755\n      F-statistic: 906 on 3 and 876 DF, p-value: <2e-16\n\nWe see that \u03b2\u00020 = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.755",
      "section_title": "F-statistic: 906 on 3 and 876 DF, p-value: <2e-16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.07 \u00d7 10\u221205 , \u03b2\u00021 = 0.355, \u03b2\u00022 = 0.300, and \u03b2\u00023 = 0.00412.\n\u0002\n\n   A commonly used special case of multiple regression is the polynomial re-\ngression model which uses powers of the predictors as well as the predictors\n\f                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.07",
      "section_title": "\u00d7 10\u221205 , \u03b2\u00021 = 0.355, \u03b2\u00022 = 0.300, and \u03b2\u00023 = 0.00412.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3 Multiple Linear Regression                      225\n                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Multiple Linear Regression                      225",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0   0.0   0.5                       \u22122     0   1   2   3\n\n\n\n\n                                                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.0   0.5                       \u22122     0   1   2   3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.4\n                aaa_dif\n\n\n\n\n                                                                                                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      0.0\n\n\n\n\n                                      cm10_dif\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                0.5\n                                                     cm30_dif\n\n\n\n\n                                                                                                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n      2\n\n\n\n\n                                                                              ff_dif\n      0\n      \u22122\n\n\n\n\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 \u22120.2   0.2   0.6                   \u22120.5   0.0   0.5\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22120.2   0.2   0.6                   \u22120.5   0.0   0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4. Scatterplot matrix of the changes in four weekly interest rates. The variable\naaa dif is the response in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "Scatterplot matrix of the changes in four weekly interest rates. The variable",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.\n\n\nthemselves. For example, when there is one X-variable, the p-degree polyno-\nmial regression model is\n\n                                Yi = \u03b20 + \u03b21 Xi + \u00b7 \u00b7 \u00b7 + \u03b2p Xip + \u0017i .\n\nAs another example, the quadratic regression model with two predictors is\n                                     2                                  2\n             Yi = \u03b20 + \u03b21 Xi,1 + \u03b22 Xi,1 + \u03b23 Xi,1 Xi,2 + \u03b24 Xi,2 + \u03b25 Xi,2 + \u0017i .\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "themselves. For example, when there is one X-variable, the p-degree polyno-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3.1 Standard Errors, t-Values, and p-Values\n\nIn this section we explain the use of several statistics included in regression\noutput. We use the output in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "1 Standard Errors, t-Values, and p-Values",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 as an illustration.\n    As noted before, the estimated coe\ufb03cients are \u03b2\u00020 = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "as an illustration.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.07 \u00d7 10\u221205 , \u03b2\u00021 =\n0.355, \u03b2\u00022 = 0.300, and \u03b2\u00023 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.07",
      "section_title": "\u00d7 10\u221205 , \u03b2\u00021 =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00412. Each of these coe\ufb03cients has three other\nstatistics associated with it.\n\u2022   The standard error (SE), which is the estimated standard deviation of the\n    least-squares estimator, tells us the precision of the estimator.\n\u2022   The t-value, is the t-statistic for testing that the coe\ufb03cient is 0. The t-\n    value is the ratio of the estimate to its standard error. For example, for\n    cm10 dif, the t-value is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00412",
      "section_title": "Each of these coe\ufb03cients has three other",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.86 = 0.355/0.0451.\n\f226       9 Regression: Basics\n\n\u2022     The p-value (Pr > |t| in the lm() output), associated with testing the\n      null hypothesis that the coe\ufb03cient is 0 versus the alternative that it is not\n      0. If a p-value for a slope parameter is small, as it is here for \u03b21 , then this\n      is evidence that the corresponding coe\ufb03cient is not 0, which means that\n      the predictor has a linear relationship with the response.\n    It is important to keep in mind that the p-value only tells us if there is\na linear relationship. The existence of a linear relationship between Yi and\nXi,j means only that the linear predictor of Yi has a nonzero slope on Xi,j ,\nor, equivalently, that partial correlation between Xi,j and Yi is not zero. (The\npartial correlation between two variables is their correlation when all other\nvariables are held \ufb01xed.) When the p-value is small (so a linear relation-\nship exists), there could also be a strong nonlinear deviation from the linear\nrelationship as in Fig. A.4g. Moreover, when the p-value is large (so no lin-\near relationship exists), there could still be a strong nonlinear relationship in\nFig. A.4f. Because of the potential for nonlinear relationships to go undetected\nin a linear regression analysis, graphical analysis of the data (e.g., Fig. 9.4)\nand residual analysis (see Chap. 10) are essential.\n    The p-values for \u03b21 and \u03b22 are very small, so we can conclude that these\nslopes are not 0. The p-value is large (0.97) for \u03b20 , so we would not reject the\nhypothesis that the intercept is 0.\n    Similarly, we would not reject the null hypothesis that \u03b23 is zero. Stated\ndi\ufb00erently, we can accept the null hypothesis that, conditional on cm10 dif\nand cm30 dif, aaa dif and ff dif are not linearly related. This result should\nnot be interpreted as stating that aaa dif and ff dif are unrelated, but only\nthat ff dif is not useful for predicting aaa dif when cm10 dif and cm30 dif\nare included in the regression model. (In fact, aaa dif and ff dif have a\ncorrelation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.86",
      "section_title": "= 0.355/0.0451.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "|t| in the lm() output), associated with testing the\n      null hypothesis that the coe\ufb03cient is 0 versus the alternative that it is not\n      0. If a p-value for a slope parameter is small, as it is here for \u03b21 , then this\n      is evidence that the corresponding coe\ufb03cient is not 0, which means that\n      the predictor has a linear relationship with the response.\n    It is important to keep in mind that the p-value only tells us if there is\na linear relationship. The existence of a linear relationship between Yi and\nXi,j means only that the linear predictor of Yi has a nonzero slope on Xi,j ,\nor, equivalently, that partial correlation between Xi,j and Yi is not zero. (The\npartial correlation between two variables is their correlation when all other\nvariables are held \ufb01xed.) When the p-value is small (so a linear relation-\nship exists), there could also be a strong nonlinear deviation from the linear\nrelationship as in Fig. A.4g. Moreover, when the p-value is large (so no lin-\near relationship exists), there could still be a strong nonlinear relationship in\nFig. A.4f. Because of the potential for nonlinear relationships to go undetected\nin a linear regression analysis, graphical analysis of the data (e.g., Fig. 9.4)\nand residual analysis (see Chap. 10) are essential.\n    The p-values for \u03b21 and \u03b22 are very small, so we can conclude that these\nslopes are not 0. The p-value is large (0.97) for \u03b20 , so we would not reject the\nhypothesis that the intercept is 0.\n    Similarly, we would not reject the null hypothesis that \u03b23 is zero. Stated\ndi\ufb00erently, we can accept the null hypothesis that, conditional on cm10 dif\nand cm30 dif, aaa dif and ff dif are not linearly related. This result should\nnot be interpreted as stating that aaa dif and ff dif are unrelated, but only\nthat ff dif is not useful for predicting aaa dif when cm10 dif and cm30 dif\nare included in the regression model. (In fact, aaa dif and ff dif have a\ncorrelation of",
        "start": 76,
        "end": 2037
      }
    ]
  },
  {
    "content": "0.25 (this is the full, not partial, correlation) and the linear\nregression of aaa dif on ff dif alone is highly signi\ufb01cant; the p-value for\ntesting that the slope is zero is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "(this is the full, not partial, correlation) and the linear",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.158 \u00d7 10\u221214 .)\n    Since the Federal Funds rate is a short-term (overnight) rate, it is not sur-\nprising that ff dif is less useful than changes in the 10- and 30-year Treasury\nrates for predicting aaa dif.\n    For#regression with one predictor variable, by (9.8) the standard error of \u03b2\u00021\n         \u0017n\n   \u0002/\nis \u03c3        i=1 (Xi \u2212 X) . When there are more than two predictor variables,\n                        2\n\nformulas of standard errors are more complex and are facilitated by the use\nof matrix notation. Because standard errors can be computed with standard\nsoftware such as lm, the formulas are not needed for applications and so are\npostponed to Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.158",
      "section_title": "\u00d7 10\u221214 .)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1.\n\f                       9.4 Analysis of Variance, Sums of Squares, and R2     227\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "9.4 Analysis of Variance, Sums of Squares, and R2     227",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 Analysis of Variance, Sums of Squares, and R2\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "Analysis of Variance, Sums of Squares, and R2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.1 ANOVA Table\n\nCertain results of a regression \ufb01t are often displayed in an analysis of variance\ntable, also called the ANOVA or AOV table. The idea behind the ANOVA\ntable is to describe how much of the variation in Y is predictable if one knows\nX1 , . . . , Xp . Here is the ANOVA table for the model in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "1 ANOVA Table",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.\n   > anova(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))\n   Analysis of Variance Table\n\n   Response: aaa_dif\n              Df Sum Sq Mean Sq F value Pr(>F)\n   cm10_dif    1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "> anova(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "anova(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))\n   Analysis of Variance Table",
        "start": 8,
        "end": 91
      }
    ]
  },
  {
    "content": "11.21    11.21 2682.61 < 2e-16 ***\n   cm30_dif    1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.21",
      "section_title": "11.21 2682.61 < 2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15    0.15   35.46 3.8e-09 ***\n   ff_dif      1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "0.15   35.46 3.8e-09 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0025 0.0025     0.61    0.44\n   Residuals 876   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0025",
      "section_title": "0.0025     0.61    0.44",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.66 0.0042\n   ---\n\nThe total variation in Y can be partitioned into two parts: the variation that\ncan be predicted by X1 , . . . , Xp and the variation that cannot be predicted.\nThe variation that can be predicted is measured by the regression sum of\nsquares, which is\n                                              n\n                        regression SS =           (Y\u0002i \u2212 Y )2 .\n                                            i=1\n\nThe regression sum of squares for the model that uses only cm10 dif is in the\n\ufb01rst row of the ANOVA table and is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.66",
      "section_title": "0.0042",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.21. The entry, 0.15, in the second row\nis the increase in the regression sum of squares when cm30 dif is added to\nthe model. Similarly, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.21",
      "section_title": "The entry, 0.15, in the second row",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0025 is the increase in the regression sum of squares\nwhen ff dif is added. Thus, rounding to two decimal places, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0025",
      "section_title": "is the increase in the regression sum of squares",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.36 = 11.21\n+ ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.36",
      "section_title": "= 11.21",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 + 0.00 is the regression sum of squares with all three predictors in the\nmodel.\n    The amount of variation in Y that cannot be predicted by a linear function\nof X1 , . . . , Xp is measured by the residual error sum of squares, which is the\nsum of the squared residuals; i.e.,\n                                                  n\n                      residual error SS =             (Yi \u2212 Y\u0002i )2 .\n                                               i=1\n\nIn the ANOVA table, the residual error sum of squares is in the last row and\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "+ 0.00 is the regression sum of squares with all three predictors in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.66. The total variation is measured by the total sum of squares (total SS),\nwhich is the sum of the squared deviations of Y from its mean; that is,\n                                        n\n                           total SS =         (Yi \u2212 Y )2 .                 (9.14)\n                                        i=1\n\f228      9 Regression: Basics\n\nIt can be shown algebraically that\n                  total SS = regression SS + residual error SS.            (9.15)\nTherefore, in Example 9.4, the total SS is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.66",
      "section_title": "The total variation is measured by the total sum of squares (total SS),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.36 + 3.66 = 15.02.\n   R-squared, denoted by R2 , is\n                        regression SS         residual error SS\n                  R2 =                 =1\u2212\n                            total SS               total SS\nand measures the proportion of the total variation in Y that can be linearly\npredicted by X. In the example, R2 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.36",
      "section_title": "+ 3.66 = 15.02.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.746 = 11.21/15.02 if only cm10 dif is\nthe model and is 11.36/",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.746",
      "section_title": "= 11.21/15.02 if only cm10 dif is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.02 = 0.756 if all three predictors are in the model.\nThis value can be found in the output displayed in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.02",
      "section_title": "= 0.756 if all three predictors are in the model.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.\n    When there is only a single X variable, then R2 = rXY 2\n                                                              = rY2\u0002 Y , where rXY\nand rY\u0002 Y are the sample correlations between Y and X and between Y and\nthe predicted values, respectively. Put di\ufb00erently, R2 is the squared correla-\ntion between Y and X and also between Y and Y\u0002 . When there are multiple\npredictors, then we still have R2 = rY2\u0002 Y . Since Y\u0002 is a linear combination of\nthe X variables, R can be viewed as the \u201cmultiple\u201d correlation between Y\nand many Xs. The residual error sum of squares is also called the error sum\nof squares or sum of squared errors and is denoted by SSE.\n    It is important to understand that sums of squares in an ANOVA table\ndepend upon the order of the predictor variables in the regression, because\nthe sum of squares for any variable is the increase in the regression sum of\nsquares when that variable is added to the predictors already in the model.\n    The table below has the same variables as before, but the order of the\npredictor variables is reversed. Now that ff dif is the \ufb01rst predictor, its sum\nof squares is much larger than before and its p-value is highly signi\ufb01cant;\nbefore it was nonsigni\ufb01cant, only ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "When there is only a single X variable, then R2 = rXY 2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.44. The sum of squares for cm30 dif is\nnow much larger than that of cm10 dif, the reverse of what we saw earlier,\nsince cm10 dif and cm30 dif are highly correlated and the \ufb01rst of them in\nthe list of predictors will have the larger sum of squares.\n      > anova(lm(aaa_dif ~ ff_dif + cm30_dif + cm10_dif))\n      Analysis of Variance Table\n\n      Response: aaa_dif\n                 Df Sum Sq Mean Sq F value Pr(>F)\n      ff_dif      1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.44",
      "section_title": "The sum of squares for cm30 dif is",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "anova(lm(aaa_dif ~ ff_dif + cm30_dif + cm10_dif))\n      Analysis of Variance Table",
        "start": 256,
        "end": 342
      }
    ]
  },
  {
    "content": "0.94    0.94   224.8 < 2e-16 ***\n      cm30_dif    1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.94",
      "section_title": "0.94   224.8 < 2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.16    10.16 2432.1 < 2e-16 ***\n      cm10_dif    1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.16",
      "section_title": "10.16 2432.1 < 2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.26    0.26    61.8 1.1e-14 ***\n      Residuals 876   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.26",
      "section_title": "0.26    61.8 1.1e-14 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.66 0.0042\n\n   The lesson here is that an ANOVA table is most useful for assessing the\ne\ufb00ects of adding predictors in some natural order. Since AAA bonds have\nmaturities closer to 10 than to 30 years, and since the Federal Funds rate is\nan overnight rate, it made sense to order the predictors as cm10 dif, cm30 dif,\nand ff dif as done initially.\n\f                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.66",
      "section_title": "0.0042",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 Analysis of Variance, Sums of Squares, and R2      229\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "Analysis of Variance, Sums of Squares, and R2      229",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.2 Degrees of Freedom (DF)\n\nThere are degrees of freedom (DF) associated with each of these sources of\nvariation. The degrees of freedom for regression is p, which is the number of\npredictor variables. The total degrees of freedom is n \u2212 1. The residual error\ndegrees of freedom is n \u2212 p \u2212 1. Here is a way to think of degrees of freedom.\nInitially, there are n degrees of freedom, one for each observation. Then one\ndegree of freedom is allocated to estimation of the intercept. This leaves a\ntotal of n \u2212 1 degrees of freedom for estimating the e\ufb00ects of the X variables\nand \u03c3 2 . Each regression parameter uses one degree of freedom for estimation.\nThus, there are (n \u2212 1) \u2212 p degrees of freedom remaining for estimation of\n\u03c3 2 using the residuals. There is an elegant geometrical theory of regression\nwhere the responses are viewed as lying in an n-dimensional vector space and\ndegrees of freedom are the dimensions of various subspaces. However, there is\nnot su\ufb03cient space to pursue this subject here.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "2 Degrees of Freedom (DF)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.3 Mean Sums of Squares (MS) and F -Tests\n\nAs just discussed, every sum of squares in an ANOVA table has an associated\ndegrees of freedom. The ratio of the sum of squares to the degrees of freedom\nis the mean sum of squares:\n                                              sum of squares\n                 mean sum of squares =                         .\n                                            degrees of freedom\n\n   The residual mean sum of squares is the unbiased estimate \u03c3 2 given\nby (9.13); that is,\n                             \u0017n          \u0002 2\n                               i=1 (Yi \u2212 Yi )\n                      \u00022 =\n                      \u03c3                                                     (9.16)\n                               n\u22121\u2212p\n                         = residual mean sum of squares\n                                 residual error SS\n                         =                              .\n                            residual degrees of freedom\n   Other mean sums of squares are used in testing. Suppose we have two\nmodels, I and II, and the predictor variables in model I are a subset of those\nin model II, so that model I is a submodel of II. A common null hypothesis is\nthat the data are generated by model I. Equivalently, in model II the slopes\nare zero for variables not also in model I. To test this hypothesis, we use the\nexcess regression sum of squares of model II relative to model I:\n\n      SS(II | I) = regression SS for model II \u2212 regression SS for model I\n                 = residual SS for model I \u2212 residual SS for model II.    (9.17)\n\nEquality (9.17) holds because (9.15) is true for all models and, in particular, for\nboth model I and model II. The degrees of freedom for SS(II | I) is the number\n\f230      9 Regression: Basics\n\nof extra predictor variables in model II compared to model I. The mean square\nis denoted as MS(II | I). Stated di\ufb00erently, if p I and p II are the number of\nparameters in models I and II, respectively, then df II| I = p II \u2212 p I and MS(II\n| I) = SS(II | I)/df II| I . The F -statistic for testing the null hypothesis is\n\n                                       MS(II|I)\n                                 F =            ,\n                                         \u00022\n                                         \u03c3\n\nwhere \u03c3\u00022 is the mean residual sum of squares for model II. Under the null\nhypothesis, the F -statistic has an F -distribution with df II| I and n \u2212 p II \u2212 1\ndegrees of freedom and the null hypothesis is rejected if the F -statistic exceeds\nthe \u03b1-upper quantile of this F -distribution.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "3 Mean Sums of Squares (MS) and F -Tests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5. Weekly interest rates\u2014Testing the one-predictor versus three-\npredictor model\n\n   In this example, the null hypothesis is that, in the three-predictor model,\nthe slopes for cm30 dif and ff dif are zero. The F -test can be computed\nusing R\u2019s anova function. The output is\n      Analysis of Variance Table\n\n      Model 1: aaa_dif ~ cm10_dif\n      Model 2: aaa_dif ~ cm10_dif + cm30_dif + ff_dif\n        Res.Df RSS Df Sum of Sq      F Pr(>F)\n      1    878 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Weekly interest rates\u2014Testing the one-predictor versus three-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.81\n      2    876 3.66   2      0.15 18.0 2.1e-08 ***\n      ---\n      Signif. codes: 0 *** ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.81",
      "section_title": "2    876 3.66   2      0.15 18.0 2.1e-08 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 ** 0.01 * 0.05 . 0.1        1\n\nIn the last row, the entry 2 in the \u201cDf\u201d column is the di\ufb00erence between the\ntwo models in the number of parameters and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "** 0.01 * 0.05 . 0.1        1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 in the \u201cSum of Sq\u201d column\nis the di\ufb00erence between the residual sum of squares (RSS) for the two models.\n    The very small p-value (",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "in the \u201cSum of Sq\u201d column",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1 \u00d7 10\u22128 ) leads us to reject the null hypothesis\nand say that the result is \u201chighly signi\ufb01cant.\u201d It is important to be aware that\nthis phrase refers to statistical signi\ufb01cance. When the sample size is as large\nas it is here, it is common to reject the null hypothesis. The reason for this\nis that the null hypothesis is rarely true exactly, and with a large sample size\nit is highly likely that even a small deviation from the null hypothesis will\nbe detected. Statistically signi\ufb01cance must be distinguished from practical\nsigni\ufb01cance. The adjusted R2 values for the two- and three-variable models\nare very similar, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.1",
      "section_title": "\u00d7 10\u22128 ) leads us to reject the null hypothesis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.746 and 0.755, respectively. Therefore, the rejection of the\ntwo-variable model may not be of practical importance.                        \u0002\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.746",
      "section_title": "and 0.755, respectively. Therefore, the rejection of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6. Weekly interest rates\u2014Testing a two-predictor versus three-\npredictor model\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Weekly interest rates\u2014Testing a two-predictor versus three-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 Model Selection     231\n\n    In this example, the null hypothesis is that, in the three predictor model,\nthe slope ff dif is zero. The F -test is again computed using R\u2019s anova func-\ntion with output:\n   Analysis of Variance Table\n\n   Model 1: aaa_dif ~ cm10_dif + cm30_dif\n   Model 2: aaa_dif ~ cm10_dif + cm30_dif + ff_dif\n     Res.Df RSS Df Sum of Sq      F Pr(>F)\n   1    877 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Model Selection     231",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.66\n   2    876 3.66   1    0.0025 0.61   0.44\n\nThe large p-value (0.44) leads us to accept the null hypothesis. Notice that\nthis is the same as the p-value for ff_dif in the ANOVA table in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.66",
      "section_title": "2    876 3.66   1    0.0025 0.61   0.44",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.1.\nThis is not a coincidence. Both p-values are the same because they are testing\nthe same hypothesis.                                                        \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4.4 Adjusted R2\n\nR2 is biased in favor of large models, because R2 is always increased by adding\nmore predictors to the model, even if they are independent of the response.\nRecall that\n                      residual error SS     n\u22121 residual error SS\n           R2 = 1 \u2212                     =1\u2212                       .\n                           total SS              n\u22121 total SS\nThe bias in R2 can be reduced by using the following \u201cadjustment,\u201d which\nreplaces both occurrences of n by the appropriate degrees of freedom:\n\n                      (n \u2212 p \u2212 1)\u22121 residual error SS     residual error MS\nadjusted R2 = 1 \u2212                                     =1\u2212                   .\n                            (n \u2212 1)\u22121 total SS                 total MS\n\nThe presence of p in the adjusted R2 penalizes the criterion for the number\nof predictor variables, so adjusted R2 can either increase or decrease when\npredictor variables are added to the model. Adjusted R2 increases if the added\nvariables decrease the residual sum of squares enough to compensate for the\nincrease in p.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "4 Adjusted R2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 Model Selection\nWhen there are many potential predictor variables, often we wish to \ufb01nd\na subset of them that provide a parsimonious regression model. F -tests are\nnot very suitable for model selection. One problem is that there are many\npossible F -tests and the joint statistical behavior of all of them is not known.\nFor model selection, it is more appropriate to use a model selection criterion\nsuch as AIC or BIC. For linear regression models, AIC is\n\f232         9 Regression: Basics\n\n                                                              \u03c3 2 ) + 2(1 + p),\n                                                  AIC = n log(\u0002\nwhere 1 + p is the number of parameters in a model with p predictor variables;\nthe intercept gives us the \ufb01nal parameter. BIC replaces 2(1 + p) in AIC by\n                                    \u03c3 2 ), is equal to, up to an additive constant\nlog(n)(1 + p). The \ufb01rst term, n log(\u0002\nthat does not a\ufb00ect model comparisons, \u22122 times the log-likelihood evaluated\nat the MLE, assuming that the noise is Gaussian.\n    In addition to AIC and BIC, there are two model selection criteria special-\nized for regression. One is adjusted R2 , which we have seen before. Another is\nCp . Cp is related to AIC and usually Cp and AIC are minimized by the same\nmodel. The primary reason for using Cp instead of AIC is that some regres-\nsion software computes only Cp , not AIC\u2014this is true of the regsubsets()\nfunction in R\u2019s leaps package which will be used in the following example.\n    To de\ufb01ne Cp , suppose there are M predictor variables. Let \u03c3      \u00022,M be the\nestimate of \u03c3 2 using all of them, and let SSE(p) be the sum of squares for\nresidual error for a model with some subset of only p \u2264 M of the predictors.\nAs usual, n is the sample size. Then Cp is\n                                                       SSE(p)\n                                          Cp =                \u2212 n + 2(p + 1).                                                                                (9.18)\n                                                        \u00022,M\n                                                        \u03c3\nOf course, Cp will depend on which particular model is used among all of\nthose with p predictors, so the notation \u201cCp \u201d may not be ideal.\n    With Cp , AIC, and BIC, smaller values are better, but for adjusted R2 ,\nlarger values are better.\n    One should not use model selection criteria blindly. Model choice should be\nguided by economic theory and practical considerations, as well as by model\nselection criteria. It is important that the \ufb01nal model makes sense to the\nuser. Subject-matter expertise might lead to adoption of a model not optimal\naccording to the criterion being used but, instead, to a model slightly below\noptimal but more parsimonious or with a better economic rationale.\n                                                           10 15 20 25 30 35\n\n\n\n\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Model Selection",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.754\n            \u22121200\n\n\n\n\n                                                                                                             adjusted R2\n      BIC\n\n\n\n\n                                                      Cp\n            \u22121210\n\n\n\n\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.754",
      "section_title": "\u22121200",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.750\n            \u22121220\n\n\n\n\n                                                           5\n\n\n\n\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.750",
      "section_title": "\u22121220",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.746\n\n\n\n\n                    1           2             3                                1           2             3                         1           2             3\n                        number of variables                                        number of variables                                 number of variables\n\n       Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.746",
      "section_title": "1           2             3                                1           2             3                         1           2             3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5. Changes in weekly interest rates. Plots for model selection.\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Changes in weekly interest rates. Plots for model selection.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6 Collinearity and Variance In\ufb02ation   233\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Collinearity and Variance In\ufb02ation   233",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.7. Weekly interest rates\u2014Model selection by AIC and BIC\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "Weekly interest rates\u2014Model selection by AIC and BIC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 contains plots of the number of predictors in the model versus\nthe optimized value of a selection criterion. By \u201coptimized value,\u201d we mean\nthe best value among all models with the given number of predictor variables.\n\u201cBest\u201d means smallest for BIC and Cp and largest for adjusted R2 . There are\nthree plots, one for each of BIC, Cp , and adjusted R2 . All three criteria are\noptimized by two predictor variables.\n    There are three models with two of the three predictors. The one that\noptimized the criteria1 is the model with cm10 dif and cm30 dif, as can be\nseen in the following output from regsubsets. Here \"*\" indicates a variable\nin the model and \" \" indicates a variable not in the model, so the three rows\nof the table indicate that the best one-variable model is cm10 dif and the\nbest two-variable model is cm10 dif and cm30 dif\u2014the third row does not\ncontain any real information since, with only three variables, there is only one\npossible three-variable model.\n     Selection Algorithm: exhaustive\n             cm10_dif cm30_dif ff_dif\n    1 ( 1 ) \"*\"       \" \"      \" \"\n    2 ( 1 ) \"*\"       \"*\"      \" \"\n    3 ( 1 ) \"*\"       \"*\"      \"*\"\n\n                                                                              \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "contains plots of the number of predictors in the model versus",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6 Collinearity and Variance In\ufb02ation\n\nIf two or more predictor variables are highly correlated with one another,\nthen it is di\ufb03cult to estimate their separate e\ufb00ects on the response. For ex-\nample, cm10 dif and cm30 dif have a correlation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Collinearity and Variance In\ufb02ation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.96 and the scatterplot\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.96",
      "section_title": "and the scatterplot",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 shows that they are highly related to each other. If we regress\naaa dif on cm10 dif, then the adjusted R2 is 0.7460, but adjusted R2 only\nincreases to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "shows that they are highly related to each other. If we regress",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7556 if we add cm30 dif as a second predictor. This suggests\nthat cm30 dif might not be related to aaa dif, but this is not the case. In\nfact, the adjusted R2 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7556",
      "section_title": "if we add cm30 dif as a second predictor. This suggests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7376 when cm30 dif is the only predictor, which\nindicates that cm30 dif is a good predictor of aaa dif, nearly as good as\ncm10 dif.\n    Another e\ufb00ect of the high correlation between the predictor variables is\nthat the regression coe\ufb03cient for each variable is very sensitive to whether\nthe other variable is in the model. For example, the coe\ufb03cient of cm10 dif is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7376",
      "section_title": "when cm30 dif is the only predictor, which",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.616 when cm10 dif is the sole predictor variable but only 0.360 if cm30 dif\nis also included.\n\n1\n    When comparing models with the same number of parameters, all three criteria\n    are optimized by the same model.\n\f234      9 Regression: Basics\n\n    The problem here is that cm10 dif and cm30 dif provide redundant in-\nformation because of their high correlation. This problem is called collinearity\nor, in the case of more than two predictors, multicollinearity. Collinearity in-\ncreases standard errors. The standard error of the \u03b2 of cm10 dif is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.616",
      "section_title": "when cm10 dif is the sole predictor variable but only 0.360 if cm30 dif",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01212\nwhen only cm10 dif is in the model, but increases to 0.0451, a 372 % increase,\nif cm30 dif is added to the model.\n    The variance in\ufb02ation factor (VIF ) of a variable tells us how much the\n                                              \u0002 of that variable is increased by\nsquared standard error, i.e., the variance of \u03b2,\nhaving the other predictor variables in the model. For example, if a variable\nhas a VIF of 4, then the variance of its \u03b2\u0002 is four times larger than it would\nbe if the other predictors were either deleted or were not correlated with it.\nThe standard error is increased by a factor of 2.\n    Suppose we have predictor variables X1 , . . . , Xp . Then the VIF of Xj is\nfound by regressing Xj on the p \u2212 1 other predictors. Let Rj2 be the R2 -value\nof this regression, so that Rj2 measures how well Xj can be predicted from the\nother Xs. Then the VIF of Xj is\n\n                                            1\n                                VIFj =           .\n                                         1 \u2212 Rj2\n\nA value of Rj2 close to 1 implies a large VIF. In other words, the more ac-\ncurately that Xj can be predicted from the other Xs, the more redundant it\nis and the higher its VIF. The minimum value of VIFj is 1 and occurs when\nRj2 is 0. There is, unfortunately, no upper bound to VIFj . Variance in\ufb02ation\nbecomes in\ufb01nite as Rj2 approaches 1.\n    When interpreting VIFs, it is important to keep in mind that VIFj tells\nus nothing about the relationship between the response and jth predictor.\nRather, it tells us only how correlated the jth predictor is with the other\npredictors. In fact, the VIFs can be computed without knowing the values of\nthe response variable.\n    The usual remedy to collinearity is to reduce the number of predictor\nvariables by using one of the model selection criteria discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01212",
      "section_title": "when only cm10 dif is in the model, but increases to 0.0451, a 372 % increase,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5.\n\n\nExample 9.8. Variance in\ufb02ation factors for the weekly interest-rate example.\n\n   The function vif() in R\u2019s faraway library returned the following VIF\nvalues for the changes in weekly interest rates:\n\n      > library(faraway)\n      > options(digits = 2)\n      > vif(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))\n      cm10_dif cm30_dif   ff_dif\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Example 9.8. Variance in\ufb02ation factors for the weekly interest-rate example.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "library(faraway)\n      > options(digits = 2)\n      > vif(lm(aaa_dif ~ cm10_dif + cm30_dif + ff_dif))\n      cm10_dif cm30_dif   ff_dif",
        "start": 213,
        "end": 359
      }
    ]
  },
  {
    "content": "14.4     14.1      1.1\n\f                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "14.1      1.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6 Collinearity and Variance In\ufb02ation   235\n\ncm10 dif and cm30 dif have large VIFs due to their high correlation with\neach other. The predictor ff dif is not highly correlated with cm10 dif and\ncm30 dif and has a lower VIF.\n    VIF values give us information about linear relationships between the pre-\ndictor variables, but not about their relationships with the response. In this\nexample, ff dif has a small VIF value but is not an important predictor be-\ncause of its low correlation with the response. Despite their high VIF values,\ncm10 dif and cm30 dif are important predictors. The high VIF values tell us\nonly that the regression coe\ufb03cients for cm10 dif and cm30 dif are impossible\nto estimate with high precision.\n    The question is whether VIF values of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Collinearity and Variance In\ufb02ation   235",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4 and 14.1 are so large that the\nnumber of predictor variables should be reduced to 1, that is, whether we\nshould use only cm10 dif. The answer is \u201cperhaps not\u201d because the model\nwith both cm10 dif and cm30 dif minimizes BIC. BIC generally selects a\nparsimonious model because of the high penalty BIC places on the number of\npredictor variables. Therefore, a model that minimizes BIC is unlikely to need\nfurther deletion of predictor variables simply to reduce VIF values. However,\nwe saw earlier that adding cm30 dif to the model with cm10 dif o\ufb00ers only\na minor increase in adjusted R2 , so the issue of whether or not to include\ncm30 dif is not clear.                                                      \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "and 14.1 are so large that the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.9. Nelson\u2013Plosser macroeconomic variables\n\n    To illustrate model selection, we now turn to an example with more pre-\ndictors. We will start with six predictors but will \ufb01nd that a model with only\ntwo predictors \ufb01ts rather well.\n    This example uses a subset of the well-known Nelson\u2013Plosser data set of\nU.S. yearly macroeconomic time series. These data are available in the \ufb01le\nnelsonplosser.csv. The variables we will use are:\n 1. sp-Stock Prices, [Index; 1941-43 = 100], [1871\u20131970].\n 2. gnp.r-Real GNP, [Billions of 1958 Dollars], [1909\u20131970],\n 3. gnp.pc-Real Per Capita GNP, [1958 Dollars], [1909\u20131970],\n 4. ip-Industrial Production Index, [1967 = 100], [1860\u20131970],\n 5. cpi-Consumer Price Index, [1967 = 100], [1860\u20131970],\n 6. emp-Total Employment, [Thousands], [1890\u20131970],\n 7. bnd-Basic Yields 30-year Corporate Bonds, [% pa], [1900\u20131970].\n    Since two of the time series start in 1909, we use only the data from\n1909 until the end of the series in 1970, a total of 62 years. The response\nwill be the di\ufb00erences of log(sp), the log returns on the stock prices. The\nregressors will be the di\ufb00erences of variables 2 through 7, with variables\n4 and 5 log-transformed before di\ufb00erencing. A di\ufb00erenced log-series contains\nthe approximate relative changes in the original variable, in the same way\nthat a log return approximates a return that is the relative change in price.\n\f236                       9 Regression: Basics\n\n    How does one decide whether to di\ufb00erence the original series, the log-\ntransformed series, or some other function of the series? Usually the aim is\nto stabilize the \ufb02uctuations in the di\ufb00erenced series. The top row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.9",
      "section_title": "Nelson\u2013Plosser macroeconomic variables",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6\nhas time series plots of changes in gnp.r, log(gnp.r), and sqrt(gnp.r) and\nthe bottom row has similar plots for ip. For ip the \ufb02uctuations in the di\ufb00er-\nenced series increase steadily over time, but this is less true if one uses the\nsquare roots or logs of the series. This is the reason why diff(log(ip)) is\nused here as a regressor. For gnp.r, the \ufb02uctuations in changes are more sta-\nble and we used diff(gnp.r) rather than diff(log(gnp.r)) as a regressor.\nIn this analysis, we did not consider using square-root transformations, since\nchanges in the square roots are less interpretable than changes in the original\nvariable or its logarithm. However, the changes in the square roots of both\nseries are reasonably stable, so square-root transformations might be consid-\nered. Another possibility would be to use the transformation that gives the\nbest-\ufb01tting model. One could, for example, put all three variables, diff(ip),\ndiff(log(ip)), and diff(sqrt(ip)), into the model and use model selec-\ntion to decide which gives the best \ufb01t. The same could be done with gnp.r\nand the other regressors.\n    Notice that the variables are transformed \ufb01rst and then di\ufb00erenced. Dif-\nferencing \ufb01rst and then taking logarithms or square roots would result in\ncomplex-valued variables, which would be di\ufb03cult to interpret, to say the\nleast.\n\n                                 gnp.r                                                       log(gnp.r)                                              sqrt(gnp.r)\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "has time series plots of changes in gnp.r, log(gnp.r), and sqrt(gnp.r) and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n               40\n\n\n\n\n                                                                                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.5 1.0\n differences\n\n\n\n\n                                                                                                                  differences\n               20\n\n\n\n\n                                                          differences\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n               0\n\n\n\n\n                                                                        \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 \u22120.05\n               \u221240 \u221220\n\n\n\n\n                                                                                                                                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                         1910   1930    1950   1970                                   1910   1930   1950   1970                               1910    1930   1950   1970\n\n                                  year                                                         year                                                     year\n\n                                   ip                                                         log(ip)                                                 sqrt(ip)\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1910   1930    1950   1970                                   1910   1930   1950   1970                               1910    1930   1950   1970",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                0.4\n differences\n\n\n\n\n                                                      differences\n\n\n\n\n                                                                                                                  differences\n               5\n\n\n\n\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                                                0.0\n               0\n\n\n\n\n                                                                        \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                \u22120.4\n               \u22125\n\n\n\n\n                         1910   1930    1950   1970                                   1910   1930   1950   1970                               1910    1930   1950   1970\n\n                                  year                                                         year                                                     year\n\n                Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6. Di\ufb00erences in gnp.r and ip with and without transformations.\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Di\ufb00erences in gnp.r and ip with and without transformations.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6 Collinearity and Variance In\ufb02ation    237\n\n    There are additional variables in this data set that could be tried in the\nmodel. The analysis presented here is only an illustration and much more\nexploration is certainly possible with this rich data set.\n    Time series and normal plots of all eight di\ufb00erenced series did not reveal\nany outliers. The normal plots were only used to check for outliers, not to check\nfor normal distributions. There is no assumption in a regression analysis that\nthe regressors are normally distributed or that the response has a marginal\nnormal distribution. It is only the conditional distribution of the response\ngiven the regressors that is assumed to be normal, and even that assumption\ncan be weakened.\n    A linear regression with all of the regressors shows that only two, diff\n(log(ip)) and diff(bnd), are statistically signi\ufb01cant at the ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Collinearity and Variance In\ufb02ation    237",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 level and\nsome have very large p-values:\n   Call:\n   lm(formula = diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc)\n      + diff(log(ip)) + diff(log(cpi))\n      + diff(emp) + diff(bnd), data = new_np)\n\n   Coefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n   (Intercept)   -2.766e-02 3.135e-02 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "level and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.882     0.3815\n   diff(gnp.r)    8.384e-03 4.605e-03    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.882",
      "section_title": "0.3815",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.821   0.0742\n   diff(gnp.pc)  -9.752e-04 9.490e-04 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.821",
      "section_title": "0.0742",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.028     0.3087\n   diff(log(ip))  6.245e-01 2.996e-01    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.028",
      "section_title": "0.3087",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.085   0.0418\n   diff(log(cpi)) 4.935e-01 4.017e-01    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.085",
      "section_title": "0.0418",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.229   0.2246\n   diff(emp)     -9.591e-06 3.347e-05 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.229",
      "section_title": "0.2246",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.287     0.7756\n   diff(bnd)     -2.030e-01 7.394e-02 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.287",
      "section_title": "0.7756",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.745     0.0082\n\n\n   A likely problem here is multicollinearity, so variance in\ufb02ation factors were\ncomputed:\n\n       diff(gnp.r)    diff(gnp.pc) diff(log(ip)) diff(log(cpi))\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.745",
      "section_title": "0.0082",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.0            31.8           3.3            1.3\n         diff(emp)        diff(bnd)\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.0",
      "section_title": "31.8           3.3            1.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.9             1.5\n\n\n    We see that diff(gnp.r) and diff(gnp.pc) have high VIF values, which\nis not surprising since they are expected to be highly correlated. In fact, their\ncorrelation is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.9",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.96.\n    Next, we search for a more parsimonious model using stepAIC(), a vari-\nable selection procedure in R that starts with a user-speci\ufb01ed model and adds\nor deletes variables sequentially. At each step it either makes the addition or\ndeletion that most improves AIC. It this example, stepAIC() will start with\nall six predictors.\n\f238       9 Regression: Basics\n\n      Here is the \ufb01rst step:\n      Start: AIC=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.96",
      "section_title": "Next, we search for a more parsimonious model using stepAIC(), a vari-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "224.92\n      diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +\n          diff(log(cpi)) + diff(emp) + diff(bnd)\n\n                         Df Sum of Sq      RSS      AIC\n      - diff(emp)         1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "224.92",
      "section_title": "diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.002    1.216 -226.826\n      - diff(gnp.pc)      1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.002",
      "section_title": "1.216 -226.826",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.024    1.238 -225.737\n      - diff(log(cpi))    1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.024",
      "section_title": "1.238 -225.737",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.034    1.248 -225.237\n      <none>                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.034",
      "section_title": "1.248 -225.237",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "",
        "start": 35,
        "end": 65
      }
    ]
  },
  {
    "content": "1.214 -224.918\n      - diff(gnp.r)       1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.214",
      "section_title": "-224.918",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.075   1.289 -223.284\n      - diff(log(ip))     1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.075",
      "section_title": "1.289 -223.284",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.098   1.312 -222.196\n      - diff(bnd)         1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.098",
      "section_title": "1.312 -222.196",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.169   1.384 -218.949\n    The listed models have either zero or one variable removed from the start-\ning model with all regressors. The models are listed in order of their AIC val-\nues. The \ufb01rst model, which has diff(emp) removed (the minus sign indicates\na variable that has been removed), has the best (smallest) AIC. Therefore, in\nthe \ufb01rst step, diff(emp) is removed. Notice that the fourth-best model has\nno variables removed.\n    The second step starts with the model without diff(emp) and exam-\nines the e\ufb00ect on AIC of removing additional variables. The removal of\ndiff(log(cpi)) leads to the largest improvement in AIC, so in the second\nstep this variable is removed:\n      Step: AIC=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.169",
      "section_title": "1.384 -218.949",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "226.83\n      diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +\n          diff(log(cpi)) + diff(bnd)\n\n                         Df Sum of Sq      RSS      AIC\n      - diff(log(cpi))    1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "226.83",
      "section_title": "diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.032    1.248 -227.236\n      <none>                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.032",
      "section_title": "1.248 -227.236",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "",
        "start": 35,
        "end": 65
      }
    ]
  },
  {
    "content": "1.216 -226.826\n      - diff(gnp.pc)      1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.216",
      "section_title": "-226.826",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.057   1.273 -226.025\n      - diff(gnp.r)       1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.057",
      "section_title": "1.273 -226.025",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.084   1.301 -224.730\n      - diff(log(ip))     1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.084",
      "section_title": "1.301 -224.730",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.096   1.312 -224.179\n      - diff(bnd)         1      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.096",
      "section_title": "1.312 -224.179",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.189   1.405 -220.032\n\nOn the third step no variables are removed and the process stops:\n      Step: AIC=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.189",
      "section_title": "1.405 -220.032",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "227.24\n      diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +\n          diff(bnd)\n\n                        Df Sum of Sq       RSS      AIC\n      <none>                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "227.24",
      "section_title": "diff(log(sp)) ~ diff(gnp.r) + diff(gnp.pc) + diff(log(ip)) +",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "",
        "start": 162,
        "end": 192
      }
    ]
  },
  {
    "content": "1.248 -227.236\n      - diff(gnp.pc)     1       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.248",
      "section_title": "-227.236",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.047   1.295 -227.001\n      - diff(gnp.r)      1       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.047",
      "section_title": "1.295 -227.001",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.069   1.318 -225.942\n      - diff(log(ip))    1       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.069",
      "section_title": "1.318 -225.942",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.122   1.371 -223.534\n      - diff(bnd)        1       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.122",
      "section_title": "1.371 -223.534",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.157   1.405 -222.001\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.157",
      "section_title": "1.405 -222.001",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.6 Collinearity and Variance In\ufb02ation     239\n\n   Notice that the removal of diff(gnp.pc) would cause only a very small in-\ncrease in AIC. We should investigate whether this variable might be removed.\nThe new model was re\ufb01t to the data.\n\n   Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n   (Intercept)  -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Collinearity and Variance In\ufb02ation     239",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.018664   0.028723   -0.65    0.518\n   diff(gnp.r)   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.018664",
      "section_title": "0.028723   -0.65    0.518",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.007743   0.004393    1.76    0.083\n   diff(gnp.pc) -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.007743",
      "section_title": "0.004393    1.76    0.083",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001029   0.000712   -1.45    0.154\n   diff(log(ip)) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001029",
      "section_title": "0.000712   -1.45    0.154",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.672924   0.287276    2.34    0.023\n   diff(bnd)    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.672924",
      "section_title": "0.287276    2.34    0.023",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.177490   0.066840   -2.66    0.010\n\n   Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.177490",
      "section_title": "0.066840   -2.66    0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 on 56 degrees of freedom\n   Multiple R-squared: 0.347,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "on 56 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n   F-statistic: 7.44 on 4 and 56 DF, p-value: 7.06e-05\n\n    Now three of the four variables are statistically signi\ufb01cant at 0.1, though\ndiff(gnp.pc) has a rather large p-value, and it seems to be worth exploring\nother possible models.\n    The R function leaps() in the leaps package will compute Cp for all\npossible models. To reduce the amount of output, only the nbest models\nwith k regressors [for each k = 1, . . . , dim(\u03b2)] are printed. The value of nbest\nis selected by the user and in this analysis nbest was set at 1, so only the\nbest model is given for each value of k. The following table gives the value of\nCp (last column) for the best k-variable models, for k = 1, . . . , 6 (k is in the\n\ufb01rst column). The remaining columns indicate with a \u201c1\u201d which variables are\nin the models. All predictors have been di\ufb00erenced, but to save space \u201cdiff\u201d\nhas been omitted from the variable names heading the columns.\n\n       gnp.r gnp.pc log(ip) log(cpi) emp bnd Cp\n   1       0     0       1        0   0   0 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "F-statistic: 7.44 on 4 and 56 DF, p-value: 7.06e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3\n   2       0     0       1        0   0   1 3.8\n   3       1     0       1        0   0   1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.3",
      "section_title": "2       0     0       1        0   0   1 3.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.6\n   4       1     1       1        0   0   1 4.5\n   5       1     1       1        1   0   1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.6",
      "section_title": "4       1     1       1        0   0   1 4.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1\n   6       1     1       1        1   1   1 7.0\n\n   We see that stepAIC stopping at the four-variable model was perhaps pre-\nmature. The model selection process was stopped at the four-variable model\nbecause the three-variable model had a slightly larger Cp -value. However, if\none continues to the best two-variable model, the minimum of Cp is obtained.\nHere is the \ufb01t to the best two-variable model:\n\n   Call:\n   lm(formula = diff(log(sp)) ~ +diff(log(ip)) + diff(bnd),\n             data = new_np)\n\n   Residuals:\n\f240      9 Regression: Basics\n\n           Min       1Q         Median          3Q        Max\n      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.1",
      "section_title": "6       1     1       1        1   1   1 7.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.44254 -0.09786        0.00377     0.10525    0.28136\n\n      Coefficients:\n                         Estimate Std. Error t value Pr(>|t|)\n      (Intercept)          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.44254",
      "section_title": "-0.09786        0.00377     0.10525    0.28136",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0166     0.0210    0.79 0.43332\n      diff(log(ip))        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0166",
      "section_title": "0.0210    0.79 0.43332",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6975     0.1683    4.14 0.00011\n      diff(bnd)           -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6975",
      "section_title": "0.1683    4.14 0.00011",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1322     0.0623   -2.12 0.03792\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1322",
      "section_title": "0.0623   -2.12 0.03792",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 on 58 degrees of freedom\n      Multiple R-squared: 0.309,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "on 58 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.285\n      F-statistic: 12.9 on 2 and 58 DF, p-value: 2.24e-05\nBoth variables are signi\ufb01cant at ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.285",
      "section_title": "F-statistic: 12.9 on 2 and 58 DF, p-value: 2.24e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05. However, it is not crucial that all regres-\nsors be signi\ufb01cant at ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "However, it is not crucial that all regres-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 or at any other predetermined level. Other models\ncould be used, especially if there were good economic reasons for doing so. One\ncannot say that the two-variable model is best, except in the narrow sense of\nminimizing Cp , and choosing instead the best three- or four-predictor model\nwould not increase Cp by much. Also, which model is best depends on the\ncriterion used. The best four-predictor model has a better adjusted R2 than\nthe best two-predictor model.                                                   \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "or at any other predetermined level. Other models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.7 Partial Residual Plots\nA partial residual plot is used to visualize the e\ufb00ect of a predictor on the re-\nsponse while removing the e\ufb00ects of the other predictors. The partial residual\nfor the jth predictor variable is\n       \u239b                   \u239e              \u239b                  \u239e\n\n Yi \u2212 \u239d\u03b2\u00020 +                                   \u0017i \u2212 \u239d\u03b2\u00020 +\n                        Xi,j \u0002 \u03b2\u0002j \u0002 \u23a0 = Y\u0002i + \u0002                      Xi,j \u0002 \u03b2\u0002j \u0002 \u23a0 = Xi,j \u03b2\u0002j + \u0002\n                                                                                                  \u0017i ,\n               j \u0002 =j                                        j \u0002 =j\n                                                                          (9.19)\nwhere the \ufb01rst equality uses (9.12) and the second uses (9.10). Notice that the\nleft-hand side of (9.19) shows that the partial residual is the response with\nthe e\ufb00ects of all predictors but the jth subtracted o\ufb00. The right-hand side\nof (9.19) shows that the partial residual is also equal to the residual with the\ne\ufb00ect of the jth variable added back. The partial residual plot is simply the\nplot of the responses against these partial residuals.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "Partial Residual Plots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.10. Partial residual plots for the weekly interest-rate example\n\n\n    Partial residual plots for the weekly interest-rate example are shown in\nFig. 9.7a, b. For comparison, scatterplots of cm10 dif and cm30 dif versus\naaa dif with the corresponding one-variable \ufb01tted lines are shown in panels\n(c) and (d). The main conclusion from examining the plots is that the slopes\nin (a) and (b) are shallower than the slopes in (c) and (d). What does this tell\n\f                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.10",
      "section_title": "Partial residual plots for the weekly interest-rate example",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.7 Partial Residual Plots     241\n\nus? It says that, due to collinearity, the e\ufb00ect of cm10 dif on aaa dif when\ncm30 dif is in the model [panel (a)] is less than when cm30 dif is not in the\nmodel [panel (c)], and similarly when the roles of cm10 dif and cm30 dif are\nreversed.\n   The same conclusion can be reached by looking at the estimated regression\ncoe\ufb03cients. From Examples ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "Partial Residual Plots     241",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1 and 9.4, we can see that the coe\ufb03cient of\ncm10 dif is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "and 9.4, we can see that the coe\ufb03cient of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.615 when cm10 dif is the only variable in the model, but the\ncoe\ufb03cient drops to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.615",
      "section_title": "when cm10 dif is the only variable in the model, but the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.355 when cm30 dif is also in the model. There is a\nsimilar decrease in the coe\ufb03cient for cm30 dif when cm10 dif is added to\nthe model.                                                                 \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.355",
      "section_title": "when cm30 dif is also in the model. There is a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.11. Nelson\u2013Plosser macroeconomic variables\u2014Partial residual\nPlots\n      Component+Residual(aaa_dif)\n\n\n\n\n                                                                          Component+Residual(aaa_dif)\n\n\n                                    a                                                                   b\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.11",
      "section_title": "Nelson\u2013Plosser macroeconomic variables\u2014Partial residual",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                    0.0\n\n\n\n\n                                                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                    \u22120.4\n\n\n\n\n                                                                                                        \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                           \u22121.0   \u22120.5      0.0     0.5                                          \u22120.5      0.0     0.5\n                                                         cm10_dif                                                       cm30_dif\n\n\n                                    c                                                                   d\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22121.0   \u22120.5      0.0     0.5                                          \u22120.5      0.0     0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                        0.6\n      aaa_dif\n\n\n\n\n                                                                          aaa_dif\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                        0.0\n                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                        \u22120.6\n\n\n\n\n                                           \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22120.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0   \u22120.5      0.0     0.5                                          \u22120.5      0.0     0.5\n                                                         cm10_dif                                                       cm30_dif\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5      0.0     0.5                                          \u22120.5      0.0     0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.7. Partial residual plots for the weekly interest rates [panels (a) and (b)] and\nscatterplots of the predictors and the response [panels (c) and (d)].\n\n\n    This example continues the analysis of the Nelson\u2013Plosser macroeco-\nnomic variables. Partial residual plots for the four-variable model selected\nby stepAIC in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "Partial residual plots for the weekly interest rates [panels (a) and (b)] and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.9 are shown in Fig. 9.8. One can see that all four\nvariables have explanatory power, since the partial residuals have linear trends\nin the variables.\n    One puzzling aspect of this model is that the slope for gnp.pc is negative.\nHowever, the p-value for this regressor is large and the minimum Cp model\n\f242                  9 Regression: Basics\n\ndoes not contain either gnp.r or gnp.pc. Often, a regressor that is highly\ncorrelated with other regressors has an estimated slope that is counterintu-\nitive. If used alone, both gnp.r and gnp.pc have positive slopes. The slope of\ngnp.pc is negative only when gnp.r is in the model.                         \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.9",
      "section_title": "are shown in Fig. 9.8. One can see that all four",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.8 Centering the Predictors\nCentering or, more precisely, mean-centering a variable means expressing it\nas a deviation from its mean. Thus, if X1,k , . . . , Xn,k are the values of the kth\npredictor and X k is their mean, then (X1,k \u2212 X k ), . . . , (Xn,k \u2212 X k ) are values\nof the centered predictor.\n    Centering is useful for two reasons:\n\n\n                            a                                                                      b\n       Component+Residual\n\n\n\n\n                                                                              Component+Residual\n\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.8",
      "section_title": "Centering the Predictors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.6\n          (diff(log(sp)))\n\n\n\n\n                                                                                 (diff(log(sp)))\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                   \u22120.4\n                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                              l\n\n\n                                        \u221240     \u221220     0         20     40                                  \u2212300   \u2212100 0        100\n                                                  diff(gnp.r)                                                       diff(gnp.pc)\n\n\n\n\n                            c                                                                      d\n       Component+Residual\n\n\n\n\n                                                                              Component+Residual\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "l",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n          (diff(log(sp)))\n\n\n\n\n                                                                                 (diff(log(sp)))\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "(diff(log(sp)))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                            \u22120.6 \u22120.2\n\n\n\n\n                                                                                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.6 \u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                         \u22120.2          0.0      0.1    0.2                                   \u22120.5   0.0      0.5        1.0\n                                                  diff(log(ip))                                                       diff(bnd)\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2          0.0      0.1    0.2                                   \u22120.5   0.0      0.5        1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.8. Partial residual plots for the Nelson\u2013Plosser U.S. economic time series.\n(a) Change in gnp.r. (b) Change in gnp.pc. (c) Change in log(ip). (d) Change\nin bnd.\n\n\n\u2022     centering can reduce collinearity in polynomial regression;\n\u2022     if all predictors are centered, then \u03b20 is the expected value of Y when\n      each of the predictors is equal to its mean. This gives \u03b20 an interpretable\n      meaning. In contrast, if the variables are not centered, then \u03b20 is the\n      expected value of Y when all of the predictors are equal to 0. Frequently,\n      0 is outside the range of some predictors, making the interpretation of \u03b20\n      of little real interest unless the variables are centered.\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.8",
      "section_title": "Partial residual plots for the Nelson\u2013Plosser U.S. economic time series.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.11 R Lab     243\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.11",
      "section_title": "R Lab     243",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.9 Orthogonal Polynomials\nAs just mentioned, centering can reduce collinearity in polynomial regression\nbecause, for example, if X is positive, then X and X 2 will be highly correlated\nbut X \u2212 X and (X \u2212 X)2 will be less correlated.\n    Orthogonal polynomials can eliminate correlation entirely, since they are\nde\ufb01ned in a way so that they are uncorrelated. This is done using the Gram\u2013\nSchmidt orthogonalization procedure discussed in textbooks on linear algebra.\nOrthogonal polynomials can be created easily in most software packages, for\ninstance, by using the poly() function in R. Orthogonal polynomials are par-\nticularly useful for polynomial regression of degree higher than 2 where center-\ning is less successful at reducing collinearity. However, the use of polynomial\nmodels of degree 4 and higher is discouraged and nonparametric regression\n(see Chap. 21) is recommended instead. Even cubic regression can be prob-\nlematic because cubic polynomials have only a limited range of shapes.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.9",
      "section_title": "Orthogonal Polynomials",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.10 Bibliographic Notes\n\nHarrell (2001) , Ryan (1997), Neter et al. (1996) and Draper and Smith (1998)\nare four of the many good introductions to regression. Faraway (2005) is an\nexcellent modern treatment of linear regression with R. See Nelson and Plosser\n(1982) for information about their data set.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.10",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.11 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.11",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.11.1 U.S. Macroeconomic Variables\n\nThis section uses the data set USMacroG in R\u2019s AER package. This data set\ncontains quarterly times series on 12 U.S. macroeconomic variables for the\nperiod 1950\u20132000. We will use the variables consumption = real consump-\ntion expenditures, dpi = real disposable personal income, government = real\ngovernment expenditures, and unemp = unemployment rate. Our goal is to\npredict changes in consumption from changes in the other variables.\n    Run the following R code to load the data, di\ufb00erence the data (since we\nwish to work with changes in these variables), and create a scatterplot matrix.\n   library(AER)\n   data(\"USMacroG\")\n   MacroDiff = as.data.frame(apply(USMacroG, 2, diff))\n   attach(MacroDiff)\n   pairs(cbind(consumption, dpi, cpi, government, unemp))\n\f244      9 Regression: Basics\n\nProblem 1 Describe any interesting features, such as outliers, seen in the\nscatterplot matrix. Keep in mind that the goal is to predict changes in\nconsumption. Which variables seem best suited for that purpose? Do you think\nthere will be collinearity problems?\n\n\nNext, run the code below to \ufb01t a multiple linear regression model to consump-\ntion using the other four variables as predictors.\n      fitLm1 = lm(consumption ~ dpi + cpi + government + unemp)\n      summary(fitLm1)\n      confint(fitLm1)\n\nProblem 2 From the summary, which variables seem useful for predicting\nchanges in consumption?\n\n\nNext, print an ANOVA table.\n      anova(fitLm1)\n\nProblem 3 For the purpose of variable selection, does the ANOVA table pro-\nvide any useful information not already in the summary?\n\n\nUpon examination of the p-values, we might be tempted to drop several vari-\nables from the regression model, but we will not do that since variables should\nbe removed from a model one at a time. The reason is that, due to correlation\nbetween the predictors, when one is removed the signi\ufb01cance of the others\nchanges. To remove variables sequentially, we will use the function stepAIC()\nin the MASS package.\n      library(MASS)\n      fitLm2 = stepAIC(fitLm1)\n      summary(fitLm2)\n\nProblem 4 Which variables are removed from the model, and in what order?\n\n\n      Now compare the initial and \ufb01nal models by AIC.\n      AIC(fitLm1)\n      AIC(fitLm2)\n      AIC(fitLm1) - AIC(fitLm2)\n\nProblem 5 How much of an improvement in AIC was achieved by removing\nvariables? Was the improvement large? Is so, can you suggest why? If not,\nwhy not?\n\f                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.11",
      "section_title": "1 U.S. Macroeconomic Variables",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.12 Exercises    245\n\nThe function vif() in the car package will compute variance in\ufb02ation factors.\nA similar function with the same name is in the faraway package. Run\n   library(car)\n   vif(fitLm1)\n   vif(fitLm2)\n\nProblem 6 Was there much collinearity in the original four-variable model?\nWas the collinearity reduced much by dropping two variables?\n\n\n   Partial residual plots, which are also called component plus residual or cr\nplots, can be constructed using the function crPlot() in the car package.\nRun\n   par(mfrow = c(2, 2))\n   sp = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.12",
      "section_title": "Exercises    245",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n   crPlot(fitLm1, dpi, span = sp, col = \"black\")\n   crPlot(fitLm1, cpi, span = sp, col = \"black\")\n   crPlot(fitLm1, government, span = sp, col = \"black\")\n   crPlot(fitLm1, unemp, span = sp, col = \"black\")\n    Besides dashed least-squares lines, the partial residual plots have solid\nlowess smooths through them unless this feature is turned o\ufb00 by specifying\nsmooth=F, as was done in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "crPlot(fitLm1, dpi, span = sp, col = \"black\")",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.8. Lowess is an earlier version of loess. The\nsmoothness of the lowess curves is determined by the parameter span, with\nlarger values of span giving smoother plots. The default is span = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.8",
      "section_title": "Lowess is an earlier version of loess. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5. In the\ncode above, span is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "In the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8 but can be changed for all four plots by changing the\nvariable sp. Lowess, loess, and span are described in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "but can be changed for all four plots by changing the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2.1. A substantial\ndeviation of the lowess curve from the least-squares line is an indication that\nthe e\ufb00ect of the predictor is nonlinear. The default color of the crPlot \ufb01gure\nis red, but this can be changed as in the code above.\n\nProblem 7 What conclusions can you draw from the partial residual plots?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "1. A substantial",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.12 Exercises\n 1. Suppose that Yi = \u03b20 + \u03b21 Xi + \u0017i , where \u0017i is N (0, 0.3), \u03b20 = 1.4, and\n    \u03b21 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.12",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.7.\n    (a) What are the conditional mean and standard deviation of Yi given\n        that Xi = 1? What is P (Yi \u2264 3|Xi = 1)?\n    (b) A regression model is a model for the conditional distribution of Yi\n        given Xi . However, if we also have a model for the marginal distribu-\n        tion of Xi , then we can \ufb01nd the marginal distribution of Yi . Assume\n        that Xi is N (1, 0.7). What is the marginal distribution of Yi ? What\n        is P (Yi \u2264 3)?\n\f246      9 Regression: Basics\n\n2. Show that if \u00171 , . . . , \u0017n are i.i.d. N (0, \u03c3 2 ), then in straight-line regression\n   the least-squares estimates of \u03b20 and \u03b21 are also the maximum likelihood\n   estimates.\n   Hint: This problem is similar to the example in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.7",
      "section_title": "(a) What are the conditional mean and standard deviation of Yi given",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9. The only di\ufb00er-\n   ence is that in that section, Y1 , . . . , Yn are independent N (\u03bc, \u03c3 2 ), while in\n   this exercise Y1 , . . . , Yn are independent N (\u03b20 + \u03b21 Xi , \u03c3 2 ).\n3. Use (7.11), (9.3), and (9.2) to show that (9.8) holds.\n4. It was stated in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9",
      "section_title": "The only di\ufb00er-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.8 that centering reduces collinearity. As an illus-\n   tration, consider the example of quadratic polynomial regression where X\n   takes 30 equally spaced values between 1 and 15.\n   (a) What is the correlation between X and X 2 ? What are the VIFs of X\n       and X 2 ?\n   (b) Now suppose that we center X before squaring. What is the correlation\n       between (X \u2212 X) and (X \u2212 X)2 ? What are the VIFs of (X \u2212 X) and\n       (X \u2212 X)2 ?\n5. A linear regression model with three predictor variables was \ufb01t to a data\n   set with 40 observations. The correlation between Y and Y\u0002 was ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.8",
      "section_title": "that centering reduces collinearity. As an illus-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.65. The\n   total sum of squares was 100.\n   (a) What is the value of R2 ?\n   (b) What is the value of the residual error SS?\n   (c) What is the value of the regression SS?\n   (d) What is the value of s2 ?\n6. A data set has 66 observations and \ufb01ve predictor variables. Three models\n   are being considered. One has all \ufb01ve predictors and the others are smaller.\n   Below is residual error SS for all three models. The total SS was 48.\n   Compute Cp and R2 for all three models. Which model should be used\n   based on this information?\n                                   Number           Residual\n                                of predictors       error SS\n                                      3               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.65",
      "section_title": "The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2\n                                      4               10.1\n                                      5               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "4               10.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.0\n7. The quadratic polynomial regression model\n                             Yi = \u03b20 + \u03b21 Xi + \u03b22 Xi2 + \u0017i\n   was \ufb01t to data. The p-value for \u03b21 was ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.0",
      "section_title": "7. The quadratic polynomial regression model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.67 and for \u03b22 was 0.84. Can we\n   accept the hypothesis that \u03b21 and \u03b22 are both 0? Discuss.\n8. Sometimes it is believed that \u03b20 is 0 because we think that E(Y |X = 0) =\n   0. Then the appropriate model is\n                                     y i = \u03b21 Xi + \u0017 i .\n      This model is usually called \u201cregression through the origin\u201d since the\n      regression line is forced through the origin. The least-squares estimator of\n      \u03b21 minimizes\n\f                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.67",
      "section_title": "and for \u03b22 was 0.84. Can we",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.12 Exercises   247\n                                    n\n                                         {Yi \u2212 \u03b21 Xi }2 .\n                                   i=1\n\n    Find a formula that gives \u03b2\u03021 as a function of the Yi s and the Xi s.\n 9. Complete the following ANOVA table for the model Yi = \u03b20 + \u03b21 Xi,1 +\n    \u03b22 Xi,2 + \u0017i :\n        Source              df          SS         MS        F          P\n        Regression           ?           ?          ?        ?          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.12",
      "section_title": "Exercises   247",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n        Error                ?           5.66       ?\n        Total               15           ?\n\n                     R-sq = ?\n\n\n10. Pairs of random variables (Xi , Yi ) were observed. They were assumed to\n    follow a linear regression with E(Yi |Xi ) = \u03b81 +\u03b82 Xi but with t-distributed\n    noise, rather than the usual normally distributed noise. More speci\ufb01cally,\n    the assumed model was that conditionally, given Xi , Yi is t-distributed\n    with mean \u03b81 + \u03b82 Xi , standard deviation \u03b83 , and degrees of freedom\n    \u03b84 . Also, the pairs (X1 , Y1 ), . . . , (Xn , Yn ) are mutually independent. The\n    model could also be expressed as\n\n                                  Y i = \u03b8 1 + \u03b8 2 Xi + \u0017 i\n    where \u00171 , . . . , \u0017n are i.i.d. t with mean 0 and standard deviation \u03b83 and\n    degrees of freedom \u03b84 . The model was \ufb01t by maximum likelihood. The R\n    code and output are\n        #(Code to input x and y not shown)\n        library(fGarch)\n        start = c(lmfit$coef, sd(lmfit$resid), 4)\n        loglik = function(theta)\n        {\n        -sum(log(dstd(y, mean = theta[1] + theta[2] * x, sd = theta[3],\n           nu = theta[4])))\n        }\n        mle = optim(start, loglik, hessian = TRUE)\n        InvFishInfo = solve(mle$hessian)\n        mle$par\n        mle$value\n        mle$convergence\n        sqrt(diag(InvFishInfo))\n        qnorm(0.975)\n\n        > mle$par\n        [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "Error                ?           5.66       ?",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mle$par\n        [1]",
        "start": 1368,
        "end": 1390
      }
    ]
  },
  {
    "content": "0.511 1.042 0.152 4.133\n        > mle$value\n        [1] -188\n\f248      9 Regression: Basics\n\n         > mle$convergence\n         [1] 0\n         > sqrt(diag(InvFishInfo))\n         [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.511",
      "section_title": "1.042 0.152 4.133",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "mle$value\n        [1] -188\n\f248      9 Regression: Basics",
        "start": 32,
        "end": 93
      },
      {
        "language": "r",
        "code": "mle$convergence\n         [1] 0\n         > sqrt(diag(InvFishInfo))\n         [1]",
        "start": 102,
        "end": 183
      }
    ]
  },
  {
    "content": "0.00697 0.11522 0.01209 0.93492\n         >\n         > qnorm(.975)\n         [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00697",
      "section_title": "0.11522 0.01209 0.93492",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qnorm(.975)\n         [1]",
        "start": 52,
        "end": 79
      }
    ]
  },
  {
    "content": "1.96\n         >\n\n      (a) What is the MLE of the slope of Yi on Xi ?\n      (b) What is the standard error of the MLE of the degrees-of-freedom\n          parameter?\n      (c) Find a 95 % con\ufb01dence interval for the standard deviation of the noise.\n      (d) Did optim converge? Why or why not?\n\n\nReferences\nDraper, N. R. and Smith, H. (1998) Applied Regression Analysis, 3rd ed.,\n  Wiley, New York.\nFaraway, J. J. (2005) Linear Models with R, Chapman & Hall, Boca Raton,\n  FL.\nHarrell, F. E., Jr. (2001) Regression Modeling Strategies, Springer-Verlag,\n  New York.\nNelson C.R., and Plosser C.I. (1982) Trends and random walks in macroeco-\n  nomic time series. Journal of Monetary Economics, 10, 139\u2013162.\nNeter, J., Kutner, M. H., Nachtsheim, C. J., and Wasserman, W. (1996)\n  Applied Linear Statistical Models, 4th ed., Irwin, Chicago.\nRyan, T. P. (1997) Modern Regression Methods, Wiley, New York.\n\f10\nRegression: Troubleshooting\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.96",
      "section_title": ">",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1 Regression Diagnostics\n\nMany things can, and often do, go wrong when data are analyzed. There\nmay be data that were entered incorrectly, one might not be analyzing the\ndata set one thinks, the variables may have been mislabeled, and so forth.\nIn Example 10.5, presented shortly, one of the weekly time series of interest\nrates began with 371 weeks of zeros, indicating missing data. However, I was\nunaware of this problem when I \ufb01rst analyzed the data. The lesson here is\nthat I should have plotted each of the data series \ufb01rst before starting to\nanalyze them, but I hadn\u2019t. Fortunately, the diagnostics presented in this\nsection showed quickly that there was some type of serious problem, and then\nafter plotting each of the time series I easily discovered the nature of the\nproblem.\n    Besides problems with the data, the assumed model may not be a good\napproximation to reality. The usual estimation methods, such as least squares\nin regression, are highly nonrobust, which means that they are particularly\nsensitive to problems with the data or the model.\n    Experienced data analysts know that they should always look at the raw\ndata. Graphical analysis often reveals any problems that exist, especially\nthe types of gross errors that can seriously degrade the analysis. However,\nsome problems are only revealed by \ufb01tting a regression model and examining\nresiduals.\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Regression Diagnostics",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. High-leverage points and residual outliers\u2014Simulated data\nexample\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                          249\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 10\n\f250     10 Regression: Troubleshooting\n\n         a                                             b\n\n\n\n             60\n\n\n\n\n                                                           60\n             40\n\n\n\n\n                                                           40\n         y\n\n\n\n\n                                                       y\n             20\n\n\n\n\n                                                           20\n             0\n\n\n\n\n                                                           0\n                  0       10   20       30   40   50            0   10   20       30   40   50\n                                    x                                         x\n\n         c\n             60\n             40\n         y\n             20\n             0\n\n\n\n\n                      2        4        6    8    10\n                                    x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "High-leverage points and residual outliers\u2014Simulated data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. (a) Linear regression with a high-leverage point that is not a residual\noutlier (solid circle). (b) Linear regression with a high-leverage point that is a resid-\nual outlier (solid circle). (c) Linear regression with a low-leverage point that is a\nresidual outlier (solid circle). Least-squares \ufb01ts are shown as solid lines.\n\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "(a) Linear regression with a high-leverage point that is not a residual",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1 uses data simulated to illustrate some of the problems that can\narise in regression. There are 11 observations. The predictor variable takes on\nvalues 1, . . . , 10 and 50, and Y = 1 + X + \u0017, where \u0017 \u223c N (0, 1). The last\nobservation is clearly an extreme value in X. Such a point is said to have high\nleverage. However, a high-leverage point is not necessarily a problem, only a\npotential problem. In panel (a), the data have been recorded correctly so that\nY is linearly related to X and the extreme X-value is, in fact, helpful as it\nincreases the precision of the estimated slope. In panel (b), the value of Y\nfor the high-leverage point has been misrecorded as ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "uses data simulated to illustrate some of the problems that can",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.254 rather than 50.254.\nThis data point is called a residual outlier. As can be seen by comparing\nthe least-squares lines in (a) and (b), the high-leverage point has an extreme\nin\ufb02uence on the estimated slope. In panel (c), X has been misrecorded for the\nhigh-leverage point as ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.254",
      "section_title": "rather than 50.254.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 instead of 50. Thus, this point is no longer high-\nleverage, but now it is a residual outlier. Its e\ufb00ect now is to bias the estimated\nintercept.\n    One should also look at the residuals after the model has been \ufb01t, because\nthe residuals may indicate problems not visible in plots of the raw data. How-\never, there are several types of residuals and, as explained soon, one type,\ncalled the externally studentized residual or rstudent, is best for diagnosing\nproblems. Ordinary (or raw) residuals are not necessarily useful for diagnosing\nproblems. For example, in Fig. 10.1b, none of the raw residuals is large, not\neven the one associated with the residual outlier. The problem is that the raw\nresiduals are too sensitive to the outliers, particularly at high-leverage points,\nand problems can remain hidden when raw residuals are plotted.                  \u0002\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "instead of 50. Thus, this point is no longer high-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1 Regression Diagnostics     251\n\n  Three important tools will be discussed for diagnosing problems with the\nmodel or the data:\n\u2022   leverages;\n\u2022   externally studentized residuals; and\n\u2022   Cook\u2019s Distance (Cook\u2019s D), which quanti\ufb01es the overall in\ufb02uence of each\n    observation on the \ufb01tted values.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Regression Diagnostics     251",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.1 Leverages\n\nThe leverage of the ith observation, denoted by Hii , measures how much\nin\ufb02uence Yi has on its own \ufb01tted value Y\u0002i . We will not go into the algebraic\ndetails until Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "1 Leverages",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1. An important result in that section is that there are\nweights Hij depending on the values of the predictor variables but not on\nY1 , . . . , Yn such that\n                                           n\n                                   Y\u0002i =         Hij Yj .                          (10.1)\n                                           j=1\n\nIn particular, Hii is the weight of Yi in the determination of Y\u0002i . It is a potential\nproblem if Hii is large since then Y\u0002i is determined too much by Yi itself and\nnot enough by the other data. The result is that the residual \u0002            \u0017i = Yi \u2212 Y\u0002i\nwill                                                                               \u0002\n   \u221a be small and not a good estimate of \u0017i . Also, the standard error of Yi is\n\u03c3 Hii , so a high value of Hii means a \ufb01tted value with low accuracy.\n    The leverage value Hii is large when the predictor variables for the ith\ncase are atypical of those values in the data, for example, because one of the\npredictor variables for that case is extremely outlying. It can be shown by\nsome elegant algebra that the average of H11 , . . . , Hnn is (p + 1)/n, where\np + 1 is the number of parameters (one intercept and p slopes) and that\ntherefore 0 < Hii < 1. A value of Hii exceeding 2(p + 1)/n, that is, over twice\nthe average value, is generally considered to be too large and therefore a cause\nfor concern Belsley et al. (1980).\n    The square matrix with i, jth element equal to Hij is called the hat matrix\nsince by (10.1) it converts Yj , j = 1, . . . , n, to Y\u0002i . The Hii are sometimes called\nthe hat diagonals.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "An important result in that section is that there are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2. Leverages in Example 10.1\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Leverages in Example 10.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 plots the leverages for the three cases in Fig. 10.1. Because the\nleverages depend only on the X-values, the leverages are the same in panels\n(a) and (b). In both panels, the high-leverage point has a leverage equal to\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "plots the leverages for the three cases in Fig. 10.1. Because the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.960. In these examples, the rule-of-thumb cuto\ufb00 point for high leverage is\nonly 2(p + 1)/n = 2 \u2217 2/11 = 0.364, so ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.960",
      "section_title": "In these examples, the rule-of-thumb cuto\ufb00 point for high leverage is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.960 is a huge leverage and close to\nthe maximum possible value of 1. In panel (c), none of the leverages is greater\nthan ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.960",
      "section_title": "is a huge leverage and close to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.364.\n\f252                10 Regression: Troubleshooting\n\n       a                                                b\n\n\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.364",
      "section_title": "252                10 Regression: Troubleshooting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                   0.8\n        leverage\n\n\n\n\n                                                        leverage\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                   0.4\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                   0.0\n                          2    4    6      8    10                       2   4    6       8       10\n                                   Index                                         Index\n\n       c                                                d\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                   0.8\n        leverage\n\n\n\n\n                                                        leverage\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                   0.4\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                   0.0\n                          2    4    6      8    10                       2   4        6       8        10\n                                   Index                                              x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2. (a)\u2013(c) Leverages plotted again case number (index) for the data sets in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "(a)\u2013(c) Leverages plotted again case number (index) for the data sets in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. Panels (a) and (b) are identical because leverages do not depend on the\nresponse values. Panel (d) plots the leverages in (c) against Xi .\n\n\n      In the special case p = 1, there is a simple formula for the leverages:\n\n                                               1     (Xi \u2212 X)2\n                                     Hii =       + \u0017n              ,                                    (10.2)\n                                                    i=1 (Xi \u2212 X)\n                                               n                 2\n\n\nIt is easy to check that in this case, H11 + \u00b7 \u00b7 \u00b7 + Hnn = p + 1 = 2, so the\naverage of the hat diagonals is, indeed, (p + 1)/n. Formula (10.2) shows\nthat Hii \u2265 1/n, Hii is equal 1/n if and only if Xi = X, and Hii increases\nquadratically with the distance between Xi and X. This behavior can be seen\nin Fig. 10.2d.                                                            \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Panels (a) and (b) are identical because leverages do not depend on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.2 Residuals\n\nThe raw residual is \u0002\u0017i = Yi \u2212 Y\u0002i . Under ideal circumstances such as a reason-\nably large sample and no outliers or high-leverage points, the raw residuals\nare approximately N (0, \u03c3 2 ), so absolute values greater than 2\u0002  \u03c3 2 are outly-\n                         2\ning and greater than 3\u0002 \u03c3 are extremely outlying. However, circumstances are\noften not ideal. When residual outliers occur at high-leverage points, they\ncan so distort the least-squares \ufb01t that they are not seen to be outlying. The\nproblem in these cases is that \u0002 \u0017i is not close to \u0017i because of the bias in the\nleast-squares \ufb01t. The bias is due to residual outliers themselves. This problem\ncan be seen in Fig. 10.1b.           \u221a\n    The standard error of \u0002\u0017i is \u03c3\u0002 1 \u2212 Hii , so the raw residuals do not have\na constant variance, and those raw residuals with large leverages close to 1\n\f                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "2 Residuals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1 Regression Diagnostics   253\n\nare much less variable than the others. To \ufb01x the problem of nonconstant\nvariance, one can use the standardized residual, sometimes called the inter-\nnally studentized\n        \u221a           residual,1 which is \u0002\u0017i divided by its standard error, that is,\n\u0002    \u03c3 1 \u2212 Hii ).\n\u0017i /(\u0002\n     There is still another problem with standardized residuals. An extreme\nresidual outlier can in\ufb02ate \u03c3  \u0002 , causing the standardized residual for the out-\nlying point to appear too small. The solution is to rede\ufb01ne the ith studen-\ntized residual with an estimate of \u03c3 that does not use the ith data point.\nThus, the externally\n                \u221a       studentized residual, often called rstudent, is de\ufb01ned to\nbe \u0002     \u03c3 ,(\u2212i) 1 \u2212 Hii }, where \u03c3\n    \u0017i /{\u0002                          \u0002 ,(\u2212i) is the estimate of \u03c3 computed by \ufb01t-\nting the model to the data with the ith observation deleted.2 For diagnostics,\nrstudent is considered the best type of residual to plot and is the type of\nresidual used in this book.\n     Warning: The terms \u201cstandardized residual\u201d and \u201cstudentized residual\u201d\ndo not have the same de\ufb01nitions in all textbooks and software packages. The\nde\ufb01nitions used here agree with R\u2019s influence.measures() function. Other\nsoftware, such as, SAS uses di\ufb00erent de\ufb01nitions.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Regression Diagnostics   253",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3. Externally studentized and raw residuals in Example 10.1\n\n    The top row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "Externally studentized and raw residuals in Example 10.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3 shows the externally studentized residuals in each\nof the three cases of simulated data in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "shows the externally studentized residuals in each",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. Case #11 is correctly identi-\n\ufb01ed as a residual outlier in data sets (b) and (c) and also correctly identi\ufb01ed in\ndata set (a) as not being a residual outlier. The bottom row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Case #11 is correctly identi-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3 shows\nthe raw residuals, rather than the externally studentized residuals. It is not\napparent from the raw residuals that in data set (b), case #11 is a residual\noutlier. This shows the inappropriateness of raw residuals for the detection of\noutliers, especially when there are high-leverage points.                       \u0002\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "shows",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.3 Cook\u2019s Distance\n\nA high-leverage value or a large absolute externally studentized residual in-\ndicates only a potential problem with a data point. Neither tells how much\nin\ufb02uence the data point actually has on the estimates. For that informa-\ntion, we can use Cook\u2019s distance, often called Cook\u2019s D, which measures how\nmuch the \ufb01tted values change if the ith observation is deleted. We say that\nCook\u2019s D measures in\ufb02uence, and any case with a large Cook\u2019s D is called a\nhigh-in\ufb02uence case. Leverage and rstudent alone do not measure in\ufb02uence.\n                                                               \u0002 obtained with\n    Let Y\u0002j (\u2212i) be the jth \ufb01tted value using estimates of the \u03b2s\nthe ith observation deleted. Then Cook\u2019s D for the ith observation is\n1\n    Studentization means dividing a statistic by its standard error.\n2\n    The notation (\u2212i) signi\ufb01es the deletion of the ith observation.\n\f254                                10 Regression: Troubleshooting\n\n                                       Dataset (a)                                          Dataset (b)                                         Dataset (c)\n                       2\n\n\n\n\n                                                                                                                                       80\nstudentized residual\n\n\n\n\n                                                           studentized residual\n\n\n\n\n                                                                                                                studentized residual\n                                                                                  0\n                       1\n\n\n\n\n                                                                                                                                       60\n                                                                                  \u22125\n                       0\n\n\n\n\n                                                                                                                                       40\n                                                                                  \u221210\n                       \u22122 \u22121\n\n\n\n\n                                                                                                                                       20\n                                                                                  \u221215\n\n\n\n\n                                                                                                                                       0\n                                   2    4    6      8 10                                2    4    6      8 10                               2    4    6      8 10\n                                            Index                                                Index                                               Index\n\n\n                                       Dataset (a)                                          Dataset (b)                                         Dataset (c)\n\n\n\n\n                                                                                                                                       40\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "3 Cook\u2019s Distance",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                                  2\n\n\n\n\n                                                                                                                                       30\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\nresidual\n\n\n\n\n                                                           residual\n\n\n\n\n                                                                                                                residual\n\n                                                                                                                                       20\n                                                                                  0\n                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "residual",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 \u22120.5\n\n\n\n\n                                                                                                                                       10\n                                                                                  \u22122\n\n\n\n\n                                                                                                                                       0\n                                                                                  \u22124\n\n\n\n\n                                   2    4    6      8 10                                2    4    6      8 10                               2    4    6      8 10\n                                            Index                                                Index                                               Index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3. Top row: Externally studentized residuals for the data sets in Fig. 10.1;\ndata set (a) is the data set in panel (a) of Fig. 10.1, and so forth. Case #11 is an\noutlier in data sets (b) and (c) but not in data set (a). Bottom row: Raw residuals\nfor the same three data sets as in the top row. For data set (b), the raw residual\ndoes not reveal that case #11 is outlying.\n\n                                                                         \u0017n            \u0002    \u0002\n                                                                                  j=1 {Yj \u2212 Yj (\u2212i)}\n                                                                                                    2\n                                                                                                      .                                                      (10.3)\n                                                                                      (p + 1)s2\n\nThe numerator in (10.3) is the sum of squared changes in the \ufb01tted values\nwhen the ith observation is deleted. The denominator standardizes this sum\nby dividing by the number of estimated parameters and an estimate of \u03c3 2 .\n    One way to use Cook\u2019s D is to plot the values of Cook\u2019s D against case\nnumber and look for unusually large values. However, it can be di\ufb03cult to\ndecide which, if any, values of Cook\u2019s D are outlying. Of course, some Cook\u2019s D\nvalues will be larger than others, but are any so large as to be worrisome?\nTo answer this question, a half-normal plot of values of Cook\u2019s D, or perhaps\nof their square roots, can be useful. Neither Cook\u2019s D nor its square root is\nnormally distributed, so one does not check for linearity. Instead, one looks\nfor values that are \u201cdetached\u201d from the rest.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "Top row: Externally studentized residuals for the data sets in Fig. 10.1;",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4. Cook\u2019s D for simulated data in Example 10.1\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "Cook\u2019s D for simulated data in Example 10.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 Checking Model Assumptions        255\n\n    The three columns of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Checking Model Assumptions        255",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4 show the values of square roots of Cook\u2019s D\nfor the three simulated data examples in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "show the values of square roots of Cook\u2019s D",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. In the top row, the square\nroots of Cook\u2019s D values are plotted versus case number (index). The bottom\nrow contains half-normal plots of the square roots of the Cook\u2019s D values.\nIn all panels, case #11 has the largest Cook\u2019s D, indicating that one should\nexamine this case to see if there is a problem. In data set (a), case #11 is a\nhigh-leverage point and has high in\ufb02uence despite not being a residual outlier.\nIn data set (b), where case #11 is both a high-leverage point and a residual\noutlier, the value of Cook\u2019s D for this case is very large, larger than in data set\n(a). In data set(c), where case #11 has low leverage, all 11 Cook\u2019s D values\nare reasonably small, at least in comparison with data sets (a) and (b), but\ncase #11 is still somewhat outlying.                                             \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "In the top row, the square",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5. Weekly interest data with missing values recorded as zeros\n\n    It was mentioned earlier that there were missing values of cm30 at the be-\nginning of the data set that were coded as zeros. In fact, there were 371 weeks\nof missing data for cm30. I started to analyze the data without realizing this\nproblem. This created a huge outlying value of cm30 dif (the \ufb01rst di\ufb00erences)\nat observation number 372 when cm30 jumps from 0 to the \ufb01rst nonmissing\nvalue. Fortunately, plots of rstudent, leverages, and Cook\u2019s D all reveal a se-\nrious problem somewhere between the 300th and 400th observations, and by\nzooming into this range of case numbers the problem was located in case #372;\nsee Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "Weekly interest data with missing values recorded as zeros",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5. The nature of the problem is not evident from these plots, only\nits existence, so I plotted each of the series aaa, cm10, and cm30. After seeing\nthe initial zero values of the latter series, the problem was obvious. Please\nremember this lesson: ALWAYS look at the data. Another lesson is that it is\nbest to use nonnumeric values for missing values. For example, R uses \u201cNA\u201d\nfor \u201cnot available.\u201d                                                          \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "The nature of the problem is not evident from these plots, only",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 Checking Model Assumptions\n\nBecause the ith residual \u0002 \u0017i estimates the \u201cnoise\u201d \u0017i , the residuals can be\nused to check the assumptions behind regression. Residual analysis generally\nconsists of various plots of the residuals, each plot being designed to check\none or more of the regression assumptions. Regression software will output\nthe several types of residuals discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Checking Model Assumptions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1.2. Externally studentized\nresiduals (rstudent) are recommended, for reasons given in that section.\n    Problems to look for include\n 1. nonnormality of the errors,\n 2. nonconstant variance of the errors,\n\f256                          10 Regression: Troubleshooting\n\n                                      Dataset (a)                                                    Dataset (b)                                                     Dataset (c)\n                       10\n\n\n\n\n                                                                                                                                                     10\n                                                                                      10\nsquare root Cook's D\n\n\n\n\n                                                               square root Cook's D\n\n\n\n\n                                                                                                                              square root Cook's D\n                       8\n\n\n\n\n                                                                                      8\n\n\n\n\n                                                                                                                                                     8\n                       6\n\n\n\n\n                                                                                      6\n\n\n\n\n                                                                                                                                                     6\n                       4\n\n\n\n\n                                                                                      4\n\n\n\n\n                                                                                                                                                     4\n                       2\n\n\n\n\n                                                                                      2\n\n\n\n\n                                                                                                                                                     2\n                       0\n\n\n\n\n                                                                                      0\n\n\n\n\n                                                                                                                                                     0\n                                  2    4     6      8     10                                     2    4     6      8     10                                      2    4     6      8     10\n\n                                            Index                                                          Index                                                           Index\n\n\n                                      Dataset (a)                                                    Dataset (b)                                                     Dataset (c)\n                                                                                      10\n                       6\n\n\n\n\n                                                          11                                                             11                                                              11\n\n\n\n\n                                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "2. Externally studentized",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\nsquare root Cook's D\n\n\n\n\n                                                               square root Cook's D\n\n\n\n\n                                                                                                                              square root Cook's D\n                       5\n\n\n\n\n                                                                                      8\n                       4\n\n\n\n\n                                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "square root Cook's D",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                                      6\n                       3\n\n\n\n\n                                                                                      4\n\n\n\n\n                                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                       2\n\n\n\n\n                                                                                                                                                                                   10\n                                                                                      2\n                       1\n\n\n\n\n                                                    10                                                             1\n\n\n\n\n                                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                       0\n\n\n\n\n                                                                                      0\n\n\n\n\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0       0.5    1.0        1.5                                0.0       0.5    1.0        1.5                                 0.0       0.5    1.0        1.5\n                             Half\u2212normal quantiles                                          Half\u2212normal quantiles                                           Half\u2212normal quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5    1.0        1.5                                0.0       0.5    1.0        1.5                                 0.0       0.5    1.0        1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4. Top row: Square roots of Cook\u2019s D for the simulated data plotted against\ncase number. Bottom row: Half-normal plots of square roots of Cook\u2019s D. Data\nset (a) Case #11 has high leverage. It is not a residual outlier but has high in\ufb02uence\nnonetheless. Data set (b) Case #11 has high leverage and is a residual outlier. It\nhas higher in\ufb02uence (as measured by Cook\u2019s D) than in data set (a). Data set (c)\nCase #11 has low leverage but is a residual outlier. It has much lower in\ufb02uence than\nin data sets (a) and (b). Note: In the top row, the vertical scale is kept constant\nto emphasize di\ufb00erences among the three cases.\n\n\n\n    3. nonlinearity of the e\ufb00ects of the predictor variables on the response, and\n    4. correlation of the errors.\nThe \ufb01rst three problems are discussed below; correlation of the errors is dis-\ncussed later in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "Top row: Square roots of Cook\u2019s D for the simulated data plotted against",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.\n\n10.2.1 Nonnormality\n\nNonnormality of the errors (noise) can be detected by a normal probability\nplot, boxplot, and histogram of the residuals. Not all three are needed, but\nlooking at a normal plot is highly recommended. Moreover, inexperienced data\nanalysts have trouble with the interpretation of normal plots. Looking at side-\nby-side normal plots and histograms (or KDEs) is helpful when learning to\ninterpret normal probability plots.\n\f                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "10.2.1 Nonnormality",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 Checking Model Assumptions               257\n\n        a                                          b\n\n\n\n\n                                                              5\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Checking Model Assumptions               257",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n        Leverage\n\n\n\n\n                                                   rstudent\n                                                              0\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "Leverage",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                              \u221210\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u221210",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n                         0 200   600     1000                       0 200    600     1000\n                                 Index                                       Index\n\n\n        c                                          d\n                   600\n\n\n\n\n                                                              600\n        Cook's D\n\n\n\n\n                                                   Cook's D\n                   300\n\n\n\n\n                                                              300\n                   0\n\n\n\n\n                                                              0\n                         0 200   600     1000                       368 370 372 374 376 378\n                                 Index                                       Index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0 200   600     1000                       0 200    600     1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5. Weekly interest data. Regression of aaa dif on cm10 dif and cm30 dif.\nFull data set including the \ufb01rst 371 weeks of data where cm30 was missing and\nassigned a value of 0. This caused severe problems at case number 372, which are\ndetected by the leverages in (a), rstudent in (b), and Cook\u2019s D in (c). Panel (d)\nzooms in on the outlier case to identify the case number as 372.\n\n\n    The residuals often appear nonnormal because there is an excess of outliers\nrelative to the normal distribution. We have de\ufb01ned a value of rstudent to be\noutlying if its absolute value exceeds 2 and extremely outlying if it exceeds\n3. Of course, these cuto\ufb00s of 2 and 3 are arbitrary and only intended to give\nrough guidelines.\n    It is the presence of outliers, particularly extreme outliers, that is a concern\nwhen we have nonnormality. A de\ufb01ciency of outliers relative to the normal\ndistribution is less of a problem, if it is a problem at all. Sometimes outliers are\ndue to errors, such as mistakes in the entry of the data or, as in Example 10.5,\nmisinterpreting a zero as a true data value rather than the indicator of a\nmissing value. If possible, outliers due to mistakes should be corrected, of\ncourse. However, in \ufb01nancial time series, outliers are often \u201cgood observations\u201d\ndue, inter alia, to excess volatility in the markets on certain days.\n    Another possible reason for an excess of both positive and negative out-\nlying residuals is nonconstant residual variance, a problem that is explained\nshortly. Normal probability plots assume that all observations come from the\nsame distribution, in particular, that they have the same variance. The pur-\npose of that plot is to determine if the common distribution is normal or not.\nIf there is no common distribution, for example, because of nonconstant vari-\nance, then the normal plot is not readily interpretable. Therefore, one should\ncheck for a constant variance before making an extended e\ufb00ort to interpret a\nnormal plot.\n\f258    10 Regression: Troubleshooting\n\n    Outliers can be a problem because they have an unduly large in\ufb02uence\non the estimation results. As discussed in Sect. 4.6, a common solution to the\nproblem of outliers is transformation of the response. Data transformation can\nbe very e\ufb00ective at handling outliers, but it does not work in all situations.\nMoreover, transformations can induce outliers. For example, if a log transfor-\nmation is applied to positive data, values very close to 0 could be transformed\nto outlying negative values since log(x) \u2192 \u2212\u221e as x \u2193 0.\n    It is always wise to check whether outliers are due to erroneous data,\nfor example, typing errors or other mistakes in data collection and entry. Of\ncourse, erroneous data should be corrected if possible and otherwise removed.\nRemoval of outliers that are not known to be erroneous is dangerous and not\nrecommended as routine statistical practice. However, reanalyzing the data\nwith outliers removed is a sound practice. If the analysis changes drastically\nwhen the outliers are deleted, then one knows there is something about which\nto worry. On the other hand, if deletion of the outliers does not change the\nconclusions of the analysis, then there is less reason to be concerned with\nwhether the outliers were erroneous data.\n    A certain amount of nonnormality of the errors is not necessarily a prob-\nlem. Least-squares estimators are unbiased even without normality. Standard\nerrors for regression coe\ufb03cients are also correct and con\ufb01dence intervals are\nnearly correct because the least-squares estimators obey a central limit theo-\nrem\u2014they are nearly normally distributed even if the errors are not normally\ndistributed. Nonetheless, outliers caused by highly skewed or heavy-tailed er-\nror distributions can cause the least-squares estimator to be highly variable\nand therefore inaccurate. Transformations of Y are commonly used when the\nerrors have skewed distributions, especially when they also have a nonconstant\nvariance. A common solution to heavy-tailed error distributions is robust re-\ngression; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "Weekly interest data. Regression of aaa dif on cm10 dif and cm30 dif.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8.\n\n10.2.2 Nonconstant Variance\n\nNonconstant residual variance means that the conditional variance of the re-\nsponse given the predictor variables is not constant as assumed by standard\nregression models. Nonconstant variance is also called heteroskedasticity. Non-\nconstant variance can be detected by an absolute residual plot, that is, by\nplotting the absolute residuals against the predicted values (Y\u0002i ) and, perhaps,\nalso against the predictor variables. If the absolute residuals show a system-\natic trend, then this is an indication of nonconstant variance. Economic data\noften have the property that larger responses are more variable. A more tech-\nnical way of stating this is that the conditional variance of the response (given\nthe predictor variables) is an increasing function of the conditional mean of\nthe response. This type of behavior can be detected by plotting the absolute\nresiduals versus the predicted values and looking for an increasing trend.\n    Often, trends are di\ufb03cult to detect just by looking at the plotted points\nand adding a so-called scatterplot smoother is very helpful. A scatterplot\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "10.2.2 Nonconstant Variance",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 Checking Model Assumptions     259\n\nsmoother \ufb01ts a smooth curve to a scatterplot. Nonparametric regression es-\ntimators such as loess and smoothing splines are commonly used scatterplot\nsmoothers available in statistical software packages. These are discussed more\nfully in Chap. 21.\n    A potentially serious problem caused by nonconstant variance is ine\ufb03-\nciency, that is, too-variable estimates, if ordinary (that is, unweighted) least\nsquares is used. Weighted least squares estimates \u03b2 e\ufb03ciently by minimizing\n                            n\n                                                   \u0002 2.\n                                 wi {Yi \u2212 f (X i ; \u03b2)}                    (10.4)\n                           i=1\n\nHere wi an estimate of the inverse (that is, reciprocal) conditional variance\nof Yi given X i , so that the more variable observations are given less weight.\nEstimation of the conditional variance function to determine the wi s is dis-\ncussed in the more advanced textbooks mentioned in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Checking Model Assumptions     259",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3. Weighted\nleast-squares for regression with GARCH errors is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "Weighted",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.12.\n    Another serious problem caused by heteroskedasticity is that standard\nerrors and con\ufb01dence intervals assume a constant variance and can be seriously\nwrong if there is substantial nonconstant variance.\n    Transformation of the response is a common solution to the problem of\nnonconstant variance; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.12",
      "section_title": "Another serious problem caused by heteroskedasticity is that standard",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4. If the response can be transformed to\nconstant variance, then unweighted least-squares will be e\ufb03cient and standard\nerrors and con\ufb01dence intervals will be valid.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "If the response can be transformed to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2.3 Nonlinearity\n\nIf a plot of the residuals versus a predictor variable shows a systematic non-\nlinear trend, then this is an indication that the e\ufb00ect of that predictor on the\nresponse is nonlinear. Nonlinearity causes biased estimates and a model that\nmay predict poorly. Con\ufb01dence intervals, which assume unbiasedness, can be\nseriously in error if there is nonlinearity. The value 100(1 \u2212 \u03b1)% is called the\nnominal value of the coverage probability of a con\ufb01dence interval and is guar-\nanteed to be the actual coverage probability only if all modeling assumptions\nare met.\n    Response transformation, polynomial regression, and nonparametric re-\ngression (e.g., splines and loess\u2014see Chap. 21) are common solutions to the\nproblem of nonlinearity.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "3 Nonlinearity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.6. Detecting nonlinearity: A simulated data example\n\n   Data were simulated to illustrate some of the techniques for diagnosing\nproblems. In the example there are two predictor variables, X1 and X2 . The\nassumed model is multiple linear regression, Yi = \u03b20 + \u03b21 Xi,1 + \u03b22 Xi,2 + \u0017i .\n\f260                           10 Regression: Troubleshooting\n\n\n\n\n                               40\n\n\n\n\n                                                                                             40\n                               30\n\n\n\n\n                                                                                             30\n      y\n\n\n\n\n                                                                                 y\n                               20\n\n\n\n\n                                                                                             20\n                               10\n\n\n\n\n                                                                                             10\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.6",
      "section_title": "Detecting nonlinearity: A simulated data example",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.2   0.4        0.6       0.8   1.0                    0.65   0.75     0.85       0.95\n                                                      x1                                                         x2\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2   0.4        0.6       0.8   1.0                    0.65   0.75     0.85       0.95",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.6. Simulated data. Responses plotted against the two predictor variables.\n\n                                          Normal QQ Plot                                            Histogram of rstudent\n                               2\n\n\n\n\n                                                                                             20\n      Theoretical Quantiles\n                               1\n\n\n\n\n                                                                                             15\n                                                                                 Frequency\n                               0\n\n\n\n\n                                                                                             10\n                               \u22121\n\n\n\n\n                                                                                             5\n                               \u22122\n\n\n\n\n                                                                                             0\n\n\n\n\n                                     \u22121    0    1      2         3     4                            \u22121   0   1    2      3    4     5\n                                           Sample Quantiles                                                  rstudent\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.6",
      "section_title": "Simulated data. Responses plotted against the two predictor variables.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.7. Simulated data. Normal plot and histogram of the studentized residuals.\nRight skewness is evident and perhaps a square root or log transformation of Y would\nbe helpful.\n\n\n\n    Figure 10.6, which shows the responses plotted against each of the predic-\ntors, suggests that the errors are heteroskedastic because there is more vertical\nscatter on the right sides of the plots. Otherwise, it is not clear whether there\nare other problems with the data or the model. The point here is that plots\nof the raw data often fail to reveal all problems. Rather, it is plots of the\nresiduals that can more reliably detect heteroskedasticity, nonnormality, and\nother di\ufb03culties.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.7",
      "section_title": "Simulated data. Normal plot and histogram of the studentized residuals.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.7 contains a normal plot and a histogram of the residuals\u2014the\nexternally standardized residuals (rstudents) are used in all examples of this\nchapter. Notice the right skewness which suggests that a response transforma-\ntion to remove right skewness, such as, a square-root or log transformation,\nshould be investigated.\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.7",
      "section_title": "contains a normal plot and a histogram of the residuals\u2014the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 Checking Model Assumptions                               261\n\n    Figure 10.8a is a plot of the residuals versus X1 . The residuals appear to\nhave a nonlinear trend. This is better revealed by adding a loess curve to the\nresiduals. The curvature of the loess \ufb01t is evident and indicates that Y is not\nlinear in X1 . A possible remedy is to add X12 as a third predictor. Figure 10.8a,\na plot of the residuals against X2 , shows somewhat random scatter, indicating\nthat Y appears to be linear in X2 . The concentration of the X2 -values near\nthe right side is not a problem. This pattern only shows that the distribution\nof X2 is left-skewed, but the regression model makes no assumptions about\nthe distributions of the predictors.\n    Before doing any more plotting, the model was augmented by adding X12\nas a predictor, so the model is now\n                                                      2\n                              Yi = \u03b20 + \u03b21 Xi,1 + \u03b22 Xi,2 + \u03b23 Xi,2 + \u0017i .                                               (10.5)\n\nFigure 10.8c is a plot of the absolute residuals versus the predicted values\nfor model (10.5). Note that the absolute residuals are largest where the \ufb01tted\nvalues are also largest, which is a clear sign of heteroskedasticity. A loess\nsmooth has been added to make the heteroskedasticity clearer.\n    To remedy the problem of heteroskedasticity, Yi was transformed to\nlog(Yi ), so the model is now\n                                                          2\n                            log(Yi ) = \u03b20 + \u03b21 Xi,1 + \u03b22 Xi,2 + \u03b23 Xi,2 + \u0017i .                                           (10.6)\n\n\n\n   Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "Checking Model Assumptions                               261",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.9 shows residual plots for model (10.6). The plots in panels (a)\nand (b) of residuals versus X1 and X2 show no patterns, indicating that the\n\n\n      a                                  b                                     c\n                                                                                               4\n                                                                               abs(rstudent)\n                                                                                               3\n                 3\n\n\n\n\n                                                     3\n      rstudent\n\n\n\n\n                                          rstudent\n\n\n\n\n                                                                                               2\n                 1\n\n\n\n\n                                                     1\n\n\n\n\n                                                                                               1\n                 \u22121\n\n\n\n\n                                                     \u22121\n\n\n\n\n                                                                                               0\n\n\n\n\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.9",
      "section_title": "shows residual plots for model (10.6). The plots in panels (a)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.4    0.8                    0.65   0.80   0.95                       10    20         30\n                              x1                                  x1                                fitted values\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4    0.8                    0.65   0.80   0.95                       10    20         30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.8. Simulated data. (a) Plot of externally studentized residuals versus X1 .\nThis plot suggests that Y is not linearly related to X1 and perhaps a model quadratic\nin X1 is needed. (b) Plot of the residuals versus X2 with a loess smooth. This plot\nsuggests that Y is linearly related to X2 so that the component of the model relating\nY to X2 is satisfactory. (c) Plot of the absolute residuals versus the predicted values\nusing a model that is quadratic in X1 . This plot reveals heteroskedasticity. A loess\nsmooth has been added to each plot.\n\f262                   10 Regression: Troubleshooting\n\n      a                                                                     b\n\n\n                       3\n\n\n\n\n                                                                                                    3\n                       2\n\n\n\n\n                                                                                                    2\n      rstudent\n\n\n\n\n                                                                            rstudent\n                       1\n\n\n\n\n                                                                                                    1\n                       \u22121\n\n\n\n\n                                                                                                    \u22121\n                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.8",
      "section_title": "Simulated data. (a) Plot of externally studentized residuals versus X1 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0        0.2    0.4        0.6   0.8   1.0                                0.65        0.75    0.85       0.95\n                                                     x1                                                                     x2\n\n\n      c                                                                     d                                         normal plot\n\n\n\n\n                                                                            Theoretical Quantiles\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2    0.4        0.6   0.8   1.0                                0.65        0.75    0.85       0.95",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                                                    2\n      abs(rstudent)\n\n\n\n\n                                                                                                    1\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                                                    0\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                    \u22122\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                   10         15      20        25    30                                        \u22121      0    1      2      3\n                                              fitted values                                                          Sample Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "10         15      20        25    30                                        \u22121      0    1      2      3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.9. Simulated data. Residual plots for \ufb01t of log(Y ) to X1 , X12 , and X2 .\n(a) Residuals versus X1 . (b) Residuals versus X2 . (c) Residuals versus Y\u0002 .\n\n\n\nmodel that is quadratic in X1 \ufb01ts well. The plot in panel (c) of absolute\nresiduals versus \ufb01tted values shows less heteroskedasticity than before, which\nshows the bene\ufb01t of the log transformation. The normal plot of the residuals\nshown in panel (d) shows much less skewness than earlier, which is another\nbene\ufb01t of the log transformation.                                           \u0002\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.9",
      "section_title": "Simulated data. Residual plots for \ufb01t of log(Y ) to X1 , X12 , and X2 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.3 Bibliographic Notes\n\nGraphical methods for detecting nonconstant variance, transform-both-sides\nregression, and weighting are discussed in Carroll and Ruppert (1988). The\nidea of using half-normal plots to detect usual values of Cook\u2019s D was borrowed\nfrom Faraway (2005).\n    Comprehensive treatments of regression diagnostics can be found in Bels-\nley et al. (1980) and in Cook and Weisberg (1982). Although variance in\ufb02ation\nfactors detect collinearity, they do not indicate what correlations are causing\nthe problem. For this purpose, one should use collinearity diagnostics. These\nare also discussed in Belsley et al. (1980).\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.3",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4 R Lab     263\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "R Lab     263",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4.1 Current Population Survey Data\n\nThis section uses the CPS1988 data set from the March 1988 Current Popu-\nlation Survey by the U.S. Census Bureau and available in the AER package.\nThese are cross-sectional data, meaning that the U.S. population was surveyed\nat a single time point. Cross-sectional data should be distinguished from lon-\ngitudinal data where individuals are followed over time. Data collected and\nanalyzed along two dimensions, that is, cross-sectionally and longitudinally,\nare called panel data by econometricians.\n    In this section, we will investigate how the variable wage (in dollars/week)\ndepends on education (in years), experience (years of potential work expe-\nrience), and ethnicity (Caucasian = \u201ccaus\u201d or African-American = \u201cafam\u201d).\nPotential experience was (age \u2212 education \u2212 6), the number of years of po-\ntential work experience assuming that education begins at age 6. Potential\nexperience was used as a proxy for actual work experience, which was not\navailable. The variable ethnicity is coded 0\u20131 for \u201ccauc\u201d and \u201cafam,\u201d so its\nregression coe\ufb03cient is the di\ufb00erence in the expected values of wage between\nan African-American and a Caucasian with the same values of education and\nexperience. Run the code below to load the data and run a multiple linear\nregression.\n   library(AER)\n   data(CPS1988)\n   attach(CPS1988)\n   fitLm1 = lm(wage ~ education + experience + ethnicity)\n\n    Next, create residual plots with the following code. In some of these plots,\nthe y-axis limits are set so as to eliminate outliers. This was done to focus\nattention on the bulk of the data. This is a very large data set with 28,155\nobservations, so scatterplots are very dense with data and almost solid black\nin places. Therefore, lowess smooths were added as thick, red lines so that they\ncan be seen clearly. Also, thick blue reference lines were added as appropriate.\n   par(mfrow = c(3, 2))\n   resid1 = rstudent(fitLm1)\n   plot(fitLm1$fit, resid1,\n     ylim = c(-1500, 1500), main = \"(a)\")\n   lines(lowess(fitLm1$fit, resid1, f = 0.2), lwd = 5, col = \"red\")\n   abline(h = 0, col = \"blue\", lwd = 5)\n\n   plot(fitLm1$fit, abs(resid1),\n     ylim = c(0, 1500), main = \"(b)\")\n   lines(lowess(fitLm1$fit, abs(resid1), f = 0.2),\n      lwd = 5, col = \"red\")\n   abline(h = mean(abs(resid1)), col = \"blue\", lwd = 5)\n\f264      10 Regression: Troubleshooting\n\n      qqnorm(resid1, datax = FALSE, main = \"(c)\")\n      qqline(resid1, datax = FALSE, lwd = 5, col = \"blue\")\n\n      plot(education, resid1, ylim = c(-1000, 1500), main = \"(d)\")\n      lines(lowess(education, resid1), lwd = 5, col = \"red\")\n      abline(h = 0, col = \"blue\", lwd = 5)\n\n      plot(experience, resid1, ylim = c(-1000, 1500), main = \"(e)\")\n      lines(lowess(experience, resid1), lwd = 5, col = \"red\")\n      abline(h = 0, col = \"blue\", lwd = 5)\n\nProblem 1 For each of the panels (a)\u2013(e) in the \ufb01gure you have just created,\ndescribe what is being plotted and any conclusions that should be drawn from\nthe plot. Describe any problems and discuss how they might be remedied.\n\n\nProblem 2 Now \ufb01t a new model where the log of wage is regressed on\neducation and experience. Create residual plots as done above for the \ufb01rst\nmodel. Describe di\ufb00erences between the residual plots for the two models. What\ndo you suggest should be tried next?\n\n\nProblem 3 Implement whatever you suggested to try next in Problem 2. De-\nscribe how well it worked. Are you satis\ufb01ed with your model? If not, try further\nenhancements of the model until arriving at a model that you feel is satisfac-\ntory. What is your \ufb01nal model?\n\n\nProblem 4 Use your \ufb01nal model to describe the e\ufb00ects of education, exper-\nience, and ethnicity on the wage. Use graphs where appropriate.\n\n\nCheck the data and your \ufb01nal model for possible problems or unusual features\nby examining the hat diagonals and Cook\u2019s D with the following code. Replace\nfitLm4 by the name of the lm object for your \ufb01nal model.\n\n      library(faraway) # required for halfnorm\n      par(mfrow=c(1, 3))\n      plot(hatvalues(fitLm4))\n      plot(sqrt(cooks.distance(fitLm4)))\n      halfnorm(sqrt(cooks.distance(fitLm4)))\n\nProblem 5 Do you see any high-leverage points or points with very high val-\nues of Cook\u2019s D? If you do, what is unusual about them?\n\f                                                                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "1 Current Population Survey Data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5 Exercises                    265\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "Exercises                    265",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5 Exercises\n 1. Residual plots and other diagnostics are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.10 for a\n    regression of Y on X. Describe any problems that you see and possible\n    remedies.\n\n a                                           b                                                           c\n         3\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.10",
      "section_title": "for a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                                                         theoretical quantiles\n                                                                                                                                 2\n         2\n\n\n\n\n                                                                                                                                 1\n                                             abs(resid)\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "theoretical quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n resid\n         1\n\n\n\n\n                                                                                                                                 0\n         0\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "resid",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                                                 \u22122 \u22121\n         \u22122\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22122 \u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                    \u22124       0 2 4 6 8                                             \u22125   0     5     10                                    \u22122        0    1   2     3\n                              x                                                             yhat                                               sample quantiles\n\n\n d                                           e                                                           f\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22124       0 2 4 6 8                                             \u22125   0     5     10                                    \u22122        0    1   2     3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                                                 0.6\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                                                                                       1\n                                             sqrt(cooks.distance(fit))\n\n\n\n\n                                                                                                                                                                 100\n                                                                                                         sqrt(CookD)\n                                                                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                         0.4\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n ACF\n\n\n\n\n                                                                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                         0.2\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n         \u22120.2\n\n\n\n\n                                                                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                                                         0.0\n\n\n\n\n                0        5    10   15   20                                     0   20 40 60 80                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0       1.0       2.0\n                             lag                                                            index                                         Half\u2212normal quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1.0       2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.10. Residual plots and diagnostics for regression of Y on X in Problem 1.\nThe residuals are rstudent values. (a) Plot of residuals versus x. (b) Plot of abso-\nlute residuals versus \ufb01tted values. (c) Normal QQ plot of residuals. (d) ACF plot\nof residuals. (e) Plot of the square root of Cook\u2019s D versus index (= observation\nnumber). (f ) Half-normal plot of square root of Cook\u2019s D.\n\f266              10 Regression: Troubleshooting\n\n 2. Residual plots and other diagnostics are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.10",
      "section_title": "Residual plots and diagnostics for regression of Y on X in Problem 1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.11 for a regres-\n    sion of Y on X. Describe any problems that you see and possible remedies.\n\n\n      a                                               b                                                         c\n\n\n\n\n                                                                                  6\n              2\n\n\n\n\n                                                                                                                theoretical quantiles\n                                                                                                                                        2\n                                                                                  5\n              0\n\n\n\n\n                                                                                                                                        1\n                                                                                  4\n                                                      abs(resid)\n      resid\n\n\n\n\n                                                                                  3\n              \u22122\n\n\n\n\n                                                                                                                                        0\n                                                                                  2\n\n\n\n\n                                                                                                                                        \u22122 \u22121\n              \u22124\n\n\n\n\n                                                                                  1\n              \u22126\n\n\n\n\n                                                                                  0\n                     \u22125   0   5       10    15                                          \u22125   0   5   10 15 20                                   \u22126     \u22124     \u22122   0         2\n                                  x                                                              yhat                                                 sample quantiles\n\n\n\n      d                                               e                                                         f\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.11",
      "section_title": "for a regres-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                                                                                         58\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "58",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                                                                                        2.0\n                                                      sqrt(cooks.distance(fit))\n\n\n\n\n                                                                                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                                                                  1.5\n\n\n\n\n                                                                                                                sqrt(CookD)\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      ACF\n\n\n\n\n                                                                                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                                                                  1.0\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                        0.5\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                                                                                                                         9\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                        0.0\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                     0    5   10       15        20                                     0    20 40 60 80                                        0.0         1.0    2.0\n                              lag                                                                index                                           Half\u2212normal quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0    5   10       15        20                                     0    20 40 60 80                                        0.0         1.0    2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.11. Residual plots and diagnostics for regression of Y on X in Problem 2.\nThe residuals are rstudent values. (a) Plot of residual versus x. (b) Plot of abso-\nlute residuals versus \ufb01tted values. (c) Normal QQ plot of residuals. (d) ACF plot\nof residuals. (e) Plot of the square root of Cook\u2019s D versus index (= observation\nnumber). (f ) Half-normal plot of square root of Cook\u2019s D.\n\f                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.11",
      "section_title": "Residual plots and diagnostics for regression of Y on X in Problem 2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.5 Exercises                   267\n\n 3. Residual plots and other diagnostics are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.5",
      "section_title": "Exercises                   267",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.12 for a regres-\n    sion of Y on X. Describe any problems that you see and possible remedies.\n\n\n  a                                           b                                                            c\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.12",
      "section_title": "for a regres-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n          2\n\n\n\n\n                                                                                                           theoretical quantiles\n                                                                                                                                   2\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n          1\n\n\n\n\n                                                                                                                                   1\n                                              abs(resid)\n  resid\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n          0\n\n\n\n\n                                                                                                                                   0\n                                                                                                                                   \u22122 \u22121\n          \u22121\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n          \u22122\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                     \u22124       0 2 4 6 8                                              \u22125   0       5   10                                   \u22122     \u22121       0   1      2\n                               x                                                          yhat                                                   sample quantiles\n\n\n  d                                           e                                                            f\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22124       0 2 4 6 8                                              \u22125   0       5   10                                   \u22122     \u22121       0   1      2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.30\n\n\n\n\n                                                                                                                                   0.30\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.30",
      "section_title": "0.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                                                                                     23\n                                              sqrt(cooks.distance(fit))\n\n\n\n\n                                                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "23",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20\n                                                                          0.20\n\n\n\n\n                                                                                                           sqrt(CookD)\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.20",
      "section_title": "0.20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n  ACF\n\n\n\n\n                                                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n                                                                          0.10\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                                   0.00\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n          \u22120.2\n\n\n\n\n                 0        5    10   15   20                                      0    20 40 60 80                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0       1.0       2.0\n                              lag                                                         index                                             Half\u2212normal quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1.0       2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.12. Residual plots and diagnostics for regression of Y on X in Problem 3.\nThe residuals are rstudent values. (a) Plot of residual versus x. (b) Plot of abso-\nlute residuals versus \ufb01tted values. (c) Normal QQ plot of residuals. (d) ACF plot\nof residuals. (e) Plot of the square root of Cook\u2019s D versus index (= observation\nnumber). (f ) Half-normal plot of square root of Cook\u2019s D.\n\f268                     10 Regression: Troubleshooting\n\n 4. Residual plots and other diagnostics are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.12",
      "section_title": "Residual plots and diagnostics for regression of Y on X in Problem 3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.13 for a\n    regression of Y on X. Describe any problems that you see and possible\n    remedies.\n\n  a                                                   b                                                               c\n          0 1 2 3 4 5\n\n\n\n\n                                                                                  5\n\n\n\n\n                                                                                                                      theoretical quantiles\n                                                                                                                                              2\n                                                                                  4\n\n\n\n\n                                                                                                                                              1\n                                                      abs(resid)\n  resid\n\n\n\n\n                                                                                  3\n\n\n\n\n                                                                                                                                              0\n                                                                                  2\n\n\n\n\n                                                                                                                                              \u22122 \u22121\n                                                                                  1\n          \u22122\n\n\n\n\n                                                                                  0\n\n                             \u22124       0 2 4 6 8                                             0     1     2     3   4                                   \u22122      0         2    4\n                                       x                                                              yhat                                                  sample quantiles\n\n\n  d                                                   e                                                               f\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.13",
      "section_title": "for a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                                                                                                89\n                                                      sqrt(cooks.distance(fit))\n\n\n\n\n                                                                                                                                                                              95\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "89",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                                                                              0.4\n                                                                                                                      sqrt(CookD)\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n  ACF\n\n\n\n\n                                                                                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                                  0.2\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n          \u22120.2\n\n\n\n\n                                                                                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                                                                  0.0\n\n\n\n\n                         0        5    10   15   20                                     0       20 40 60 80                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0         1.0       2.0\n                                      lag                                                             index                                            Half\u2212normal quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1.0       2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.13. Residual plots and diagnostics for regression of Y on X in Problem 4.\nThe residuals are rstudent values. (a) Plot of residual versus x. (b) Plot of abso-\nlute residuals versus \ufb01tted values. (c) Normal QQ plot of residuals. (d) ACF plot\nof residuals. (e) Plot of the square root of Cook\u2019s D versus index (= observation\nnumber). (f ) Half-normal plot of square root of Cook\u2019s D.\n\n\n 5. It was noticed that a certain observation had a large leverage (hat diago-\n    nal) but a small Cook\u2019s D. How could this happen?\n\n\nReferences\nBelsley, D. A., Kuh, E., and Welsch, R. E. (1980) Regression Diagnostics,\n  Wiley, New York.\nCarroll, R. J., and Ruppert, D. (1988) Transformation and Weighting in Re-\n  gression, Chapman & Hall, New York.\nCook, R. D., and Weisberg, S. (1982) Residuals and In\ufb02uence in Regression,\n  Chapman & Hall, New York.\nFaraway, J. J. (2005) Linear Models with R, Chapman & Hall, Boca Ra-\n  ton, FL.\n\f11\nRegression: Advanced Topics\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.13",
      "section_title": "Residual plots and diagnostics for regression of Y on X in Problem 4.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1 The Theory Behind Linear Regression\n\nThis section provides some theoretical results about linear least-squares esti-\nmation. The study of linear regression is facilitated by the use of matrices.\nEquation (9.1) can be written more succinctly as\n\n                         Yi = xT\n                               i \u03b2 + \u0017i ,    i = 1, . . . , n           (11.1)\n\nwhere xi = (1 Xi,1 \u00b7 \u00b7 \u00b7 Xi,p )T and \u03b2 = (\u03b20 \u03b21 \u00b7 \u00b7 \u00b7 \u03b2p )T . Let\n                    \u239b \u239e              \u239b    \u239e              \u239b \u239e\n                        Y1             x1                    \u00171\n                    \u239c .. \u239f           \u239c .. \u239f              \u239c .. \u239f\n              Y = \u239d . \u23a0 , X = \u239d . \u23a0 , and = \u239d . \u23a0 .\n                        Yn             xn                       \u0017n\n\nThen, the n equations in (11.1) can be expressed as\n\n                          Y = 1234\n                         1234  X             \u03b2 + 1234 ,                 (11.2)\n                                            1234\n                         n\u00d71    n\u00d7(p+1) (p+1)\u00d71       n\u00d71\n\n\nwith the matrix dimensions indicated by underbraces.\n   The least-squares estimate of \u03b2 minimizes\n\n \u0012Y \u2212X\u03b2\u00122 = (Y \u2212X\u03b2)T (Y \u2212X\u03b2) = Y T Y \u22122\u03b2 T X T Y +\u03b2 T X T X\u03b2. (11.3)\n\nBy setting the derivatives of (11.3) with respect to \u03b20 , . . . , \u03b2p equal to 0\nand simplifying the resulting equations, one \ufb01nds that the least-squares\nestimator is\n                           \u0002 = (X T X)\u22121 X T Y .\n                           \u03b2                                             (11.4)\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                            269\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 11\n\f270      11 Regression: Advanced Topics\n\n                                                  \u0002\nUsing (7.9), one can \ufb01nd the covariance matrix of \u03b2:\n          \u0002 1 , . . . , xn ) = (X T X)\u22121 X T COV(Y |x1 , . . . , xn )X(X T X)\u22121\n      COV(\u03b2|x\n                           = (X T X)\u22121 X T (\u03c3 2 I)X(X T X)\u22121\n                           = \u03c3 2 (X T X)\u22121 ,\n\nsince COV(Y |x1 , . . . , xn ) = COV( ) = \u03c3 2 I, where I is the n \u00d7 n identity\nmatrix. Therefore, the standard error of \u03b2\u0002j is the square root of the jth\ndiagonal element of \u03c3 2 (X T X)\u22121 .\n    The vector of \ufb01tted values is\n                           \u0002 = {X(X T X)\u22121 X T }Y = HY ,\n                    Y\u0002 = X \u03b2\n\nwhere H = X(X T X)\u22121 X T is the hat matrix. The leverage of the ith obser-\nvation is Hii , the ith diagonal element of H.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "The Theory Behind Linear Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1.1 Maximum Likelihood Estimation for Regression\n\nIn this section, we assume a linear regression model with noise that may not\nbe normally distributed and independent.\n    For example, consider the special case of i.i.d. errors. It is useful to put\nthe scale parameter explicitly into the regression model, so we assume that\n\n                                  Yi = xT\n                                        i \u03b2 + \u03c3\u0017i ,\n\nwhere {\u0017i } are i.i.d. with a known density f that has variance equal to 1\nand \u03c3 is the unknown noise standard deviation. For example, f could be a\nstandardized t-density. Then the likelihood of Y1 , . . . , Yn is\n                             5n      \u000e          \u000f\n                                 1     Yi \u2212 xT\n                                             i\u03b2\n                                   f              .\n                             i=1\n                                 \u03c3         \u03c3\n\nThe maximum likelihood estimator maximizes the log-likelihood\n                                   n     ! \u000e            \u000f\"\n                                             Yi \u2212 xTi\u03b2\n           L(\u03b2, \u03c3) = \u2212n log(\u03c3) +      log f                .\n                                  i=1\n                                                 \u03c3\n\nFor normally distributed errors, log{f (x)} = \u2212 12 x2 \u2212 12 log(2\u03c0), and for the\npurpose of maximization, the constant \u2212 12 log(2\u03c0) can be ignored. Therefore,\nthe log-likelihood is\n                                              n \u0007             \b2\n                                           1       Yi \u2212 xT i\u03b2\n               LGAUSS\n                      (\u03b2, \u03c3) = \u2212n log(\u03c3) \u2212                       .\n                                           2 i=1       \u03c3\n\nIt should be obvious that the least-squares estimator is the MLE of \u03b2. Also,\nmaximizing LGAUSS (\u03b2,\u0002 \u03c3) in \u03c3, where \u03b2 has been replaced by the least-squares\nestimate, is a standard calculus exercise and the result is\n\f                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "1 Maximum Likelihood Estimation for Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2 Nonlinear Regression     271\n                                        n\n                         \u0002MLE\n                         \u03c3 2\n                              = n\u22121         (Yi \u2212 xT \u0002 2\n                                                   i \u03b2) .\n                                      i=1\n\nIn can be shown that \u03c3\u0002MLE\n                       2\n                             is biased but that the bias is eliminated if n\u22121 is\n                         \u22121\nreplaced by {n \u2212 (p + 1)} where p + 1 is the dimension of \u03b2. This give us\nthe estimator (9.16).\n    Now assume that has a covariance matrix \u03a3 and, for some function f ,\ndensity\n                   |\u03a3|\u22121/2 f {(Y \u2212 X\u03b2)T \u03a3 \u22121 (Y \u2212 X\u03b2)}.\nThen the log-likelihood is\n              1             \u0018                           \u0019\n             \u2212 log |\u03a3| + log f {(Y \u2212 X\u03b2)T \u03a3 \u22121 (Y \u2212 X\u03b2)} .\n              2\n    In the important special case where has a mean-zero multivariate normal\ndistribution, the density of is\n                     !                \"    \u000e         \u000f\n                             1                1 T \u22121\n                                        exp \u2212    \u03a3     ,              (11.5)\n                       |\u03a3|1/2 (2\u03c0)p/2         2\n\nIf \u03a3 is known, then the MLE of \u03b2 minimizes\n\n                         (Y \u2212 X\u03b2)T \u03a3 \u22121 (Y \u2212 X\u03b2)\n\nand is called the generalized least-squares estimator (GLS estimator). If\n\u00171 , . . . , \u0017n are uncorrelated but with possibly di\ufb00erent variances, then \u03a3 is\nthe diagonal matrix of these variances and the generalized least-squares esti-\nmator is the weighted least-squares estimator (10.4).\n      The GLS estimator is\n                      \u0002         T \u22121\n                                     X)\u22121 X T \u03a3 \u22121 Y .\n                      \u03b2 GLS = (X \u03a3                                           (11.6)\n\nTypically, \u03a3 is unknown and must be replaced by an estimate, for example,\nfrom an ARMA model for the errors.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "Nonlinear Regression     271",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2 Nonlinear Regression\nOften we can derive a theoretical model relating predictor variables and a\nresponse, but the model we derive is not linear. In particular, models derived\nfrom economic theory are commonly used in \ufb01nance and many are not linear.\n   The nonlinear regression model is\n\n                             Yi = f (X i ; \u03b2) + \u0017i ,                         (11.7)\n\nwhere Yi is the response measured on the ith observation, X i is a vector\nof observed predictor variables for the ith observation, f (\u00b7 ; \u00b7) is a known\n\f272      11 Regression: Advanced Topics\n\nfunction, \u03b2 is an unknown parameter vector, and \u00171 , . . . , \u0017n are i.i.d. with\n                                                     \u0002 minimizes\nmean 0 and variance \u03c3 2 . The least-squares estimate \u03b2\n                                n\n                                                       2\n                                      {Yi \u2212 f (X i ; \u03b2)} .\n                                i=1\n\n                                        \u0002 and the residuals are \u0002\nThe predicted values are Y\u0002i = f (X i ; \u03b2)                       \u0017i = Yi \u2212 Y\u0002i .\n    Since the model is nonlinear, \ufb01nding the least-squares estimate requires\nnonlinear optimization. Because of the importance of nonlinear regression,\nalmost every statistical software package will have routines for nonlinear least-\nsquares estimation. This means that most of the di\ufb03cult programming has\nalready been done for us. However, we do need to write an equation that\nspeci\ufb01es the model we are using.1 In contrast, when using linear regression\nonly the predictor variables need to be speci\ufb01ed.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "Nonlinear Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1. Simulated bond prices\n\n    Consider prices of par $1000 zero-coupon bonds issued by a particular\nborrower, perhaps the Federal government or a corporation. Suppose that\nthere are several times to maturity, the ith being denoted by Ti . Suppose also\nthat the yield to maturity is a constant, say r. The assumption that YT = r for\nall T is not realistic and is used only to keep this example simple. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "Simulated bond prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3\nmore realistic models will be used.\n    The rate r is determined by the market and can be estimated from prices.\nUnder the assumption of a constant value of r, the present price of a bond\nwith maturity Ti is\n                               Pi = 1000 exp(\u2212rTi ).                       (11.8)\nThere is some random variation in the observed prices. One reason is that\nthe price of a bond can only be determined by the sale of the bond, so the\nobserved prices have not been determined simultaneously. Prices that may\nno longer re\ufb02ect current market values are called stale. Each bond\u2019s price was\ndetermined at the time of the last trade of a bond of that maturity, and r may\nhave had a somewhat di\ufb00erent value then. It is only as a function of time to\nmaturity that r is assumed constant, so r may vary with calendar time. Thus,\nwe augment model (11.8) by including a noise term to obtain the regression\nmodel\n                         Pi = 1000 exp(\u2212rTi ) + \u0017i .                     (11.9)\nAn estimate of r can be determined by least squares, that is, by minimizing\nover r the sum of squares:\n                        n \u000e                      \u000f2\n                            Pi \u2212 1,000 exp(\u2212rTi ) .\n                          i=1\n1\n    Even this work can sometimes be avoided, since some nonlinear regression soft-\n    ware has many standard models already programmed.\n\f                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "more realistic models will be used.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2 Nonlinear Regression   273\n\nThe least-squares estimator is denoted by r\u0002.\n\n\n\n                               *\n\n                         900\n                         800       **\n                                        *\n                         700\n                 price\n\n\n\n\n                                            **\n                         600\n\n\n\n\n                                             *\n                         500\n\n\n\n\n                                                            *\n                         400\n\n\n\n\n                                                                       *\n                                        5              10         15\n                                            maturity\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "Nonlinear Regression   273",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1. Plot of bond prices against maturities with the predicted price from the\nnonlinear least-squares \ufb01t.\n\n\n    Since it is unlikely that market data will have a constant r, this example\nuses simulated data. The data were generated with r \ufb01xed at ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "Plot of bond prices against maturities with the predicted price from the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06 and plot-\nted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "and plot-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1. The nonlinear least-squares estimate of r was found using\nR\u2019s nls() function. Nonlinear optimization requires starting values for the\nparameters, and a starting value of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "The nonlinear least-squares estimate of r was found using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 was used for r.\n\n   bondprices = read.table(\"bondprices.txt\", header = TRUE)\n   attach(bondprices)\n   fit = nls(price ~ 1000 * exp(-r * maturity), start = list(r = 0.04))\n   summary(fit)\n   detach(bondprices)\n\nThe output is:\n   Formula: price ~ 1000 * exp(-r * maturity)\n\n   Parameters:\n     Estimate Std. Error t value Pr(>|t|)\n   r ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "was used for r.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05850     0.00149    39.3 1.9e-10 ***\n   ---\n\n   Residual standard error: 20 on 8 degrees of freedom\n\n   Number of iterations to convergence: 4\n   Achieved convergence tolerance: 5.53e-08\n\f274    11 Regression: Advanced Topics\n\nNotice that r\u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05850",
      "section_title": "0.00149    39.3 1.9e-10 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0585 and the standard error of this estimate is 0.00149. The\npredicted price curve using nonlinear regression is shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0585",
      "section_title": "and the standard error of this estimate is 0.00149. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1.      \u0002\n\n    As mentioned, in nonlinear regression the form of the regression function\nis nonlinear but known up to a few unknown parameters. For example, the\nregression function has an exponential form in model (11.9). For this reason,\nnonlinear regression would best be called nonlinear parametric regression to\ndistinguish it from nonparametric regression, where the regression function is\nalso nonlinear but not of a known parametric form. Nonparametric regression\nis discussed in Chap. 21.\n    Polynomial regression may appear to be nonlinear since polynomials are\nnonlinear functions. For example, the quadratic regression model\n\n                          Yi = \u03b20 + \u03b21 Xi + \u03b22 Xi2 + \u0017i                   (11.10)\n\nis nonlinear in Xi . However, by de\ufb01ning Xi2 as a second predictor variable,\nthis model is linear in (Xi , Xi2 ) and therefore is an example of multiple linear\nregression. What makes model (11.10) linear is that the right-hand side is a\nlinear function of the parameters \u03b20 , \u03b21 , and \u03b22 , and therefore can be inter-\npreted as a linear regression with the appropriate de\ufb01nition of the variables.\nIn contrast, the exponential model\n\n                               Y i = \u03b2 0 e\u03b2 1 X i + \u0017 i\n\nis nonlinear in the parameter \u03b21 , so it cannot be made into a linear model by\nrede\ufb01ning the predictor variable.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2. Estimating default probabilities\n\n    This example illustrates both nonlinear regression and the detection of\nheteroskedasticity by residual plotting.\n    Credit risk is the risk to a lender that a borrower will default on con-\ntractual obligations, for example, that a loan will not be repaid in full. A\nkey parameter in the determination of credit risk is the probability of de-\nfault. Bluhm, Overbeck, and Wagner (2003) illustrate how one can calibrate\nMoody\u2019s credit rating to estimate default probabilities. These authors use\nobserved default frequencies for bonds in each of 16 Moody\u2019s ratings from\nAaa (best credit rating) to B3 (worse rating). They convert the credit ratings\nto a 1 to 16 scale (Aaa = 1, . . . , B3 = 16). Figure 11.2a shows default fre-\nquencies (as fractions, not percentages) plotted against the ratings. The data\nare from Bluhm, Overbeck, and Wagner (2003). The relationship is clearly\nnonlinear. Not surprisingly, Bluhm, Overbeck, and Wagner used a nonlinear\nmodel, speci\ufb01cally\n\n                  P r{default|rating} = exp{\u03b20 + \u03b21 rating}.              (11.11)\n\f                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "Estimating default probabilities",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2 Nonlinear Regression       275\n\nTo use this model they \ufb01t a linear function to the logarithms of the default\nfrequencies. One di\ufb03culty with doing this is that six of the default frequencies\nare zero giving a log transformation of \u2212\u221e.\n\n     a                                                           b\n                                                             *\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "Nonlinear Regression       275",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n\n\n\n\n                             exponential                                                               BOW\n\n\n\n\n                                                                                            0\n                                                                 log(default probability)\n                             data                                                                      nonlinear\n                         *                                                                             tbs\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "exponential                                                               BOW",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n      frequency\n\n\n\n\n                                                                                                       data\n\n\n\n\n                                                                                            \u22125\n                                                        *\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "frequency",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n\n\n\n\n                                                                                            \u221210\n                                                    *\n                                                *\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "\u221210",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                         ************\n                                5          10           15                                                 5            10   15\n                                      rating                                                                       rating\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "************",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2. (a) Default frequencies with an exponential \ufb01t. \u201cRating\u201d is a conversion\nof the Moody\u2019s rating to a 1 to 16-point scale as follows: 1 = Aaa, 2 = Aa1, 3 = Aa3,\n4 = A1, . . . , 16 = B 3. (b) Estimation of default probabilities by Bluhm, Overbeck,\nand Wagner\u2019s (2003) linear regression with ratings removed that have no observed\ndefaults (BOW) and by nonlinear regression with all data (nonlinear). Because some\ndefault frequencies are zero, when plotting the data on a semilog plot,10\u22126 was added\nto the default frequencies. This constant was not added when estimating default\nfrequencies, only for plotting the raw data. The six observations along the bottom\nof the plot are the ones removed by Bluhm, Overbeck, and Wagner. \u201cTBS\u201d is the\ntransform-both-sides estimate, which will be discussed soon.\n\n\n    Bluhm, Overbeck, and Wagner (2003) address this issue by labeling default\nfrequencies equal to zero as \u201cunobserved\u201d and not using them in the estimation\nprocess. The problem with their technique is that they have deleted the data\nwith the lowest observed default frequencies. This biases their estimates of\ndefault probabilities in an upward direction. Bluhm, Overbeck, and Wagner\nargue that an observed default frequency of zero does not imply that the\ntrue default probability is zero. This is certainly true. However, the default\nfrequencies, even when they are zero, are unbiased estimates of the true default\nprobabilities. There is no intent here to be critical of their book, which is well-\nwritten and useful. However, one can avoid the bias of their method by using\nnonlinear regression with model (11.11). The advantage of \ufb01tting (11.11) by\nnonlinear regression is that it avoids the use of a logarithm transformation thus\nallowing the use of all the data, even data with a default frequency of zero. The\n\ufb01ts by the Bluhm, Overbeck, and Wagner method and by nonlinear regression\nusing model (11.11) are shown in Fig. 11.2b with a log scale on the vertical axis\nso that the \ufb01tted functions are linear. Notice that at good credit ratings the\nestimated default probabilities are lower using nonlinear regression compared\nto Bluhm, Overbeck, and Wagner\u2019s biased method. The di\ufb00erences between\nthe two sets of estimated default probabilities can be substantial. Bluhm,\n\f276                       11 Regression: Advanced Topics\n\nOverbeck, and Wagner estimate the default probability of an Aaa bond as\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "(a) Default frequencies with an exponential \ufb01t. \u201cRating\u201d is a conversion",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005 %. In contrast, the unbiased estimate by nonlinear regression is only 40 %\nof that \ufb01gure, speci\ufb01cally, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "%. In contrast, the unbiased estimate by nonlinear regression is only 40 %",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0020 %. Thus, the bias in the Bluhm, Overbeck,\nand Wagner estimate leads to a substantial overestimate of the credit risk of\nAaa bonds and similar overestimation at other good credit ratings.\n\n\n      a                                                             b\n\n\n\n\n                                                                                            2\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0020",
      "section_title": "%. Thus, the bias in the Bluhm, Overbeck,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.006\n\n\n\n\n                                                                    theoretical quantiles\n      absolute residual\n\n\n\n\n                                                                                            1\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.006",
      "section_title": "theoretical quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.003\n\n\n\n\n                                                                                            0\n                                                                                            \u22121\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.003",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                                                                            \u22122\n                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00     0.04    0.08     0.12                                \u22120.006        0.000   0.004\n                                             fitted values                                                sample quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.04    0.08     0.12                                \u22120.006        0.000   0.004",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. (a) Residuals for estimation of default probabilities by nonlinear regres-\nsion. Absolute studentized residuals plotted against \ufb01tted values with a loess smooth.\nSubstantial heteroskedasticity is indicated because the data on the left side are less\nscattered than elsewhere. (b) Normal probability plot of the residuals. Notice the\noutliers caused by the nonconstant variance.\n\n\n\n    A plot of the absolute residuals versus the \ufb01tted values in Fig. 11.3a gives\na clear indication of heteroskedasticity. Heteroskedasticity does not cause bias\nbut it does cause ine\ufb03cient estimates. In Sect. 11.4, this problem is \ufb01xed by a\nvariance-stabilizing transformation. Figure 11.3b is a normal probability plot\nof the residuals. Outliers with both negative and positive values can be seen.\nThese are due to the nonconstant variance and are not necessarily a sign of\nnonnormality. This plot illustrates the danger of attempting to interpret a\nnormal plot when the data have a nonconstant variance. One should apply a\nvariance-stabilizing transformation \ufb01rst before checking for normality.       \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "(a) Residuals for estimation of default probabilities by nonlinear regres-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3 Estimating Forward Rates from Zero-Coupon\nBond Prices\nIn practice, the forward-rate function r(t) is unknown. Only bond prices are\nknown. If the prices P (Ti ) of zero-coupon bonds are available on a relatively\n\ufb01ne grid of values of T1 < T2 < \u00b7 \u00b7 \u00b7 < Tn , then using (3.24) we can estimate\nthe forward-rate curve at Ti with\n                                       \u0394 log{P (Ti )}    log{P (Ti )} \u2212 log{P (Ti\u22121 )}\n                                   \u2212                  =\u2212                               .                                       (11.12)\n                                           \u0394Ti                    Ti \u2212 Ti\u22121\n\f                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "Estimating Forward Rates from Zero-Coupon",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3 Estimating Forward Rates from Zero-Coupon Bond Prices                                                                277\n\nWe will call these the empirical forward-rate estimates. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "Estimating Forward Rates from Zero-Coupon Bond Prices                                                                277",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4 shows\nprices and empirical forward-rate estimates from data to be described soon\nin Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "shows",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. As can be seen in the plot, the empirical forward-rate esti-\nmates can be rather noisy when the denominators in (11.12) are small because\nthe maturities are spaced closely together. If the maturities were more widely\nspaced, then bias rather than variance would be the major problem. Despite\nthese di\ufb03culties, the empirical forward-rate estimates give a general impres-\nsion of the forward-rate curve and are useful for comparing with estimates\nfrom parametric models, which are discussed next.\n\n\n   a                                                       b\n            100\n\n\n\n\n                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "As can be seen in the plot, the empirical forward-rate esti-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07\n                                                              empirical forward rate\n            80\n\n\n\n\n                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "empirical forward rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n    price\n\n\n\n\n                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "price",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n            60\n\n\n\n\n                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "60",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n            40\n\n\n\n\n                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n            20\n\n\n\n\n                  0   5   10      15      20   25   30                                        0        5     10      15      20   25   30\n                               maturity                                                                           maturity\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4. (a) U.S. STRIPS prices. (b) Empirical forward-rate estimates from the\nprices.\n\n   We can estimate r(t) from the bond prices using nonlinear regression.\nAn example of estimating r(t) was given in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "(a) U.S. STRIPS prices. (b) Empirical forward-rate estimates from the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.2 assuming that r(t)\nwas constant and using as data the prices of zero-coupon bonds of di\ufb00erent\nmaturities. In this section, we estimate r(t) without assuming it is constant.\n   Parametric estimation of the forward-rate curves starts with a parametric\nfamily r(t; \u03b8) of forward rates and the correspond yield curves\n                                         \u0013 T\n                                      \u22121\n                           yT (\u03b8) = T        r(t; \u03b8) dt\n                                                          0\n\nand model for the price of a par-$1 bond:\n                                                                                                  \u0013 T\n                      PT (\u03b8) = exp{\u2212T yT (\u03b8)} = exp \u2212                                                      r(t; \u03b8) dt .\n                                                                                                   0\n\nFor example, suppose that r(t; \u03b8) is a pth-degree polynomial, so that\n\n                                       r(t; \u03b8) = \u03b80 + \u03b81 t + \u00b7 \u00b7 \u00b7 + \u03b8p tp\n\nfor some unknown parameters \u03b80 , . . . , \u03b8p . Then\n\f278    11 Regression: Advanced Topics\n                 \u0013 T\n                                                T2              T p+1\n                       r(t; \u03b8) dt = \u03b80 T + \u03b81      + \u00b7 \u00b7 \u00b7 + \u03b8p       ,\n                  0                             2                 p\n\nand the yield curve is\n                          \u0013 T\n                                                       T             Tp\n              yT = T \u22121         r(t; \u03b8)dt = \u03b80 + \u03b81      + \u00b7 \u00b7 \u00b7 + \u03b8p .\n                            0                          2             p\n\n   A popular model is the Nelson\u2013Siegel family with forward-rate and yield\ncurves\n\n          r(t; \u03b8) = \u03b80 + (\u03b81 + \u03b82 t) exp(\u2212\u03b83 t),\n                         \u0007          \b\n                                \u03b82 1 \u2212 exp(\u2212\u03b83 t) \u03b82\n           yt (\u03b8) = \u03b80 + \u03b81 +                    \u2212    exp(\u2212\u03b83 t).\n                                \u03b83         \u03b83 t    \u03b83\n\nThe six-parameter Svensson model extends the Nelson\u2013Siegel model by adding\nthe term \u03b84 t exp(\u2212\u03b85 t) to the forward rate.\n    The nonlinear regression model for estimating the forward-rate curve\nstates that the price of the ith bond in the sample with maturity Ti expressed\nas a fraction of par value is\n                                                \u0013 Ti\n               Pi = D(Ti ) + \u0017i = exp \u2212                r(t; \u03b8) dt   + \u0017i ,   (11.13)\n                                                  0\n\n\nwhere D is the discount function and \u0017i is an \u201cerror\u201d due to problems such as\nprices being somewhat stale and the bid\u2013ask spread.2\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.2",
      "section_title": "assuming that r(t)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. Estimating forward rates from STRIPS prices\n\n    We now look at an example using data on U.S. STRIPS, a type of zero-\ncoupon bond. STRIPS is an acronym for \u201cSeparate Trading of Registered\nInterest and Principal of Securities.\u201d The interest and principal on Trea-\nsury bills, notes, and bonds are traded separately through the Federal Re-\nserve\u2019s book-entry system, in e\ufb00ect creating zero-coupon bonds by repackaging\ncoupon bonds.3\n    The data are from December 31, 1995. The prices are given as a percentage\nof par value. Price is plotted against maturity in years in Fig. 11.4a. There are\n117 prices and the maturities are nearly equally spaced from 0 to 30 years.\nWe can see that the price drops smoothly with maturity and that there is\nnot much noise in the price data. The empirical forward-rate estimates in\nFig. 11.4b are much noisier than the prices.\n2\n  A bond dealer buys bonds at the bid price and sells them at the ask price, which\n  is slightly higher than the bid price. The di\ufb00erence is called the bid\u2013ask spread\n  and covers the trader\u2019s administrative costs and pro\ufb01t.\n3\n  Jarrow(2002, p. 15).\n\f           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "Estimating forward rates from STRIPS prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3 Estimating Forward Rates from Zero-Coupon Bond Prices        279\n\n    Three models for the forward curve were \ufb01t: quadratic polynomial, cu-\nbic polynomial, and quadratic polynomial spline with a knot at T = 15.\nThe latter splices two quadratic functions together at T = 15 so that the\nresulting curve is continuous and with a continuous \ufb01rst derivative. The\nspline\u2019s second derivative jumps at T = 15. One way to write the spline is\n\n                      r(t) = \u03b20 + \u03b21 t + \u03b22 t2 + \u03b23 (t \u2212 15)2+ ,          (11.14)\n\nwhere the positive-part function is x+ = x if x \u2265 0 and x+ = 0 if x < 0. Also,\nx2+ means (x+ )2 , that is, take the positive part \ufb01rst. See Chap. 21 for further\ninformation about splines. From (11.14), one obtains\n          \u0013 T\n                                      T2      T3      (T \u2212 15)3+\n                r(t) dt = \u03b20 T + \u03b21      + \u03b22    + \u03b23            ,        (11.15)\n           0                          2       3           3\n\nand therefore the yield curve is\n\n                                     T      T2      (T \u2212 15)3+\n                    yT = \u03b2 0 + \u03b2 1     + \u03b22    + \u03b23            .          (11.16)\n                                     2      3          3T\nFrom (11.15), the model for a bond price (as a percentage of par) is\n               \u000e \u0007                                        \b\u000f\n                              T2      T3       (T \u2212 15)3+\n       100 exp \u2212 \u03b20 T + \u03b21       + \u03b22    + \u03b23                .       (11.17)\n                               2       3           3\n\nR code to \ufb01t the quadratic spline and plot its forward-rate estimate is\n   fitSpline = nls(price ~ 100 * exp(-beta0 * T\n        - (beta1 * T^2)/2 - (beta2 * T^3) / 3\n        - (T > 15) * (beta3 * (T - 15)^3) / 3), data = dat,\n        start = list(beta0 = 0.03, beta1 = 0, beta2 = 0, beta3 = 0))\n   coefSpline = summary(fitSpline)$coef[ , 1]\n   forwardSpline = coefSpline[1] + (coefSpline[2] * t) +\n       (coefSpline[3] * t^2) + (t > 15) * (coefSpline[4] * (t - 15)^2)\n   plot(t, forwardSpline, lty = 2, lwd = 2)\n\nOnly slight changes in the code are needed to \ufb01t the quadratic or cubic poly-\nnomial models.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "Estimating Forward Rates from Zero-Coupon Bond Prices        279",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "15) * (beta3 * (T - 15)^3) / 3), data = dat,\n        start = list(beta0 = 0.03, beta1 = 0, beta2 = 0, beta3 = 0))\n   coefSpline = summary(fitSpline)$coef[ , 1]\n   forwardSpline = coefSpline[1] + (coefSpline[2] * t) +\n       (coefSpline[3] * t^2) + (t > 15) * (coefSpline[4] * (t - 15)^2)\n   plot(t, forwardSpline, lty = 2, lwd = 2)",
        "start": 1693,
        "end": 2028
      }
    ]
  },
  {
    "content": "11.5 contains all three estimates of the forward rate and the empir-\nical forward rates. The cubic polynomial and quadratic spline models follow\nthe empirical forward rates much more closely than the quadratic polynomial\nmodel. The cubic polynomial and quadratic spline \ufb01ts both use four param-\neters and are similar to each other, though the spline has a slightly smaller\nresidual sum of squares. The summary of the spline model\u2019s \ufb01t is\n\f280      11 Regression: Advanced Topics\n\n                                                              *\n\n\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.5",
      "section_title": "contains all three estimates of the forward rate and the empir-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07\n                                                               *        ** ******\n                                                  ** ** * * * * ****** ** ** * *** * *\n                                              **** * * * *                        * ** ** **\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "*        ** ******",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06                    *  *    *                        * * ** *\n                                           * *     *  **                                     * *\n                                   * ********\n          forward rate\n\n\n                                                                                                   *\n                                      ** *\n                                **** ** * * *                                               ** * **\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "*  *    *                        * * ** *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n                                    *                                                         * **\n                                                                                               **\n                                                                                              * **\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "*                                                         * **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n\n\n\n\n                                    quadratic\n                                    cubic\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "quadratic",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n\n\n\n\n                                    spline\n                                *   empirical                                                *\n                                0         5          10          15         20          25         30\n                                                             maturity\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "spline",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.5. Polynomial and spline estimates of forward rates of U.S. Treasury bonds.\nThe empirical forward rates are also shown.\n\n\n      > summary(fitSpline)\n\n      Formula: price ~ 100 * exp(-beta0 * T - (beta1 * T^2)/2\n        - (beta2 * T^3)/3 - (T > 15) * (beta3 * (T - 15)^3)/3)\n\n      Parameters:\n              Estimate Std. Error t value Pr(>|t|)\n      beta0 4.947e-02 9.221e-05 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.5",
      "section_title": "Polynomial and spline estimates of forward rates of U.S. Treasury bonds.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "summary(fitSpline)",
        "start": 131,
        "end": 153
      },
      {
        "language": "r",
        "code": "15) * (beta3 * (T - 15)^3)/3)",
        "start": 246,
        "end": 279
      }
    ]
  },
  {
    "content": "536.52      <2e-16 ***\n      beta1 1.605e-03 3.116e-05     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "536.52",
      "section_title": "<2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "51.51   <2e-16 ***\n      beta2 -2.478e-05 1.820e-06 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "51.51",
      "section_title": "<2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.62     <2e-16 ***\n      beta3 -1.763e-04 5.755e-06 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.62",
      "section_title": "<2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "30.64     <2e-16 ***\n      ---\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "30.64",
      "section_title": "<2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0667 on 113 degrees of freedom\n\n      Number of iterations to convergence: 5\n      Achieved convergence tolerance: 1.181e-07\n\nNotice that all coe\ufb03cients have very small p-values. The small p-value of\nbeta3 is further evidence that the spline model \ufb01ts better than the quadratic\npolynomial model, since the two models di\ufb00er only in that beta3 is 0 for the\nquadratic model.\n    R\u2019s nls function could not \ufb01nd the least-squares estimator for the Nelson\u2013\nSiegel model, but the least-squares estimator was found using the optim non-\nlinear optimization function with the sum of squares as the objective function.\nThe \ufb01t of the Nelson\u2013Siegel model was noticeably inferior to that of the cubic\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0667",
      "section_title": "on 113 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4 Transform-Both-Sides Regression      281\n\npolynomial and quadratic spline models. In fact, the Nelson\u2013Siegel model did\nnot \ufb01t even as well as the quadratic polynomial model.\n    The Svensson model is likely to \ufb01t better than the Nelson\u2013Siegel model,\nbut the four-parameter cubic polynomial and quadratic spline models \ufb01t su\ufb03-\nciently well that it did not seem worthwhile to try the six-parameter Svensson\nmodel.                                                                       \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "Transform-Both-Sides Regression      281",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4 Transform-Both-Sides Regression\n\nSuppose we have a theoretical model that states that in the absence of any\nnoise,\n                              Yi = f (X i ; \u03b2).                    (11.18)\nModel (11.18) is identical to the model\n\n                              h{Yi } = h{f (X i ; \u03b2)},                         (11.19)\n\nwhere h is any one-to-one function, such as, a strictly increasing function. In\nthe absence of noise, one choice of h is as good as any other and one might as\nwell stick with model (11.18), but when noise exists, this is no longer true.\n   When we have noisy data, Eq. (11.19) can be converted to the nonlinear\nregression model\n                          h{Yi } = h{f (X i ; \u03b2)} + \u0017i .                (11.20)\nModel (11.20) is called the transform-both-sides (TBS) regression model be-\ncause both sides of Eq. (11.19) have been transformed by the same function\nh. Typically, h will be one of the Box\u2013Cox transformations and h is chosen\nto stabilize the variation and to induce nearly normally distributed errors. To\nestimate \u03b2 for a \ufb01xed h, one minimizes\n                         n    \u001a             (            )\u001b2\n                                                      \u0002\n                                  h{Yi } \u2212 h f (X i ; \u03b2)     .                 (11.21)\n                        i=1\n\nVarious choices of h can be compared by residual plots. The h that gives\napproximately normally distributed residuals with a constant variance is used\nfor the \ufb01nal analysis.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "Transform-Both-Sides Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4. TBS regression for the default frequency data\n\n    TBS regression was applied to the default frequency data. The Box\u2013Cox\ntransformation h(y) = y (\u03b1) was tried with various positive values of \u03b1. It was\nfound that \u03b1 = 1/2 gave residuals that appeared normally distributed with a\nconstant variance, so the square-root transformation was used for estimation;\nsee Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "TBS regression for the default frequency data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6. With this transformation, \u03b2 is estimated by minimizing\n\f282                 11 Regression: Advanced Topics\n\n        a                                                           b\n\n\n\n\n                                                                                            2\n                                                                    Theoretical Quantiles\n        absolute residual\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "With this transformation, \u03b2 is estimated by minimizing",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015\n\n\n\n\n                                                                                            1\n                                                                                            0\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.015",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005\n\n\n\n\n                                                                                            \u22121\n                                                                                            \u22122\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.1       0.2     0.3                                  \u22120.02         0.00   0.01   0.02\n                                            fitted values                                                Sample Quantiles\n                                                                     \u221a\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.1       0.2     0.3                                  \u22120.02         0.00   0.01   0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6. (a) Transform-both-sides regression (TBS) with h(y) = y. Absolute\nstudentized residuals plotted against \ufb01tted values with a loess smooth. (b) Normal\nplot of the studentized residuals.\n\n                                                n   \u001a\u0011                            \u001b2\n                                                      Yi \u2212 exp{\u03b20 /2 + (\u03b21 /2)Xi } ,                                           (11.22)\n                                            i=1\n\nwhere Yi is the ith default frequency and Xi is the ith rating. The square-root\ntransformation of the model is accomplished by dividing \u03b20 and \u03b21 by 2.\n    The R code to \ufb01t the TBS model and create Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "(a) Transform-both-sides regression (TBS) with h(y) = y. Absolute",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6 is below. The \ufb01tted\nvalues fit tbs are computed by subtracting the residuals from the responses;\nthis is done because the function summary() does not return the \ufb01tted values.\n      DefaultData = read.table(\"DefaultData.txt\", header = TRUE)\n      attach(DefaultData)\n      freq2 = freq / 100\n      fit_tbs = nls(sqrt(freq2) ~ exp(b1 / 2 + b2 * rating / 2),\n         start = list(b1 = -6, b2 = 0.5))\n      sum_tbs = summary(fit_tbs)\n      par(mfrow = c(1, 2))\n      fitted_tbs = sqrt(freq2) - sum_tbs$resid\n      plot(fitted_tbs,abs(sum_tbs$resid), xlab = \"fitted values\",\n         ylab = \"absolute residual\")\n      fit_loess_tbs = loess( abs(sum_tbs$resid) ~ fitted_tbs,\n         span = 1, deg = 1)\n      ord_tbs = order(fitted_tbs)\n      lines(fitted_tbs[ord_tbs], fit_loess_tbs$fit[ord_tbs])\n      qqnorm(sum_tbs$resid, datax = TRUE, main = \"\")\n      qqline(sum_tbs$resid, datax = TRUE)\n      detact(DefaultData)\n\n    Using TBS regression, the estimated default probability of Aaa bonds is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "is below. The \ufb01tted",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0008 %, only 16 % of the estimate given by Bluhm, Overbeck, and Wagner\n(2003) and only 40 % of the estimate given by nonlinear regression without\na transformation. Of course, a reduction in estimated risk by 84 % is a huge\nchange. This shows how proper statistical modeling\u2014e.g., using all the data\nand an appropriate transformation\u2014can have a major impact on \ufb01nancial risk\n\f                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0008",
      "section_title": "%, only 16 % of the estimate given by Bluhm, Overbeck, and Wagner",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4 Transform-Both-Sides Regression             283\n\nanalysis. TBS allows one to use all the data (for unbiasedness) and, as de-\nscribed next, to e\ufb00ectively weight the data by the reciprocals of their variances\nfor high e\ufb03ciency.\n                                                                               \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "Transform-Both-Sides Regression             283",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.4.1 How TBS Works\n\nTBS in e\ufb00ect weights the data. To appreciate this, we use a Taylor series\nlinearization4 to obtain\n    n     \u001a             (            )\u001b2     n     \u001a    (            )\u001b2 (                  )2\n                                  \u0002\n              h(Yi ) \u2212 h f (X i ; \u03b2)     =                        \u0002\n                                                    h(1) f (X i ; \u03b2)                     \u0002\n                                                                           Yi \u2212 f (X i ; \u03b2)    .\n    i=1                                      i=1\n                                      \u001a                  \u001b2\n                                                     \u0002\nThe weight of the ith observation is h(1) {f (X i ; \u03b2)}     . Since the best weights\nare inverse variances, the most appropriate transformation h solves\n                                     \u001a                  \u001b\u22122\n                                                     \u0002\n                      Var(Yi |X i ) \u221d h(1) {f (X i ; \u03b2)}      .              (11.23)\n\nFor example, if h(y) = log(y), then h(1) (y) = 1/y and (11.23) becomes\n\n                                                           \u0002 2,\n                                 Var(Yi |X i ) \u221d {f (X i ; \u03b2)}                           (11.24)\n\nso that the conditional standard deviation of the response is proportional to\nits conditional mean. This occurs frequently. For example, if the response is\nexponentially distributed then (11.24) must hold. Equation (11.24) holds also\nif the response is lognormally distributed and the log-variance is constant. In\nthis case, it is not surprising that the log transformation is best since the log\ntransforms to i.i.d. normal noise.\n    The coe\ufb03cient of variation of a random variable is the ratio of its stan-\ndard deviation to its expected value. When (11.24) holds, the response has a\nconstant coe\ufb03cient of variation.\n    A transformation that causes that conditional variance to be constant is\ncalled the variance-stabilizing transformation. We have just shown that when\nthe coe\ufb03cient of variation is constant, then the variance-stabilizing transfor-\nmation is the logarithm.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.4",
      "section_title": "1 How TBS Works",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.5. Poisson responses\n\n   Assume Yi |X i is Poisson distributed with mean f (X i ; \u03b2), as might, for\nexample, happen if Yi were of the number of companies declaring bankruptcy\n4\n    A Taylor series linearization of the function h about the point x is h(y) \u2248 h(x) +\n    h(1) (x)(y \u2212 x), where h(1) is the \ufb01rst derivative of h. See any calculus textbook\n    for further discussion of Taylor series.\n\f284     11 Regression: Advanced Topics\n\nin a year, with f (X i ; \u03b2) modeling how that expected number depends on\nmacroeconomic variables in X i . The variance equals the mean for the Poisson\ndistribution, so\n                            Var(Yi |X i ) = f (X i ; \u03b2).\nUsing the same type of reasoning as in the previous example, it follows that one\nshould use \u03b1 = 1/2; the square-root transformation is the variance-stabilizing\ntransformation for Poisson-distributed responses.                             \u0002\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.5",
      "section_title": "Poisson responses",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.5 Transforming Only the Response\nThe so-called Box\u2013Cox transformation model is\n                       (\u03b1)\n                     Yi      = \u03b20 + Xi,1 \u03b21 + \u00b7 \u00b7 \u00b7 + Xi,p \u03b2p + \u0017i ,           (11.25)\n\nwhere \u00171 , . . . , \u0017n are i.i.d. N (0, \u03c3 2 ) for some \u03c3 . In contrast to the TBS model,\nonly the response is transformed. The goal of transforming the response is to\nachieve three objectives:\n                             (\u03b1)\n 1. a simple model: Yi           is linear in predictors Xi,1 , . . . , Xi,p and in the\n    parameters \u03b21 , . . . , \u03b2p ;\n 2. constant residual variance; and\n 3. Gaussian noise.\nIn contrast, 2 and 3 but not 1 are the goals of the TBS model.\n     Model (11.25) was introduced by Box and Cox (1964) who suggested es-\ntimation of \u03b1 by maximum likelihood. The function boxcox() in R\u2019s MASS\npackage will compute the pro\ufb01le log-likelihood for \u03b1 along with a con\ufb01dence\ninterval. Usually, \u03b1   \u0002 is taken to be some round number, e.g., \u22121, \u22121/2, 0, 1/2,\nor 1, in the con\ufb01dence interval. The reason for selecting one of these numbers\nis that then the transformation is readily interpretable, that is, it is the square\nroot, log, inverse, or some other familiar function. Of course, one can use the\nvalue of \u03b1 that maximizes the pro\ufb01le log-likelihood if one is not concerned\nwith having a familiar transformation. After \u03b1    \u0002 has been selected in this way,\n                                                          \u03b1)\n                                                         (\u0002\n\u03b20 , . . . , \u03b2p and \u03c3 2 can be estimated by regressing Yi on Xi,1 , . . . , Xi,p .\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.5",
      "section_title": "Transforming Only the Response",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6. Simulated data\u2014Box Cox transformation\n\n\n  This example uses the simulated data introduced in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "Simulated data\u2014Box Cox transformation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.6. The\nmodel is\n                (\u03b1)                    2\n              Yi = \u03b20 + \u03b21 Xi,1 + \u03b22 Xi,1 + \u03b23 Xi,2 + \u0017i .     (11.26)\nThe pro\ufb01le likelihood for \u03b1 was produced by the boxcox() function in R and\nis plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.6",
      "section_title": "The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.7. The code to produce the \ufb01gure is:\n\f                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.7",
      "section_title": "The code to produce the \ufb01gure is:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.5 Transforming Only the Response                                             285\n\n                                                    95%\n\n\n\n\n                                           \u221260\n                                           \u221280\n                          log\u2212likelihood\n                                           \u2212100\n                                           \u2212120\n                                           \u2212140\n\n\n\n\n                                                   \u22122             \u22121                    0                            1                  2\n                                                                                         \u03bb\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.5",
      "section_title": "Transforming Only the Response                                             285",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.7. Pro\ufb01le likelihood for the Box\u2013Cox model applied to the simulated data.\n\n\n   boxcox(y ~ poly(x1,2) + x2, ylab = \"log-likelihood\")\n\nWe see that the MLE is near \u22121 and \u22121 is well within the con\ufb01dence interval;\nthese results suggest that we use \u22121/Yi as the response.\n\n\n\n     a                                                                         b\n                3\n\n\n\n\n                                                                                                       3\n                2\n\n\n\n\n                                                                                                       2\n     rstudent\n\n\n\n\n                                                                               rstudent\n                1\n\n\n\n\n                                                                                                       1\n                0\n\n\n\n\n                                                                                                       0\n                \u22122 \u22121\n\n\n\n\n                                                                                                       \u22122 \u22121\n\n\n\n\n                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.7",
      "section_title": "Pro\ufb01le likelihood for the Box\u2013Cox model applied to the simulated data.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10                          \u22120.06                                                  0.0   0.2    0.4        0.6   0.8   1.0\n                                                  fitted values                                                                   x1\n\n\n     c                                                                         d\n                3\n\n\n\n\n                                                                               theoretical quantiles\n                                                                                                       2\n                2\n\n\n\n\n                                                                                                       1\n     rstudent\n                1\n\n\n\n\n                                                                                                       0\n                0\n                \u22122 \u22121\n\n\n\n\n                                                                                                       \u22122\n\n\n\n\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "\u22120.06                                                  0.0   0.2    0.4        0.6   0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.65                  0.75       0.85     0.95                                         \u22122    \u22121      0         1     2     3\n                                                        x2                                                               sample quantiles\n\n   Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.65",
      "section_title": "0.75       0.85     0.95                                         \u22122    \u22121      0         1     2     3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8. Residuals for the Box\u2013Cox model applied to the simulated data.\n\f286      11 Regression: Advanced Topics\n\n    Residual plots with response \u22121/Yi are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "Residuals for the Box\u2013Cox model applied to the simulated data.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8. We see in\npanel (a) that there is no sign of heteroskedasticity, since the vertical scatter\nof the residuals does not change from left to right. In panels (b) and (c) we\nsee uniform vertical scatter which shows that the model that is quadratic in\nX1 and linear in X2 \ufb01ts \u22121/Yi well. Finally, in panel (d), we see that the\nresiduals appear normally distributed.                                         \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "We see in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6 Binary Regression\nA binary response Y can take only two values, 0 or 1, which code two possible\noutcomes, for example, that a company goes into default on its loans or that\nit does not default. Binary regression models the conditional probability that\na binary response is 1, given the values of the predictors Xi,1 , . . . , Xi,p . Since\na probability is constrained to lie between 0 and 1, a linear model is not\nappropriate for a binary response. However, linear models are so convenient\nthat one would like a model that has many of the features of a linear model.\nThis has motivated the development of generalized linear models, often called\nGLMs.\n    Generalized linear models for binary responses are of the form\n\n      P (Yi = 1|Xi,1 , . . . , Xi,p ) = H(\u03b20 + \u03b21 Xi,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xi,p ) = H(xT\n                                                                                i \u03b2),\n\nwhere H(x) is a function that increases from 0 to 1 as x increases from \u2212\u221e\nto \u221e, so that H(x) is a CDF, and the last expression uses the vector notation\nof (11.1). The most common GLMs for a binary responses are probit regres-\nsion, where H(x) = \u03a6(x), the N (0, 1) CDF; and logistic regression, where\nH(x) is the logistic CDF, which is H(x) = 1/{1 + exp(\u2212x)}. The parameter\nvector \u03b2 can be estimated by maximum likelihood. Assume that conditional on\nx1 , . . . , xn the binary responses Y1 , . . . , Yn are mutually independent. Then,\nusing (A.8), the likelihood (conditional on x1 , . . . , xn ) is\n\n                         5\n                         n\n                                -    . Yi             1\u2212Yi\n                               H xT\n                                  i\u03b2      1 \u2212 H(xT\n                                                 i \u03b2)      .                     (11.27)\n                         i=1\n\nThe MLEs can be found by standard software, e.g., the function glm() in R.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "Binary Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.7. Who gets a credit card ?\n\n    In this example, we will analyze the data in the CreditCard data set in\nR\u2019s AER package. The following variables are included in the data set:\n\n 1. card = Was the application for a credit card accepted?\n 2. reports = Number of major derogatory reports\n 3. income = Yearly income (in USD 10,000)\n 4. age = Age in years plus 12ths of a year\n\f                                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.7",
      "section_title": "Who gets a credit card ?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6 Binary Regression                                      287\n\n 5. owner = Does the individual own his or her home?\n 6. dependents = Number of dependents\n 7. months = Months living at current address\n 8. share = Ratio of monthly credit card expenditure to yearly income\n 9. selfemp = Is the individual self-employed?\n10. majorcards = Number of major credit cards held\n11. active = Number of active credit accounts\n12. expenditure = Average monthly credit card expenditure\n\n    The \ufb01rst variable, card, is binary and will be the response. Variables 2\u20138\nwill be used as predictors. The goal of the analysis is to discover which of\nthe predictors in\ufb02uences the probability that an application is accepted. R\u2019s\ndocumentation mentions that there are some values of the variable age under\none year. These cases must be in error and they were deleted from the analysis.\nFigure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "Binary Regression                                      287",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9 contains histograms of the predictors. The variable share is highly\nright-skewed, so log(share) will be used in the analysis. The variable reports\n\n                               reports                                         income                                                    share\n\n\n\n                                                                                                                  1000\n                                                                500\n    Frequency\n\n\n\n\n                                                    Frequency\n\n\n\n\n                                                                                                      Frequency\n                600\n\n\n\n\n                                                                0 200\n\n\n\n\n                                                                                                                  0 400\n                0\n\n\n\n\n                          0    4     8     12                            0     4        8   12                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "contains histograms of the predictors. The variable share is highly",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0       0.4       0.8\n                               reports                                          income                                                    share\n\n\n\n                                   age                                         owner                                                 dependents\n                150 300\n    Frequency\n\n\n\n\n                                                                                                      Frequency\n                                                                                                                  300\n                0\n\n\n\n\n                                                                                                                  0\n\n\n\n\n                          20   40    60        80                                                                                0       2     4         6\n                                   age                                             no yes                                             dependents\n\n\n                               months                                         log(share)                                        log(reports+1)\n                                                                                                                  0 400 1000\n                                                                                                      Frequency\n    Frequency\n\n\n\n\n                                                    Frequency\n                                                                200\n                400\n                0\n\n\n\n\n                                                                0\n\n\n\n\n                          0    200       400                            \u221210        \u22126       \u22122 0                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4       0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0      1.0    2.0\n                               months                                          log(share)                                            log(reports + 1)\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1.0    2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9. Histograms of variables for potential use in a model to predict whether\na credit card application will be accepted.\n\f288      11 Regression: Advanced Topics\n\nis also extremely right-skewed; most values of reports are 0 or 1 but the\nmaximum value is 14. To reduce the skewness, log(reports+1) will be used\ninstead of reports. The \u201c1\u201d is added to avoid taking the logarithm of 0. There\nare no assumptions in regression about the distributions of the predictors, so\nskewed predictor variables can, in principle, be used. However, highly skewed\npredictors have high-leverage points and are less likely to be linearly related\nto the response. It is a good idea at least to consider transformation of highly\nskewed predictors. In fact, the logistic model was also \ufb01t with reports and\nshare untransformed, but this increased AIC by more than 3 compared to\nusing the transformed predictors.\n    First, a logistic regression model is \ufb01t with all seven predictors using the\nglm() function. The R code is:\n      library(\"AER\")\n      library(\"faraway\")\n      data(\"CreditCard\")\n      CreditCard_clean = CreditCard[CreditCard$age > 18, ]\n      names(CreditCard)\n      fit1 = glm(card ~ log(reports + 1) + income + log(share) + age\n         + owner + dependents + months,\n         family = \"binomial\", data = CreditCard_clean)\n      summary(fit1)\n      stepAIC(fit1)\n\n      Call:\n      glm(formula = card ~ log(reports + 1) + income + log(share) +\n          age + owner + dependents + months, family = \"binomial\",\n          data = CreditCard_clean)\n\n      Coefficients:\n                        Estimate Std. Error z value Pr(>|z|)\n      (Intercept)      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "Histograms of variables for potential use in a model to predict whether",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "18, ]\n      names(CreditCard)\n      fit1 = glm(card ~ log(reports + 1) + income + log(share) + age\n         + owner + dependents + months,\n         family = \"binomial\", data = CreditCard_clean)\n      summary(fit1)\n      stepAIC(fit1)",
        "start": 1123,
        "end": 1360
      }
    ]
  },
  {
    "content": "21.473930   3.674325   5.844 5.09e-09 ***\n      log(reports + 1) -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.473930",
      "section_title": "3.674325   5.844 5.09e-09 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.908644   1.097604 -2.650 0.00805 **\n      income            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.908644",
      "section_title": "1.097604 -2.650 0.00805 **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.903315   0.189754   4.760 1.93e-06 ***\n      log(share)        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.903315",
      "section_title": "0.189754   4.760 1.93e-06 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.422980   0.530499   6.452 1.10e-10 ***\n      age               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.422980",
      "section_title": "0.530499   6.452 1.10e-10 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.022682   0.021895   1.036 0.30024\n      owneryes          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.022682",
      "section_title": "0.021895   1.036 0.30024",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.705171   0.533070   1.323 0.18589\n      dependents       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.705171",
      "section_title": "0.533070   1.323 0.18589",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.664933   0.267404 -2.487 0.01290 *\n      months           -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.664933",
      "section_title": "0.267404 -2.487 0.01290 *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005723   0.003988 -1.435 0.15130\n      ---\n\n      (Dispersion parameter for binomial family taken to be 1)\n\n          Null deviance: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005723",
      "section_title": "0.003988 -1.435 0.15130",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1398.53   on 1311   degrees of freedom\n      Residual deviance: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1398.53",
      "section_title": "on 1311   degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "139.79    on 1304   degrees of freedom\n      AIC: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "139.79",
      "section_title": "on 1304   degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "155.79\n\n      Number of Fisher Scoring iterations: 11\n\f                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "155.79",
      "section_title": "Number of Fisher Scoring iterations: 11",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.6 Binary Regression     289\n\nSeveral of the regressors have large p-values, so stepAIC() was used to \ufb01nd\na more parsimonious model. The \ufb01nal step where no more variables were\ndeleted is\n\n   Step: AIC=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.6",
      "section_title": "Binary Regression     289",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "154.22\n   card ~ log(reports + 1) + income + log(share) + dependents\n\n                        Df Deviance    AIC\n   <none>                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "154.22",
      "section_title": "card ~ log(reports + 1) + income + log(share) + dependents",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "",
        "start": 121,
        "end": 142
      }
    ]
  },
  {
    "content": "144.22 154.22\n   - dependents          1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "144.22",
      "section_title": "154.22",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "150.28 158.28\n   - log(reports + 1)    1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "150.28",
      "section_title": "158.28",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "164.18 172.18\n   - income              1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "164.18",
      "section_title": "172.18",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "173.62 181.62\n   - log(share)          1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "173.62",
      "section_title": "181.62",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1079.61 1087.61\n\nBelow is the \ufb01t using the model selected by stepAIC(). For convenience later,\neach of the regressors was mean-centered; \u201c_c\u201d appended to a variable name\nindicates centering.\n\n   glm(formula = card ~ log_reports_c + income_c + log_share_c +\n       dependents_c, family = \"binomial\", data = CreditCard_clean)\n\n   Coefficients:\n                   Estimate Std. Error z value Pr(>|z|)\n   (Intercept)       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1079.61",
      "section_title": "1087.61",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5238     1.7213   5.533 3.15e-08 ***\n   log_reports_c    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5238",
      "section_title": "1.7213   5.533 3.15e-08 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.8953     1.0866 -2.664 0.00771 **\n   income_c          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.8953",
      "section_title": "1.0866 -2.664 0.00771 **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8717     0.1724   5.056 4.28e-07 ***\n   log_share_c       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8717",
      "section_title": "0.1724   5.056 4.28e-07 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.3102     0.4942   6.698 2.11e-11 ***\n   dependents_c     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.3102",
      "section_title": "0.4942   6.698 2.11e-11 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5506     0.2505 -2.198 0.02793 *\n   ---\n\n   (Dispersion parameter for binomial family taken to be 1)\n\n       Null deviance: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5506",
      "section_title": "0.2505 -2.198 0.02793 *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1398.53      on 1311   degrees of freedom\n   Residual deviance: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1398.53",
      "section_title": "on 1311   degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "144.22       on 1307   degrees of freedom\n   AIC: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "144.22",
      "section_title": "on 1307   degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "154.22\n\n   Number of Fisher Scoring iterations: 11\n\n    It is important to understand what the logistic regression model is telling\nus about the probability of an application being accepted. Qualitatively, we see\nthat the probability of having an application accepted increases with income\nand share and decreases with reports and dependents. To understand these\ne\ufb00ects quantitatively, \ufb01rst consider the intercept. Since the predictors have\nbeen mean-centered, the probability of an application being accepted when\nall variables are at their mean is simply H(9.5238) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "154.22",
      "section_title": "Number of Fisher Scoring iterations: 11",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.999927. Since reports\nand dependents are integer-valued and cannot exactly equal their means,\nthis probability only provides an idea of what the intercept ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.999927",
      "section_title": "Since reports",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5238 signi\ufb01es.\nFigure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5238",
      "section_title": "signi\ufb01es.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.10 plots the probability that a credit card application is accepted as\n\f290               11 Regression: Advanced Topics\n\nfunctions of reports, income, log(share), and dependents. In each plot, the\nother variables are \ufb01xed at their means. Clearly, the variable with the largest\ne\ufb00ect is share, the ratio of monthly credit card expenditure to yearly income.\nWe see that applicants who spend little of their income through credit cards\nare unlikely to have their applications accepted.\n    In Fig. 11.11, panel (a) is a plot of card, which takes value 0 if an ap-\nplication is rejected and 1 if it is accepted, versus log(share). It should be\nemphasized that panel (a) is a plot of the data, not a \ufb01t from the model.\nWe see that an application is always accepted if log(share) exceeds \u22126,\nwhich translates into share exceeding ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.10",
      "section_title": "plots the probability that a credit card application is accepted as",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0025. Thus, in this data set, among\nthe group of applicants whose average monthly credit card expenses exceeded\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0025",
      "section_title": "Thus, in this data set, among",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 % of yearly income, all credit card applications were accepted. How do\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "% of yearly income, all credit card applications were accepted. How do",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                          0.8\n      P(accept)\n\n\n\n\n                                                              P(accept)\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                          0.4\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                          0.0\n\n\n\n\n                         0   2    4     6   8      10    14                     0   2   4       6       8       10 12 14\n                                       reports                                                  income\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                          0.8\n      P(accept)\n\n\n\n\n                                                              P(accept)\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                          0.4\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                          0.0\n\n\n\n\n                             \u22128   \u22126        \u22124      \u22122   0                      0   1       2       3       4     5   6\n                                      log(share)                                        dependents\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.10. Plots of probabilities of a credit card application being accepted as\nfunctions of single predictors with other predictors \ufb01xed at their means. The variables\nvary over their ranges in the data.\n\n\nthese applicants look on the other variables? Panels (b)\u2013(d) plot reports,\nincome, and majorcards versus log(share). The variable majorcards was\nnot used in the logistic regression analysis but is included here.\n   An odd feature in Fig. 11.11c is a group of points following a smooth curve.\nThis is a group of 316 applications who had the product of share times income\nexactly equal to 0.0012, the minimum value of this product. Oddly, share is\nnever 0. Perhaps because of some coding artifact, these 316 had 0 credit card\nexpenditures rather than the reported values. Another interesting feature of\nthe data is that among these 316 applications, only 21 were accepted. Among\nthe remaining 996 applications, all were accepted.\n\f                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.10",
      "section_title": "Plots of probabilities of a credit card application being accepted as",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.7 Linearizing a Nonlinear Model                291\n\n    Besides illustrating logistic regression, this example demonstrates that\nreal-world data often contain errors, or perhaps we should call them idiosyn-\ncracies, and that a thorough graphical analysis of the data is always a good\nthing.                                                                     \u0002\n\n\n     a                                                 b\n\n\n\n\n                                                                    12\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.7",
      "section_title": "Linearizing a Nonlinear Model                291",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                       reports\n     card\n\n\n\n\n                                                                    8\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "reports",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                    4\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                    0\n                       \u22128   \u22126   \u22124      \u22122    0                          \u22128   \u22126   \u22124      \u22122   0\n                            log(share)                                         log(share)\n\n\n     c                                                 d\n              12\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                                       majorcards\n     income\n              8\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "majorcards",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n              4\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n              0\n\n\n\n\n                       \u22128   \u22126   \u22124      \u22122    0                          \u22128   \u22126   \u22124      \u22122   0\n                            log(share)                                         log(share)\n\n                    Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.11. Plots of log(share) versus other variables.\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.11",
      "section_title": "Plots of log(share) versus other variables.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.7 Linearizing a Nonlinear Model\n\nSometimes a nonlinear model can be linearized by applying a transformation\nto both the model and the response. In such cases, should one use a linearizing\ntransformation or, instead, apply nonlinear regression to the original model?\nThe answer is that linearization can sometimes be a good thing, but not\nalways. Fortunately, residual analysis can help us decide whether a linearizing\ntransformation should be used.\n    For example, consider the model\n\n                                         Yi = \u03b21 exp(\u03b22 Xi ).                                    (11.28)\n\nThis model is \u201cequivalent\u201d to the linear model\n\n                                         log(Yi ) = \u03b1 + \u03b22 Xi ,                                  (11.29)\n\nwhere \u03b1 = log(\u03b21 ). \u201cEquivalent\u201d is in quotes, because the two models are no\nlonger equivalent when noise is present.\n\f292             11 Regression: Advanced Topics\n\n      Suppose (11.28) has i.i.d. additive noise, so that\n                                                Yi = \u03b21 exp(\u03b22 Xi ) + \u0017i ,                                        (11.30)\nwhere \u00171 , . . . , \u0017n are i.i.d. Then applying the log transformation to (11.29)\ngives us the model\n                                         log(Yi ) = log {\u03b21 exp(\u03b22 Xi ) + \u0017i }                                    (11.31)\nwith nonadditive noise. Because the noise is not additive, the variation of\nlog(Yi ) about the model log {\u03b21 exp(\u03b22 Xi )} will have nonconstant variation\nand skewness, even if \u00171 , . . . , \u0017n are i.i.d. Gaussian.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.7",
      "section_title": "Linearizing a Nonlinear Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8. Linearizing transformation\u2014Simulated data\n\n    Figure 11.12a shows a simulated sample from model (11.28) with \u03b21 = 1,\n\u03b22 = \u22121, and \u03c3 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "Linearizing transformation\u2014Simulated data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02. The Xi are equally spaced from \u22121 to 2.5 by\nincrements of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "The Xi are equally spaced from \u22121 to 2.5 by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025. Panel (b) shows log(Yi ) plotted against Xi . One can see\nthat the transformation has linearized the relationship between the variables\nbut has introduced nonconstant residual variation. Panels (c) and (d) show\nresidual plots using the linearized model. Notice the nonlinear normal plot\nand the severe nonconstant variance.                                        \u0002\n\n\n        a                                                         b\n                                                                             0\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.025",
      "section_title": "Panel (b) shows log(Yi ) plotted against Xi . One can see",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                  log(y)\n        y\n\n\n\n\n                                                                             \u22122\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "log(y)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                             \u22124\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22124",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                          \u22121.0     0.0      1.0       2.0                           \u22121.0   0.0     1.0      2.0\n                                           x                                                      x\n\n        c                                                         d\n                                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22121.0     0.0      1.0       2.0                           \u22121.0   0.0     1.0      2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                    2\n        quantiles\n\n\n\n\n                                                                  residual\n                                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                    0\n\n\n\n\n                                                                             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                    \u22122\n\n\n\n\n                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5         \u22120.5       0.5     1.0                     \u22122.5   \u22121.5    \u22120.5     0.5\n                                   sample quantiles                                          fitted value\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u22120.5       0.5     1.0                     \u22122.5   \u22121.5    \u22120.5     0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12. Example where the log transformation linearizes a model but induces\nsubstantial heteroskedasticity and skewness. (a) Raw data. (b) Data after log trans-\nformation of the response. (c) Normal plot of residuals after linearization. (d) Ab-\nsolute residual plot after linearization.\n\f                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "Example where the log transformation linearizes a model but induces",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8 Robust Regression      293\n\n   Linearizing is not always a bad thing. Suppose the noise is multiplicative\nand lognormal so that (11.28) becomes\n\n               Yi = \u03b21 exp(\u03b22 Xi ) exp(\u0017i ) = \u03b21 exp(\u03b22 Xi + \u0017i ),         (11.32)\n\nwhere \u00171 , . . . , \u0017n are i.i.d. Gaussian. Then the log transformation converts\n(11.32) to\n                                log(Yi ) = \u03b1 + \u03b22 Xi + \u0017i ,             (11.33)\nwhich is a linear model satisfying all of the usual assumptions.\n   In summary, a linearizing transformation may or may not cause the data\nto better follow the assumptions of regression analysis. Residual analysis can\nhelp one decide whether a transformation is appropriate.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "Robust Regression      293",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.8 Robust Regression\nA robust regression estimator should be relatively immune to two types of\noutliers. The \ufb01rst are bad data, meaning contaminants that are not part of\nthe population, for example, due to undetected recording errors. The second\nare outliers due to the noise distribution having heavy tails. There are a large\nnumber of robust regression estimators, and their sheer number has been an\nimpediment to their use. Many data analysts are confused as to which robust\nestimator is best and consequently are reluctant to use any. Rather than\ndescribe many of these estimators, which might contribute to this problem,\nwe mention just one, the least-trimmed sum of squares estimator, often called\nthe LTS.\n    Recall the trimmed mean, a robust estimator of location for a univariate\nsample. The trimmed mean is simply the mean of the sample after a certain\npercentage of the largest observations and the same percentage of the smallest\nobservations have been removed. This trimming removes some non-outliers,\nwhich, under the ideal conditions of no outliers, causes some loss of precision,\nbut not an unacceptable amount. The trimming also removes outliers, and\nthis causes the estimator to be robust. Trimming is easy for a univariate\nsample because we know which observations to trim, the very largest and\nthe very smallest. This is not the case in regression. Consider the data in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.8",
      "section_title": "Robust Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.13. There are 26 observations that fall closely along a line plus two\nresidual outliers that are far from this line. Notice that the residual outliers\nhave neither extreme X-values nor extreme Y -values. They are outlying only\nrelative to the linear regression \ufb01t to the other data.\n    The residual outliers are obvious in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.13",
      "section_title": "There are 26 observations that fall closely along a line plus two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.13 because there is only a sin-\ngle predictor. When there are many predictors, outliers can only be identi\ufb01ed\nwhen we have a model and good estimates of the parameters in that model.\nThe di\ufb03culty, then, is that estimation of the parameters requires the identi-\n\ufb01cation of the outliers, and vice versa. One can see from the \ufb01gure that the\nleast-squares line is changed by including the residual outliers in the data used\n\f294    11 Regression: Advanced Topics\n\nfor estimation. In some cases, e.g., Fig. 10.1b, the e\ufb00ect of a residual outlier\ncan be so severe that it totally changes the least-squares estimates. This is\nlikely to happen if the residual outlier occurs at a high-leverage point.\n    The LTS estimator simultaneously identi\ufb01es residual outliers and estimates\nrobustly the parameters of a model. Let 0 < \u03b1 \u2264 1/2 be the trimming propor-\ntion and let k equal n\u03b1 rounded to an integer. The trimmed sum of squares\nabout a set of values of the regression parameters is de\ufb01ned as follows: Form\nthe residuals from the model evaluated at these parameters, square the residu-\nals, then order the squared residuals and remove the k largest, and \ufb01nally sum\nthe remaining squared residuals. The LTS estimates are the set of parameter\nvalues that minimize the trimmed sum of squares. The LTS estimator can be\ncomputed using the ltsReg() function in R\u2019s robust package.\n    If the noise distribution is heavy-tailed, then an alternative to a robust\nregression analysis is to use a heavy-tailed distribution as a model for the noise\nand then to estimate the parameters by maximum likelihood. For example,\none could assume that the noise has a double-exponential or t-distribution. In\nthe latter case, one could either estimate the degrees of freedom or simply \ufb01x\nthe degrees of freedom at a low value, which implies heavier tails; see Lange,\nLittle, and Taylor (1989). This strategy is called robust modeling rather than\nrobust estimation. The distinction is that in robust estimation one assumes\na fairly restrictive model such as a normal noise distribution, but \ufb01nds a\nrobust alternative to maximum likelihood. In robust modeling, one uses a\nmore \ufb02exible model so that maximum likelihood estimation is itself robust.\nWhen there is a single gross residual outlier, particularly at a high-leverage\npoint, robust regression is a better alternative than the MLE with a heavy-\ntailed noise distribution; see the next example.\n    Another possibility is that residual outliers are due to nonconstant stan-\ndard deviations, with the outliers mainly in the data with a higher noise\nstandard deviation. The remedy to this problem is to apply a variance stabi-\nlization transformation or to model the nonconstant standard deviation, say\nby one of the GARCH models discussed in Chap. 14.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.13",
      "section_title": "because there is only a sin-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9. Simulated data in Example 10.1\u2014Robust regression\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "Simulated data in Example 10.1\u2014Robust regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.14 compares least-squares \ufb01t, the LTS \ufb01t, and the MLE assuming\nt-distributed noise for the simulated data in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.14",
      "section_title": "compares least-squares \ufb01t, the LTS \ufb01t, and the MLE assuming",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1. In panel (a) with\nno residuals outliers, the three \ufb01ts coincide. In panels (b) and (c), the LTS \ufb01ts\nare not a\ufb00ected by the residual outliers and \ufb01t the nonoutlying data very well.\nIn these panels, the LS and MLE \ufb01ts are highly a\ufb00ected by the outlier and\nnearly identical. For these examples, the MLE assuming t-distributed noise is\nnot robust.                                                                    \u0002\n\f                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "In panel (a) with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9 Regression and Best Linear Prediction      295\n\n\n\n\n                   50\n                            *\n                   40\n                                                                   *\n                   30\n               Y\n                   20\n\n                                     l\n                                                     o   good data\n                                                         residual outliers\n                   10\n\n\n\n\n                                                     *   with outliers\n                                                         w/o outliers\n\n                        0        2       4       6          8           10\n                                             X\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "Regression and Best Linear Prediction      295",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.13. Straight-line regression with two residual outliers showing least-squares\n\ufb01ts with and without the outliers.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.13",
      "section_title": "Straight-line regression with two residual outliers showing least-squares",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9 Regression and Best Linear Prediction\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "Regression and Best Linear Prediction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.1 Best Linear Prediction\n\nOften we observe a random variable X and we want to predict an unobserved\nrandom variable Y that is related to X. For example, Y could be the future\nprice of an asset and X might be the most recent change in that asset\u2019s price.\nPrediction has many practical uses, and it is also important in theoretical\nstudies.\n    The predictor of Y that minimizes the expected squared prediction error\nis E(Y |X) (see Appendix A.19), but E(Y |X) is often a nonlinear function\nof X and di\ufb03cult to compute. A common solution to this di\ufb03culty it to\nconsider only linear functions of X as possible predictors. This is called linear\nprediction. In this section, we will show that linear prediction is closely related\nto linear regression.\n    A linear predictor of Y based on X is a function \u03b20 + \u03b21 X where \u03b20 and\n\u03b21 are parameters that we can choose. Best linear prediction means \ufb01nding\n\u03b20 and \u03b21 so that expected squared prediction error, which is given by\n\n                                E{Y \u2212 (\u03b20 + \u03b21 X)}2 ,                         (11.34)\n\nis minimized. Doing this makes the predictor as close as possible, on average,\nto Y . The expected squared prediction error can be rewritten as\n\n    E{Y \u2212 (\u03b20 + \u03b21 X)}2\n\n        = E(Y 2 ) \u2212 2\u03b20 E(Y ) \u2212 2\u03b21 E(XY ) + \u03b202 + 2\u03b20 \u03b21 E(X) + \u03b212 E(X 2 ).\n\f296        11 Regression: Advanced Topics\n\na                                                      b\n      60\n\n\n\n\n                                                            60\n                   LTS                                                  LTS\n                   LS                                                   LS\n      50\n\n\n\n\n                                                            50\n                   t                              *                     t\n      40\n\n\n\n\n                                                            40\n      30\n\n\n\n\n                                                            30\ny\n\n\n\n\n                                                        y\n      20\n\n\n\n\n                                                            20\n      10\n\n\n\n\n                                                            10\n                 ***                                                   ***\n           *******                                               *******                         *\n      0\n\n\n\n\n                                                            0\n           0       10        20       30   40     50             0     10     20       30   40   50\n                                  x                                                x\n\n\nc\n      60\n\n\n\n\n                   LTS\n                   LS\n      50\n\n\n\n\n                   t              *\n      40\n      30\ny\n      20\n      10\n\n\n\n\n                                           ***\n           *******\n      0\n\n\n\n\n               2         4            6    8      10\n                                  x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "1 Best Linear Prediction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.14. Simulated data in Example 10.1 with LS \ufb01ts (dashed red) and LTS \ufb01ts\n(solid black) and MLEs assuming t-distributed noise (dotted blue). In (a) the three\n\ufb01ts are too close together to distinguish between them. In (b) and (c) the LS and t\n\ufb01ts are nearly identical and di\ufb03cult to distinguish.\n\n\nTo \ufb01nd the minimizers, we set the partial derivatives of this expression to zero\nto obtain\n\n                                  0 = \u2212E(Y ) + \u03b20 + \u03b21 E(X),                                 (11.35)\n                                  0 = \u2212E(XY ) + \u03b20 E(X) + \u03b21 E(X 2 ).                        (11.36)\n\nAfter some algebra we \ufb01nd that\n                                                           2\n                                                \u03b21 = \u03c3XY /\u03c3X                                 (11.37)\n\nand\n                        \u03b20 = E(Y ) \u2212 \u03b21 E(X) = E(Y ) \u2212 \u03c3XY /\u03c3X\n                                                             2\n                                                               E(X).                         (11.38)\n\f                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.14",
      "section_title": "Simulated data in Example 10.1 with LS \ufb01ts (dashed red) and LTS \ufb01ts",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9 Regression and Best Linear Prediction      297\n\nOne can check that the matrix of second derivatives of (11.34) is positive\nde\ufb01nite so that the solution (\u03b20 , \u03b21 ) to (11.35) and (11.36) minimizes (11.34).\nThus, the best linear predictor of Y is\n                                          \u03c3XY\n          Y\u0002 Lin (X) = \u03b20 + \u03b21 X = E(Y ) + 2 {X \u2212 E(X)}.                   (11.39)\n                                           \u03c3X\n                                                                               2\nIn practice, (11.39) cannot be used directly unless E(X), E(Y ), \u03c3XY , and \u03c3X\nare known, which is often not the case. Linear regression analysis is essentially\nthe use of (11.39) with these unknown parameters replaced by least-squares\nestimates\u2014see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "Regression and Best Linear Prediction      297",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.3.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.2 Prediction Error in Best Linear Prediction\n\nIn this section, assume that Y\u0002 is the best linear predictor of Y . The prediction\nerror is Y \u2212 Y\u0002 . It is easy to show that E{Y \u2212 Y\u0002 } = 0 so that the prediction\nis unbiased. With a little algebra we can show that the expected squared\nprediction error is\n                                          2\n                                     \u03c3\n                  E{Y \u2212 Y\u0002 }2 = \u03c3Y2 \u2212 XY\n                                       2 = \u03c3Y (1 \u2212 \u03c1XY ).\n                                            2       2\n                                                                           (11.40)\n                                      \u03c3X\n\n    How much does X help us predict Y ? To answer this question, notice \ufb01rst\nthat if we do not observe X, then we must predict Y using a constant, which\nwe denote by c. It is easy to show that the best predictor has c equal to E(Y ).\nNotice \ufb01rst that the expected squared prediction error is E(Y \u2212 c)2 . Some\nalgebra shows that\n\n                     E(Y \u2212 c)2 = Var(Y ) + {c \u2212 E(Y )}2 ,                  (11.41)\n\nwhich, since Var(Y ) does not depend on c, shows that the expected squared\nprediction error is minimized by c = E(Y ). Thus, when X is unobserved, the\nbest predictor of Y is E(Y ) and the expected squared prediction error is \u03c3Y2 ,\nbut when X is observed, then the expected squared prediction error is smaller,\n\u03c3Y2 (1 \u2212 \u03c12XY ). Therefore, \u03c12XY is the fraction by which the prediction error is\nreduced when X is known. This is an important fact that we will see again.\n\nResult ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "2 Prediction Error in Best Linear Prediction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1 Prediction when Y is independent of all available information:\n    If Y is independent of all presently available information, that is, Y is\nindependent of all random variables that have been observed, then the best\npredictor of Y is E(Y ) and the expected value of the squared prediction error\nis \u03c3Y2 . We say that Y \u201ccannot be predicted\u201d when there exists no predictor\nbetter than its expected value.\n\f298    11 Regression: Advanced Topics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "Prediction when Y is independent of all available information:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.3 Regression Is Empirical Best Linear Prediction\n\nFor the case of a single predictor, note the similarity between the best linear\npredictor,\n                                     \u03c3XY\n                        Y\u0002 = E(Y ) + 2 {X \u2212 E(X)},\n                                      \u03c3X\nand the least-squares line,\n                                      sXY\n                              Y\u0002 = Y + 2 (X \u2212 X).\n                                       sX\n\n    The least-squares line is a sample version of the best linear predictor. Also,\n\u03c12XY , the squared correlation between X and Y , is the fraction of variation in\nY that can be predicted using the linear predictor, and the sample version of\n\u03c12XY is R2 = rXY\n               2\n                   = rY2\u0002 Y .\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "3 Regression Is Empirical Best Linear Prediction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.4 Multivariate Linear Prediction\n\nSo far we have assumed that there is only a single random variable, X, avail-\nable to predict Y . More commonly, Y is predicted using a set of observed\nrandom variables, X1 , . . . , Xn .\n   Let Y and X by p \u00d7 1 and q \u00d7 1 random vectors. As before in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "4 Multivariate Linear Prediction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3.1,\nde\ufb01ne\n                   \u03a3 Y,X = E{Y \u2212 E(Y )}{X \u2212 E(X)}T ,\nso that the i, jth element of \u03a3 Y,X is the covariance between Yi and Xj . Then\nthe best linear predictor of Y given X is\n\n                    Y\u0002 = E(Y ) + \u03a3 Y,X \u03a3 \u22121\n                                         X {X \u2212 E(X)}.                    (11.42)\n\nNote the similarity between (11.39) and (11.42), the best linear predictors in\nthe univariate and multivariate cases.\n   The sample analog of multivariate linear prediction is multiple regression.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.3",
      "section_title": "1,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.10 Regression Hedging\n\nAn interesting application of regression is determining the optimal hedge of\na bond position. Market makers buy securities at a bid price and make a\npro\ufb01t by selling them at a higher ask price. Suppose a market maker has just\npurchased a bond from a pension fund. Ideally, the market maker would sell\nthe bond immediately after purchasing it. However, many bonds are illiquid,\nso it may take some time before the bond can be sold. During the period that\na market maker is holding a bond, the market maker is at risk that the bond\nprice could drop due to a change in interest rates. The change could wipe out\nthe pro\ufb01t due to the small bid\u2013ask spread. The market maker would prefer to\n\f                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.10",
      "section_title": "Regression Hedging",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.10 Regression Hedging      299\n\nhedge this risk by assuming another risk which is likely to be in the opposite\ndirection. To hedge the interest-rate risk of the bond being held, the market\nmaker can sell other, more liquid, bonds short. Suppose that the market maker\ndecides to sell short a 30-year Treasury bond, which is more liquid.\n    Regression hedging determines the optimal amount of the 30-year Treasury\nbonds to sell short to hedge the risk of the bond just purchased. The goal is\nthat the price of the portfolio long in the \ufb01rst bond and short in the Treasury\nbond changes as little as possible as yields change. Suppose the \ufb01rst bond has\na maturity of 25 years. One can determine the sensitivity of price to yield\nchanges using results from Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.10",
      "section_title": "Regression Hedging      299",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.8. Let y30 be the yield on 30-year bonds,\nlet P30 be the price of $1 in face amount of 30-year bonds, and let DUR30 be\nthe duration. The change in price, \u0394P30 , and the change in yield, \u0394y30 , are\nrelated by\n                           \u0394P30 \u2248 \u2212P30 DUR30 \u0394y30\nfor small values of \u0394y30 . A similar result holds for 25-year bonds.\n    Consider a portfolio that holds face amount F25 in 25-year bonds and is\nshort face amount F30 in 30-year bonds. The value of the portfolio is\n                                F25 P25 \u2212 F30 P30 .\nIf \u0394y25 and \u0394y30 are the changes in the yields, then the change in value of\nthe portfolio is approximately\n                 {F30 P30 DUR30 \u0394y30 \u2212 F25 P25 DUR25 \u0394y25 }.                (11.43)\nSuppose that the regression of \u0394y30 on \u0394y25 is\n                              \u0394y30 = \u03b2\u00020 + \u03b2\u00021 \u0394y25                         (11.44)\n\nand \u03b2\u00020 \u2248 0, as is usually the case for regression of changes in interest rates, as\nin Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.8",
      "section_title": "Let y30 be the yield on 30-year bonds,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1. Substituting (11.44) into (11.43), the change in price of the\nportfolio is approximately\n                   {F30 P30 DUR30 \u03b2\u00021 \u2212 F25 P25 DUR25 }\u0394y25 .               (11.45)\nThis change is approximately zero for all values of \u0394y25 if\n                                         P25 DUR25\n                            F30 = F25                 .                     (11.46)\n                                        P30 DUR30 \u03b2\u00021\nEquation (11.46) tells us how much face value of the 30-year bond to sell short\nin order to hedge F25 face value of the 25-year bond. All quantities on the\nright-hand side of (11.46) are known or readily calculated: F25 is the current\nposition in the 25-year bond, P25 and P30 are known bond prices, calculation\nof DUR25 and DUR30 is discussed in Chap. 3, and \u03b2\u00021 is the slope of the\nregression of \u0394y30 on \u0394y25 .\n    The higher the R2 of the regression, the better the hedge works. Hedging\nwith two or more liquid bonds, say a 30-year and a 10-year, can be done by\nmultiple regression and might produce a better hedge.\n\f300     11 Regression: Advanced Topics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "Substituting (11.44) into (11.43), the change in price of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.11 Bibliographic Notes\nAtkinson (1985) has nice coverage of transformations and residual plotting and\nmany good examples. For more information on nonlinear regression, see Bates\nand Watts (1988) and Seber and Wild (1989). Graphical methods for detect-\ning a nonconstant variance, transform-both-sides regression, and weighting\nare discussed in Carroll and Ruppert (1988). Hosmer and Lemeshow (2000)\nis an in-depth treatment of logistic regression. Faraway (2006) covers general-\nized linear models including logistic regression. See Tuckman (2002) for more\ndiscussion of regression hedging.\n    The Nelson\u2013Siegel and Svensson models are from Nelson and Siegel (1985)\nand Svensson (1994).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.11",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12.1 Nonlinear Regression\n\nIn this section, you will be \ufb01tting short-rate models. Let rt be the short rate\n(the risk-free rate for short-term borrowing) at time t. It is assumed that the\nshort rate satis\ufb01es the stochastic di\ufb00erential equation\n\n                         drt = \u03bc(t, rt ) dt + \u03c3(t, rt ) dWt ,               (11.47)\n\nwhere \u03bc(t, rt ) is a drift function, \u03c3(t, rt ) is a volatility function, and Wt is a\nstandard Brownian motion. We will use a discrete approximation to (11.47):\n\n                (rt \u2212 rt\u22121 ) = \u03bc(t \u2212 1, rt\u22121 ) + \u03c3(t \u2212 1, rt\u22121 ) \u0017t\u22121       (11.48)\n\nwhere \u00171 , . . . , \u0017n\u22121 are i.i.d. N (0, 1).\n   We will start with the Chan, Karolyi, Longsta\ufb00, and Sanders (1992)\n(CKLS) model, which assumes that\n\n                            \u03bc(t, r) = \u03bc(r) = a (\u03b8 \u2212 r)                      (11.49)\n\nfor some unknown parameters a and \u03b8, and\n\n                                   \u03c3(t, r) = \u03c3r\u03b3                            (11.50)\n\nfor some \u03c3 and \u03b3. Be careful to distinguish between the volatility function\n\u03c3(t, r) and the constant volatility parameter \u03c3.\n    We will use the Irates data set in the Ecdat package. This data set has\ninterests rates for maturities from 1 to 120 months. We will use the \ufb01rst\ncolumn, which has the one-month maturity rates, since we want the short\nrate.\n    Run the following code to input the data, compute the lagged and di\ufb00er-\nenced short-rate series, and construct some basic plots.\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "1 Nonlinear Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12 R Lab     301\n\n   library(Ecdat)\n   data(Irates)\n   r1 = Irates[,1]\n   n = length(r1)\n   lag_r1 = lag(r1)[-n]\n   delta_r1 = diff(r1)\n   n = length(lag_r1)\n   par(mfrow = c(3, 2))\n   plot(r1, main = \"(a)\")\n   plot(delta_r1, main = \"(b)\")\n   plot(delta_r1^2, main = \"(c)\")\n   plot(lag_r1, delta_r1, main = \"(d)\")\n   plot(lag_r1, delta_r1^2, main = \"(e)\")\n\nProblem 1 What is the maturity of the interest rates in the \ufb01rst column?\nWhat is the sampling frequency of this data set\u2014daily, weekly, monthly, or\nquarterly? What country are the data from? Are the rates expressed as per-\ncentages or fractions (decimals)?\n\n    In the plot you have just created, panels (a), (b), and (c) show how the\nshort rate, changes in the short rate, and squared changes in the short rate\ndepend on time. The plots of changes in the short rate are useful for choosing\nthe drift \u03bc(t \u2212 1, rt\u22121 ) while squared changes in the short rate are helpful for\nselecting the volatility \u03c3(t \u2212 1, rt\u22121 ).\n\nProblem 2 Model (11.49) states that \u03bc(t, r) = \u03bc(r), that is, that the drift\ndoes not depend on t. Use your plots to discuss whether this assumption seems\nvalid. Assuming for the moment that this assumption is valid, any trend in\nthe plot in panel (d) would give us information about the form of \u03bc(r). Do\nyou see any trend?\n\nNow run the following code to \ufb01t model (11.49) and \ufb01ll in the \ufb01rst two panels\nof a \ufb01gure. This \ufb01gure will be continued next.\n   #   CKLS (Chan, Karolyi, Longstaff, Sanders)\n\n   nlmod_CKLS = nls(delta_r1 ~ a * (theta-lag_r1),\n      start=list(theta = 5,   a = 0.01),\n      control = list(maxiter = 200))\n   param = summary(nlmod_CKLS)$parameters[ , 1]\n   par(mfrow = c(2, 2))\n   t = seq(from = 1946, to = 1991 + 2 / 12, length = n)\n   plot(lag_r1, ylim = c(0, 16), ylab = \"rate and theta\",\n      main = \"(a)\", type = \"l\")\n   abline(h = param[1], lwd = 2, col = \"red\")\n\nProblem 3 What are the estimates of a and \u03b8 and their 95 % con\ufb01dence\nintervals?\n\f302      11 Regression: Advanced Topics\n\nNote that the nonlinear regression analysis estimates \u03c3 2 (r), not \u03c3(r), since\nthe response variable is the squared residual. Here A = \u03c3 2 and B = 2\u03b3.\n      res_sq = residuals(nlmod_CKLS)^2\n      nlmod_CKLS_res <- nls(res_sq ~ A*lag_r1^B,\n         start = list(A = 0.2, B = 1/2))\n      param2 = summary(nlmod_CKLS_res)$parameters[ , 1]\n      plot(lag_r1, sqrt(res_sq), pch = 5, ylim = c(0, 6),\n         main = \"(b)\")\n      attach(as.list(param2))\n      curve(sqrt(A * x^B), add = T, col = \"red\", lwd = 3)\n\nProblem 4 What are the estimates of \u03c3 and \u03b3 and their 95 % con\ufb01dence\nintervals?\n\nFinally, re\ufb01t model (11.49) using weighted least squares.\n\n      nlmod_CKLS_wt = nls(delta_r1 ~ a * (theta-lag_r1),\n         start = list(theta = 5, a = 0.01),\n         control = list(maxiter = 200),\n         weights = 1 / fitted(nlmod_CKLS_res))\n\n      plot(lag_r1, ylim = c(0, 16), ylab = \"rate and theta\",\n         main = \"(c)\", type = \"l\")\n      param3 = summary(nlmod_CKLS_wt)$parameters[ , 1]\n      abline(h = param3[1], lwd = 2, col = \"red\")\n\nProblem 5 How do the unweighted estimate of \u03b8 shown in panel (a) and\nthe weighted estimate plotted in panel (d) di\ufb00er? Why do they di\ufb00er in this\nmanner?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "R Lab     301",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12.2 Response Transformations\n\nThis section uses the HousePrices data set in the AER package. This is a\ncross-sectional data set on house prices and other features, e.g., the number\nof bedrooms of houses in Windsor, Ontario. The data were gathered during\nthe summer of 1987. Accurate modeling of house prices is important for the\nmortgage industry. Run the code below to read the data and regress price on\nthe other variables; the period on the right-hand side of the formula \u201cprice~.\u201d\nspeci\ufb01es that the predictors should include all variables except, of course, the\nresponse.\n\n      library(AER)\n      data(HousePrices)\n      fit1 = lm(price ~ ., data = HousePrices)\n      summary(fit1)\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "2 Response Transformations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12 R Lab     303\n\nNext construct a pro\ufb01le log-likelihood plot for the transformation parameter\n\u03b1 in model (11.25)\n\n   library(MASS)\n   fit2 = boxcox(fit1, xlab = expression(alpha))\n\nProblem 6 What is the MLE of \u03b1? (Hint: Type ?boxcox to learn what is\nreturned by this function.)\n\n\nNext, \ufb01t a linear model with price transformed by \u03b1     \u0002 (the MLE). Here the\nfunction bcPower() in the AER package computes a Box\u2013Cox transformation\nfor a given value of \u03b1 and must be distinguished from boxcox(), which com-\nputes the pro\ufb01le log-likelihood for \u03b1. In the following code, replace 1/2 by the\nMLE of \u03b1.\n\n   library(car)\n   alphahat = 1/2\n   fit3 = lm(bcPower(price, alphahat) ~ ., data = HousePrices)\n   summary(fit3)\n   AIC(fit1)\n   AIC(fit3)\n\nProblem 7 Does the Box\u2013Cox transformation o\ufb00er a substantial improve-\nment in \ufb01t compared to the regression with no transformation of price?\n\n\nProblem 8 Would it be worthwhile to check the residuals for correlation?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "R Lab     303",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12.3 Binary Regression: Who Owns an Air Conditioner?\n\nThis section uses the HousePrices data set used in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "3 Binary Regression: Who Owns an Air Conditioner?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12.2. The goal\nhere is to investigate how the presence or absence of air conditioning is related\nto the other variables. The code below \ufb01ts a logistic regression model to all\npotential predictor variables and then uses stepAIC() to \ufb01nd a parsimonious\nmodel.\n\n   library(AER)\n   data(HousePrices)\n   fit1 = glm(aircon ~ ., family = \"binomial\",\n      data = HousePrices)\n   summary(fit1)\n   library(MASS)\n   fit2 = stepAIC(fit1)\n   summary(fit2)\n\f304      11 Regression: Advanced Topics\n\nProblem 9 Which variables are most useful for predicting whether a home\nhas air conditioning? Describe qualitatively the relationships between these\nvariables and the variable aircon. Are there any variables in the model selected\nby stepAIC() that you think might be dropped?\n\n\nProblem 10 Estimate the probability that a house will have air conditioning\nif it has the following characteristics:\n\n      price lotsize bedrooms bathrooms stories driveway recreation\n      42000    5850        3         1       2      yes         no\n      fullbase gasheat garage prefer\n       yes      no      1     no\n\n(Hint: The R function plogis() computes the logistic function.)\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "2. The goal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.13 Exercises\n\n 1. When we were \ufb01nding the best linear predictor of Y given X, we derived\n    the equations\n\n                        0 = \u2212E(Y ) + \u03b20 + \u03b21 E(X)\n                        0 = \u2212E(XY ) + \u03b20 E(X) + \u03b21 E(X 2 ).\n\n      Show that their solution is\n                                           \u03c3XY\n                                    \u03b21 =     2\n                                            \u03c3X\n      and\n                                                     \u03c3XY\n                    \u03b20 = E(Y ) \u2212 \u03b21 E(X) = E(Y ) \u2212     2 E(X).\n                                                      \u03c3X\n 2. Suppose one has a long position of F20 face value in 20-year Treasury\n    bonds and wants to hedge this with short positions in both 10- and 30-\n    year Treasury bonds. The prices and durations of 10-, 20-, and 30-year\n    Treasury bonds are P10 , DUR10 , P20 , DUR20 , P30 , and DUR30 and are\n    assumed to be known. A regression of changes in the 20-year yield on\n    changes in the 10- and 30-year yields is \u0394y20 = \u03b2\u00020 + \u03b2\u00021 \u0394y10 + \u03b2\u00022 \u0394y30 .\n    The p-value of \u03b2\u00020 is large and it is assumed that \u03b20 is close enough to\n    zero to be ignored. What face amounts F10 and F30 of 10- and 30-year\n    Treasury bonds should be shorted to hedge the long position in 20-year\n    Treasury bonds? (Express F10 and F30 in terms of the known quantities\n    P10 , P20 , P30 , DUR10 , DUR20 , DUR30 , \u03b2\u00021 , \u03b2\u00022 , and F20 .)\n 3. The maturities (T ) in years and prices in dollars of zero-coupon bonds are\n    in \ufb01le ZeroPrices.txt on the book\u2019s website. The prices are expressed\n\f                                                                     References   305\n\n   as percentages of par. A popular model is the Nelson\u2013Siegel family with\n   forward rate\n\n                 r(T ; \u03b81 , \u03b82 , \u03b83 , \u03b84 ) = \u03b81 + (\u03b82 + \u03b83 T ) exp(\u2212\u03b84 T ).\n\n   Fit this forward rate to the prices by nonlinear regression using R\u2019s\n   optim() function.\n   (a) What are your estimates of \u03b81 , \u03b82 , \u03b83 , and \u03b84 ?\n   (b) Plot the estimated forward rate and estimated yield curve on the same\n        \ufb01gure. Include the \ufb01gure with your work.\n4. Least-squares estimators are unbiased in linear models, but in nonlinear\n   models they can be biased. Simulation studies (including bootstrap re-\n   sampling) can be used to estimate the amount of bias. In Example 11.1,\n   the data were simulated with r = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.13",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06 and r\u0002 = 0.0585. Do you think this\n   is a sign of bias or simply due to random variability? Justify your answer.\n\n\nReferences\n\nAtkinson, A. C. (1985) Plots, Transformations and Regression, Clarendon,\n  Oxford.\nBates, D. M., and Watts, D. G. (1988) Nonlinear Regression Analysis and Its\n  Applications, Wiley, New York.\nBluhm, C., Overbeck, L., and Wagner, C. (2003) An Introduction to Credit\n  Risk Modelling, Chapman & Hall/CRC, Boca Raton, FL.\nBox, G. E. P., and Dox, D. R. (1964) An analysis of transformations. Journal\n  of the Royal Statistical Society, Series B, 26 211\u2013246.\nCarroll, R. J., and Ruppert, D. (1988) Transformation and Weighting in\n  Regression, Chapman & Hall, New York.\nChan, K. C., Karolyi, G. A., Longsta\ufb00, F. A., and Sanders, A. B. (1992) An\n  empirical comparison of alternative models of the short-term interest rate.\n  Journal of Finance, 47, 1209\u20131227.\nFaraway, J. J. (2006) Extending the Linear Model with R, Chapman & Hall,\n  Boca Raton, FL.\nHosmer, D., and Lemeshow, S. (2000) Applied Logistic Regression, 2nd ed.,\n  Wiley, New York.\nJarrow, R. (2002) Modeling Fixed-Income Securities and Interest Rate\n  Options, 2nd Ed., Stanford University Press, Stanford, CA.\nLange, K. L., Little, R. J. A., and Taylor, J. M. G. (1989) Robust statisti-\n  cal modeling using the t-distribution. Journal of the American Statistical\n  Association, 84, 881\u2013896.\nNelson, C. R., and Siegel, A. F. (1985) Parsimonious modelling of yield curves.\n  Journal of Business, 60, 473\u2013489.\n\f306    11 Regression: Advanced Topics\n\nSeber, G. A. F., and Wild, C. J. (1989) Nonlinear Regression, Wiley,\n  New York.\nSvensson, L. E. (1994) Estimating and interpreting forward interest rates:\n  Sweden 1992\u201394, Working paper. International Monetary Fund, 114.\nTuckman, B. (2002) Fixed Income Securities, 2nd ed., Wiley, Hoboken, NJ.\n\f12\nTime Series Models: Basics\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "and r\u0002 = 0.0585. Do you think this",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1 Time Series Data\n\nA time series is a sequence of observations in chronological order, for example,\ndaily log returns on a stock or monthly values of the Consumer Price Index\n(CPI). A common simplifying assumption is that the data are equally spaced\nwith a discrete-time observation index; however, this may only hold approx-\nimately. For example, daily log returns on a stock may only be available\nfor weekdays, with additional gaps on holidays, and monthly values of the\nCPI are equally spaced by month, but unequally spaced by days. In either\ncase, the consecutive observations are commonly regarded as equally spaced,\nfor simplicity. In this chapter, we study statistical models for time series.\nThese models are widely used in econometrics, business forecasting, and many\nscienti\ufb01c applications.\n    A stochastic process is a sequence of random variables and can be viewed as\nthe \u201ctheoretical\u201d or \u201cpopulation\u201d analog of a time series\u2014conversely, a time\nseries can be considered a sample from a stochastic process. \u201cStochastic\u201d is a\nsynonym for random. One of the most useful methods for obtaining parsimony\nin a time series model is to assume some form of distributional invariance over\ntime, or stationarity, a property discussed next.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "Time Series Data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2 Stationary Processes\n\nWhen we observe a time series, the \ufb02uctuations appear random, but often with\nthe same type of stochastic behavior from one time period to the next. For\nexample, returns on stocks or changes in interest rates can be very di\ufb00erent\nfrom the previous year, but the mean, standard deviation, and other statistical\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                             307\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 12\n\f308       12 Time Series Models: Basics\n\nproperties often are similar from one year to the next.1 Similarly, the demand\nfor many consumer products, such as sunscreen, winter coats, and electricity,\nhas random as well as seasonal variation, but each summer is similar to past\nsummers, each winter to past winters, at least over shorter time periods.\nStationary stochastic processes are probability models for time series with\ntime-invariant behavior.\n    A process is said to be strictly stationary if all aspects of its behavior\nare unchanged by shifts in time. Mathematically, stationarity is de\ufb01ned as\nthe requirement that for every m and n, the distributions of (Y1 , . . . , Yn )\nand (Y1+m , . . . , Yn+m ) are the same; that is, the probability distribution of\na sequence of n observations does not depend on their time origin. Strict\nstationarity is a very strong assumption, because it requires that \u201call aspects\u201d\nof stochastic behavior be constant in time. Often, it will su\ufb03ce to assume less,\nnamely, weak stationarity. A process is weakly stationary if its mean, variance,\nand covariance are unchanged by time shifts. More precisely, Y1 , Y2 , . . . is a\nweakly stationary process if\n\u2022     E(Yt ) = \u03bc (a \ufb01nite constant) for all t;\n\u2022     Var(Yt ) = \u03c3 2 (a positive \ufb01nite constant) for all t; and\n\u2022     Cov(Yt , Ys ) = \u03b3(|t \u2212 s|) for all t and s for some function \u03b3(h).\n    Thus, the mean and variance do not change with time and the covariance\nbetween two observations depends only on the lag, the time distance |t \u2212 s|\nbetween them, not the indices t or s directly. For example, if the process\nis weakly stationary, then the covariance between Y2 and Y5 is the same as\nthe covariance between Y7 and Y10 , since each pair is separated by three\nunits of time. The adjective \u201cweakly\u201d in \u201cweakly stationary\u201d refers to the\nfact that we are only assuming that means, variance, and covariances, not\nother distributional characteristics such as quantiles, skewness, and kurtosis,\nare stationary. Weakly stationary is also sometimes referred to as covariance\nstationary. The term stationary will sometimes be used as a shorthand for\nstrictly stationary.\n    The function \u03b3 is called the autocovariance function of the process.\nNote that \u03b3(h) = \u03b3(\u2212h). Why? Assuming weak stationarity, the correla-\ntion between Yt and Yt+h is denoted by \u03c1(h). The function rho is called the\nautocorrelation function. Note that \u03b3(0) = \u03c3 2 and that \u03b3(h) = \u03c3 2 \u03c1(h). Also,\n\u03c1(h) = \u03b3(h)/\u03c3 2 = \u03b3(h)/\u03b3(0).\n    As mentioned, many \ufb01nancial time series do not exhibit stationarity, but\noften the changes in them, perhaps after applying a log transformation, are\napproximately stationary. For this reason, stationary time series models have\nbroad applicability and wide ranging applications. From the viewpoint of\n\n\n1\n    It is the returns, not the stock prices, that have time-invariant behavior. Stock\n    prices themselves tend to increase over time, so this year\u2019s stock prices tend to\n    be higher and more variable than those a decade or two ago.\n\f                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "Stationary Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2 Stationary Processes     309\n\nstatistical modeling, it is not important whether it is the time series itself\nor changes in the time series that are stationary, because either way we get a\nparsimonious model.\n     The beauty of a stationary process is that it can be modeled with relatively\nfew parameters. For example, we do not need a di\ufb00erent expectation for each\nYt ; rather they all have a common expectation, \u03bc. This implies that \u03bc can\nbe estimated accurately by Y . If instead we did not assume stationarity and\neach Yt had its own unique expectation, \u03bct , then it would not be possible to\nestimate \u03bct accurately\u2014\u03bct could only be estimated by the single observation\nYt itself.\n     When a time series is observed, a natural question is whether it appears\nto be stationary. This is not an easy question to address, and we can never be\nabsolutely certain of the answer. However, visual inspection of the time series\nand changes in the time series can be helpful. A time series plot is a plot of\nthe series in chronological order. This very basic plot is useful for assessing\nstationary behavior, though it can be supplemented with other plots, such as\nthe plot of the sample autocorrelation function that will be introduced later.\nIn addition, there are statistical tests of stationarity\u2014these are discussed in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "Stationary Processes     309",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10.\n     A time series plot of a stationary series should show random oscillation\naround some \ufb01xed level, a phenomenon called mean-reversion. If the series\nwanders without returning repeatedly to some \ufb01xed level, then the series\nshould not be modeled as a stationary process.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "A time series plot of a stationary series should show random oscillation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1. In\ufb02ation rates and changes in in\ufb02ation rates\u2014time series\nplots\n\n    The one-month in\ufb02ation rate (annual rate, in percent) is shown in\nFig. 12.1a. The data come from the Mishkin data set in R\u2019s Ecdat package.\nThe series may be wandering without reverting to a \ufb01xed mean, or it may be\nslowly reverting to a mean of approximately 4 %, as would be expected with\na stationary time series. In panel (b), the \ufb01rst di\ufb00erences, that is, the changes\nfrom one month to the next, are shown. In contrast to the original series, the\ndi\ufb00erenced series certainly oscillate around a \ufb01xed mean that is 0 %, or nearly\nso. The di\ufb00erenced series appears stationary, but whether or not the original\nseries is stationary needs further investigation. We will return to this question\nlater.                                                                          \u0002\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "In\ufb02ation rates and changes in in\ufb02ation rates\u2014time series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2. Air passengers\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "Air passengers",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2 is a plot of monthly total international airline passengers for\nthe years 1949 to 1960. The data come from the AirPassengers data set in\nR\u2019s Datasets package. There are three types of nonstationarity seen in the\n\f310       12 Time Series Models: Basics\n\nplot. First is the obvious upward trend, second is the seasonal variation with\nlocal peaks in summer and troughs in winter months, and third is the increase\nover time in the size of the seasonal oscillations.                         \u0002\n\n\n           a\n           Inflation Rate\n                            15\n                            5\n                            \u22125\n\n\n\n\n                                  1950   1960      1970        1980   1990\n                                                    Year\n\n           b\n           Change in Rate\n                            20\n                            0\n                            \u221220\n\n\n\n\n                                  1950   1960      1970        1980   1990\n                                                    Year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "is a plot of monthly total international airline passengers for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1. Time series plots of (a) one-month in\ufb02ation rate (annual rate, in percent)\nand (b) \ufb01rst di\ufb00erences (changes) in the one-month in\ufb02ation rate. It is unclear if\nthe series in (a) is stationary, but the di\ufb00erenced series in (b) seems suitable for\nmodeling as stationary.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "Time series plots of (a) one-month in\ufb02ation rate (annual rate, in percent)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2.1 White Noise\n\nWhite noise is the simplest example of a stationary process. We will de\ufb01ne\nseveral types of white noise with increasingly restrictive assumptions.\n   The sequence Y1 , Y2 , . . . is a weak white noise process with mean \u03bc and\nvariance \u03c3 2 , which will be shortened to \u201cweak WN(\u03bc, \u03c3 2 ),\u201d if\n\u2022     E(Yt ) = \u03bc (a \ufb01nite constant) for all t;\n\u2022     Var(Yt ) = \u03c3 2 (a positive \ufb01nite constant) for all t; and\n\u2022     Cov(Yt , Ys ) = 0 for all t = s.\nIf the mean is not speci\ufb01ed, then it is assumed that \u03bc = 0. A weak white noise\nprocess is weakly stationary with\n\n                                          \u03b3(0) = \u03c3 2 ,\n                                          \u03b3(h) = 0 if h = 0,\n\f                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "1 White Noise",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2 Stationary Processes     311\n\nso that\n\n                                            \u03c1(0) = 1,\n                                            \u03c1(h) = 0 if h = 0.\n\n                            600\n                            500\n               Passengers\n                            400\n                            300\n                            200\n                            100\n\n\n\n\n                                    1950    1952     1954    1956     1958       1960\n                                                         Time\n\n  Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "Stationary Processes     311",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2. Time series plot of monthly totals of air passengers (in thousands).\n\n\n\n    If Y1 , Y2 , . . . is an i.i.d. process, then we call it an i.i.d. white noise process\nor simply i.i.d. WN(\u03bc, \u03c3 2 ). Weak white noise is weakly stationary, while i.i.d.\nwhite noise is strictly stationary. An i.i.d. white noise process with \u03c3 2 \ufb01nite\nis also a weak white noise process, but not vice versa.\n    If, in addition, Y1 , Y2 . . . is an i.i.d. process with a speci\ufb01c marginal distri-\nbution, then this might be noted. For example, if Y1 , Y2 . . . are i.i.d. normal\nrandom variables, then the process is called a Gaussian white noise process.\nSimilarly, if Y1 , Y2 . . . are i.i.d. t random variables with \u03bd degrees of freedom,\nthen it is called a t\u03bd white noise process.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "Time series plot of monthly totals of air passengers (in thousands).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2.2 Predicting White Noise\n\nBecause of the lack of dependence, past values of a white noise process contain\nno information that can be used to predict future values. More precisely,\nsuppose that Y1 , Y2 , . . . is an i.i.d. WN(\u03bc, \u03c3 2 ) process. Then\n\n                                  E(Yt+h |Y1 , . . . , Yt ) = \u03bc for all h \u2265 1.              (12.1)\n\nWhat this equation is saying is that one cannot predict the future deviations\nof a white noise process from its mean, because its future is independent of\nits past and present. Therefore, the best predictor of any future value of the\n\f312      12 Time Series Models: Basics\n\nprocess is simply the mean \u03bc, what you would use even if Y1 , . . . , Yt had not\nbeen observed. For weak white noise, (12.1) need not be true, but it is still\ntrue that the best linear predictor2 of Yt+h given Y1 , . . . , Yt is \u03bc.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "2 Predicting White Noise",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3 Estimating Parameters of a Stationary Process\n\nSuppose we observe Y1 , . . . , Yn from a weakly stationary process. To estimate\nthe mean \u03bc and variance \u03c3 2 of the process, we can use the sample mean Y\nand sample variance s2 . To estimate the autocovariance function, we use the\nsample autocovariance function\n                n\u2212h                                  n\n \u0002(h) = n\u22121\n \u03b3                    (Yt+h \u2212 Y )(Yt \u2212 Y ) = n\u22121           (Yt \u2212 Y )(Yt\u2212h \u2212 Y ). (12.2)\n                t=1                                t=h+1\n\nEquation (12.2) is an example of the usefulness of parsimony induced by the\nstationarity assumption. Because the covariance between Yt and Yt+h does\nnot depend on t, all n \u2212 h pairs of data points that are separated by a lag of\nh time units can be used to estimate \u03b3(h). Some authors de\ufb01ne \u03b3    \u0002(h) with the\nfactor n\u22121 in (12.2) replaced by (n \u2212 h)\u22121 , but this change has little e\ufb00ect if\nn is reasonably large and h is small relative to n, as is typically the case.\n    To estimate \u03c1(\u00b7), we use the sample autocorrelation function (sample ACF )\nde\ufb01ned as\n                                          \u0002(h)\n                                          \u03b3\n                                  \u03c1\u0002(h) =      .\n                                          \u0002(0)\n                                          \u03b3\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "Estimating Parameters of a Stationary Process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3.1 ACF Plots and the Ljung\u2013Box Test\n\nMost statistical software will plot a sample ACF with test bounds. These\nbounds are used to test the null hypothesis that an autocorrelation coe\ufb03cient\nis 0. The null hypothesis is rejected if the sample autocorrelation is outside\nthe bounds. The usual level of the test is 0.05, so one can expect to see about\n1 out of 20 sample autocorrelations outside the test bounds simply by chance.\n    An alternative to using the bounds to test the autocorrelations one at\na time is to use a simultaneous test. A simultaneous test is one that tests\nwhether a group of null hypotheses are all true versus the alternative that\nat least one of them is false. The null hypothesis of the Ljung\u2013Box test is\nH0 : \u03c1(1) = \u03c1(2) = \u00b7 \u00b7 \u00b7 = \u03c1(K) = 0 for some K, say K = 5 or 10. If the Ljung\u2013\nBox test rejects, then we conclude that one or more of \u03c1(1), \u03c1(2), \u00b7 \u00b7 \u00b7 , \u03c1(K) is\nnonzero.\n\n\n\n2\n    Best linear prediction is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "1 ACF Plots and the Ljung\u2013Box Test",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.1.\n\f                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3 Estimating Parameters of a Stationary Process           313\n\n    If, in fact, the autocorrelations 1 to K are all zero, then there is only a 1\nin 20 chance of falsely concluding that they are not all zero, assuming a level\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "Estimating Parameters of a Stationary Process           313",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 test. In contrast, if the autocorrelations are tested one at time, then there\nis a much higher chance of concluding that one or more is nonzero.\n    The Ljung\u2013Box test is sometimes called simply the Box test, though the\nformer name is preferable since the test is based on a joint paper of Ljung\nand Box.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "test. In contrast, if the autocorrelations are tested one at time, then there",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3. In\ufb02ation rates and changes in the in\ufb02ation rate\u2014sample ACF\nplots and the Ljung\u2013Box test\n\n       a                                       b\n\n\n\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "In\ufb02ation rates and changes in the in\ufb02ation rate\u2014sample ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n             0.8\n\n\n\n\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n       ACF\n\n\n\n\n                                               ACF\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                     0.2\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                     \u22120.4\n\n\n\n\n                   0   5   10   15   20   25                0   5   10   15   20   25\n                             Lag                                     Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3. Sample ACF plots of the one-month in\ufb02ation rate (a) and changes in\nthe in\ufb02ation rate (b).\n\n\n    We return to the in\ufb02ation rate data used in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "Sample ACF plots of the one-month in\ufb02ation rate (a) and changes in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1. Figure 12.3\ncontains plots of (a) the sample ACF of the one-month in\ufb02ation rate and\n(b) the sample ACF of changes in the in\ufb02ation rate.\n1 data(Mishkin, package = \"Ecdat\")\n2 y = as.vector(Mishkin[,1])\n3 par(mfrow=c(1,2))\n\n4 acf(y)\n\n5 acf(diff(y))\n\n\n\nIn (a) we see that the sample ACF decays to zero slowly. This is a sign\nof either nonstationarity or possibly of stationarity with long-memory dep-\nendence, which is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "Figure 12.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5. In contrast, the sample ACF in\n(b) decays to zero quickly, indicating clearly that the di\ufb00erenced series is\nstationary. Thus, the sample ACF plots agree with the conclusions reached\nby examining the time series plots in Fig. 12.1, speci\ufb01cally that the di\ufb00erenced\nseries is stationary and the original series might not be. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "In contrast, the sample ACF in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10 we will\nuse hypothesis testing to further address the question of whether or not the\noriginal series is stationary.\n\f314     12 Time Series Models: Basics\n\n    Several of the autocorrelations of the rate change series fall outside the\ntest bounds, which suggests that the series is not white noise. To check,\nthe Ljung\u2013Box test was implemented using R\u2019s Box.test() function. K is\ncalled lag when Box.test() is called and df in the output, and we specify\ntype=\"Ljung-Box\" for the Ljung\u2013Box test.\n6   Box.test(diff(y), lag = 10, type = \"Ljung-Box\")\n\nThe Ljung\u2013Box test statistic with K = 10 is 79.92, which has an extremely\nsmall p-value, 5.217e\u221213, so the null hypothesis of white noise is strongly\nrejected. Other choices of K give similar results.                       \u0002\n     Although a stationary process is somewhat parsimonious with parameters,\nat least relative to a general nonstationary process, a stationary process is still\nnot su\ufb03ciently parsimonious for most purposes. The problem is that there are\nstill an in\ufb01nite number of parameters, \u03c1(1), \u03c1(2), . . . . What we need is a class\nof stationary time series models with only a \ufb01nite, preferably small, number\nof parameters. The ARIMA models of this chapter are precisely such a class.\nThe simplest ARIMA models are autoregressive (AR) models, and we turn to\nthese \ufb01rst.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "we will",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4 AR(1) Processes\nTime series models with correlation can be constructed from white noise. The\nsimplest correlated stationary processes are autoregressive processes, where\nYt is modeled as a weighted average of past observations plus a white noise\n\u201cerror,\u201d which is also called the \u201cnoise\u201d or \u201cdisturbance.\u201d We start with AR(1)\nprocesses, the simplest autoregressive processes.\n    Let \u00171 , \u00172 , . . . be weak WN(0,\u03c3 2 ). We say that Y1 , Y2 , . . . is an AR(1) pro-\ncess if for some constant parameters \u03bc and \u03c6,\n\n                             Yt \u2212 \u03bc = \u03c6(Yt\u22121 \u2212 \u03bc) + \u0017t                           (12.3)\n\nfor all t. The parameter \u03bc is the mean of the process, hence (Yt \u2212 \u03bc) has\nmean zero for all t. We may interpret the term \u03c6(Yt\u22121 \u2212 \u03bc) as representing\n\u201cmemory\u201d or \u201cfeedback\u201d of the past into the present value of the process. The\nprocess {Yt }+\u221e\n              t=\u2212\u221e is correlated because the deviation of Yt\u22121 from its mean is\nfed back into Yt . The parameter \u03c6 determines the amount of feedback, with a\nlarger absolute value of \u03c6 resulting in more feedback and \u03c6 = 0 implying that\nYt = \u03bc + \u0017t , so that Yt is weak WN(\u03bc, \u03c3 2 ). In applications in \ufb01nance, one can\nthink of \u0017t as representing the e\ufb00ect of \u201cnew information.\u201d For example, if Yt\nis the log return on an asset at time t, then \u0017t represents the e\ufb00ect on the\nasset\u2019s price of business and economic information that is revealed at time t.\nInformation that is truly new cannot be anticipated, so the e\ufb00ects of today\u2019s\nnew information should be independent of the e\ufb00ects of yesterday\u2019s news. This\nis why we model new information as white noise.\n\f                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "AR(1) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4 AR(1) Processes      315\n\n    If Y1 , Y2 , . . . is a weakly stationary process, then |\u03c6| < 1. To see this,\nnote that stationarity implies that the variances of (Yt \u2212 \u03bc) and (Yt\u22121 \u2212 \u03bc)\nin (12.3) are equal, say, to \u03c3Y2 . Therefore, \u03c3Y2 = \u03c62 \u03c3Y2 + \u03c3 2 , which requires\nthat |\u03c6| < 1. The mean of this process is \u03bc. Simple algebra shows that (12.3)\ncan be rewritten as\n                               Yt = (1 \u2212 \u03c6)\u03bc + \u03c6Yt\u22121 + \u0017t .                (12.4)\n    Recall the linear regression model Yt = \u03b20 + \u03b21 Xt + \u0017t from your statis-\ntics courses or see Chap. 9 for an introduction to regression analysis. Equa-\ntion (12.4) is just a linear regression model with intercept \u03b20 = (1 \u2212 \u03c6)\u03bc and\nslope \u03b21 = \u03c6. The term autoregression refers to the regression of the process\non its own past values.\n    If |\u03c6| < 1, then repeated substitution of (12.3) shows that\n                                                          \u221e\n            Yt = \u03bc + \u0017t + \u03c6\u0017t\u22121 + \u03c62 \u0017t\u22122 + \u00b7 \u00b7 \u00b7 = \u03bc +         \u03c6h \u0017t\u2212h ,   (12.5)\n                                                          h=0\n\nassuming that time index t of Yt and \u0017t can be extended to negative values so\nthat the white noise process is {\u0017t }+\u221e\n                                     t=\u2212\u221e and (12.3) is true for all integers t.\nEquation (12.5) is called the in\ufb01nite moving average [MA(\u221e)] representation\nof the process. This equation shows that Yt is a weighted average of all past\nvalues of the white noise process. This representation should be compared to\nthe AR(1) representation that shows Yt as depending only on Yt\u22121 and \u0017t .\nSince |\u03c6| < 1, \u03c6h \u2192 0 as the lag h \u2192 \u221e. Thus, the weights given to the\ndistant past are small. In fact, they are quite small. For example, if \u03c6 = 0.5,\nthen \u03c610 = 0.00098, so \u0017t\u221210 has virtually no e\ufb00ect on Yt . For this reason,\nthe sum in (12.5) could be truncated at a \ufb01nite number of terms, with no\npractical need to assume that the processes existed in the in\ufb01nite past.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "AR(1) Processes      315",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4.1 Properties of a Stationary AR(1) Process\n\nWhen an AR(1) process is weakly stationary, which implies that |\u03c6| < 1, then\n\n                     E(Yt ) = \u03bc   \u2200t,                                       (12.6)\n                                              \u03c32\n                 Var(Yt ) = \u03b3(0) = \u03c3Y2 =             \u2200t,                    (12.7)\n                                            1 \u2212 \u03c62\n                                            \u03c32\n            Cov(Yt , Yt+h ) = \u03b3(h) = \u03c6|h|          \u2200t and \u2200h, and           (12.8)\n                                          1 \u2212 \u03c62\n            Corr(Yt , Yt+h ) = \u03c1(h) = \u03c6|h|   \u2200t and \u2200h.                     (12.9)\n\nIt is important to remember that formulas (12.6) to (12.9) hold only if |\u03c6| < 1\nand only for AR(1) processes. Moreover, for Yt to be stationary, Y0 must start\nin the stationary distribution so that E(Y0 ) = \u03bc and Var(Y0 ) = \u03c3 2 /(1 \u2212 \u03c62 ).\nOtherwise, Yt is not stationary though it eventually converges to stationarity.\n\f316      12 Time Series Models: Basics\n\n   These formulas can be proved using (12.5). For example, using (7.11) in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "1 Properties of a Stationary AR(1) Process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.3.2,\n                                \u221e                           \u221e\n                                       h                2                   \u03c32\n          Var(Yt ) = Var              \u03c6 \u0017t\u2212h       =\u03c3             \u03c62h =          ,   (12.10)\n                                                                          1 \u2212 \u03c62\n                                h=0                         h=0\n\nwhich proves (12.7). In (12.10) the formula for summation of a geometric series\nwas used. This formula is\n                                 \u221e\n                                               1\n                                      ri =        if |r| < 1.                        (12.11)\n                                i=0\n                                              1\u2212r\n\nAlso, for h > 0,\n                      \u239b                                      \u239e\n                          \u221e                 \u221e\n                                                                            \u03c32\n                 Cov \u239d          \u0017t\u2212i \u03c6i ,         \u0017t+h\u2212j \u03c6j \u23a0 = \u03c6|h|             ,   (12.12)\n                          i=0               j=0\n                                                                          1 \u2212 \u03c62\n\nthus verifying (12.8). Then (12.9) follows by dividing (12.8) by (12.7).\n    Be sure to distinguish between \u03c3 2 , which is the variance of the white noise\nprocess \u00171 , \u00172 , . . ., and \u03b3(0), which is the variance, \u03c3Y2 , of the stationary AR(1)\nprocess Y1 , Y2 , . . . . We can see from (12.7) that \u03b3(0) is larger than \u03c3 2 unless\n\u03c6 = 0, in which case Yt = \u03bc + \u0017t , such that Yt and \u0017t have the same variance.\n    The ACF (autocorrelation function) of an AR(1) process depends upon\nonly one parameter, \u03c6. This is a remarkable amount of parsimony, but it\ncomes at a price. The ACF of an AR(1) process has only a limited range\nof shapes, as can be seen in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.3",
      "section_title": "2,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0,\n                      \u239b                                      \u239e\n                          \u221e                 \u221e\n                                                                            \u03c32\n                 Cov \u239d          \u0017t\u2212i \u03c6i ,         \u0017t+h\u2212j \u03c6j \u23a0 = \u03c6|h|             ,   (12.12)\n                          i=0               j=0\n                                                                          1 \u2212 \u03c62",
        "start": 769,
        "end": 1185
      }
    ]
  },
  {
    "content": "12.4. The magnitude of its ACF decays\ngeometrically to zero, either slowly as when \u03c6 = 0.95, moderately slowly as\nwhen \u03c6 = 0.75, or rapidly as when \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "The magnitude of its ACF decays",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2. If \u03c6 < 0, then the sign of the ACF\nalternates as its magnitude decays geometrically. If the sample ACF of the\ndata does not behave in one of these ways, then an AR(1) model is unsuitable.\nThe remedy is to use more AR parameters, or to switch to another class of\nmodels such as the moving average (MA) or autoregressive moving average\n(ARMA) models. We investigate these alternatives in this chapter.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "If \u03c6 < 0, then the sign of the ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4.2 Convergence to the Stationary Distribution\n\nSuppose that Y0 is an arbitrary starting value not chosen from the station-\nary distribution and that (12.3) holds for t = 1, 2, . . . . Then the process is\nnot stationary, but converges to the stationary distribution satisfying (12.6)\nto (12.9) as t \u2192 \u221e.3 For example, since Yt \u2212 \u03bc = \u03c6(Yt\u22121 \u2212 \u03bc) + \u0017t , we have\n\n3\n    However, there is a technical issue here. It must be assumed that Y0 has a \ufb01nite\n    mean and variance, since otherwise Yt will not have a \ufb01nite mean and variance\n    for any t > 0.\n\f                                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "2 Convergence to the Stationary Distribution",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.",
        "start": 547,
        "end": 639
      }
    ]
  },
  {
    "content": "12.4 AR(1) Processes           317\n\n\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "AR(1) Processes           317",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                      1.0\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                      0.5\n         \u03c1(h)\n\n\n\n\n                                                               \u03c1(h)\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                      0.0\n                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 \u22120.5\n\n\n\n\n                                                                      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 \u22120.5\n                                               \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.95                                           \u03c6 = 0.75\n\n                            0          5         10       15                      0       5         10       15\n                                           h                                                  h\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.95",
      "section_title": "\u03c6 = 0.75",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                      1.0\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                      0.5\n        \u03c1(h)\n\n\n\n\n                                                               \u03c1(h)\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                      0.0\n                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 \u22120.5\n\n\n\n\n                                                                      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 \u22120.5\n                                               \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2                                            \u03c6 = \u22120.9\n\n                            0          5         10       15                      0       5         10       15\n                                           h                                                  h\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u03c6 = \u22120.9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4. Autocorrelation functions of AR(1) processes with \u03c6 equal to 0.95, 0.75,\n0.2, and \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "Autocorrelation functions of AR(1) processes with \u03c6 equal to 0.95, 0.75,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9.\n\n\nE(Y1 ) \u2212 \u03bc = \u03c6{E(Y0 ) \u2212 \u03bc}, and E(Y2 ) \u2212 \u03bc = \u03c62 {E(Y0 ) \u2212 \u03bc}, and so forth,\nso that\n                                    E(Yt ) = \u03bc + \u03c6t {E(Y0 ) \u2212 \u03bc} for all t > 0.                                   (12.13)\nSince |\u03c6| < 1, \u03c6 \u2192 0 and E(Yt ) \u2192 \u03bc as t \u2192 \u221e. The convergence of Var(Yt )\n                                t\n\nto \u03c3 2 /(1 \u2212 \u03c62 ) can be proved in a somewhat similar manner. The convergence\nto the stationary distribution can be very rapid when |\u03c6| is not too close to\n1. For example, if \u03c6 = 0.5, then \u03c610 = 0.00097, so by (12.13), E(Y10 ) is very\nclose to \u03bc unless E(Y0 ) was extremely far from \u03bc.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "E(Y1 ) \u2212 \u03bc = \u03c6{E(Y0 ) \u2212 \u03bc}, and E(Y2 ) \u2212 \u03bc = \u03c62 {E(Y0 ) \u2212 \u03bc}, and so forth,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.                                   (12.13)\nSince |\u03c6| < 1, \u03c6 \u2192 0 and E(Yt ) \u2192 \u03bc as t \u2192 \u221e. The convergence of Var(Yt )\n                                t",
        "start": 166,
        "end": 322
      }
    ]
  },
  {
    "content": "12.4.3 Nonstationary AR(1) Processes\n\nIf |\u03c6| \u2265 1, then the AR(1) process is nonstationary, and the mean, variance,\ncovariances and and correlations are not constant.\n\nRandom Walk (\u03c6 = 1)\n\nIf \u03c6 = 1, then\n                                                      Yt = Yt\u22121 + \u0017t\nand the process is not stationary. This is the random walk process we saw in\nChap. 2.\n\f318     12 Time Series Models: Basics\n\n   Suppose we start the process at an arbitrary point Y0 . It is easy to see\nthat\n                        Yt = Y0 + \u00171 + \u00b7 \u00b7 \u00b7 + \u0017t .\nThen E(Yt |Y0 ) = Y0 for all t, which is constant but depends entirely on the\narbitrary starting point. Moreover, Var(Yt |Y0 ) = t\u03c3 2 , which is not stationary\nbut rather increases linearly with time. The increasing variance makes the\nrandom walk \u201cwander\u201d in that Yt takes increasingly longer excursions away\nfrom its conditional mean of Y0 and therefore is not mean-reverting.\n\nAR(1) Processes When |\u03c6| > 1\n\nWhen |\u03c6| > 1, an AR(1) process has explosive behavior. This can be seen\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "3 Nonstationary AR(1) Processes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1",
        "start": 936,
        "end": 941
      },
      {
        "language": "r",
        "code": "1, an AR(1) process has explosive behavior. This can be seen\nin Fig.",
        "start": 950,
        "end": 1021
      }
    ]
  },
  {
    "content": "12.5. This \ufb01gure shows simulations of 200 observations from AR(1)\nprocesses with various values of \u03c6. The explosive case where \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "This \ufb01gure shows simulations of 200 observations from AR(1)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.01 clearly\nis di\ufb00erent from the other cases where |\u03c6| \u2264 1. However, the case where\n\u03c6 = 1 is not that much di\ufb00erent from \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.01",
      "section_title": "clearly",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98 even though the former is\nnonstationary while the latter is stationary. Longer time series would help\ndistinguish between \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98",
      "section_title": "even though the former is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98 and \u03c6 = 1.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98",
      "section_title": "and \u03c6 = 1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5 Estimation of AR(1) Processes\n\nR has the function arima() for \ufb01tting AR and other time series models. The\nfunction arima() and similar functions in other software packages have two\nprimary estimation methods, conditional least-squares and maximum likeli-\nhood. The two methods are explained in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "Estimation of AR(1) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5.2. They are similar and\ngenerally give nearly the same estimates. In this book, we use the default\nmethod in R\u2019s arima(), which is the MLE with the conditional least-squares\nestimate used as the starting value for computing the MLE by iterative non-\nlinear optimization.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "2. They are similar and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5.1 Residuals and Model Checking\n\nOnce \u03bc and \u03c6 have been estimated, one can estimate the white noise process\n\u00171 , . . . , \u0017n . Rearranging Eq. (12.3), we have\n\n                           \u0017t = (Yt \u2212 \u03bc) \u2212 \u03c6(Yt\u22121 \u2212 \u03bc).                        (12.14)\n\nIn analogy with (12.14), the residuals, \u0002\n                                        \u00172 , \u0002\n                                             \u00173 , . . . , \u0002\n                                                          \u0017n , are de\ufb01ned as\n\n                      \u0002\n                      \u0017t = (Yt \u2212 \u03bc    \u0002 t\u22121 \u2212 \u03bc\n                                 \u0002) \u2212 \u03c6(Y     \u0002),         t \u2265 2,               (12.15)\n\nand estimate \u00172 , . . . , \u0017n . The \ufb01rst noise, \u00171 , cannot be estimated directly since\nit is assumed that the observations start at Y1 so that Y0 is not available.\n\f                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "1 Residuals and Model Checking",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5 Estimation of AR(1) Processes              319\n\n                                 \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "Estimation of AR(1) Processes              319",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98                                   \u03c6 = \u22120.6\n\n\n\n\n               6\n\n\n\n\n                                                             4\n               4\n\n\n\n\n                                                             2\n               2\n               \u22122 0\n          Yt\n\n\n\n\n                                                        Yt\n\n                                                             0\n                                                             \u22122\n               \u22126\n\n\n\n\n                                                             \u22124\n                      0     50     100      150   200              0   50     100      150   200\n                                 Time (t)                                   Time (t)\n\n                                  \u03c6=1                                       \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98",
      "section_title": "\u03c6 = \u22120.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.01\n\n\n\n\n                                                             0\n               0\n\n\n\n\n                                                             \u221220\n               \u22125\n          Yt\n\n\n\n\n                                                        Yt\n               \u221210\n\n\n\n\n                                                             \u221240\n\n\n\n\n                      0     50     100      150   200              0   50     100      150   200\n                                 Time (t)                                   Time (t)\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.01",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5. Simulations of 200 observations from AR(1) processes with various values\nof \u03c6 and \u03bc = 0. The white noise process 1 , 2 , . . . , 200 is the same for all four AR(1)\nprocesses.\n\n\nThe residuals can be used to check the assumption that Y1 , Y2 , . . . , Yn is an\nAR(1) process; any autocorrelation in the residuals is evidence against the\nassumption of an AR(1) process.\n   To appreciate why residual autocorrelation indicates a possible problem\nwith the model, suppose that we are \ufb01tting an AR(1) model, Yt = \u03bc + \u03c6\n(Yt\u22121 \u2212 \u03bc) + \u0017t , but the true model is an AR(2) process4 given by\n\n                          Yt \u2212 \u03bc = \u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) + \u0017t .\n\nSince we are \ufb01tting the incorrect AR(1) model, there is no hope of estimating\n\u03c62 since it is not in the model. Moreover, \u03c6\u0002 does not necessarily estimate\n\u03c61 because of bias caused by model misspeci\ufb01cation. Let \u03c6\u2217 be the expected\n         \u0002 For the purpose of illustration, assume that \u03bc\nvalue of \u03c6.                                                \u0002 \u2248 \u03bc and \u03c6\u0002 \u2248 \u03c6\u2217 .\nThis is a sensible approximation if the sample size n is large enough. Then\n\n               \u0017t \u2248 (Yt \u2212 \u03bc) \u2212 \u03c6\u2217 (Yt\u22121 \u2212 \u03bc)\n               \u0002\n                  = \u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) + \u0017t \u2212 \u03c6\u2217 (Yt\u22121 \u2212 \u03bc)\n                      = (\u03c61 \u2212 \u03c6\u2217 )(Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) + \u0017t .\n\n4\n    We discuss higher-order AR models in more detail soon.\n\f320      12 Time Series Models: Basics\n\nThus, the residuals do not estimate the white noise process as they would if\nthe correct AR(2) model were used. Even if there is no bias in the estimation\nof \u03c61 by \u03c6\u0002 so that \u03c61 = \u03c6\u2217 and the term (\u03c61 \u2212 \u03c6\u2217 )(Yt\u22121 \u2212 \u03bc) drops out, the\npresence of \u03c62 (Yt\u22122 \u2212 \u03bc) in the residuals causes them to be autocorrelated.\n    To check for residual autocorrelation, one can use the test bounds of ACF\nplots. Any residual ACF value outside the test bounds is signi\ufb01cantly di\ufb00erent\nfrom 0 at the ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "Simulations of 200 observations from AR(1) processes with various values",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 level. As discussed earlier, the danger here is that some\nsample ACF values will be signi\ufb01cant merely by chance, and to guard against\nthis danger, one can use the Ljung\u2013Box test that simultaneously tests that all\nautocorrelations up to a speci\ufb01ed lag are zero. When the Ljung\u2013Box test is\napplied to residuals, a correction is needed to account for the use of \u03c6\u0002 in place\nof the unknown \u03c6. Some software makes this correction automatically. In R\nthe correction is not automatic but is done by setting the fitdf parameter in\nBox.test() to the number of autoregressive coe\ufb03cient parameters that were\nestimated, so for an AR(1) model fitdf should be 1.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "level. As discussed earlier, the danger here is that some",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4. Daily log returns for BMW stock\u2014ACF plots and AR \ufb01t\n\n    The daily log returns for BMW stock between January 1973 and July\n1996 from the bmw data set in R\u2019s evir package are shown in Fig. 12.6a. Their\nsample ACF and quantiles are shown in Fig. 12.6b and c, respectively. The\nestimated autocorrelation coe\ufb03cient at lag 1 is well outside the test bounds,\nso the series has some dependence. Also, the Ljung\u2013Box test that the \ufb01rst lag\nautocorrelations are 0 was performed using R\u2019s Box.test() function.\n7    data(bmw, package = \"evir\")\n8    Box.test(bmw, lag = 5, type = \"Ljung-Box\")\n\nThe parameter lag, which speci\ufb01es the number of autocorrelation coe\ufb03cients\nto test, was set equal to 5, though other choices give similar results. The\noutput was\n              Box-Ljung test\n\n      data: bmw\n      X-squared = 44.987, df = 5, p-value = 1.460e-08\nThe p-value is very small, indicating that at least one of the \ufb01rst \ufb01ve autocor-\nrelations is nonzero. Whether the amount of dependence is of any practical\nimportance is debatable, but an AR(1) model to account for the small amount\nof autocorrelation might be appropriate.\n    Next, an AR(1) model was \ufb01t using the arima() command in R.\n9    fitAR1 = arima(bmw, order = c(1,0,0))\n10   print(fitAR1)\n\nThe order parameter will be explained later, but for an AR(1) process it\nshould be c(1,0,0). A summary of the output is below.\n\f                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "Daily log returns for BMW stock\u2014ACF plots and AR \ufb01t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5 Estimation of AR(1) Processes                                          321\n\n           a\n\n           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "Estimation of AR(1) Processes                                          321",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n           \u22120.05\n           \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n\n\n\n\n                                     Jan 01   Jan 01        Jan 01   Jan 03    Jan 01                      Jan 03        Jan 01       Jan 02   Jul 22\n                                      1973     1976          1979     1982      1985                        1988          1991         1994    1996\n\n\n\n     b                                                                          c\n           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "Jan 01   Jan 01        Jan 01   Jan 03    Jan 01                      Jan 03        Jan 01       Jan 02   Jul 22",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                Sample Quantiles\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n     ACF\n\n\n\n\n                                                                                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n                                                                                                   \u22120.15\n\n\n\n\n                                     0        5        10       15     20                                  \u22124       \u22122            0        2        4\n                                                   Lag                                                              Theoretical Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "\u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.6. (a) Daily log returns for BMW stock from January 1973 until July\n1996, and their (b) sample ACF and (c) sample quantiles relative to the normal\ndistribution.\n\n\n   Call:\n   arima(x = bmw, order = c(1, 0, 0))\n\n   Coefficients:\n              ar1                                  intercept\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.6",
      "section_title": "(a) Daily log returns for BMW stock from January 1973 until July",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.081116                                   0.000340\n   s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.081116",
      "section_title": "0.000340",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.012722                                    0.000205\n\n   sigma^2 estimated as 0.000216260:                                                                       log-likelihood = 17212.34,\n   aic = -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.012722",
      "section_title": "0.000205",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "34418.68\n    We see that \u03c6\u0002 = 0.081 and \u03c3    \u00022 = 0.00022. Although \u03c6\u0002 is small, it is\nstatistically signi\ufb01cant since it is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "34418.68",
      "section_title": "We see that \u03c6\u0002 = 0.081 and \u03c3    \u00022 = 0.00022. Although \u03c6\u0002 is small, it is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.4 times its standard error 0.013, so its\np-value is near zero. As just mentioned, whether this small, but nonzero,\nvalue of \u03c6\u0002 is of practical signi\ufb01cance is another matter. A non-zero value\nof \u03c6 means that there is some information in today\u2019s return that could be\nused for prediction of tomorrow\u2019s return, but a small value of \u03c6 means that\nthe prediction will not be very accurate. The potential for pro\ufb01t might be\nnegated by trading costs.\n\f322          12 Time Series Models: Basics\n\n             a\n\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.4",
      "section_title": "times its standard error 0.013, so its",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n              \u22120.05\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n\n\n\n\n                                        Jan 01   Jan 01     Jan 01   Jan 03   Jan 01                      Jan 03        Jan 01       Jan 02   Jul 22\n                                         1973     1976       1979     1982     1985                        1988          1991         1994    1996\n\n\n       b                                                                       c\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "Jan 01   Jan 01     Jan 01   Jan 03   Jan 01                      Jan 03        Jan 01       Jan 02   Jul 22",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                               Sample Quantiles\n                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n       ACF\n\n\n\n\n                                                                                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n                                                                                                  \u22120.15\n\n\n\n\n                                        0        5    10        15     20                                 \u22124       \u22122            0       2         4\n\n                                                      Lag                                                          Theoretical Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "\u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.7. A (a) time series plot, (b) sample ACF and (c) normal quantile plot of\nresiduals from an AR(1) \ufb01t to the daily log returns for BMW stock.\n\n\n\n    The sample ACF of the residuals is plotted in Fig. 12.7b. None of the\nautocorrelations at low lags are outside the test bounds. A few at higher\nlags are outside the bounds, but this type of behavior is expected to occur\nby chance or because, with a large sample size, very small but nonzero true\ncorrelations can be detected. The Ljung\u2013Box test was applied, with lag equal\nto 5 and fitdf=1.\n11   Box.test(residuals(fitAR1), lag = 5, type = \"Ljung-Box\", fitdf = 1)\n\n      Box-Ljung test\n\n      data: residuals(fitAR1)\n      X-squared = 6.8669, df = 4, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "A (a) time series plot, (b) sample ACF and (c) normal quantile plot of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1431\nThe large p-value indicates that we should accept the null hypothesis that the\nresiduals are uncorrelated, at least at small lags. This is a sign that the AR(1)\nmodel provides an adequate \ufb01t. However, the Ljung\u2013Box test was repeated\nwith lag equal to 10, 15, and 20 and the p-values were 0.041, 0.045, and 0.004,\nrespectively. These values are \u201cstatistically signi\ufb01cant\u201d using the conventional\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1431",
      "section_title": "The large p-value indicates that we should accept the null hypothesis that the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5 Estimation of AR(1) Processes         323\n\ncuto\ufb00 of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "Estimation of AR(1) Processes         323",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05. The sample size is 6146, so it is not surprising that even a\nsmall amount of autocorrelation can be statistically signi\ufb01cant. The practical\nsigni\ufb01cance of this autocorrelation is very doubtful.\n    We conclude that the AR(1) model is adequate for the BMW daily returns,\nbut at longer lags some slight amount of autocorrelation appears to remain.\nHowever, the time series plot and normal quantile plot of the AR(1) resid-\nuals in Fig. 12.7a and c show volatility clustering and heavy tails. These\nare common features of economic data and will be modeled in subsequent\nchapters.                                                                   \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "The sample size is 6146, so it is not surprising that even a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5. In\ufb02ation rate\u2014AR(1) \ufb01t and checking residuals\n\n    This example uses the in\ufb02ation rate time series used earlier in Example\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "In\ufb02ation rate\u2014AR(1) \ufb01t and checking residuals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1. Although there is some doubt as to whether this series is stationary, we\nwill \ufb01t an AR(1) model. The ACF of the residuals are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "Although there is some doubt as to whether this series is stationary, we",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8 and\nthere is considerable residual autocorrelation, which indicates that the AR(1)\nmodel is not adequate. A Ljung\u2013Box test con\ufb01rms this result.\n12 data(Mishkin, package = \"Ecdat\")\n13 y = as.vector(Mishkin[,1])\n14 fit = arima(y, order = c(1,0,0))\n\n15 Box.test(fit$resid, type = \"Ljung\", lag = 24, fitdf = 1)\n\n\n\n     Box-Ljung test\n\n     data: fit$resid\n     X-squared = 138.5776, df = 23, p-value < 2.2e-16\n\nOne might try \ufb01tting an AR(1) to the changes in the in\ufb02ation rate, since this\nseries is clearly stationary. However, the AR(1) model also does not \ufb01t the\nchanges in the in\ufb02ation rate. We will return to this example when we have a\nlarger collection of models in our statistics toolbox.                     \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5.2 Maximum Likelihood and Conditional Least-Squares\n\nEstimators for AR processes can be computed automatically by most sta-\ntistical software packages, and the user need not know what is \u201cunder the\nhood\u201d of the software. Nonetheless, for readers interested in the estimation\nmethodology, this section has been provided.\n    To \ufb01nd the joint density for Y1 , . . . , Yn , we use (A.41) and the fact that\n\n               fYt |Y1 ,...,Yt\u22121 (yt |y1 , . . . , yt\u22121 ) = fYt |Yt\u22121 (yt |yt\u22121 )   (12.16)\n\f324         12 Time Series Models: Basics\n\n                               Inflation rate                                  Residuals from AR(1)\n\n\n\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "2 Maximum Likelihood and Conditional Least-Squares",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                     1.0\n                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n            0.8\n\n\n\n\n                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n            0.6\n      ACF\n\n\n\n\n                                                               ACF\n                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n            0.4\n\n\n\n\n                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n            0.2\n\n\n\n\n                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n            0.0\n\n\n\n\n                    0     5     10     15       20   25                    0   5    10     15   20    25\n                                     Lag                                                 Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8. Sample ACF for the in\ufb02ation rate time series and residual series from\nan AR(1) \ufb01t.\n\n\nfor t = 2, 3, . . . , n. A stochastic process with property (12.16) is called a\nMarkov process. By (A.41) and (12.16), we have\n\n                                                               5\n                                                               n\n                  fY1 ,...,Yn (y1 , . . . , yn ) = fY1 (y1 )         fYt |Yt\u22121 (yt |yt\u22121 ).            (12.17)\n                                                               t=2\n\nIf we assume the errors are from a Gaussian white noise process, then by (12.6)\nand (12.7), we know that Y1 is N {\u03bc, \u03c3 2 /(1 \u2212 \u03c62 )} for a stationary AR(1)\nprocess. Given Yt\u22121 , the only random component of Yt is \u0017t , so that Yt given\nYt\u22121 is N {\u03bc + \u03c6(Yt\u22121 \u2212 \u03bc), \u03c3 2 }. It then follows that the joint density for\nY1 , . . . , Yn is\n                                                                           \u239b\n                                                                        \u000e \u000f2 \u239e\n\u0003        \u0004n \u0005           \u0006               \u0007\bn      \u239c Yi \u2212 \u03bc + \u03c6(Yi\u22121 \u2212 \u03bc)      \u239f\n     1                     (Y1 \u2212 \u03bc)2             \u239c                           \u239f\n  \u221a           1 \u2212 \u03c62 exp \u2212 2                 exp \u239c\u2212                          \u239f.\n    2\u03c0\u03c3\u0004                  2\u03c3\u0004 /(1 \u2212 \u03c6 )\n                                     2\n                                         i=2\n                                                 \u239d          2\u03c3\u0004\n                                                              2              \u23a0\n\n\n                                                                                                       (12.18)\n\nThe maximum likelihood estimator maximizes the logarithm of (12.18) over\n(\u03bc, \u03c6, \u03c3 ). A somewhat simpler estimator deletes the marginal density of Y1\nfrom the likelihood and maximizes the logarithm of\n                              \u239b \u001a                         \u001b2 \u239e\n        \u0007        \bn\u22121 5n         Y   \u2212 \u03bc + \u03c6(Y       \u2212 \u03bc)\n              1               \u239c    t             t\u22121         \u239f\n           \u221a              exp \u239d\u2212               2             \u23a0.     (12.19)\n             2\u03c0\u03c3      t=2\n                                            2\u03c3\n\n    This estimator is called the conditional least-squares estimator. It is \u201ccon-\nditional\u201d because it uses the conditional density of Y2 , . . . , Yn given Y1 . It is\na least-squares estimator because the estimates of \u03bc and \u03c6 minimize\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "Sample ACF for the in\ufb02ation rate time series and residual series from",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.6 AR(p) Models        325\n                            n    \u001a                          \u001b2\n                                     Yt \u2212 \u03bc + \u03c6(Yt\u22121 \u2212 \u03bc)        .                (12.20)\n                           t=2\n\nThe default method for the function arima() in R is to use the conditional\nleast-squares estimates as starting values for maximum likelihood. The MLE\nis returned, along with approximate standard errors. The default option is\nused in the examples in this book.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.6",
      "section_title": "AR(p) Models        325",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.6 AR(p) Models\nWe have seen that the ACF of an AR(1) process decays geometrically to zero if\n|\u03c6| < 1 and also alternates in sign if \u03c6 < 0. This is a limited range of behavior\nand many time series do not behave in this way. To get a more \ufb02exible class of\nmodels, but one that is still parsimonious, we can use a model that regresses\nthe current value of the process on several of the recent past values, not just\nthe most recent. Thus, we let the last p values of the process, Yt\u22121 , . . . , Yt\u2212p ,\nfeed back into the current value Yt .\n    Here is a formal de\ufb01nition. The stochastic process Yt is an AR(p) process if\n\n        Yt \u2212 \u03bc = \u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) + \u00b7 \u00b7 \u00b7 + \u03c6p (Yt\u2212p \u2212 \u03bc) + \u0017t ,\n\nwhere \u00171 , \u00172 , . . . is weak WN(0, \u03c3 2 ).\n    This is a multiple linear regression5 model with lagged values of the time\nseries as the \u201cx-variables.\u201d The model can also be expressed as\n\n                      Yt = \u03b20 + \u03c61 Yt\u22121 + \u00b7 \u00b7 \u00b7 + \u03c6p Yt\u2212p + \u0017t ,\n\nwhere \u03b20 = {1\u2212(\u03c61 + \u00b7 \u00b7 \u00b7 +\u03c6p )}\u03bc. The parameter \u03b20 is called the \u201cconstant\u201d or\n\u201cintercept\u201d as in an AR(1) model. It can be shown that {1\u2212(\u03c61 + \u00b7 \u00b7 \u00b7 +\u03c6p )}>0\nfor a stationary process, so \u03bc = 0 if and only if \u03b20 is zero.\n    Formulas for the ACFs of AR(p) processes with p > 1 are more compli-\ncated than for an AR(1) process and can be found in the time series textbooks\nlisted in the \u201cReferences\u201d section. However, software is available for comput-\ning and plotting the ACF of any AR processes, as well as for the MA and\nARMA processes to be introduced soon. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.6",
      "section_title": "AR(p) Models",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1 are more compli-\ncated than for an AR(1) process and can be found in the time series textbooks\nlisted in the \u201cReferences\u201d section. However, software is available for comput-\ning and plotting the ACF of any AR processes, as well as for the MA and\nARMA processes to be introduced soon. Figure",
        "start": 1226,
        "end": 1521
      }
    ]
  },
  {
    "content": "12.9 is a plot of the ACFs of\nthree AR(2) process. The ACFs were computed using R\u2019s ARMAacf() function.\nNotice the wide variety of ACFs that are possible with two AR parameters.\n    Most of the concepts we have discussed for AR(1) models generalize easily\nto AR(p) models. The conditional least squares or maximum likelihood esti-\nmators can be calculated using software such as R\u2019s arima() function. The\nresiduals are de\ufb01ned by\n\n             \u0017t = Yt \u2212 {\u03b2\u00020 + \u03c6\u00021 Yt\u22121 + \u00b7 \u00b7 \u00b7 + \u03c6\u0002t\u2212p Yt\u2212p },\n             \u0002                                                       t \u2265 p + 1.\n\n\n5\n    See Chap. 9 for an introduction to multiple regression.\n\f326      12 Time Series Models: Basics\n\n                                    ACF of three AR(2) processes\n\n\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "is a plot of the ACFs of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                       x*\n                       o\n                                x\n                                            x         x\n                            x          x         x           x     x\n                                                                          x   x\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "x*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5         o\n                                o\n                            *          o\n          ACF\n\n\n\n\n                                            o\n                                                 o    o      o     o      o   o\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                 *    *      *     *      *   *\n                                *           *\n                                       *                  * (0.5, \u22120.3)\n                                                          o (0.5, 0.15)\n                \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "*    *      *     *      *   *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                          x (0.15, 0.8)\n\n                       0        2           4         6            8          10\n                                                lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "x (0.15, 0.8)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9. ACF of three AR(2) processes; the legend gives the values of \u03c61 and \u03c62 .\n\n\nIf the AR(p) model \ufb01ts the time series well, then the residuals should look\nlike white noise. Residual autocorrelation can be detected by examining the\nsample ACF of the residuals and using the Ljung\u2013Box test. Any signi\ufb01cant\nresidual autocorrelation is a sign that the AR(p) model does not \ufb01t well.\n    One problem with AR models is that they often need a rather large value\nof p to \ufb01t a data set. The problem is illustrated by the following two examples.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "ACF of three AR(2) processes; the legend gives the values of \u03c61 and \u03c62 .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.6. Changes in the in\ufb02ation rate\u2014AR(p) models\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.6",
      "section_title": "Changes in the in\ufb02ation rate\u2014AR(p) models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10 is a plot of AIC and BIC versus p for AR(p) \ufb01ts to the\nchanges in the in\ufb02ation rate. Both criteria suggest that p should be large.\nAIC decreases steadily as p increases from 1 to 19, though there is a local\nminimum at 8. Even the conservative BIC criterion indicates that p should be\nas large as 6. Thus, AR models are not parsimonious for this example. The\nremedy is to use a MA or ARMA model, which are the topics of the next\nsections.\n    Many statistical software packages have functions to automate the search\nfor the AR model that optimizes AIC or other criteria. The auto.arima func-\ntion in R\u2019s forecast package found that p = 8 is the \ufb01rst local minimum of\nAIC.\n16   library(forecast)\n17   auto.arima(diff(y), max.p = 20, max.q = 0, d = 0, ic = \"aic\")\n\n      Series: diff(y)\n      ARIMA(8,0,0) with zero mean\n\f                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "is a plot of AIC and BIC versus p for AR(p) \ufb01ts to the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.6 AR(p) Models     327\n\n                                  AIC and BIC for AR fits to changes in inflation rate\n\n\n\n\n                     2560\n                            o                                      * AIC\n                            * o                                    o BIC                 o\n                                                                                    o\n                     2520\n                              *                                                 o o\n                                                                         o o\n                                   o                           o o o\n         criterion\n\n\n\n\n                                       o                 o o\n                                   *       o o o o o\n                     2480\n\n\n\n\n                                       *\n                                           *\n                                               * * * *\n                                                         * * * * * *\n                                                                              * * * * *\n                     2440\n\n\n\n\n                                           5             10              15              20\n                                                           p\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.6",
      "section_title": "AR(p) Models     327",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10. Fitting AR(p) models to changes in the one-month in\ufb02ation rate; AIC\nand BIC plotted against p.\n\n\n     Coefficients:\n               ar1                    ar2          ar3       ar4        ar5         ar6\n           -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "Fitting AR(p) models to changes in the one-month in\ufb02ation rate; AIC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6274                -0.4977      -0.5158   -0.4155    -0.3443     -0.2560\n     s.e.   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6274",
      "section_title": "-0.4977      -0.5158   -0.4155    -0.3443     -0.2560",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0456                 0.0536       0.0576    0.0606     0.0610      0.0581\n               ar7                    ar8\n           -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0456",
      "section_title": "0.0536       0.0576    0.0606     0.0610      0.0581",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1557                -0.1051\n     s.e.   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1557",
      "section_title": "-0.1051",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0543                 0.0459\n\n     sigma^2 estimated as 8.539: log likelihood=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0543",
      "section_title": "0.0459",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1221.2\n     AIC=2460.4   AICc=2460.78   BIC=2498.15\n\nThe \ufb01rst local minimum of BIC is at p = 6.\n18   auto.arima(diff(y), max.p = 20, max.q = 0, d = 0, ic = \"bic\")\n\n     Series: diff(y)\n     ARIMA(6,0,0) with zero mean\n\n     Coefficients:\n               ar1                    ar2          ar3       ar4        ar5         ar6\n           -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1221.2",
      "section_title": "AIC=2460.4   AICc=2460.78   BIC=2498.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6057                -0.4554      -0.4558   -0.3345    -0.2496     -0.1481\n     s.e.   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6057",
      "section_title": "-0.4554      -0.4558   -0.3345    -0.2496     -0.1481",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0454                 0.0522       0.0544    0.0546     0.0526      0.0457\n\n     sigma^2 estimated as 8.699:                    log likelihood=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0454",
      "section_title": "0.0522       0.0544    0.0546     0.0526      0.0457",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1225.67\n     AIC=2465.33   AICc=2465.56                     BIC=2494.69\n\n   We will see later that a more parsimonious \ufb01t can be obtained by going\nbeyond AR models.                                                       \u0002\n\f328      12 Time Series Models: Basics\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1225.67",
      "section_title": "AIC=2465.33   AICc=2465.56                     BIC=2494.69",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.7. In\ufb02ation rates\u2014AR(p) models\n\n   Since it is uncertain whether or not the in\ufb02ation rates are stationary, one\nmight \ufb01t an AR model to the in\ufb02ation rates themselves, rather than their\ndi\ufb00erences. An AR(p) models was \ufb01t to the in\ufb02ation rates with p selected via\nan information criterion by auto.arima(). The BIC method chose p = 2 and\nAIC selected p = 7. The results for p = 7 are below.\n\n      Series: y\n      ARIMA(7,0,0) with non-zero mean\n\n      Coefficients:\n              ar1   ar2       ar3     ar4     ar5     ar6     ar7    intercept\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "In\ufb02ation rates\u2014AR(p) models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.366 0.129    -0.020   0.099   0.065   0.080   0.119        3.987\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.366",
      "section_title": "0.129    -0.020   0.099   0.065   0.080   0.119        3.987",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.045 0.048      0.048   0.048   0.049   0.048   0.046        0.784\n\n      sigma^2 estimated as 8.47: log likelihood=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.045",
      "section_title": "0.048      0.048   0.048   0.049   0.048   0.046        0.784",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1221.8\n      AIC=2461.6   AICc=2461.9   BIC=2499.3\n\nThe in\ufb02ation rate and its residual series from an AR(7) \ufb01t and their sample\nACFs are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1221.8",
      "section_title": "AIC=2461.6   AICc=2461.9   BIC=2499.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.11.                                            \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.11",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.7 Moving Average (MA) Processes\nAs we saw in Example 12.6, there is a potential need for large values of p\nwhen \ufb01tting AR processes. A remedy for this problem is to add a moving ave-\nrage component to an AR(p) process. The result is an autoregressive-moving\naverage process, often called an ARMA process. Before introducing ARMA\nprocesses, we start with pure moving average (MA) processes.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "Moving Average (MA) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.7.1 MA(1) Processes\n\nThe idea behind AR processes is to feed past data back into the current value\nof the process. This induces correlation between the past and present. The\ne\ufb00ect is to have at least some correlation at all lags. Sometimes data show\ncorrelation at only short lags, for example, only at lag 1 or only at lags 1 and 2.\nSee, for example, Fig. 12.3b where the sample ACF of changes in the in\ufb02ation\nrate is approximately \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "1 MA(1) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 at lag 1, but then is approximately 0.1 or less in\nmagnitude after one lag. AR processes do not behave this way and, as already\nseen in Example 12.6, do not provide a parsimonious \ufb01t. In such situations, a\nuseful alternative to an AR model is a moving average (MA) model. A process\nYt is a moving average process if Yt can be expressed as a weighted average\n(moving average) of the past values of the white noise process {\u0017t }.\n\f                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "at lag 1, but then is approximately 0.1 or less in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.7 Moving Average (MA) Processes               329\n\n             a                                                              b\n\n\n                                 20\n\n\n\n\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "Moving Average (MA) Processes               329",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                Inflation Rate\n                                 5 10\n\n\n\n\n                                                                            ACF\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "Inflation Rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                 \u22125 0\n\n\n\n\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22125 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                        1950   1960   1970    1980   1990               0   5   10     15   20   25\n                                                      Year                                           Lag\n\n\n      c                                                                     d\n\n\n\n\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1950   1960   1970    1980   1990               0   5   10     15   20   25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n       Inflation Rate\n\n                                 5 10\n\n\n\n\n                                                                            ACF\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "Inflation Rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                 0\n\n\n\n\n                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                 \u221210\n\n\n\n\n                                        1950   1960   1970    1980   1990               0   5   10     15   20   25\n                                                      Year                                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u221210",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.11. (a) The in\ufb02ation rate series and (b) its sample ACF; (c) the residuals\nseries from an AR(7) \ufb01t to the in\ufb02ation rates (d) ACF of residuals.\n\n\n     The MA(1) (moving average of order 1) process is\n\n                                                              Yt \u2212 \u03bc = \u0017t + \u03b8\u0017t\u22121 ,                               (12.21)\n\nwhere as before the \u0017t are weak WN(0, \u03c3 2 ).6\n   One can show that\n\n                                                              E(Yt ) = \u03bc,\n                                                             Var(Yt ) = \u03c3 2 (1 + \u03b82 ),\n                                                                \u03b3(1) = \u03b8\u03c3 2 ,\n                                                                \u03b3(h) = 0 if |h| > 1,\n                                                                          \u03b8\n                                                                \u03c1(1) =         ,                                  (12.22)\n                                                                       1 + \u03b82\n                                                                \u03c1(h) = 0 if |h| > 1.                              (12.23)\n\nNotice the implication of (12.22) and (12.23)\u2014an MA(1) model has zero cor-\nrelation at all lags except lag 1 (and of course lag 0). It is relatively easy to\nderive these formulas and this is left as an exercise for the reader.\n\n6\n    Some textbooks and some software write MA models with the signs reversed so\n    that model (12.21) is written as Yt \u2212 \u03bc = t \u2212 \u03b8 t\u22121 . We have adopted the same\n    form of MA models as R\u2019s arima() function. These remarks apply as well to the\n    general MA and ARMA models given by Eqs. (12.24) and (12.25).\n\f330      12 Time Series Models: Basics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.11",
      "section_title": "(a) The in\ufb02ation rate series and (b) its sample ACF; (c) the residuals",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1,\n                                                                          \u03b8\n                                                                \u03c1(1) =         ,                                  (12.22)\n                                                                       1 + \u03b82\n                                                                \u03c1(h) = 0 if |h| > 1.                              (12.23)",
        "start": 713,
        "end": 1117
      }
    ]
  },
  {
    "content": "12.7.2 General MA Processes\n\nThe MA(q) process is\n\n                      Yt = \u03bc + \u0017t + \u03b81 \u0017t\u22121 + \u00b7 \u00b7 \u00b7 + \u03b8q \u0017t\u2212q .          (12.24)\n\n     One can show that \u03b3(h) = 0 and \u03c1(h) = 0 if |h| > q. Formulas for \u03b3(h)\nand \u03c1(h) when |h| \u2264 q are given in time series textbooks and these functions\ncan be computed in R by the function ARMAacf().\n     Unlike AR(p) models where the \u201cconstant\u201d in the model is not the same\nas the mean, in an MA(q) model \u03bc, the mean of the process, is the same as\n\u03b20 , the \u201cconstant\u201d in the model. This fact can be appreciated by examining\nthe right-hand side of Eq. (12.24), where \u03bc is the \u201cintercept\u201d or \u201cconstant\u201d\nin the model and is also the mean of Yt because \u0017t , . . . , \u0017t\u2212q have mean zero.\nMA(q) models can be \ufb01t easily using, for example, the arima() function in R.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.7",
      "section_title": "2 General MA Processes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "q. Formulas for \u03b3(h)\nand \u03c1(h) when |h| \u2264 q are given in time series textbooks and these functions\ncan be computed in R by the function ARMAacf().\n     Unlike AR(p) models where the \u201cconstant\u201d in the model is not the same\nas the mean, in an MA(q) model \u03bc, the mean of the process, is the same as\n\u03b20 , the \u201cconstant\u201d in the model. This fact can be appreciated by examining\nthe right-hand side of Eq. (12.24), where \u03bc is the \u201cintercept\u201d or \u201cconstant\u201d\nin the model and is also the mean of Yt because \u0017t , . . . , \u0017t\u2212q have mean zero.\nMA(q) models can be \ufb01t easily using, for example, the arima() function in R.",
        "start": 185,
        "end": 795
      }
    ]
  },
  {
    "content": "12.8. Changes in the in\ufb02ation rate\u2014MA models\n\n    MA(q) models were \ufb01t to the changes in the in\ufb02ation rate. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "Changes in the in\ufb02ation rate\u2014MA models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12\nshows plots of AIC and BIC versus q. BIC suggests that an MA(2) model is\nadequate, while AIC suggests an MA(3) model. We \ufb01t the MA(3) model. The\nLjung\u2013Box test was applied to the residuals with fitdf = 3 and lag equal\nto 5, 10, and 15 and gave p-values of 0.65, 0.76, and 0.32, respectively. The\nMA(2) also provided an adequate \ufb01t with the p-values from the Ljung\u2013Box\ntest all above ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "shows plots of AIC and BIC versus q. BIC suggests that an MA(2) model is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08. The output for the MA(3) model is below.\n19   fitMA3 = arima(diff(y), order = c(0,0,3))\n20   fitMA3\n\n      Series: diff(y)\n      ARIMA(0,0,3) with non-zero mean\n\n      Coefficients:\n               ma1    ma2          ma3     intercept\n            -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "The output for the MA(3) model is below.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.633 -0.103       -0.108         0.000\n      s.e.   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.633",
      "section_title": "-0.103       -0.108         0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.046  0.051        0.047         0.021\n\n      sigma^2 estimated as 8.5:          log likelihood=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.046",
      "section_title": "0.051        0.047         0.021",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1220.3\n      AIC=2450.5   AICc=2450.7           BIC=2471.5\n\n   Thus, if an MA model is used, then only two or three MA parameters are\nneeded. This is a strong contrast with AR models, which require far more\nparameters, perhaps as many as six.                                    \u0002\n\f                                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1220.3",
      "section_title": "AIC=2450.5   AICc=2450.7           BIC=2471.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8 ARMA Processes    331\n\n                                                        AIC and BIC for MA fits to changes in inflation rate\n\n\n\n\n                    2450 2460 2470 2480 2490 2500\n                                                                                                               o\n                                                        * AIC                                          o\n                                                        o BIC\n                                                                                                o\n                                                                                        o\n        criterion\n\n\n\n\n                                                    o                            o\n                                                    *                    o\n                                                           o      o\n                                                                                                       *       *\n                                                           *                     *      *       *\n                                                                  *       *\n\n                                                           2              4             6              8\n                                                                                 q\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "ARMA Processes    331",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12. Fitting MA(q) models to changes in the one-month in\ufb02ation rate; AIC\nand BIC plotted against q.\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "Fitting MA(q) models to changes in the one-month in\ufb02ation rate; AIC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8 ARMA Processes\n\nStationary time series with complex autocorrelation behavior often are more\nparsimoniously modeled by mixed autoregressive and moving average (ARMA)\nprocesses than by either a pure AR or pure MA process. For example, it is\nsometimes the case that a model with one AR and one MA parameter, called\nan ARMA(1, 1) model, will provide a more parsimonious \ufb01t than a pure AR\nor pure MA model. This section introduces ARMA processes.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "ARMA Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8.1 The Backwards Operator\n\nThe backwards operator B is a simple notation with a fancy name. It is use-\nful for describing ARMA (and ARIMA) models. The backwards operator is\nde\ufb01ned by\n                                                                       B Yt = Yt\u22121\nand, more generally,\n                                                                      B h Yt = Yt\u2212h .\nThus, B backs up time one unit while B h does this repeatedly so that time is\nbacked up h time units. Note that B c = c for any constant c, since a constant\ndoes not change with time. The backwards operator is sometimes called the\nlag operator.\n\f332      12 Time Series Models: Basics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "1 The Backwards Operator",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8.2 The ARMA Model\nAn ARMA(p, q) model combines both AR and MA terms and is de\ufb01ned by\nthe equation\n (Yt \u2212\u03bc) = \u03c61 (Yt\u22121 \u2212\u03bc)+\u00b7 \u00b7 \u00b7+\u03c6p (Yt\u2212p \u2212\u03bc)+\u0017t +\u03b81 \u0017t\u22121 +\u00b7 \u00b7 \u00b7+\u03b8q \u0017t\u2212q , (12.25)\nwhich shows how Yt depends on lagged values of itself and lagged values of\nthe white noise process. Equation (12.25) can be written more succinctly with\nthe backwards operator as\n      (1 \u2212 \u03c61 B \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p B p )(Yt \u2212 \u03bc) = (1 + \u03b81 B + \u00b7 \u00b7 \u00b7 + \u03b8q B q )\u0017t .   (12.26)\n   A white noise process is ARMA(0,0) since if p = q = 0, then (12.26)\nreduces to\n                                     (Yt \u2212 \u03bc) = \u0017t .\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "2 The ARMA Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8.3 ARMA(1,1) Processes\nThe ARMA(1,1) model is commonly used in practice and is simple enough\nto study theoretically. In this section, formulas for its variance and ACF will\nbe derived. Without loss of generality, one can assume that \u03bc = 0 when\ncomputing the variance and ACF. Multiplying the model\n                               Yt = \u03c6Yt\u22121 + \u03b8\u0017t\u22121 + \u0017t                          (12.27)\nby \u0017t and taking expectations, one has\n                             Cov(Yt , \u0017t ) = E(Yt \u0017t ) = \u03c3 2 ,                  (12.28)\nsince \u0017t is independent of \u0017t\u22121 and Yt\u22121 . From (12.27) and (12.28),\n                        \u03b3(0) = \u03c62 \u03b3(0) + (1 + \u03b82 )\u03c3 2 + 2\u03c6\u03b8\u03c3 2 ,                (12.29)\nand then solving (12.29) for \u03b3(0) gives us the formula\n                                       (1 + \u03b82 + 2\u03c6\u03b8)\u03c3 2\n                              \u03b3(0) =                     .                      (12.30)\n                                            1 \u2212 \u03c62\nBy similar calculations, multiplying (12.27) by Yt\u22121 and taking expecta-\ntions yields a formula for \u03b3(1). Dividing this formula by the right-hand side\nof (12.29) gives us\n                                   (1 + \u03c6\u03b8)(\u03c6 + \u03b8)\n                           \u03c1(1) =                  .                  (12.31)\n                                    1 + \u03b82 + 2\u03c6\u03b8\n    For h \u2265 2, multiplying (12.27) by Yt\u2212h and taking expectations results in\nthe formula\n                          \u03c1(h) = \u03c6\u03c1(h \u2212 1), h \u2265 2.                    (12.32)\nBy (12.32), after one lag, the ACF of an ARMA(1,1) process decays in the\nsame way as the ACF of an AR(1) process with the same \u03c6.\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "3 ARMA(1,1) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8 ARMA Processes        333\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "ARMA Processes        333",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8.4 Estimation of ARMA Parameters\n\nThe parameters of ARMA models can be estimated by maximum likelihood\nor conditional least-squares. These methods were introduced for AR(1) pro-\ncesses is Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "4 Estimation of ARMA Parameters",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.5. The estimation methods for AR(p) models are very similar\nto those for AR(1) models. For MA and ARMA, because the noise terms\n\u00171 , . . . , \u0017n are unobserved, there are complications that are best left for adv-\nanced time series texts.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.5",
      "section_title": "The estimation methods for AR(p) models are very similar",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9. Changes in risk-free returns\u2013ARMA models\n\n    This example uses the monthly changes in the risk-free returns shown\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "Changes in risk-free returns\u2013ARMA models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3. In Table 12.1, AIC and BIC are shown for ARMA models with\np, q = 0, 1, 2. We see that AIC and BIC are both minimized by the ARMA(1,1)\nmodel, though the MA(2) model is a very close second. The ARMA(1,1) and\nMA(2) \ufb01t nearly equally well, and it is di\ufb03cult to decide between them.\n    Sample ACF, normal, and time series plots of the residuals from the\nARMA(1,1) model are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.3",
      "section_title": "In Table 12.1, AIC and BIC are shown for ARMA models with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.13. The ACF plot shows no short-term\nautocorrelation, which is another sign that the ARMA(1,1) model is satisfac-\ntory. However, the normal plot shows heavy tails and the residual time series\nplot shows volatility clustering. These problems will be addressed in later\nchapters.                                                                  \u0002\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "The ACF plot shows no short-term",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1. AIC and BIC for ARMA models \ufb01t to the monthly changes in the risk-\nfree interest returns. The minimum values of both criteria are shown in boldface. To\nimprove the appearance of the table, 1290 was added to all AIC and BIC values.\n                                 p   q AIC BIC\n                                 0   0 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "AIC and BIC for ARMA models \ufb01t to the monthly changes in the risk-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "29.45 37.8\n                                 0   1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "29.45",
      "section_title": "37.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.21 21.8\n                                 0   2 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.21",
      "section_title": "21.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.00 19.8\n                                 1   0 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.00",
      "section_title": "19.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.86 27.5\n                                 1   1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.86",
      "section_title": "27.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.67 19.5\n                                 1   2 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.67",
      "section_title": "19.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.67 25.7\n                                 2   0 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.67",
      "section_title": "25.7",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.61 22.4\n                                 2   1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.61",
      "section_title": "22.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.98 28.0\n                                 2   2 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.98",
      "section_title": "28.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.89 30.1\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.89",
      "section_title": "30.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8.5 The Di\ufb00erencing Operator\n\nThe di\ufb00erencing operator is another useful notation and is de\ufb01ned as \u0394 =\n1 \u2212 B, where B is the backwards operator, so that\n\n                         \u0394Yt = Yt \u2212 B Yt = Yt \u2212 Yt\u22121 .\n\f334              12 Time Series Models: Basics\n\n      a                                                     b\n\n\n\n\n                                                                                    3\n                                                            Theoretical Quantiles\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "5 The Di\ufb00erencing Operator",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                                    2\n                                                                                    1\n      ACF\n\n\n\n\n                                                                                    0\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                    \u22121\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                    \u22123\n                        0      5   10          15   20                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22123",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4   \u22120.2    0.0     0.2    0.4\n                                   Lag                                                            Sample Quantiles\n\n\n      c\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2    0.0     0.2    0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                 0.2\n      Residual\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                 \u22120.2\n                 \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                        1960            1970             1980                                   1990            2000\n                                                           Time\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1960            1970             1980                                   1990            2000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.13. Residual sample ACF, normal quantile, and time series plots for the\nARMA(1, 1) \ufb01t to the monthly changes in the risk-free returns.\n\n\nFor example, if pt = log(Pt ) is the log price, then the log return is\n                                                    rt = \u0394pt .\nDi\ufb00erencing can be iterated. For example,\n                 \u03942 Yt = \u0394(\u0394Yt ) = \u0394(Yt \u2212 Yt\u22121 ) = (Yt \u2212 Yt\u22121 ) \u2212 (Yt\u22121 \u2212 Yt\u22122 )\n                                    = Yt \u2212 2Yt\u22121 + Yt\u22122 .\n\n\u0394k is called the kth-order di\ufb00erencing operator. A general formula for \u0394k can\nbe derived from a binomial expansion:\n                                         k \u0007 \b\n                                             k\n                  \u0394k Yt = (1 \u2212 B)k Yt =           (\u22121)\u0002 Yt\u2212\u0002 .         (12.33)\n                                              \u0007\n                                                           \u0002=0\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "Residual sample ACF, normal quantile, and time series plots for the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9 ARIMA Processes\nOften the \ufb01rst or perhaps second di\ufb00erences of nonstationary time series are\nstationary. For example, the \ufb01rst di\ufb00erences of a random walk (nonstationary)\nare white noise (stationary). In this section, autoregressive integrated moving\naverage (ARIMA) processes are introduced. They include stationary as well\nas nonstationary processes.\n\f                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "ARIMA Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9 ARIMA Processes      335\n\n    A time series Yt is said to be an ARIMA(p, d, q) process if \u0394d Yt is\nARMA(p, q). For example, if log returns on an asset are ARMA(p, q), then the\nlog prices are ARIMA(p, 1, q). An ARIMA(p, d, q) is stationary only if d = 0.\nOtherwise, only its di\ufb00erences of order d or above are stationary.\n    Notice that an ARIMA(p, 0, q) model is the same as an ARMA(p, q) model.\nARIMA(p, 0, 0), ARMA(p, 0), and AR(p) models are the same. Similarly,\nARIMA(0, 0, q), ARMA(0, q), and MA(q) models are the same. A random\nwalk is an ARIMA(0, 1, 0) model, and white noise is an ARIMA(0, 0, 0) model.\n    The inverse of di\ufb00erencing is \u201cintegrating.\u201d The integral of a process Yt is\nthe process wt , where\n\n                            wt = wt0 + Yt0 +1 + \u00b7 \u00b7 \u00b7 + Yt .                 (12.34)\n\nHere t0 is an arbitrary starting time point and wt0 is the starting value of the\nwt process. It is easy to check that\n\n                                     \u0394wt = Yt ,                              (12.35)\n\nso integrating and di\ufb00erencing are inverse processes.7\n    We will say that a process is I(d) if it is stationary after being di\ufb00erenced\nd times. For example, a stationary process is I(0). An ARIMA(p, d, q) process\nis I(d). An I(d) process is said to be \u201cintegrated to order d.\u201d\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "ARIMA Processes      335",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14 shows an AR(1) process, its integral, and its second integral,\nmeaning the integral of its integral. These three processes are I(0), I(1), and\nI(2), respectively. The three processes behave in entirely di\ufb00erent ways. The\nAR(1) process is stationary and varies randomly about its mean, which is 0;\none says that the process reverts to its mean. The integral of this process\nbehaves much like a random walk in having no \ufb01xed level to which it reverts.\nThe second integral has momentum. Once the process starts moving upward\nor downward, it tends to continue in that direction. If data show momentum\nlike this, then the momentum is an indication that d = 2. The AR(1) process\nwas generated by the R function arima.sim(). This process was integrated\ntwice with R\u2019s cumsum() function.\n21 set.seed(4631)\n22 y1 = arima.sim(n = 500, list(ar = c(0.4)))\n23 y2 = cumsum(y1)\n\n24 y3 = cumsum(y2)\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "shows an AR(1) process, its integral, and its second integral,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10. Fitting an ARIMA model to CPI data\n\n\n\n\n7\n     An analog is, of course, di\ufb00erentiation and integration in calculus, which are\n     inverses of each other.\n\f336       12 Time Series Models: Basics\n\n      a\n\n\n           3\n           1\n      y1\n\n           \u22123 \u22121\n\n\n                   0      100         200          300         400         500\n                                            Time\n\n\n      b\n           20\n           0\n      y2\n\n           \u221230\n\n\n\n\n                   0      100         200          300         400         500\n                                            Time\n\n\n      c\n           2000\n      y3\n\n           \u22121000\n\n\n\n\n                   0      100         200          300         400         500\n                                            Time\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "Fitting an ARIMA model to CPI data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14. The (a) top plot is of an AR(1) process with \u03bc = 0 and \u03c6 = 0.4. The\n(b) middle and (c) bottom plots are, respectively, the integral and second integral of\nthis AR(1) process. Thus, from top to bottom, the series are I(0), I(1), and I(2),\nrespectively.\n\n\n    This example uses the CPI.dat.csv data set. CPI is a seasonally adjusted\nU.S. Consumer Price Index. The data are monthly. Only data from January\n1977 to December 1987 are used in this example. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "The (a) top plot is of an AR(1) process with \u03bc = 0 and \u03c6 = 0.4. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15 shows time\nseries plots of log(CPI) and the \ufb01rst and second di\ufb00erences of this series. The\noriginal series shows the type of momentum that is characteristic of an I(2)\nseries. The \ufb01rst di\ufb00erences show no momentum, but they do not appear to\nbe mean-reverting and so they may be I(1). The second di\ufb00erences appear to\nbe mean-reverting and therefore seem to be I(0). ACF plots in Fig. 12.16a,b,\nand c provide additional evidence that the log(CPI) is I(2).\n    Notice that the ACF of \u03942 log(CPI) has large correlations at the \ufb01rst\ntwo lags and then small autocorrelations after that. This suggests using an\nMA(2) for \u03942 log(CPI) or, equivalently, an ARIMA(0,2,2) model for log(CPI).\nThe ACF of the residuals from this \ufb01t is shown in Fig. 12.16d. The residual\nACF has small correlations at short lags, which is an indication that the\nARIMA(0,2,2) model \ufb01ts well. Also, the residuals pass Ljung\u2013Box tests for\nvarious choices of lag, for example, with a p-value of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "shows time",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08 at lag = 20, with\nfitdf = 2.                                                                   \u0002\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "at lag = 20, with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9 ARIMA Processes    337\n\n a\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "ARIMA Processes    337",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.7\n  log(CPI)\n\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.7",
      "section_title": "log(CPI)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.4\n              4.1\n\n\n\n\n                              1978   1980   1982           1984      1986        1988\n                                                   year\n\n b\n \u0394 log(CPI)\n\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.4",
      "section_title": "4.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005 0.005\n\n\n\n\n                             1978    1980   1982           1984      1986        1988\n                                                   year\n\n\n c\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "0.005",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005\n \u0394 log(CPI)\n\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "\u0394 log(CPI)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005\n2\n\n\n\n\n                             1978    1980   1982          1984       1986        1988\n                                                   year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15. (a) log(CPI), (b) \ufb01rst di\ufb00erences of log(CPI), and (c) second di\ufb00erences\nof log(CPI).\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "(a) log(CPI), (b) \ufb01rst di\ufb00erences of log(CPI), and (c) second di\ufb00erences",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.11. Fitting an ARIMA model to industrial production (IP) data\n\n   This example uses the IP.dat data set. The variable, IP, is a seasonally\nadjusted U.S. industrial production index. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.11",
      "section_title": "Fitting an ARIMA model to industrial production (IP) data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.17 panels (a) and (b)\nshow time series plots of log(IP) and \u0394 log(IP) and panel (c) has the sample\nACF of \u0394 log(IP). The log(IP) series appears to be I(1), implying that we\nshould \ufb01t an ARMA model to \u0394 log(IP). AR(1), AR(2), and ARMA(1,1)\neach \ufb01t \u0394 log(IP) reasonably well and the ARMA(1,0) model is selected using\nthe BIC criterion with R\u2019s auto.arima() function. The ACF of the residuals\nin Fig. 12.17d indicates a satisfactory \ufb01t to the ARMA(1,0) model since it\nshows virtually no short-term autocorrelation. In summary, log(IP) is well \ufb01t\nby an ARIMA(1,1,0) model.                                                  \u0002\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.17",
      "section_title": "panels (a) and (b)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9.1 Drifts in ARIMA Processes\n\nIf a nonstationary process has a constant mean, then the \ufb01rst di\ufb00erences of this\nprocess have mean zero. For this reason, it is often assumed that a di\ufb00erenced\nprocess has mean zero. The arima() function in R makes this assumption.\n\f338         12 Time Series Models: Basics\n\n      a                    log(CPI)                b                      \u0394 log(CPI}\n\n\n\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "1 Drifts in ARIMA Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                         1.0\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                         0.6\n      ACF\n\n\n\n\n                                                   ACF\n                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n            0.2\n\n\n\n\n                                                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n            \u22120.2\n\n\n\n\n                   0   5      10         15   20                0     5        10      15     20\n                              Lag                                              Lag\n\n\n      c                    \u03942 log(CPI}             d                residuals, ARIMA(0,2,2)\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                         1.0\n                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n            0.6\n      ACF\n\n\n\n\n                                                   ACF\n                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n            0.2\n\n\n\n\n                                                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n            \u22120.2\n\n\n\n\n                   0   5      10         15   20                0     5        10      15     20\n                              Lag                                              Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.16. Sample ACF of (a) log(CPI), (b) \ufb01rst di\ufb00erences of log(CPI), (c) sec-\nond di\ufb00erences of log(CPI), and (d) residuals from an ARIMA(0,2,2) model \ufb01t to\nlog(CPI).\n\n\n    Instead of a constant mean, sometimes a nonstationary process has a mean\nwith a deterministic linear trend, e.g., E(Yt ) = \u03b20 + \u03b21 t. Then, \u03b21 is called\nthe drift of Yt . Note that E(\u0394Yt ) = \u03b21 , so if Yt has a nonzero drift then \u0394Yt\nhas a nonzero mean. The R function auto.arima() discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.16",
      "section_title": "Sample ACF of (a) log(CPI), (b) \ufb01rst di\ufb00erences of log(CPI), (c) sec-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.11\nallows a di\ufb00erenced process to have a nonzero mean, which is called the drift\nin the output.\n    These ideas can be extended to higher-degree polynomial trends and\nhigher-order di\ufb00erencing. If E(Yt ) has an mth-degree polynomial trend, then\nthe mean of E(\u0394d Yt ) has an (m \u2212 d)th-degree trend for d \u2264 m. For d > m,\nE(\u0394d Yt ) = 0.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.11",
      "section_title": "allows a di\ufb00erenced process to have a nonzero mean, which is called the drift",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "m,\nE(\u0394d Yt ) = 0.",
        "start": 316,
        "end": 337
      }
    ]
  },
  {
    "content": "12.10 Unit Root Tests\nWe have seen that it can be di\ufb03cult to tell whether a time series is best\nmodeled as stationary or nonstationary. To help decide between these two\npossibilities, it can be helpful to use hypothesis testing.\n   What is meant by a unit root? Recall that an ARMA(p, q) process can be\nwritten as\n (Yt \u2212\u03bc) = \u03c61 (Yt\u22121 \u2212\u03bc)+\u00b7 \u00b7 \u00b7+\u03c6p (Yt\u2212p \u2212\u03bc)+\u0017t +\u03b81 \u0017t\u22121 +\u00b7 \u00b7 \u00b7+\u03b8q \u0017t\u2212q . (12.36)\n\f                                                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "Unit Root Tests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10 Unit Root Tests          339\n\n     a                                                         b\n\n\n\n\n                                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "Unit Root Tests          339",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01 0.03\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "0.03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.30 4.40 4.50\n\n\n\n\n                                                               diff(logIP)\n     logIP\n\n\n                                                                                                                  l\n\n\n\n\n                                                                             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.30",
      "section_title": "4.40 4.50",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n                              0   20 40 60 80        120                                 0    20 40 60 80             120\n                                        Index                                                        Index\n\n     c                                                         d\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "0   20 40 60 80        120                                 0    20 40 60 80             120",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                             1.0\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                             0.6\n     ACF\n\n\n\n\n                                                               ACF\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             0.2\n             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             \u22120.2\n                              0     5   10      15    20                                 0       5    10     15        20\n                                        Lag                                                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.17. Time series plots of (a) log(IP) and (b) \u0394 log(IP), and sample ACF\nplots of (c) \u0394 log(IP) and (d) residual from ARMA(1,0) \ufb01t to \u0394 log(IP).\n\n\nThe condition for {Yt } to be stationary is that all roots of the polynomial\n                                                1 \u2212 \u03c6 1 x \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p xp                                               (12.37)\nhave absolute values greater than one. (See Appendix A.21 for information\nabout complex roots of polynomials and the absolute value of a complex\nnumber.) For example, when p = 1, then (12.37) is\n                                                           1 \u2212 \u03c6x\nand has one root, 1/\u03c6. We know that the process is stationary if |\u03c6| < 1,\nwhich, of course, is equivalent to |1/\u03c6| > 1.\n   If there is a unit root, that is, a root with an absolute value equal to 1,\nthen the ARMA process is nonstationary and behaves much like a random\nwalk. Not surprisingly, this is called the unit root case. The explosive case is\nwhen a root has an absolute value less than 1.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.17",
      "section_title": "Time series plots of (a) log(IP) and (b) \u0394 log(IP), and sample ACF",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1.\n   If there is a unit root, that is, a root with an absolute value equal to 1,\nthen the ARMA process is nonstationary and behaves much like a random\nwalk. Not surprisingly, this is called the unit root case. The explosive case is\nwhen a root has an absolute value less than 1.",
        "start": 729,
        "end": 1012
      }
    ]
  },
  {
    "content": "12.12. In\ufb02ation rates\n\n   Recall from Examples ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "In\ufb02ation rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.1 and 12.3 that we have had di\ufb03culty deciding\nwhether the in\ufb02ation rates are stationary or not. If we \ufb01t stationary ARMA\nmodels to the in\ufb02ation rates, then auto.arima() selects an ARMA(2,1) model\nand the AR coe\ufb03cients are \u03c6\u00021 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.1",
      "section_title": "and 12.3 that we have had di\ufb03culty deciding",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.229 and \u03c6\u00022 = \u22120.233. The roots of\n                                1 \u2212 \u03c6\u00021 x \u2212 \u03c6\u00022 x2\n\f340      12 Time Series Models: Basics\n\ncan be found easily using R\u2019s polyroot() function and are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.229",
      "section_title": "and \u03c6\u00022 = \u22120.233. The roots of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0053 and 4.2694.\n25   polyroot(c(1, -1.229, +0.233))\nBoth roots have absolute values greater than 1, indicating possible station-\narity; however, the \ufb01rst is very close to 1, and since the roots are estimated\nwith error, there is reason to suspect that this series may be nonstationary. \u0002\n    Unit root tests are used to decide if an AR model has an absolute root\nequal to 1. One popular unit root test is the augmented Dickey\u2013Fuller test,\noften called the ADF test. The null hypothesis is that there is a unit root.\nThe usual alternative is that the process is stationary, but one can instead\nuse the alternative that the process is explosive.\n    Another unit root test is the Phillips\u2013Perron test. It is similar to the\nDickey\u2013Fuller test, but di\ufb00ers in some details.\n    A third test is the KPSS test. The null hypothesis for the KPSS test\nis stationarity and the alternative is a unit root, just the opposite of the\nhypotheses for the Dickey\u2013Fuller and Phillips\u2013Perron tests.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0053",
      "section_title": "and 4.2694.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.13. In\ufb02ation rates\u2014unit root tests\n\n    Recall that we were undecided as to whether or not the in\ufb02ation rate time\nseries was stationary. The unit root tests might help resolve this issue, but\nunfortunately they do not provide unequivocal evidence in favor of stationar-\nity. Both the augmented Dickey\u2013Fuller and Phillips\u2013Perron tests, which were\nimplemented in R with the functions adf.test() and pp.test(), respectively,\nhave small p-values, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "In\ufb02ation rates\u2014unit root tests",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.016 for the former and less than 0.01 for the latter; see\nthe output below. The functions adf.test(), pp.test(), and kpss.test()\n(used below) are in R\u2019s tseries package. Therefore, at level ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.016",
      "section_title": "for the former and less than 0.01 for the latter; see",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 the null\nhypothesis of a unit root is rejected by both tests in favor of the alternative\nof stationarity, the default alternative hypothesis for both adf.test() and\npp.test().\n26   library(tseries)\n27   adf.test(y)\n\n      Augmented Dickey-Fuller Test\n      data: y\n      Dickey-Fuller = -3.8651, Lag order = 7, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "the null",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01576\n      alternative hypothesis: stationary\n\n28   pp.test(y)\n\n      Phillips-Perron Unit Root Test\n      data: y\n      Dickey-Fuller Z(alpha) = -248.75, Truncation lag parameter = 5,\n        p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01576",
      "section_title": "alternative hypothesis: stationary",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n      alternative hypothesis: stationary\n\f                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "alternative hypothesis: stationary",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10 Unit Root Tests      341\n\n   Although the augmented Dickey\u2013Fuller and Phillips\u2013Perron tests suggest\nthat the in\ufb02ation rate series is stationary since the null hypothesis of a unit\nroot is rejected, the KPSS test leads one to the opposite conclusion. The null\nhypothesis for the KPSS is stationarity and it is rejected with a p-value smaller\nthan ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "Unit Root Tests      341",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01. Here is the R output.\n29   kpss.test(y)\n\n     KPSS Test for Level Stationarity\n     data: y\n     KPSS Level = 2.51, Truncation lag parameter = 5, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "Here is the R output.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n\nThus, the unit root tests are somewhat contradictory. Perhaps the in\ufb02ation\nrates are stationary with long-term memory. Long-memory processes will be\nintroduced in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "Thus, the unit root tests are somewhat contradictory. Perhaps the in\ufb02ation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5.                                                \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.10.1 How Do Unit Root Tests Work?\n\nA full discussion of the theory behind unit root tests is beyond the scope of\nthis book. Here, only the basic idea will be mentioned. See Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.10",
      "section_title": "1 How Do Unit Root Tests Work?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14 for\nmore information. The Dickey\u2013Fuller test is based on the AR(1) model\n\n                                 Yt = \u03c6Yt\u22121 + \u0017t .                             (12.38)\n\nThe null hypothesis (H0 ) is that there is a unit root, that is, \u03c6 = 1, and the\nalternative (H1 ) is stationarity, which is equivalent to \u03c6 < 1, assuming, as\nseems reasonable, that \u03c6 > \u22121. The AR(1) model (12.38) is equivalent to\n\u0394Yt = (\u03c6 \u2212 1)Yt\u22121 + \u0017t , or\n\n                                \u0394Yt = \u03c0Yt\u22121 + \u0017t ,                             (12.39)\n\nwhere \u03c0 = \u03c6 \u2212 1. Stated in terms of \u03c0, H0 is \u03c0 = 0 and H1 is \u03c0 < 0.\nThe Dickey\u2013Fuller test regresses \u0394Yt on Yt\u22121 and tests H0 . Because Yt\u22121 is\nnonstationary under H0 , the t-statistic for \u03c0 has a nonstandard distribution\nso special tables need to be developed in order to compute p-values.\n    The augmented Dickey\u2013Fuller test expands model (12.39) by adding a time\ntrend and lagged values of \u0394Yt . Typically, the time trend is linear so that the\nexpanded model is\n                                                 p\n                    \u0394Yt = \u03b20 + \u03b21 t + \u03c0Yt\u22121 +         \u03b3j \u0394Yt\u2212j + \u0017t .          (12.40)\n                                                j=1\n\nThe hypotheses are still H0 : \u03c0 = 0 and H1 : \u03c0 < 0. There are several meth-\nods for selecting p. The adf.test() function has a default value of p equal\nto trunc((length(y)-1)^(1/3)), where y is the input series (Yt in our\nnotation).\n\f342      12 Time Series Models: Basics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "for",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "\u22121. The AR(1) model (12.38) is equivalent to\n\u0394Yt = (\u03c6 \u2212 1)Yt\u22121 + \u0017t , or",
        "start": 351,
        "end": 427
      }
    ]
  },
  {
    "content": "12.11 Automatic Selection of an ARIMA Model\nIt is useful to have an automatic method for selecting an ARIMA model. As\nalways, an automatically selected model should not be accepted blindly, but\nit makes sense to start model selection with something chosen quickly and by\nan objective criterion.\n    The R function auto.arima() can select all three parameters, p, d, and\nq, for an ARIMA model. The di\ufb00erencing parameter d is selected using the\nKPSS test. If the null hypothesis of stationarity is accepted when the KPSS\nis applied to the original time series, then d = 0. Otherwise, the series is\ndi\ufb00erenced until the KPSS accepts the null hypothesis. After that, p and q\nare selected using either AIC or BIC.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.11",
      "section_title": "Automatic Selection of an ARIMA Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14. In\ufb02ation rates\u2014automatic selection of an ARIMA model\n\n    In this example, auto.arima() is applied to the in\ufb02ation rates. The\nARIMA (1,1,1) model is selected by auto.arima() using either AIC or BIC\nto select p and q after d = 1 is selected by the KPSS test.\n30   auto.arima(y, max.p = 5, max.q = 5, ic = \"bic\", trace = FALSE)\n\n      Series: y\n      ARIMA(1,1,1)\n\n      Coefficients:\n              ar1     ma1\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "In\ufb02ation rates\u2014automatic selection of an ARIMA model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.238 -0.877\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.238",
      "section_title": "-0.877",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.055    0.027\n\n      sigma^2 estimated as 8.55: log likelihood=-",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.055",
      "section_title": "0.027",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1221.6\n      AIC=2449.2   AICc=2449.3   BIC=2461.8\n\nThis is a very parsimonious model and residual diagnostics (not shown) show\nthat it \ufb01ts well.\n    AICc in the output from auto.arima() is the value of the corrected AIC\ncriterion de\ufb01ned by (5.34). The sample size is 491 so, not surprisingly, AICc\nis equal to AIC, at least after rounding to the nearest integer.            \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1221.6",
      "section_title": "AIC=2449.2   AICc=2449.3   BIC=2461.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12 Forecasting\n\nForecasting means predicting future values of a time series using the current\ninformation set, which is the set of present and past values of the time series.\nIn some contexts, the information set could include other variables related to\nthe time series, but in this section the information set contains only the past\nand present values of the time series that is being predicted.\n\f                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "Forecasting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12 Forecasting      343\n\n   ARIMA models are often used for forecasting. Consider forecasting using\n                                                                          \u0002\nan AR(1) process. Suppose that we have data Y1 , . . . , Yn and estimates \u03bc\n    \u0002 We know that\nand \u03c6.\n\n                        Yn+1 = \u03bc + \u03c6(Yn \u2212 \u03bc) + \u0017n+1 .                       (12.41)\n\nSince \u0017n+1 is independent of the past and present, by Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "Forecasting      343",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.1 in Sect. 11.9.2\nthe best predictor of \u0017n+1 is its expected value, which is 0. We know, of course,\nthat \u0017n+1 is not 0, but 0 is our best guess at its value. On the other hand,\nwe know or have estimates of all other quantities in (12.41). Therefore, we\npredict Yn+1 by\n                             Y\u0002n+1 = \u03bc    \u0002 n\u2212\u03bc\n                                      \u0002 + \u03c6(Y     \u0002).\nBy the same reasoning we forecast Yn+2 by\n\n                Y\u0002n+2 = \u03bc   \u0002 Y\u0002n+1 \u2212 \u03bc\n                        \u0002 + \u03c6(        \u0002) = \u03bc   \u0002 \u03c6(Y\n                                           \u0002 + \u03c6{ \u0002 n\u2212\u03bc\n                                                      \u0002)},                  (12.42)\n\nand so forth. Notice that in (12.42) we do not use Yn+1 , which is unknown\nat time n, but rather the forecast Y\u0002n+1 . Continuing in this way, we \ufb01nd the\ngeneral formula for the k-step-ahead forecast:\n\n                                   \u0002 + \u03c6\u0002k (Yn \u2212 \u03bc\n                           Y\u0002n+k = \u03bc             \u0002).                        (12.43)\n    \u0002 < 1, as is true for a stationary series, then as k increases, the forecasts\nIf |\u03c6|\nwill converge geometrically fast to \u03bc\u0002.\n    Formula (12.43) is valid only for AR(1) processes, but forecasting other\nAR(p) processes is similar. For an AR(2) process,\n\n                           \u0002 + \u03c6\u00021 (Yn \u2212 \u03bc\n                   Y\u0002n+1 = \u03bc             \u0002) + \u03c6\u00022 (Yn\u22121 \u2212 \u03bc\n                                                          \u0002)\n\nand\n                           \u0002 + \u03c6\u00021 (Y\u0002n+1 \u2212 \u03bc\n                   Y\u0002n+2 = \u03bc                \u0002) + \u03c6\u00022 (Yn \u2212 \u03bc\n                                                           \u0002),\nand so on.\n   Forecasting ARMA and ARIMA processes is similar to forecasting AR\nprocesses. Consider the MA(1) process, Yt \u2212 \u03bc = \u0017t + \u03b8\u0017t\u22121 . Then the next\nobservation will be\n                         Yn+1 = \u03bc + \u0017n+1 + \u03b8\u0017n .                   (12.44)\nIn the right-hand side of (12.44) we replace \u03bc and \u03b8 by estimates and \u0017n by\nthe residual \u0002\n             \u0017n . Also, since \u0017n+1 is independent of the observed data, it is\nreplaced by its mean 0. Then the forecast is\n\n                                       \u0002 + \u03b8\u0002 \u0002\n                               Y\u0002n+1 = \u03bc      \u0017n .\n\nThe two-step-ahead forecast of Yn+2 = \u03bc + \u0017n+2 + \u03b8\u0017n+1 is simply Y\u0002n+2 = \u03bc  \u0002,\nsince \u0017n+1 and \u0017n+2 are independent of the observed data. Similarly, Y\u0002n+k = \u03bc\n                                                                             \u0002\nfor all k > 2.\n\f344      12 Time Series Models: Basics\n\n      To forecast the ARMA(1,1) process\n\n                        Yt \u2212 \u03bc = \u03c6(Yt\u22121 \u2212 \u03bc) + \u0017t + \u03b8\u0017t\u22121 ,\n\nwe use\n                           Y\u0002n+1 = \u03bc   \u0002 n\u2212\u03bc\n                                   \u0002 + \u03c6(Y \u0002) + \u03b8\u0002 \u0002\n                                                   \u0017n\nas the one-step-ahead forecast and\n\n                        Y\u0002n+k = \u03bc   \u0002 Y\u0002n+k\u22121 \u2212 \u03bc\n                                \u0002 + \u03c6(          \u0002), k \u2265 2\n\nfor forecasting two or more steps ahead.\n    As a \ufb01nal example, suppose that Yt is ARIMA(1,1,0), so that \u0394Yt is AR(1).\nTo forecast Yn+k , k \u2265 1, one \ufb01rst \ufb01ts an AR(1) model to the \u0394Yt process and\nforecasts \u0394Yn+k , k \u2265 1. Let the forecasts be denoted by \u0394Y n+k , k \u2265 1. Then,\nsince\n                               Yn+1 = Yn + \u0394Yn+1 ,\nthe forecast of Yn+1 is\n                               Y\u0002n+1 = Yn + \u0394Y n+1 ,\nand similarly\n\n                Y\u0002n+2 = Y\u0002n+1 + \u0394Y n+2 = Yn + \u0394Y n+1 + \u0394Y n+2 ,\nand so on.\n    Most time series software packages o\ufb00er functions to automate forecasting.\nR\u2019s predict() function forecasts using an \u201cobject\u201d returned by the arima()\n\ufb01tting function.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.1",
      "section_title": "in Sect. 11.9.2",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "2.\n\f344      12 Time Series Models: Basics",
        "start": 2395,
        "end": 2441
      }
    ]
  },
  {
    "content": "12.12.1 Forecast Errors and Prediction Intervals\n\nWhen making forecasts, one would of course like to know the uncertainty of\nthe predictions. To this end, one \ufb01rst computes the variance of the forecast\nerror. Then a (1 \u2212 \u03b1)100 % prediction interval is the forecast itself plus or\nminus the forecast error\u2019s standard deviation times z\u03b1/2 (the normal upper\nquantile). The use of z\u03b1/2 assumes that \u00171 , \u00172 , . . . is Gaussian white noise. If the\nresiduals are heavy-tailed, then we might be reluctant to make the Gaussian\nassumption. One way to avoid this assumption is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "1 Forecast Errors and Prediction Intervals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12.2.\n    Computation of the forecast error variance and the prediction interval is\nautomated by modern statistical software, so we need not present general\nformulas for the forecast error variance. However, to gain some understanding\nof general principles, we will look at two special cases, one stationary and the\nother nonstationary.\n\f                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12 Forecasting      345\n\nStationary AR(1) Forecast Errors\n\nWe will \ufb01rst consider the errors made when forecasting a stationary AR(1)\nprocess. The error in the \ufb01rst prediction is\n\n         Yn+1 \u2212 Y\u0002n+1 = {\u03bc + \u03c6(Yn \u2212 \u03bc) + \u0017n+1 } \u2212 {\u0002     \u0002 n\u2212\u03bc\n                                                    \u03bc + \u03c6(Y    \u0002)}\n                      = (\u03bc \u2212 \u03bc           \u0002            \u0002\n                               \u0002) + (\u03c6 \u2212 \u03c6)Yn \u2212 (\u03c6\u03bc \u2212 \u03c6\u0002\n                                                       \u03bc) + \u0017n+1   (12.45)\n                      \u2248 \u0017n+1 .                                     (12.46)\n\nHere (12.45) is the exact error and (12.46) is a \u201clarge-sample\u201d approximation.\nThe basis for (12.46) is that as the sample size increases \u03bc\u0002 \u2192 \u03bc and \u03c6\u0002 \u2192 \u03c6,\nso the \ufb01rst three terms in (12.45) converge to 0, but the last term remains\nunchanged. The large-sample approximation simpli\ufb01es formulas and helps us\nfocus on the main components of the forecast error. Using the large-sample\napproximation again, so \u03bc \u0002 is replaced by \u03bc and \u03c6\u0002 by \u03c6, the error in the two-\nsteps-ahead forecast is\n\n       Yn+2 \u2212 Y\u0002n+2 = {\u03bc + \u03c6(Yn+1 \u2212 \u03bc) + \u0017n+2 } \u2212 {\u03bc + \u03c6(Y\u0002n+1 \u2212 \u03bc)}\n                    = \u03c6(Yn+1 \u2212 Y\u0002n+1 ) + \u0017n+1\n                    = \u03c6\u0017n+1 + \u0017n+2 .                                      (12.47)\n\nContinuing in this manner, we \ufb01nd that the k-step-ahead forecasting error is\n\n    Yn+k \u2212 Y\u0002n+k \u2248 {\u03bc + \u03c6(Yn+k\u22121 \u2212 \u03bc) + \u0017n+k } \u2212 {\u03bc + \u03c6(Y\u0002n+k\u22121 \u2212 \u03bc)}\n                  = \u03c6k\u22121 \u0017n+1 + \u03c6k\u22122 \u0017n+2 + \u00b7 \u00b7 \u00b7 + \u03c6\u0017n+k\u22121 + \u0017n+k .      (12.48)\n\nBy the formula for the sum of a \ufb01nite geometric series, the variance of the\nright-hand side of (12.48) is\n         (                                   )      \u0007         \b\n                                                      1 \u2212 \u03c62k\n           \u03c62(k\u22121) + \u03c62(k\u22122) + \u00b7 \u00b7 \u00b7 + \u03c62 + 1 \u03c3 2 =             \u03c32\n                                                      1 \u2212 \u03c62\n                                                      \u03c32\n                                                  \u2192        as k \u2192 \u221e. (12.49)\n                                                    1 \u2212 \u03c62\n    An important point here is that the variance of the forecast error does not\ndiverge as k \u2192 \u221e, but rather the variance converges to \u03b3(0), the marginal\ncovariance of the AR(1) process given by (12.7). This is an example of the\ngeneral principle that for any stationary ARMA process, the variance of the\nforecast error converges to the marginal variance.\n\nForecasting a Random Walk\n\nFor the random walk process, Yn+1 = \u03bc + Yn + \u0017n+1 , many of the formulas\njust derived for the AR(1) process still hold, but with \u03c6 = 1. An exception is\n\f346    12 Time Series Models: Basics\n\nthat the last result in (12.49) does not hold because the summation formula\nfor a geometric series does not apply when \u03c6 = 1. One result that does still\nhold is\n              Yn+k \u2212 Y\u0002n+k = \u0017n+1 + \u0017n+2 + \u00b7 \u00b7 \u00b7 + \u0017n+k\u22121 + \u0017n+k\nso the variance of the k-step-ahead forecast error is k\u03c3 2 and, unlike for the\nstationary AR(1) case, the forecast error variance diverges to \u221e as k \u2192 \u221e.\n\nForecasting ARIMA Processes\n\nAs mentioned before, in practice we do not need general formulas for the\nforecast error variance of ARIMA processes, since statistical software can\ncompute the variance. However, it is worth repeating a general principle: For\nstationary ARMA processes, the variance of the k-step-ahead forecast error\nvariance converges to a \ufb01nite value as k \u2192 \u221e, but for a nonstationary ARIMA\nprocess this variance converges to \u221e. The result of this principle is that for\na nonstationary process, the forecast limits diverge away from each other as\nk \u2192 \u221e, but for a stationary process the forecast limits converge to parallel\nhorizontal lines.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "Forecasting      345",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15. Forecasting the one-month in\ufb02ation rate\n\n    We saw in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "Forecasting the one-month in\ufb02ation rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.8 that an MA(3) model provided a parsimonious\n\ufb01t to the changes in the one-month in\ufb02ation rate. This implies that an\nARIMA(0,1,3) model will be a good \ufb01t to the in\ufb02ation rates themselves. The\ntwo models are, of course, equivalent, but they forecast di\ufb00erent series. The\n\ufb01rst model gives forecasts and con\ufb01dence limits for the changes in the in\ufb02a-\ntion rate, while the second model provides forecasts and con\ufb01dence limits for\nthe in\ufb02ation rate itself.\n    Figures ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.8",
      "section_title": "that an MA(3) model provided a parsimonious",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.18 and 12.19 plot forecasts and forecast limits from the two\nmodels out to 100 steps ahead. One can see that the forecast limits diverge\nfor the second model and converge to parallel horizontal lines for the \ufb01rst\nmodel.                                                                     \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.18",
      "section_title": "and 12.19 plot forecasts and forecast limits from the two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12.2 Computing Forecast Limits by Simulation\n\nSimulation can be used to compute forecasts limits. This is done by simulating\nrandom forecasts and \ufb01nding their \u03b1/2-upper and -lower sample quantiles. A\nset of random forecasts up to m time units ahead is generated for an ARMA\nprocess by recursion:\n\n                \u0002 + \u03c6\u00021 (Y\u0002n+t\u22121 \u2212 \u03bc\n        Y\u0002n+t = \u03bc                  \u0002) + \u00b7 \u00b7 \u00b7 + \u03c6\u0002p (Y\u0002n+t\u2212p \u2212 \u03bc\n                                                               \u0002)\n                +\u0002          \u0002\n                  \u0017n+t + \u03b81 \u0002                  \u0002\n                              \u0017n+t\u22121 + \u00b7 \u00b7 \u00b7 + \u03b8q \u0002\n                                                  \u0017n+t\u2212q , t = 1, . . . , m, (12.50)\n\f                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "2 Computing Forecast Limits by Simulation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12 Forecasting                                                                347\n\nin which\n\u2022 \u0002\n  \u0017k is the kth residual if k \u2264 n,\n\u2022 {\u0002\n   \u0017k : k = n + 1, . . . , n + m} is a resample from the residuals.\n                                      15\n\n\n                                               o   data\n                                               *   predictions\n                                      10\n\n\n\n\n                                                   lower CL\n           Change in inflation rate\n\n\n\n\n                                                   upper CL\n                                      5\n\n\n\n\n                                                                                   *\n\n                                                              l\n                                                                                   **\n                                      0\n\n\n\n\n                                                                                    *************************************************************************************************\n                                                        l\n                                      \u22125\n\n\n\n\n                                           1975        1980       1985          1990                                  1995\n                                                                         year\n\n                                           Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "Forecasting                                                                347",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.18. Forecasts of changes in in\ufb02ation rate.\n\n    Thus, Y\u0002n+1 is generated from Yn+1\u2212p , . . . , Yn , \u0002                    \u0017n+1 , then Y\u0002n+2\n                                                            \u0017n+1\u2212q , . . . , \u0002\nis generated from Yn+2\u2212p , . . . , Yn , Y\u0002n+1 , \u0002                \u0017n+2 , then Y\u0002n+3 is gener-\n                                                \u0017n+2\u2212q , . . . , \u0002\n                                \u0002      \u0002\nated from Yn+3\u2212p , . . . , Yn , Yn+1 , Yn+2 , \u0002\n                                              \u0017n+3\u2212q , . . . , \u0002\n                                                               \u0017n+3 , and so forth.\n    A large number, call it B, of sets of random forecasts are generated in\nthis way. They di\ufb00er because their sets of future errors generated in step two\nare mutually independent. For each t = 1, . . . , m, the \u03b1/2-upper and -lower\nsample quantiles of the B random values of Y\u0002n+h are the forecast limits for\nYn+h .\n    To obtain forecasts, rather than forecast limits, one uses \u0002             \u0017k = 0, for k =\nn + 1, . . . , n + m, in step two. The forecasts are nonrandom, conditional given\nthe data, and therefore need to be computed only once.\n    If Yt = \u0394Wt for some nonstationary series {W1 , . . . , Wn }, then random\nforecasts of {Wn+1 , . . .} can be obtained as partial sums of {Wn , Y\u0002n+1 , . . .}.\nFor example,\n                 \u001fn+1 = Wn + Y\u0002n+1 ,\n                 W\n                                      \u001fn+2 = W\n                                      W      \u001fn+1 + Y\u0002n+2 = Wn + Y\u0002n+1 + Y\u0002n+2 ,\n                                      \u001fn+3 = W\n                                      W      \u001fn+2 + Y\u0002n+3 = Wn + Y\u0002n+1 + Y\u0002n+2 + Y\u0002n+3 ,\n\nand so forth. Then, upper and lower quantiles of the randomly generated\n\u001fn+h can be used as forecast limits for Wn+h .\nW\n\f348    12 Time Series Models: Basics\n\n\n\n\n                         15\n                         10\n        Inflation rate\n                         5\n\n\n                                                                       ***************************************************************************************************\n                                                                       *\n                         0\n\n\n\n\n                                  o    data\n                         \u22125\n\n\n\n\n                                  *    predictions\n                                       lower CL\n                         \u221210\n\n\n\n\n                                       upper CL\n\n                               1975        1980       1985          1990                                   1995\n                                                             year\n\n                                      Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.18",
      "section_title": "Forecasts of changes in in\ufb02ation rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.19. Forecasts of in\ufb02ation rate.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.19",
      "section_title": "Forecasts of in\ufb02ation rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.16. Forecasting the one-month in\ufb02ation rate and changes in the\nin\ufb02ation rate by simulation\n\n    To illustrate the amount of random variation in the forecasts, a small\nnumber (\ufb01ve) of sets of random forecasts of the changes in the in\ufb02ation rate\nwere generated out to 30 months ahead. These are plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.16",
      "section_title": "Forecasting the one-month in\ufb02ation rate and changes in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.20. Notice\nthe substantial random variation between the random forecasts. Because of\nthis large variation, to calculate forecasts limits a much larger number of\nrandom forecasts should be used. In this example, B = 50,000 sets of random\nforecasts are generated. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.20",
      "section_title": "Notice",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.21 shows the forecast limits, which are the\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.21",
      "section_title": "shows the forecast limits, which are the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 % upper and lower sample quantiles. For comparison, the forecast limits\ngenerated by R\u2019s function arima() are also shown. The two sets of forecast\nlimits are very similar even though the arima() limits assume Gaussian noise,\nbut the residuals are heavy-tailed. Thus, the presence of heavy tails does not\ninvalidate the Gaussian limits in this example with 95 % forecast limits. If a\nlarger con\ufb01dence coe\ufb03cient were used, that is, one very close to 1, then the\nforecast intervals based on sampling heavy-tailed residuals would be wider\nthan those based on a Gaussian assumption.\n    As described above, forecasts for future in\ufb02ation rates were obtained by\ntaking partial sums of random forecasts of changes in the in\ufb02ation rate and\nthe forecast limits (upper and lower quantiles) are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "% upper and lower sample quantiles. For comparison, the forecast limits",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.22. As\nexpected for a nonstationary process, the forecast limits diverge.          \u0002\n\f                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.22",
      "section_title": "As",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.13 Partial Autocorrelation Coe\ufb03cients      349\n\n\n\n\n                      10\n                      5\n               rate\n                      0\n                      \u22125\n                      \u221210\n\n\n\n\n                            0    5      10      15      20      25         30\n                                             month ahead\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "Partial Autocorrelation Coe\ufb03cients      349",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.20. Five random sets of forecasts of changes in the in\ufb02ation rate computed\nby simulation.\n\n\n   There are two important advantages to using simulation for forecasting:\n 1. simulation can be used in situations where standard software does not\n    compute forecast limits, and\n 2. simulation does not require that the noise series be Gaussian.\n    The \ufb01rst advantage will be important in some future examples, such as,\nmultivariate AR processes \ufb01t by R\u2019s ar() function. The second advantage is\nless important if one is generating 90 % or 95 % forecast limits, but if one\nwishes more extreme quantiles, say 99 % forecast limits, then the second adv-\nantage could be more important since in most applications the noise series\nhas heavier than Gaussian tails.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.20",
      "section_title": "Five random sets of forecasts of changes in the in\ufb02ation rate computed",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.13 Partial Autocorrelation Coe\ufb03cients\nThe partial autocorrelation function (PACF) can be useful for identifying the\norder of an AR process. The kth partial autocorrelation, denoted by \u03c6k,k , for\na stationary process Yt is the correlation between Yt and Yt+k , conditional on\nYt+1 , . . . , Yt+k\u22121 . For k = 1, Yt+1 , . . . , Yt+k\u22121 is an empty set, so the partial\nautocorrelation coe\ufb03cient is simply equal to the autocorrelation coe\ufb03cient,\nthat is, \u03c61,1 = \u03c1(1). Let \u03c6\u0002k,k denote the estimate of \u03c6k,k . \u03c6\u0002k,k can be calculated\nby \ufb01tting the regression model\n\n                      Yt = \u03c60,k + \u03c61,k Yt\u22121 + \u00b7 \u00b7 \u00b7 + \u03c6k,k Yt\u2212k + \u0017k,t .\n\f350     12 Time Series Models: Basics\n\n\n\n\n                     10\n                     5\n              rate\n                     0\n                     \u22125\n                     \u221210\n\n\n\n\n                           0   5   10      15      20     25      30\n                                        month ahead\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "Partial Autocorrelation Coe\ufb03cients",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.21. Forecast limits of changes in the in\ufb02ation rate computed by simulation\n(solid), computed by arima() (dotted), and the mean of the forecast (dashed). Notice\nthat the two sets of future limits are very similar and nearly overprint each other,\nso they are di\ufb03cult to distinguish visually.\n\n\nIf Yt is an AR(p) process, then \u03c6k,k = 0 for k > p. Therefore, a sign that a\ntime series can be \ufb01t by an AR(p) model is that the sample PACF will be\nnonzero up to p and then will be nearly zero for larger lags.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.21",
      "section_title": "Forecast limits of changes in the in\ufb02ation rate computed by simulation",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "p. Therefore, a sign that a\ntime series can be \ufb01t by an AR(p) model is that the sample PACF will be\nnonzero up to p and then will be nearly zero for larger lags.",
        "start": 342,
        "end": 507
      }
    ]
  },
  {
    "content": "12.17. PACF for BMW log returns\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.17",
      "section_title": "PACF for BMW log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.23 is the sample PACF for the BMW log returns computed using\nthe R function pacf(). The large value of \u03c6\u00021,1 and the smaller values of \u03c6\u0002k,k\nfor k = 2, . . . , 9 are a sign that this time series can be \ufb01t by an AR(1) model,\nin agreement with the results in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.23",
      "section_title": "is the sample PACF for the BMW log returns computed using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4. Note that \u03c6\u0002k,k is outside the\ntest bounds for some values of k > 9, particularly for k = 19. This is likely\ndue to random variation.                                                        \u0002\n    When computing resources were expensive, the standard practice was to\nidentify a tentative ARMA model using the sample ACF and PACF, \ufb01t this\nmodel, and then check the ACF and PACF of the residuals. If the residual\nACF and PACF revealed some lack of \ufb01t, then the model could be enlarged.\nAs computing has become much cheaper and faster and the use of information-\nbased model selection criteria has become popular, this practice has changed.\nNow many data analysts prefer to start with a relatively large set of models\nand compare them with selection criteria such as AIC and BIC. This can\nbe done automatically by auto.arima() in R or similar functions in other\nsoftware packages.\n\f                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "Note that \u03c6\u0002k,k is outside the",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "9, particularly for k = 19. This is likely\ndue to random variation.                                                        \u0002\n    When computing resources were expensive, the standard practice was to\nidentify a tentative ARMA model using the sample ACF and PACF, \ufb01t this\nmodel, and then check the ACF and PACF of the residuals. If the residual\nACF and PACF revealed some lack of \ufb01t, then the model could be enlarged.\nAs computing has become much cheaper and faster and the use of information-\nbased model selection criteria has become popular, this practice has changed.\nNow many data analysts prefer to start with a relatively large set of models\nand compare them with selection criteria such as AIC and BIC. This can\nbe done automatically by auto.arima() in R or similar functions in other\nsoftware packages.",
        "start": 70,
        "end": 931
      }
    ]
  },
  {
    "content": "12.13 Partial Autocorrelation Coe\ufb03cients   351\n\n\n\n\n                             15\n                             10\n             rate\n                             5\n                             0\n                             \u22125\n\n\n\n\n                                   0        5        10        15    20     25        30\n                                                          month ahead\n\n    Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.13",
      "section_title": "Partial Autocorrelation Coe\ufb03cients   351",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.22. Forecast limits for the in\ufb02ation rate computed by simulation.\n\n                                       Sample PACF for daily BMW stock log returns\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.22",
      "section_title": "Forecast limits for the in\ufb02ation rate computed by simulation.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n                           0.04\n             Partial ACF\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n                           \u22120.04\n\n\n\n\n                                   0    5       10        15    20   25   30     35\n                                                               Lag\n\n        Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "\u22120.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.23. Sample PACF for the daily BMW stock log returns.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.23",
      "section_title": "Sample PACF for the daily BMW stock log returns.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.18. PACF for changes in the in\ufb02ation rate\n\n   Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.18",
      "section_title": "PACF for changes in the in\ufb02ation rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.24 is the sample PACF for the changes in the in\ufb02ation rate.\nThe sample PACF decays slowly to zero, rather than dropping abruptly to\nzero as for an AR process. This is an indication that this time series should\nnot be \ufb01t by a pure AR process. An MA or ARMA process would be prefer-\nable. In fact, we saw previously that an MA(2) or MA(3) model provides a\nparsimonious \ufb01t.                                                           \u0002\n\f352    12 Time Series Models: Basics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.24",
      "section_title": "is the sample PACF for the changes in the in\ufb02ation rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14 Bibliographic Notes\nThere are many books on time series analysis and only a few will be men-\ntioned. Box, Jenkins, and Reinsel (2008) did so much to popularize ARIMA\nmodels that these are often called \u201cBox\u2013Jenkins models.\u201d Hamilton (1994)\nis a comprehensive treatment of time series. Brockwell and Davis (1991) is\nparticularly recommended for those with a strong mathematical preparation\nwishing to understand the theory of time series analysis. Brockwell and Davis\n(2003) is a gentler introduction to time series and is suited for those wishing\n\n\n                                           Change in inflation rate\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                            0.0\n              Partial ACF\n                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n                            \u22120.2\n                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                                   0   5       10          15     20   25\n                                                     Lag\n\n          Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "0   5       10          15     20   25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.24. Sample PACF for changes in the in\ufb02ation rate.\n\n\nto concentrate on applications. Enders (2004) and Tsay (2005) are time series\ntextbooks concentrating on economic and \ufb01nancial applications; Tsay (2005)\nis written at a somewhat more advanced level than Enders (2004). Gourieroux\nand Jasiak (2001) has a chapter on the applications of univariate time series\nin \ufb01nancial econometrics, and Alexander (2001) has a chapter on time series\nmodels. Pfa\ufb00 (2006) covers both the theory and application of unit root tests.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.24",
      "section_title": "Sample PACF for changes in the in\ufb02ation rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15.1 T-bill Rates\n\nRun the following code to input the TbGdpPi.csv data set and plot the three\nquarterly time series, as well as their auto- and cross-correlation functions.\nThe last three lines of code run augmented Dickey\u2013Fuller tests on the three\nseries.\n\f                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "1 T-bill Rates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15 R Lab     353\n\n 1 TbGdpPi = read.csv(\"TbGdpPi.csv\", header=TRUE)\n 2 # r = the 91-day treasury bill rate\n 3 #  y = the log of real GDP\n 4 #  pi = the inflation rate\n 5 TbGdpPi = ts(TbGdpPi, start = 1955, freq = 4)\n\n 6 library(tseries)\n\n 7 plot(TbGdpPi)\n\n 8 acf(TbGdpPi)\n\n 9 adf.test(TbGdpPi[,1])\n\n10 adf.test(TbGdpPi[,2])\n\n11 adf.test(TbGdpPi[,3])\n\n\n\nProblem 1\n\n(a) Describe the signs of nonstationarity seen in the time series and ACF\n    plots.\n(b) Use the augmented Dickey\u2013Fuller tests to decide which of the series are\n    nonstationary. Do the tests corroborate the conclusions of the time series\n    and ACF plots?\n\nNext run the augmented Dickey\u2013Fuller test on the di\ufb00erenced series and plot\nthe di\ufb00erenced series using the code below. Notice that the pairs() function\ncreates a scatterplot matrix, but the plot() function applied to time series\ncreates time series plots. [The plot() function would create a scatterplot\nmatrix if the data were in a data.frame rather than having \u201cclass\u201d time series\n(ts). Check the class of diff_rate with attr(diff_rate,\"class\").] Both\ntypes of plots are useful. The former shows cross-sectional associations, while\nthe time series plots are helpful when deciding whether di\ufb00erencing once is\nenough to induce stationarity. You should see that the \ufb01rst-di\ufb00erenced data\nlook stationary.\n12 diff_rate = diff(TbGdpPi)\n13 adf.test(diff_rate[,1])\n14 adf.test(diff_rate[,2])\n\n15 adf.test(diff_rate[,3])\n\n16 pairs(diff_rate)              #   scatterplot matrix\n17 plot(diff_rate)               #   time series plots\n\nNext look at the autocorrelation functions of the di\ufb00erenced series. These will\nbe on the diagonal of a 3 \u00d7 3 matrix of plots. The o\ufb00-diagonal plots are cross-\ncorrelation functions, which will be discussed in Chap. 13 and can be ignored\nfor now.\n18   acf(diff_rate)           # auto- and cross-correlations\n\f354      12 Time Series Models: Basics\n\nProblem 2\n 1. Do the di\ufb00erenced series appear stationary according to the augmented\n    Dickey\u2013Fuller tests?\n 2. Do you see evidence of autocorrelations in the di\ufb00erenced series? If so,\n    describe these correlations.\n\nFor the remainder of this lab, we will focus on the analysis of the 91-day\nT-bill rate. Since the time series are quarterly, it is good to see whether the\nmean depends on the quarter. One way to check for such e\ufb00ects is to compare\nboxplots of the four quarters. The following code does this. Note the use of\nthe cycle() function to obtain the quarterly period of each observation; this\ninformation is embedded in the data and cycle() simply extracts it.\n19   par(mfrow=c(1,1))\n20   boxplot(diff_rate[,1] ~ cycle(diff_rate))\n\nProblem 3 Do you see any seasonal di\ufb00erences in the boxplots? If so,\ndescribe them.\n\nRegardless of whether seasonal variation is present, for now we will look at\nnonseasonal models. Seasonal models are introduced in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "R Lab     353",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1. Next, use\nthe auto.arima() function in the forecast package to \ufb01nd a \u201cbest-\ufb01tting\u201d\nnonseasonal ARIMA model for the T-bill rates. The speci\ufb01cations max.P=0\nand max.Q=0 force the model to be nonseasonal, since max.P and max.Q are\nthe number of seasonal AR and MA components.\n21   library(forecast)\n22   auto.arima(TbGdpPi[,1], max.P=0, max.Q=0, ic=\"aic\")\n\nProblem 4\n 1. What order of di\ufb00erencing is chosen? Does this result agree with your\n    previous conclusions?\n 2. What model was chosen by AIC?\n 3. Which goodness-of-\ufb01t criterion is being used here?\n 4. Change the criterion to BIC. Does the best-\ufb01tting model then change?\n 5. Carefully express the \ufb01tted model chosen by the BIC criterion in mathe-\n    matical notation.\n\nFinally, re\ufb01t the best-\ufb01tting model with the following code, and check for any\nresidual autocorrelation. You will need to replace the three question marks by\nthe appropriate numerical values for the best-\ufb01tting model.\n23 fit1 = arima(TbGdpPi[,1], order=c(?,?,?))\n24 acf(residuals(fit1))\n25 Box.test(residuals(fit1), lag = 12, type=\"Ljung\", fitdf=?)\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Next, use",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15 R Lab     355\n\nProblem 5 Do you think that there is residual autocorrelation? If so, describe\nthis autocorrelation and suggest a more appropriate model for the T-bill series.\n\nGARCH e\ufb00ects, that is, volatility clustering, can be detected by looking for\nauto-correlation in the mean-centered squared residuals. Another possibility\nis that some quarters are more variable than others. This can be detected for\nquarterly data by autocorrelation in the squared residuals at time lags that\nare a multiple of 4. Run the following code to look at autocorrelation in the\nmean-centered squared residuals.\n26 resid2 = (residuals(fit1) - mean(residuals(fit1)))^2\n27 acf(resid2)\n28 Box.test(resid2, lag = 12, type=\"Ljung\")\n\n\n\nProblem 6 Do you see evidence of GARCH e\ufb00ects?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "R Lab     355",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15.2 Forecasting\n\nThis example shows how to forecast a time series using R. Run the follow-\ning code to \ufb01t a nonseasonal ARIMA model to the quarterly in\ufb02ation rate.\nThe code also uses the predict() function to forecast 36 quarters ahead.\nThe standard errors of the forecasts are also returned by predict() and can\nbe used to create prediction intervals. Note the use of col to specify colors.\nReplace c(?,?,?) by the speci\ufb01cation of the ARIMA model that minimizes\nBIC.\n1  TbGdpPi = read.csv(\"TbGdpPi.csv\", header=TRUE)\n2  attach(TbGdpPi)\n 3 #  r = the 91-day treasury bill rate\n 4 #  y = the log of real GDP\n 5 #  pi = the inflation rate\n 6 #  fit the non-seasonal ARIMA model found by auto.arima()\n 7 #  quarterly observations from 1955-1 to 2013-4\n 8 year = seq(1955,2013.75, by=0.25)\n\n 9 library(forecast)\n\n10 auto.arima(pi, max.P=0, max.Q=0, ic=\"bic\")\n\n11 fit = arima(pi, order=c(?,?,?))\n\n12 forecasts = predict(fit, 36)\n\n13 plot(year,pi,xlim=c(1980,2023), ylim=c(-7,12), type=\"b\")\n\n14 lines(seq(from=2014, by=.25, length=36), forecasts$pred, col=\"red\")\n\n15 lines(seq(from=2014, by=.25, length=36),\n\n16       forecasts$pred + 1.96*forecasts$se, col=\"blue\")\n17 lines(seq(from=2014, by=.25, length=36),\n\n18       forecasts$pred - 1.96*forecasts$se, col=\"blue\")\n\f356    12 Time Series Models: Basics\n\nProblem 7 Include the plot with your work.\n(a) Why do the prediction intervals (blue curves) widen as one moves farther\n    into the future?\n(b) Why are the predictions (red) constant throughout?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "2 Forecasting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.16 Exercises\n 1. This problem and the next use CRSP daily returns. First, get the data\n    and plot the ACF in two ways:\n     1 library(Ecdat)\n\n     2 data(CRSPday)\n\n     3 crsp=CRSPday[,7]\n\n     4 acf(crsp)\n\n     5 acf(as.numeric(crsp))\n\n    (a) Explain what \u201clag\u201d means in the two ACF plots. Why does lag di\ufb00er\n        between the plots?\n    (b) At what values of lag are there signi\ufb01cant autocorrelations in the\n        CRSP returns? For which of these values do you think the statistical\n        signi\ufb01cance might be due to chance?\n 2. Next, \ufb01t AR(1) and AR(2) models to the CRSP returns:\n     6 arima(crsp,order=c(1,0,0))\n\n     7 arima(crsp,order=c(2,0,0))\n\n    (a) Would you prefer an AR(1) or an AR(2) model for this time series?\n        Explain your answer.\n    (b) Find a 95 % con\ufb01dence interval for \u03c6 for the AR(1) model.\n 3. Consider the AR(1) model\n\n                              Yt = 5 \u2212 0.55Yt\u22121 + \u0017t\n\n    and assume that \u03c3 2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.16",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2.\n    (a) Is this process stationary? Why or why not?\n    (b) What is the mean of this process?\n    (c) What is the variance of this process?\n    (d) What is the covariance function of this process?\n 4. Suppose that Y1 , Y2 , . . . is an AR(1) process with \u03bc = 0.5, \u03c6 = 0.4, and\n    \u03c3 2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "(a) Is this process stationary? Why or why not?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2.\n    (a) What is the variance of Y1 ?\n    (b) What are the covariances between Y1 and Y2 and between Y1 and Y3 ?\n    (c) What is the variance of (Y1 + Y2 + Y3 )/2?\n 5. An AR(3) model has been \ufb01t to a time series. The estimates are \u03bc\u0302 = 104,\n    \u03c6\u03021 = 0.4, \u03c6\u03022 = 0.25, and \u03c6\u03023 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "(a) What is the variance of Y1 ?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. The last four observations were\n    Yn\u22123 = 105, Yn\u22122 = 102, Yn\u22121 = 103, and Yn = 99. Forecast Yn+1 and\n    Yn+2 using these data and estimates.\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "The last four observations were",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.16 Exercises   357\n\n 6. Let Yt be an MA(2) process,\n                          Yt = \u03bc + \u0017t + \u03b81 \u0017t\u22121 + \u03b82 \u0017t\u22122 .\n    Find formulas for the autocovariance and autocorrelation functions of Yt .\n 7. Let Yt be a stationary AR(2) process,\n                  (Yt \u2212 \u03bc) = \u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) + \u0017t .\n    (a) Show that the ACF of Yt satis\ufb01es the equation\n                           \u03c1(k) = \u03c61 \u03c1(k \u2212 1) + \u03c62 \u03c1(k \u2212 2)\n       for all values of k > 0. (These are a special case of the Yule\u2013Walker\n       equations.)\n       [Hint: \u03b3(k) = Cov(Yt , Yt\u2212k ) = Cov{\u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) +\n       \u0017t , Yt\u2212k } and \u0017t and Yt\u2212k are independent if k > 0.]\n   (b) Use part (a) to show that (\u03c61 , \u03c62 ) solves the following system of\n       equations:\n                           \u0007      \b \u0007              \b\u0007 \b\n                             \u03c1(1)         1   \u03c1(1)     \u03c61\n                                   =                        .\n                             \u03c1(2)       \u03c1(1)   1       \u03c62\n\n    (c) Suppose that \u03c1(1) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.16",
      "section_title": "Exercises   357",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0. (These are a special case of the Yule\u2013Walker\n       equations.)\n       [Hint: \u03b3(k) = Cov(Yt , Yt\u2212k ) = Cov{\u03c61 (Yt\u22121 \u2212 \u03bc) + \u03c62 (Yt\u22122 \u2212 \u03bc) +\n       \u0017t , Yt\u2212k } and \u0017t and Yt\u2212k are independent if k > 0.]\n   (b) Use part (a) to show that (\u03c61 , \u03c62 ) solves the following system of\n       equations:\n                           \u0007      \b \u0007              \b\u0007 \b\n                             \u03c1(1)         1   \u03c1(1)     \u03c61\n                                   =                        .\n                             \u03c1(2)       \u03c1(1)   1       \u03c62",
        "start": 443,
        "end": 977
      }
    ]
  },
  {
    "content": "0.4 and \u03c1(2) = 0.2. Find \u03c61 , \u03c62 , and \u03c1(3).\n 8. Use (12.11) to verify Eq. (12.12).\n 9. Show that if wt is de\ufb01ned by (12.34) then (12.35) is true.\n10. For a univariate, discrete time process, what is the di\ufb00erence between a\n    strictly stationary process and a weakly stationary process?\n11. The time series in the middle and bottom panels of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "and \u03c1(2) = 0.2. Find \u03c61 , \u03c62 , and \u03c1(3).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.14 are both\n    nonstationary, but they clearly behave in di\ufb00erent manners. The time\n    series in the bottom panel exhibits \u201cmomentum\u201d in the sense that once\n    it starts moving upward or downward, it often moves consistently in that\n    direction for a large number of steps. In contrast, the series in the middle\n    panel does not have this type of momentum and a step in one direction\n    is quite likely to be followed by a step in the opposite direction. Do you\n    think the time series model with momentum would be a good model for\n    the price of a stock? Why or why not?\n12. The MA(2) model Yt = \u03bc + \u0017t + \u03b81 \u0017t\u22121 + \u03b82 \u0017t\u22122 was \ufb01t to data and the\n    estimates are\n                               Parameter Estimate\n                                       \u03bc       45\n                                      \u03b81      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.14",
      "section_title": "are both",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n                                      \u03b82    \u22120.15\n\f358      12 Time Series Models: Basics\n\n      The last two values of the observed time series and residuals are\n\n                                       t Yt     \u0002\n                                                \u0017t\n                                   n \u2212 1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "\u03b82    \u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "39.8 \u22124.3\n                                       n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "39.8",
      "section_title": "\u22124.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "42.7 1.5\n\n    Find the forecasts of Yn+1 and Yn+2 .\n13. The ARMA(1,2) model Yt = \u03bc + \u03c61 Yt\u22121 + \u0017t + \u03b81 \u0017t\u22121 + \u03b82 \u0017t\u22122 was \ufb01t to\n    data and the estimates are\n\n                                Parameter Estimate\n                                        \u03bc      103\n                                      \u03c61       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "42.7",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                       \u03b81      0.4\n                                       \u03b82    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u03b81      0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25\n\n      The last two values of the observed time series and residuals are\n\n                                      t    Yt   \u0002\n                                                \u0017t\n                                  n \u2212 1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "The last two values of the observed time series and residuals are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "120.1 \u22122.3\n                                      n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "120.1",
      "section_title": "\u22122.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "118.3 2.6\n\n    Find the forecasts of Yn+1 and Yn+2 .\n14. To decide the value of d for an ARIMA(p, d, q) model for a time series y,\n    plots were created using the R program:\n     8 par(mfrow=c(3,2))\n\n     9 plot(y,type=\"l\")\n\n    10 acf(y)\n\n    11 plot(diff(y),type=\"l\")\n\n    12 acf(diff(y))\n\n    13 plot(diff(y,d=2),type=\"l\")\n\n    14 acf(diff(y,d=2))\n\n    The output was the following \ufb01gure:\n\f                                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "118.3",
      "section_title": "2.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.16 Exercises           359\n\n                                                                                      Series y\n                  5\n\n\n\n\n                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.16",
      "section_title": "Exercises           359",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.4 0.8\n                                                        ACF\n                  \u221215 \u22125\ny\n\n\n\n\n                           0   50   100     150   200                       0   5       10     15       20\n                                    Index                                                Lag\n\n                                                                                    Series diff(y)\n\n\n\n\n                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.4 0.8\n                  1\n diff(y)\n\n\n\n\n                                                        ACF\n                  \u22123 \u22121\n\n\n\n\n                           0   50   100     150   200                       0   5       10       15      20\n                                    Index                                                Lag\n\n                                                                                Series diff(y, d = 2)\n diff(y, d = 2)\n                  2\n\n\n\n\n                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n                                                        ACF\n                  0\n\n\n\n\n                                                              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n                  \u22124\n\n\n\n\n                           0   50   100     150   200                       0   5       10       15      20\n                                    Index                                                Lag\n\n\n    What value of d do you recommend? Why?\n15. This problem \ufb01ts an ARIMA model to the logarithms monthly one-month\n    T-bill rates in the data set Mishkin in the Ecdat package. Run the fol-\n    lowing code to get the variable:\n    15 library(Ecdat)\n\n    16 data(Mishkin)\n\n    17 tb1 = log(Mishkin[,3])\n\n    (a) Use time series and ACF plots to determine the amount of di\ufb00erencing\n        needed to obtain a stationary series.\n    (b) Next use auto.arima to determine the best-\ufb01tting nonseasonal ARIMA\n        models. Use both AIC and BIC and compare the results.\n    (c) Examine the ACF of the residuals for the model you selected. Do you\n        see any problems?\n16. Suppose you \ufb01t an AR(2) model to a time series Yt , t = 1, . . . , n, and\n    the estimates were \u03bc  \u0002 = 100.1, \u03c6\u00021 = 0.5, and \u03c6\u00022 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "\u22124",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. The last three\n    observations were Yn\u22122 = 101.0, Yn\u22121 = 99.5, and Yn = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "The last three",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "102.3. What are\n    the forecasts of Yn+1 , Yn+2 , and Yn+3 ?\n17. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "102.3",
      "section_title": "What are",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.9.1, it was stated that \u201cif E(Yt ) has an mth-degree polynomial\n    trend, then the mean of E(\u0394d Yt ) has an (m\u2212d)th-degree trend for d \u2264 m.\n    For d > m, E(\u0394d Yt ) = 0.\u201d Prove these assertions.\n\f360    12 Time Series Models: Basics\n\nReferences\nAlexander, C. (2001) Market Models: A Guide to Financial Data Analysis,\n  Wiley, Chichester.\nBox, G. E. P., Jenkins, G. M., and Reinsel, G. C. (2008) Times Series Analysis:\n  Forecasting and Control, 4th ed., Wiley, Hoboken, NJ.\nBrockwell, P. J. and Davis, R. A. (1991) Time Series: Theory and Methods,\n  2nd ed., Springer, New York.\nBrockwell, P. J. and Davis, R. A. (2003) Introduction to Time Series and\n  Forecasting, 2nd ed., Springer, New York.\nEnders, W. (2004) Applied Econometric Time Series, 2nd Ed., Wiley,\n  New York.\nGourieroux, C., and Jasiak, J. (2001) Financial Econometrics, Princeton\n  University Press, Princeton, NJ.\nHamilton, J. D. (1994) Time Series Analysis, Princeton University Press,\n  Princeton, NJ.\nPfa\ufb00, B (2006) Analysis of Integrated and Cointegrated Time Series with R,\n  Springer, New York.\nTsay, R. S. (2005) Analysis of Financial Time Series, 2nd ed., Wiley,\n  New York.\n\f13\nTime Series Models: Further Topics\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.9",
      "section_title": "1, it was stated that \u201cif E(Yt ) has an mth-degree polynomial",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "m, E(\u0394d Yt ) = 0.\u201d Prove these assertions.\n\f360    12 Time Series Models: Basics",
        "start": 154,
        "end": 238
      }
    ]
  },
  {
    "content": "13.1 Seasonal ARIMA Models\nEconomic time series often exhibit strong seasonal variation. For example,\nan investor in mortgage-backed securities might be interested in predicting\nfuture housing starts, and these are usually much lower in the winter months\ncompared to the rest of the year. Figure 13.1a is a time series plot of the\nlogarithms of quarterly urban housing starts in Canada from the \ufb01rst quarter\nof 1960 to \ufb01nal quarter of 2001. The data are in the data set Hstarts in R\u2019s\nEcdat package.\n\n\n     a                                             b                         c\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Seasonal ARIMA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 10.0\n\n\n\n\n                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "10.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 10.0\n                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "10.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n     log(starts)\n\n\n\n\n                                                                             log(starts)\n                                                   ACF\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "log(starts)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.0\n\n\n\n\n                                                                                           9.0\n                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.0",
      "section_title": "9.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                   8.5\n\n\n\n\n                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "8.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5\n                                                         0.0\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.5",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.0\n\n\n\n\n                                                                                           8.0\n\n\n\n\n                              1960   1980   2000               0 1 2 3 4 5                            1   2    3    4\n                                     year                           lag                                   quarter\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.0",
      "section_title": "8.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1. Logarithms of quarterly urban housing starts in Canada: (a) time series\nplot; (b) sample ACF; (c) boxplots by quarter.\n\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Logarithms of quarterly urban housing starts in Canada: (a) time series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1 shows one and perhaps two types of nonstationarity: (1) There\nis strong seasonality, and (2) it is unclear whether the seasonal sub-series\nrevert to a \ufb01xed mean and, if not, then this is a second type of nonstationarity\nbecause the process is integrated. These e\ufb00ects can also be seen in the ACF\nplot in Fig. 13.1b. At lags that are a multiples of four, the autocorrelations\n\n\u00a9 Springer Science+Business Media New York 2015                                                                         361\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 13\n\f362    13 Time Series Models: Further Topics\n\nare large, and decay slowly to zero. At other lags, the autocorrelations are\nsmaller but also decay somewhat slowly. The boxplots in Fig. 13.1c give us\na better picture of the seasonal e\ufb00ects. Housing starts are much lower in the\n\ufb01rst quarter than other quarters, jump to a peak in the second quarter, and\nthen drop o\ufb00 slightly in the last two quarters.\n    Other time series might have only seasonal nonstationarity. For example,\nmonthly average temperatures in a city with a temperate climate will show\na strong seasonal e\ufb00ect, but if we plot temperatures for any single month of\nthe year, say July, we will see mean-reversion.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "shows one and perhaps two types of nonstationarity: (1) There",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1.1 Seasonal and Nonseasonal Di\ufb00erencing\n\nNonseasonal di\ufb00erencing is the type of di\ufb00erencing that we have been using so\nfar. The series Yt is replaced by \u0394Yt = Yt \u2212Yt\u22121 if the di\ufb00erencing is \ufb01rst-order,\nand so forth for higher-order di\ufb00erencing. Nonseasonal di\ufb00erencing does not\nremove seasonal nonstationarity and does not alone create a stationary series;\nsee the top row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "1 Seasonal and Nonseasonal Di\ufb00erencing",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2.\n    To remove seasonal nonstationarity, one uses seasonal di\ufb00erencing. Let s\nbe the period. For example, s = 4 for quarterly data and s = 12 for monthly\ndata. De\ufb01ne \u0394s = 1 \u2212 B s so that \u0394s Yt = Yt \u2212 Yt\u2212s .\n    Be careful to distinguish between \u0394s = 1\u2212B s and \u0394s = (1\u2212B)s . Note that\n\u0394s = 1\u2212B s is the \ufb01rst-order seasonal di\ufb00erencing operator and \u0394s = (1\u2212B)s\nis the sth-order nonseasonal di\ufb00erencing operator. For example, \u03942 Yt = Yt \u2212\nYt\u22122 but \u03942 Yt = \u0394(\u0394Yt ) = Yt \u2212 2Yt\u22121 + Yt\u22122 .\n    The series \u0394s Yt is called the seasonally di\ufb00erenced series. See the middle\nrow of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "To remove seasonal nonstationarity, one uses seasonal di\ufb00erencing. Let s",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2 for the seasonally di\ufb00erenced logarithm of housing starts and\nits Sample ACF.\n    One can combine seasonal and nonseasonal di\ufb00erencing by using, for\nexample, for \ufb01rst-order di\ufb00erences\n         \u0394(\u0394s Yt ) = \u0394(Yt \u2212 Yt\u2212s ) = (Yt \u2212 Yt\u2212s ) \u2212 (Yt\u22121 \u2212 Yt\u2212s\u22121 ).\nThe order in which the seasonal and nonseasonal di\ufb00erence operators are\napplied does not matter, since one can show that\n                             \u0394(\u0394s Yt ) = \u0394s (\u0394Yt ).\n    For a seasonal time series, seasonal di\ufb00erencing is necessary, but whether\nalso to use nonseasonal di\ufb00erencing will depend on the particular time series.\nFor the housing starts data, the seasonally di\ufb00erenced series appears station-\nary so only seasonal di\ufb00erencing is absolutely needed, but combining seasonal\nand nonseasonal di\ufb00erencing might be preferred since it results in a simpler\nmodel.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "for the seasonally di\ufb00erenced logarithm of housing starts and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1.2 Multiplicative ARIMA Models\n\nOne of the simplest seasonal models is the ARIMA{(1, 1, 0)\u00d7(1, 1, 0)s } model,\nwhich puts together the nonseasonal ARIMA(1,1,0) model\n\f                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "2 Multiplicative ARIMA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1 Seasonal ARIMA Models               363\n\na                                   nonseasonal differencing        b                  nonseasonal differencing\n\n\n\n\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Seasonal ARIMA Models               363",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                      0.5\ndiff(x)\n\n\n\n\n                                                                     ACF\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                      \u22120.5\n\n\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                             1960     1970   1980    1990    2000                  0     1    2     3    4      5\n                                              year                                                lag\n\n\nc                                    seasonal differencing          d                   seasonal differencing\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1960     1970   1980    1990    2000                  0     1    2     3    4      5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                            0.8\ndiff(x, 4)\n\n\n\n\n                                                                     ACF\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                      \u22120.5\n\n\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                             1960     1970   1980    1990    2000                  0     1    2     3    4      5\n                                              year\n                                                                                                  lag\n\ne                        seasonal & nonseasonal differencing        f          seasonal & nonseasonal differencing\ndiff(diff(x, 1), 4)\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1960     1970   1980    1990    2000                  0     1    2     3    4      5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                            0.5\n                                                                     ACF\n                      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                            \u22120.5\n\n\n\n\n                             1960     1970   1980    1990    2000                  0     1    2     3    4      5\n                                              year                                                lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2. Time series (left column) and sample ACF plots (right column) of the\nlogarithms of quarterly urban housing starts with nonseasonal di\ufb00erencing (top row),\nseasonal di\ufb00erencing (middle row), and both seasonal and nonseasonal di\ufb00erencing\n(bottom row). Note: in the sample ACF plots, lag = 1 means a lag of one year, which\nis four observations for quarterly data.\n\n\n                                                       (1 \u2212 \u03c6B)(\u0394Yt \u2212 \u03bc) = \u0017t                                     (13.1)\n\nand a purely seasonal ARIMA(1,1,0)s model\n\n                                                     (1 \u2212 \u03c6\u2217 B s )(\u0394s Yt \u2212 \u03bc) = \u0017t                                (13.2)\n\nto obtain the multiplicative model\n\n                                             (1 \u2212 \u03c6B) (1 \u2212 \u03c6\u2217 B s ) {\u0394(\u0394s Yt ) \u2212 \u03bc} = \u0017t .                        (13.3)\n\nModel (13.2) is called \u201cpurely seasonal\u201d and has a subscript \u201cs\u201d since it uses\nonly B s and \u0394s ; it is obtained from the ARIMA(1,1,0) by replacing B and\n\u0394 by B s and \u0394s . For a monthly time series (s = 12), model (13.2) gives\n12 independent processes, one for Januaries, a second for Februaries, and so\nforth. Model (13.3) uses the components from (13.1) to tie these 12 series\ntogether.\n\f364       13 Time Series Models: Further Topics\n\n      The ARIMA{(p, d, q) \u00d7 (ps , ds , qs )s } process is\n     (1 \u2212 \u03c61 B \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p B p ) {1 \u2212 \u03c6\u22171 B s \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6\u2217ps (B s )ps } {\u0394d (\u0394ds s Yt ) \u2212 \u03bc}\n     = (1 + \u03b81 B + . . . + \u03b8q B q ) {1 + \u03b81\u2217 B s + . . . + \u03b8q\u2217s (B s )qs } \u0017t .               (13.4)\nThis process multiplies together the AR components, the MA components,\nand the di\ufb00erencing components of two processes: the nonseasonal ARIMA\n(p, d, q) process\n       (1 \u2212 \u03c61 B \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p B p ) {(\u0394d Yt ) \u2212 \u03bc} = (1 + \u03b81 B + . . . + \u03b8q B q ) \u0017t\nand the seasonal ARIMA(ps , ds , qs )s process\n{1 \u2212 \u03c6\u22171 B s \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6\u2217ps (B s )ps } {(\u0394ds s Yt ) \u2212 \u03bc} = {1 + \u03b81\u2217 B s + . . . + \u03b8q\u2217s (B s )qs } \u0017t .\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "Time series (left column) and sample ACF plots (right column) of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1. ARIMA{(1, 1, 1) \u00d7 (0, 1, 1)4 } model for housing starts\n\n    We return to the housing starts data. The \ufb01rst question is whether to dif-\nference only seasonally, or both seasonally and nonseasonally. The seasonally\ndi\ufb00erenced quarterly series in the middle row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "ARIMA{(1, 1, 1) \u00d7 (0, 1, 1)4 } model for housing starts",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2 is possibly station-\nary, so perhaps seasonal di\ufb00erencing is su\ufb03cient. However, the ACF of the\nseasonally and nonseasonally di\ufb00erenced series in the bottom row has a sim-\npler ACF than the data that are only seasonally di\ufb00erenced. By di\ufb00erencing\nboth ways, we should be able \ufb01nd a more parsimonious ARMA model.\n    Two models with seasonal and nonseasonal di\ufb00erencing were tried, ARIMA\n{(1, 1, 1) \u00d7 (1, 1, 1)4 } and ARIMA{(1, 1, 1) \u00d7 (0, 1, 1)4 }. Both provided good\n\ufb01ts and had residuals that passed the Ljung\u2013Box test. The second of the two\nmodels was selected, because it has one fewer parameter than the \ufb01rst, though\nthe other model would have been a reasonable choice. The results from \ufb01tting\nthe chosen model are below.\n 1 data(Hstarts, package=\"Ecdat\")\n 2 x = ts(Hstarts[,1], start=1960, frequency=4)\n 3 fit2 = arima(x, c(1,1,1), seasonal = list(order = c(0,1,1),\n\n 4   period = 4))\n 5 fit2\n\n\n      Call:\n      arima(x = hst, order = c(1, 1, 1), seasonal\n      = list(order = c(0, 1, 1), period = 4))\n\n      Coefficients:\n              ar1     ma1           sma1\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "is possibly station-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.675 -0.890          -0.822\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.675",
      "section_title": "-0.890          -0.822",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.142    0.105          0.051\n\n      sigma^2 estimated as 0.0261: log-likelihood = 62.9,\n         aic = -118\n\f                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.142",
      "section_title": "0.105          0.051",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2 Box\u2013Cox Transformation for Time Series                                                  365\n\nThus, the \ufb01tted model is\n\n                              (1 \u2212 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "Box\u2013Cox Transformation for Time Series                                                  365",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.675 B)Yt\u2217 = (1 \u2212 0.890 B)(1 \u2212 0.822 B4 ) \u0017t\n\nwhere Yt\u2217 = \u0394(\u03944 Yt ) and \u0017t is white noise with mean zero and variance ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.675",
      "section_title": "B)Yt\u2217 = (1 \u2212 0.890 B)(1 \u2212 0.822 B4 ) \u0017t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0261.\nFigure 13.3 shows forecasts from this model for the four years following the\nend of the time series.                                                      \u0002\n\n                                o   data\n                                *   predictions\n                                    lower CL\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0261",
      "section_title": "Figure 13.3 shows forecasts from this model for the four years following the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.0\n\n\n\n\n                                    upper CL\n\n\n\n                                                                               *               *               *               *\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.0",
      "section_title": "upper CL",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5\n         log(starts)\n\n\n\n\n                                                                                   *               *               *               *\n                                                                                       *               *               *               *\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "log(starts)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.0\n\n\n\n\n                                                                           *               *               *               *\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.0",
      "section_title": "*               *               *               *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5\n\n\n\n\n                         1992       1994          1996   1998     2000   2002                  2004                            2006\n                                                                year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.5",
      "section_title": "1992       1994          1996   1998     2000   2002                  2004                            2006",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3. Forecasting logarithms of quarterly urban housing starts using the\nARIMA{(1, 1, 1) \u00d7 (0, 1, 1)4 } model. The dashed line connects the data, the dotted\nline connects the forecasts, and the solid lines are the forecast limits.\n\n\n    When the size of the seasonal oscillations increases, as with the air passen-\nger data in Fig. 12.2, some type of preprocessing is needed before di\ufb00erencing.\nOften, taking logarithms stabilizes the size of the oscillations. This can be seen\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Forecasting logarithms of quarterly urban housing starts using the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4. Box, Jenkins, and Reinsel (2008) obtain a parsimonious \ufb01t to the\nlog passengers with an ARIMA(0, 1, 1) \u00d7 (0, 1, 1)12 model.\n    For the housing starts series, the data come as logarithms in the Ecdat\npackage. If they had come untransformed, then we would have needed to apply\nsome type of transformation.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Box, Jenkins, and Reinsel (2008) obtain a parsimonious \ufb01t to the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2 Box\u2013Cox Transformation for Time Series\nAs just discussed, it is often desirable to transform a time series to stabilize the\nsize of the variability, both seasonal and random. Although a transformation\ncan be selected by trial-and-error, another possibility is automatic selection\nby maximum likelihood estimation using the model\n\f366     13 Time Series Models: Further Topics\n\n\n\n\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "Box\u2013Cox Transformation for Time Series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.5\n                      6.0\n         passengers\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.5",
      "section_title": "6.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5\n                      5.0\n\n\n\n\n                                  1950    1952     1954        1956   1958    1960\n                                                        Time\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "5.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4. Time series plot of the logarithms of the monthly total international\nairline passengers (in thousands).\n\n\n                            (\u03b1)                   (\u03b1)                        (\u03b1)\n                 (\u0394d Yt           \u2212 \u03bc) = \u03c61 (\u0394d Yt\u22121 \u2212 \u03bc) + \u00b7 \u00b7 \u00b7 + \u03c6p (\u0394d Yt\u2212p \u2212 \u03bc)\n                                         + \u0017t + \u03b81 \u0017t\u22121 + \u00b7 \u00b7 \u00b7 + \u03b8q \u0017t\u2212q ,            (13.5)\n\nwhere \u00171 , \u00172 , . . . is Gaussian white noise. Model (13.5) states that after a Box\u2013\nCox transformation, Yt follows an ARIMA model with Gaussian noise that\nhas a constant variance. The transformation parameter \u03b1 is considered unk-\nnown and is estimated by maximum likelihood along with the AR and MA\nparameters and the noise variance. For notational simplicity, (13.5) uses a\nnonseasonal model, but a seasonal ARIMA model could just as easily have\nbeen used.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Time series plot of the logarithms of the monthly total international",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.2. Selecting a transformation for the housing starts\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.2",
      "section_title": "Selecting a transformation for the housing starts",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5 show the pro\ufb01le likelihood for \u03b1 for the housing starts series\n(not the logarithms). The ARIMA model was ARIMA{(1, 1, 1) \u00d7 (1, 1, 1)4 }.\nThe \ufb01gure was created by the BoxCox.Arima() function in R\u2019s FitAR package.\nThis function denotes the transformation parameter by \u03bb. The MLE of \u03b1 is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "show the pro\ufb01le likelihood for \u03b1 for the housing starts series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.34 and the 95 % con\ufb01dence interval is roughly from 0.15 to 0.55. Thus,\nthe log transformation (\u03b1 = 0) is somewhat outside the con\ufb01dence interval,\nbut the square-root transformation is in the interval. Nonetheless, the log\ntransformation worked satisfactorily in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.34",
      "section_title": "and the 95 % con\ufb01dence interval is roughly from 0.15 to 0.55. Thus,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1 and might be retained.\n    Without further analysis, it is not clear why \u03b1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "and might be retained.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.34 achieves a bet-\nter \ufb01t than the log transformation. Better \ufb01t could mean that the ARIMA\nmodel \ufb01ts better, that the noise variability is more nearly constant, that\nthe noise is closer to being Gaussian, or some combination of these e\ufb00ects.\n\f                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.34",
      "section_title": "achieves a bet-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression       367\n\nIt would be interesting to compare forecasts using the log and square-root\ntransformations to see in what ways, if any, the square-root transformation\noutperforms the log transformation for forecasting. The forecasts would need\nto be back-transformed to the original scale in order for them to be compa-\nrable. One might use the \ufb01nal year as test data to see how well housing starts\nin that year are forecast.                                                  \u0002\n\n                               Relative Likelihood Analysis\n                                95% Confidence Interval\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression       367",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                0.8\n\n\n\n\n                                                          ^\n                                                          \u03bb = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.337\n                0.6\n         R(\u03bb)\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.337",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                0.2\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                      0.0          0.2           0.4          0.6\n                                             \u03bb\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0          0.2           0.4          0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5. Pro\ufb01le likelihood for \u03b1 (called \u03bb in the legend) in the housing start\nexample. Values of \u03bb with R(\u03bb) (the pro\ufb01le likelihood) above the horizontal line are\nin the 95 % con\ufb01dence limit.\n\n\n    Data transformations can stabilize some types of variation in time series,\nbut not all types. For example, in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Pro\ufb01le likelihood for \u03b1 (called \u03bb in the legend) in the housing start",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.2 the seasonal oscillations in the\nnumbers of air passengers increase as the series itself increases, and we can see\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.2",
      "section_title": "the seasonal oscillations in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 that a log transformation stabilizes these oscillations. In contrast,\nthe S&P 500 returns in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "that a log transformation stabilizes these oscillations. In contrast,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 exhibit periods of low and high volatility even\nthough the returns maintain a mean near 0. Transformations cannot remove\nthis type of volatility clustering. Instead, these changes of volatility could be\nmodeled by a GARCH process; this topic is pursued in Chap. 14.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.1",
      "section_title": "exhibit periods of low and high volatility even",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression\nIn a multiple linear regression model (9.1) the errors \u0017i are assumed to be\nmutually independent. However, if the data {(X i , Yi ), i = 1, . . . , n} are time\nseries, then it is likely that the errors are correlated, a problem we will call\nresidual correlation; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.1.\n\f368    13 Time Series Models: Further Topics\n\n   Residual correlation causes standard errors and con\ufb01dence intervals (which\nincorrectly assume uncorrelated noise) to be incorrect. In particular, the\ncoverage probability of con\ufb01dence intervals can be much lower than the\nnominal value. A solution to this problem is to adjust or correct the esti-\nmated covariance matrix of the coe\ufb03cient estimates; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.2. An alte-\nrnative solution is to model the noise as an ARMA process, assuming that\nthe residuals are stationary; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "2. An alte-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.1 Residual Correlation and Spurious Regressions\n\nIn the extreme case where the residuals are an integrated process, the least-\nsquares estimator is inconsistent, meaning that it will not converge to the true\nparameter as the sample size converges to \u221e. If an I(1) process is regressed\non another I(1) process and the two processes are independent (so that the\nregression coe\ufb03cient is 0), it is quite possible to obtain a highly signi\ufb01cant\nresult, that is, to strongly reject the true null hypothesis that the regression\ncoe\ufb03cient is 0. This is called a spurious regression. The problem, of course,\nis that the test is based on the incorrect assumption of independent error.\nThe residuals from the The problem of correlated noise can be detected by\nlooking at the sample ACF of the residuals. Sometimes the presence of resid-\nual correlation is obvious. In other cases, it is not so clear and a statistical\ntest is desirable. The Durbin\u2013Watson test can be used to test the null hyp-\nothesis of no residual autocorrelation. More precisely, the null hypothesis of\nthe Durbin\u2013Watson test is that the \ufb01rst p autocorrelation coe\ufb03cients are all\n0, where p can be selected by the user. The p-value for a Durbin\u2013Watson\ntest is not trivial to compute, and di\ufb00erent implementations use di\ufb00erent\ncomputational methods. In the R function durbinWatsonTest() in the car\npackage, p is called max.lag and has a default value of 1. The p-value is com-\nputed by durbinWatsonTest() using bootstrapping. The lmtest package of R\nhas another function, dwtest(), that computes the Durbin\u2013Watson test, but\nonly with p = 1. The function dwtest() uses either a normal approximation\n(default) or an exact algorithm to calculate the p-value.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "1 Residual Correlation and Spurious Regressions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3. Residual plots for weekly interest changes\n\n    Using the interest rate data from Chap. 9, Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Residual plots for weekly interest changes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.6 contains residual plots\nfor the regression of aaa dif on cm10 dif and cm30 dif. The normal plot in\npanel (a) shows heavy tails. A t-distribution was \ufb01t to the residuals, and the\nestimated degrees of freedom was 2.99, again indicating heavy tails. Panel (b)\nshows a QQ plot of the residuals and the quantiles of the \ufb01tted t-distribution\nwith a 45o reference line. There is excellent agreement between the data and\nthe t-distribution.\n    Panel (c) is a plot of the ACF of the residuals. There is some evidence\nof autocorrelation. The Durbin\u2013Watson test was performed three times with\n\f                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.6",
      "section_title": "contains residual plots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression                                            369\n\n      a                                        Normal plot                     b                                                  t\u2212plot\n\n\n\n\n                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression                                            369",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                              1 2 3\n\n\n\n\n                                                                               theoretical Quantiles\n      theoretical quantiles\n\n\n\n\n                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1 2 3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                                                       0.0\n                              \u22121\n                              \u22123\n\n\n\n\n                                                                                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                          \u22120.2        0.0    0.2        0.4                                              \u22120.2      0.0     0.2     0.4\n                                               sample quantiles                                                                    resid\n\n\n      c                                                                        d\n\n\n\n                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2        0.0    0.2        0.4                                              \u22120.2      0.0     0.2     0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                              0.8\n\n\n\n\n                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                               Residuals\n      ACF\n\n\n\n\n                                                                                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "Residuals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.0\n                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                              0.0\n\n\n\n\n                                      0    5     10     15   20    25    30                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6          \u22120.2 0.0    0.2   0.4\n                                                       lag                                                                      fitted values\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22120.2 0.0    0.2   0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.6. Residual plots for the regression of aaa dif on cm10 dif and cm30 dif.\n\nR\u2019s durbinWatsonTest() using max.lag =1 and gave p-values of 0.006, 0.004,\nand ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.6",
      "section_title": "Residual plots for the regression of aaa dif on cm10 dif and cm30 dif.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.012. This shows the substantial random variation due to bootstrapping\nwith the default of B = 1000 resamples. Using a larger number of resamples\nwill compute the p-value with more accuracy. For example, when the number\nof resamples was increased to 10,000, three p-values were 0.0112, 0.0096, and\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.012",
      "section_title": "This shows the substantial random variation due to bootstrapping",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0106. Using dwtest(), the approximate p-value was 0.01089 and the exact\np-value could not be computed. Despite some uncertainty about the p-value,\nit is clear that the p-value is small, so there is at least some residual autocor-\nrelation.\n     To further investigate autocorrelation, ARMA models were \ufb01t to the resid-\nuals using the auto.arima() function in R to automatically select the order.\nUsing BIC, the selected model is ARIMA(0,0,0), that is, white noise. Using\nAIC, the selected model is ARIMA(0,0,3) with estimates:\n6   auto.arima(resid, ic=\"aic\")\n\n    Series: resid\n    ARIMA(0,0,3) with zero mean\n\n    Coefficients:\n              ma1                                   ma2         ma3\n          -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0106",
      "section_title": "Using dwtest(), the approximate p-value was 0.01089 and the exact",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0857                                0.0770      0.0888\n\f370                 13 Time Series Models: Further Topics\n\n      s.e.                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0857",
      "section_title": "0.0770      0.0888",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0336     0.0338         0.0342\n\n      sigma^2 estimated as 0.004075: log likelihood=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0336",
      "section_title": "0.0338         0.0342",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1172.54\n      AIC=-2337.09   AICc=-2337.04   BIC=-2317.97\n\nSeveral of the coe\ufb03cients are large relative to their standard errors. There is\nevidence of some autocorrelation, but not a great deal and the BIC-selected\nmodel does not have any autocorrelation. The sample size is 880, so there are\nenough data to detect small autocorrelations. The autocorrelation that was\nfound seems of little practical signi\ufb01cance and could perhaps be ignored; see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1172.54",
      "section_title": "AIC=-2337.09   AICc=-2337.04   BIC=-2317.97",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.2 for further investigation. The plot of residuals versus \ufb01tted values\nin panel (d) shows no sign of heteroskedasticity.                              \u0002\n\n\n\n       a                                                         b                   Residuals\n\n                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "2 for further investigation. The plot of residuals versus \ufb01tted values",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                     0.5\n        Residuals\n\n\n\n\n                                                                 ACF\n                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                     \u22120.5\n\n\n\n\n                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                            Feb 16   Jan 06   Jan 07    Jan 01               0   5   10   15    20   25   30\n                             1977     1982     1987      1992\n                                                                                          Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "Feb 16   Jan 06   Jan 07    Jan 01               0   5   10   15    20   25   30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.7. Time series plot and ACF plot of residuals when aaa is regressed on cm10\nand cm30. The plots indicate that the residuals are nonstationary.\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.7",
      "section_title": "Time series plot and ACF plot of residuals when aaa is regressed on cm10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4. Residual plots for weekly interest rates without di\ufb00erencing\n\n    The reader may have noticed that di\ufb00erenced time series have been used\nin the examples. There is a good reason for this. Many, if not most, \ufb01nan-\ncial time series are nonstationary or, at least, have very high and long-term\nautocorrelation. When one nonstationary series is regressed upon another, it\nhappens frequently that the residuals are nonstationary. This is a substantial\nviolation of the assumption of uncorrelated noise and can lead to serious prob-\nlems. An estimator is said to be consistent if it converges to the true value of\nthe parameter as the sample size increases to \u221e. The least-squares estimator\nis not consistent when the errors are an integrated process.\n    As an example, we regressed aaa on cm10 and cm30. These are the weekly\ntime series of AAA, 10-year Treasury, and 30-year Treasury interest rates,\nwhich, when di\ufb00erenced, gave us aaa dif, cm10 dif, and cm30 dif used in\nprevious examples. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Residual plots for weekly interest rates without di\ufb00erencing",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.7 contains time series and ACF plots of the resid-\nuals. The residuals are very highly correlated and perhaps are nonstationary.\nUnit root tests provide more evidence that the residuals are nonstationary.\nThe p-values of augmented Dickey\u2013Fuller tests are on one side of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.7",
      "section_title": "contains time series and ACF plots of the resid-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 or the\n\f                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "or the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression     371\n\nother, depending on the order. With the default lag order in the adf.test()\nfunction from the tseries package in R, the p-value is 0.12, so one would not\nreject the null hypothesis of nonstationarity at level ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression     371",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 or even level 0.1.\nThe kpss.test() function does reject the null hypothesis of stationarity.\n    Let us compare the estimates from regression using the original series with\nthe estimates from the di\ufb00erenced series. First, what should we expect when\nwe make this comparison? Suppose that Xt and Yt are time series following\nthe regression model\n                          Y t = \u03b1 + \u03b2 0 t + \u03b2 1 Xi + \u0017 t .                  (13.6)\nNote the linear time trend \u03b20 t. Then, upon di\ufb00erencing, we have\n\n                         \u0394Yt = \u03b20 + \u03b21 \u0394Xi + \u0394\u0017t ,                          (13.7)\n\nso the original intercept \u03b1 is removed, and the time trend\u2019s slope \u03b20 in (13.6)\nbecomes an intercept in (13.7). The time trend could be omitted in (13.6)\nif the intercept in (13.7) is not signi\ufb01cant, as happens in this example. The\nslope \u03b21 in (13.6) remains unchanged in (13.7). However, if \u0017t is I(1), then\nthe regression of Yt on Xt will not provide a consistent estimate of \u03b21 , but\nthe regression of \u0394Yt on \u0394Xi will consistently estimate \u03b21 , so the estimates\nfrom the two regressions could be very di\ufb00erent. This is what happens with\nthis example.\n    The results from regression with the original series without the time trend\nare\n\n   Call:\n   lm(formula = aaa ~ cm10 + cm30)\n\n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)\n   (Intercept)   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "or even level 0.1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9803     0.0700   14.00 < 2e-16 ***\n   cm10          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9803",
      "section_title": "0.0700   14.00 < 2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3183     0.0445    7.15 1.9e-12 ***\n   cm30          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3183",
      "section_title": "0.0445    7.15 1.9e-12 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6504     0.0498   13.05 < 2e-16 ***\n\nThe results with the di\ufb00erenced series are\n   Call:\n   lm(formula = aaa_dif ~ cm10_dif + cm30_dif)\n\n   Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n   (Intercept) -9.38e-05   2.18e-03   -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6504",
      "section_title": "0.0498   13.05 < 2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04     0.97\n   cm10_dif     3.60e-01   4.45e-02    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "0.97",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.09 2.0e-15 ***\n   cm30_dif     2.97e-01   4.98e-02    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.09",
      "section_title": "2.0e-15 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.96 3.7e-09 ***\n\n   The estimated slopes for cm10 and cm10 dif, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.96",
      "section_title": "3.7e-09 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3183 and 0.360, are some-\nwhat similar. However, the estimated slopes for cm30 and cm30 dif, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3183",
      "section_title": "and 0.360, are some-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.650\nand 0.297, are quite dissimilar relative to their standard errors. This is to\n\f372      13 Time Series Models: Further Topics\n\nbe expected if the estimators using the undi\ufb00erenced series are not consis-\ntent; also, their standard errors are not valid because they are based on the\nassumption of uncorrelated noise. In the analysis with the di\ufb00erenced data,\nthe p-value for the intercept is 0.97, so we can accept the null hypothesis that\nthe intercept is zero; this justi\ufb01es the omission of the time trend when using\nthe undi\ufb00erenced series.                                                      \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.650",
      "section_title": "and 0.297, are quite dissimilar relative to their standard errors. This is to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5. Simulated independent AR processes\n\n    To illustrate further the problems caused by regressing nonstationary\nseries, or even stationary series with high correlation, we simulated two inde-\npendent AR process, both of length 200 with \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Simulated independent AR processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99.\n7  set.seed(997711)\n8  n = 200\n 9 x = arima.sim(list(order=c(1,0,0),ar=.99),n=n)\n\n10 y = arima.sim(list(order=c(1,0,0),ar=.99),n=n)\n\n11 fit1 = lm(y~x)\n\n12 fit5 = lm(diff(y)~diff(x))\n\n\n   These processes are stationary but near the borderline of being nonstat-\nionary. After simulating these processes, one process was regressed on the\nother. We repeated this three more times. Since the processes are independent,\nthe true slope is 0. In each case, the estimated slope was far from the true\nvalue of 0 and was statistically signi\ufb01cant according to the (incorrect) p-value.\nThe results are below.\n                    Estimate Std. Error t value Pr(>|t|)\n      (Intercept)      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.99",
      "section_title": "7  set.seed(997711)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.40      0.269     -31 1.9e-78\n      x                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.40",
      "section_title": "0.269     -31 1.9e-78",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.48      0.036      13 1.6e-29\n\n                    Estimate Std. Error t value Pr(>|t|)\n      (Intercept)       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.48",
      "section_title": "0.036      13 1.6e-29",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.96      0.328    18.2 4.9e-44\n      x                -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.96",
      "section_title": "0.328    18.2 4.9e-44",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.43      0.088    -4.8 2.6e-06\n\n                    Estimate Std. Error t value Pr(>|t|)\n      (Intercept)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.43",
      "section_title": "0.088    -4.8 2.6e-06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.154      0.213   -24.2 4.5e-61\n      x                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.154",
      "section_title": "0.213   -24.2 4.5e-61",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.095      0.031     3.1 2.3e-03\n\n                    Estimate Std. Error t value Pr(>|t|)\n      (Intercept)      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.095",
      "section_title": "0.031     3.1 2.3e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.51      0.312    -1.6 1.1e-01\n      x                -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.51",
      "section_title": "0.312    -1.6 1.1e-01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.53      0.079    -6.7 2.3e-10\n\nNotice how the estimated intercepts and slope randomly vary between the\nfour simulations. The standard errors and p-values are based on the invalid\nassumption of independent errors and are erroneous and very misleading, a\nproblem that is called spurious regression. Fortunately, the violation of the\nindependence assumption would be easy to detect by plotting the residuals.\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.53",
      "section_title": "0.079    -6.7 2.3e-10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression     373\n\n   We also regressed the di\ufb00erenced series and obtained completely di\ufb00erent\nresults:\n                 Estimate Std. Error t value Pr(>|t|)\n   (Intercept)      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression     373",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.082      0.069    1.18     0.24\n   diff(x)         -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.082",
      "section_title": "0.069    1.18     0.24",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.023      0.068   -0.34     0.73\n\n                 Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.023",
      "section_title": "0.068   -0.34     0.73",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.027      0.064   -0.41     0.68\n   diff(x)         -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.027",
      "section_title": "0.064   -0.41     0.68",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.021      0.063   -0.33     0.74\n\n                 Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.021",
      "section_title": "0.063   -0.33     0.74",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.015      0.071   -0.21     0.83\n   diff(x)         -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.015",
      "section_title": "0.071   -0.21     0.83",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.022      0.076   -0.29     0.77\n\n                 Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.022",
      "section_title": "0.076   -0.29     0.77",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025      0.077   -0.32     0.75\n   diff(x)          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.025",
      "section_title": "0.077   -0.32     0.75",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.022      0.078    0.28     0.78\n\nNotice that now the estimated slopes are all near the true value of 0. All the\np-values are large and lead one to the correct conclusion that the true slope\nis 0.\n    When the noise process is stationary, an alternative to di\ufb00erencing is to\nuse an ARMA model for the noise process; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.022",
      "section_title": "0.078    0.28     0.78",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3.                  \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3.                  \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.2 Heteroscedasticity and Autocorrelation Consistent (HAC)\nStandard Errors\n\nWe now consider the e\ufb00ect of correlated noise and heteroskedasticity on stan-\ndard errors and con\ufb01dence intervals in multiple linear regression models. If\nCOV( ) = \u03c3 2 I but rather COV( ) = \u03a3 for some matrix \u03a3 , then\n        \u0002 1 , . . . , xn ) = (X T X)\u22121 X T COV(Y |x1 , . . . , xn )X(X T X)\u22121\n    COV(\u03b2|x\n                           = (X T X)\u22121 X T \u03a3 X(X T X)\u22121 .                  (13.8)\n\nThis result lets us see the e\ufb00ect of correlation or nonconstant variance among\n\u00171 , . . . , \u0017 n .\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "2 Heteroscedasticity and Autocorrelation Consistent (HAC)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.6. Regression with AR(1) errors\n\n     Suppose that \u00171 , . . . , \u0017n is a stationary AR(1) process so that \u0017t = \u03c6\u0017t\u22121 +\nut , where |\u03c6| < 1 and u1 , u2 , . . . is weak WN(0, \u03c3u2 ). Then\n                               \u239b                                  \u239e\n                                    1       \u03c6     \u03c62   \u00b7 \u00b7 \u00b7 \u03c6n\u22121\n                               \u239c \u03c6          1      \u03c6   \u00b7\u00b7\u00b7 \u03c6  n\u22122\n                                                                  \u239f\n                \u03a3 = \u03c32 \u239c       \u239d ..         ..     ..  ..      .. \u239f\n                                                                  \u23a0.          (13.9)\n                                    .        .      .      .    .\n                             \u03c6n\u22121    \u03c6n\u22122     \u03c6n\u22123   \u00b7\u00b7\u00b7    1\n\f374    13 Time Series Models: Further Topics\n\nAs an example, suppose that n = 21, X1 , . . . , Xn are equally spaced between\n\u221210 and 10, and \u03c3 2 = 1. Substituting (13.9) into (13.8) gives the cov-\nariance matrix of the estimator (\u03b2\u00020 , \u03b2\u00021 ), and taking the square roots of\nthe diagonal elements gives the standard errors. This was done with \u03c6 =\n\u22120.75, \u22120.5, \u22120.25, 0, 0.25, 0.5, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.6",
      "section_title": "Regression with AR(1) errors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.75.\n    Figure 13.8 plots the ratios of standard errors for the independent case\n(\u03c6 = 0) to the standard errors for the true value of \u03c6. These ratios are the\nfactors by which the standard errors are miscalculated if we assume that\n\u03c6 = 0, but it is not. Notice that negative values of \u03c6 result in a conservative\n(too large) standard error, but positive values of \u03c6 give a standard error that\nis too small. In the case of \u03c6 = 0.75, assuming independence gives standard\nerrors that are only about half as large as they should be. As discussed in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.75",
      "section_title": "Figure 13.8 plots the ratios of standard errors for the independent case",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3, this problem can be \ufb01xed by assuming (correctly) that the noise\nprocess is AR(1).                                                            \u0002\n    As discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3, this problem can be \ufb01xed by assuming (correctly) that the noise",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.1, if the errors in a regression model are an\nintegrated process, such as a random walk, the least-squares estimator is\ninconsistent. However, when the dependence between the errors is not too\nstrong, there are mild conditions under which the least-squares estimator is\nconsistent, meaning that it will converge to the true parameter as the sam-\nple size converges to \u221e. For the latter case there are methods available to\nestimate consistent standard errors for the coe\ufb03cient estimates. Two simple\nand widely used approaches are the heteroskedasticity consistent (HC) and\nheteroskedasticity and autocorrelation consistent (HAC) estimators.\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "1, if the errors in a regression model are an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n\n\n\n\n                                                       o   intercept\n                                                       *   slope\n                              *\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "o   intercept",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n             SE ratio\n\n\n\n\n                                   *\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "SE ratio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n\n\n\n                                         *\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                              *\n                                                   *\n                                                            *\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                   *\n\n                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5       0.0           0.5\n                                             \u03c6\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.0           0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8. Factor by which the standard error is changed when \u03c6 deviates from 0\nfor intercept (solid) and slope (dashed).\n\f                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "Factor by which the standard error is changed when \u03c6 deviates from 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression         375\n\n    Let \u0017\u02c6i denote the OLS residuals, let \u03a3     \u0002 \u02c6 = diag{\u02c6\n                                                           \u001721 , . . . , \u0017\u02c62n } denote a\ndiagonal matrix of the squared residuals, and let C    \u0002HC = X T \u03a3          \u0002 \u02c6 X. Then\na heteroskedasticity consistent (HC) estimator (see White, 1980) of the cov-\nariance matrix for the coe\ufb03cient estimates is\n                      \u0002 1 , . . . , xn ) = (X T X)\u22121 C\n              \u0002 HC (\u03b2|x\n             COV                                     \u0002HC (X T X)\u22121 .              (13.10)\nThe corresponding HC standard errors are de\ufb01ned as the square roots of the\ndiagonal entries of (13.10).\n    A heteroskedasticity and autocorrelation consistent (HAC) estimator (see\nNewey and West, 1987) of the covariance matrix for the coe\ufb03cient estimates\nis similarly de\ufb01ned as\n             \u0002 HAC (\u03b2|x\n            COV        \u0002 1 , . . . , xn ) = (X T X)\u22121 C\n                                                      \u0002HAC (X T X)\u22121 , (13.11)\nin which\n                            L          n      &                                         '\n             \u0002HC +\n      \u0002HAC = C                   w\u0002               X i \u0017\u02c6i \u0017\u02c6i\u2212\u0002 X T   +      \u0017\n                                                                             \u02c6   \u0017\n                                                                                 \u02c6    T\n      C                                                           i\u2212\u0002   X i\u2212\u0002 i\u2212\u0002 i X i ,\n                           \u0002=1        i=\u0002+1\n                                                                                        (13.12)\nwhere w\u0002 = 1 \u2212 \u0007/(L + 1) denotes the Bartlett weight function, although other\nweight functions w\u0002 can also be used. The corresponding HAC standard errors\nare de\ufb01ned as the square root of the diagonal entries of (13.11).\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression         375",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.7. HC and HAC estimates for regression of weekly interest\nchanges\n\n    In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.7",
      "section_title": "HC and HAC estimates for regression of weekly interest",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.1 a regression of aaa dif on cm10 dif and cm30 dif produced\nresiduals that exhibited minor autocorrelation; AIC suggested an MA(3)\nmodel for the residuals while BIC selected ARIMA(0,0,0), i.e., white noise.\nWe now consider whether ignoring the small autocorrelations has a practical\nimpact on inference. The previous regression results are obtained from the\nfollowing R commands.\n13 dat = read.table(file=\"WeekInt.txt\", header=T)\n14 attach(dat)\n15 cm10_dif = diff(cm10)\n\n16 aaa_dif = diff(aaa)\n\n17 cm30_dif = diff(cm30)\n\n18 fit = lm(aaa_dif ~ cm10_dif + cm30_dif)\n\n19 round(summary(fit)$coef, 4)\n\n    The HC and HAC covariance matrix estimates can be computed using the\nNeweyWest() function from the R package sandwich. The \ufb01rst argument is\na \ufb01tted model object, in this case fit. In both cases we set prewhite = F.\nFor the HAC estimate, the argument lag corresponds to the maximal lag L\nused in the Bartlett weight function above. If no value is speci\ufb01ed, one is\nselected automatically via the bwNeweyWest() function (see the help \ufb01le for\nmore information). For the HC estimate we specify lag = 0. The HC estimate\nand HAC estimate with L = 3 are shown below.\n\f376      13 Time Series Models: Further Topics\n\n20 library(sandwich)\n21 options(digits=2)\n22 NeweyWest(fit, lag = 0, prewhite = F)\n\n\n                    (Intercept) cm10_dif cm30_dif\n      (Intercept)       4.7e-06 7.3e-06 -1.1e-05\n      cm10_dif          7.3e-06 6.3e-03 -6.2e-03\n      cm30_dif         -1.1e-05 -6.2e-03 6.7e-03\n23   NeweyWest(fit, lag = 3, prewhite = F)\n                    (Intercept) cm10_dif cm30_dif\n      (Intercept)       4.6e-06 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "1 a regression of aaa dif on cm10 dif and cm30 dif produced",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00003 2.6e-05\n      cm10_dif         -3.0e-05 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00003",
      "section_title": "2.6e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00666 -6.6e-03\n      cm30_dif          2.6e-05 -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00666",
      "section_title": "-6.6e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00662 7.0e-03\n    The OLS regression results, as well as the HC and HAC estimated standard\nerrors, and their corresponding t values are summarized in Table ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00662",
      "section_title": "7.0e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1. Recall\nthat the HC and HAC standard error estimates are computed as the square\nroots of the diagonal entries of the covariance matrix estimates.\n24   sqrt(diag(NeweyWest(fit, lag = 0, prewhite = F)))\n25   sqrt(diag(NeweyWest(fit, lag = 3, prewhite = F)))\nThe corresponding t values are the OLS coe\ufb03cient estimates divided by their\nstandard error estimates.\n26   coef(fit)/sqrt(diag(NeweyWest(fit, lag = 0, prewhite = F)))\n27   coef(fit)/sqrt(diag(NeweyWest(fit, lag = 3, prewhite = F)))\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Recall",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1. Regression estimates of aaa dif on cm10 dif and cm30 dif: the OLS\nestimates, estimated standard errors, and t values are shown on the left; the esti-\nmated HC standard errors and corresponding t values are shown in the middle; and\nthe estimated HAC standard errors with L = 3 and corresponding t values are shown\non the right.\n                          OLS                    HC              HACL=3\n      Coe\ufb03cient Estimate Std. Err. t value Std. Err. t value Std. Err. t value\n      (Intercept) \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Regression estimates of aaa dif on cm10 dif and cm30 dif: the OLS",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0001 0.0022 \u22120.043      0.0022 \u22120.043     0.0021 \u22120.044\n      cm10 dif     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0001",
      "section_title": "0.0022 \u22120.043      0.0022 \u22120.043     0.0021 \u22120.044",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3602 0.0445 8.091       0.0791 4.553      0.0816 4.415\n      cm30 dif     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3602",
      "section_title": "0.0445 8.091       0.0791 4.553      0.0816 4.415",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2968 0.0498 5.956       0.0816 3.637      0.0836 3.551\n\n\n    From Table ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2968",
      "section_title": "0.0498 5.956       0.0816 3.637      0.0836 3.551",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1 we see that the HC and HAC estimates produced similar\nresults. The estimated standard error for the intercept are stable, while the\nestimated HC and HAC standard errors for the cm10 dif and cm30 dif coe\ufb03-\ncients are about twice as large as the OLS estimates of the standard errors, and\nas a result, the corresponding t values are about half as large in magnitude.\nIn this case, however, the cm10 dif and cm30 dif coe\ufb03cients remain statis-\ntically signi\ufb01cant, with both estimates over three standard errors above zero.\nThe minor serial correlation (and heteroskedasticity) in the OLS residuals\ndoes not appear to have a practical impact on inference in this example. \u0002\n\f                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "we see that the HC and HAC estimates produced similar",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression      377\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression      377",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3 Linear Regression with ARMA Errors\n\nWhen residual analysis shows that the residuals are correlated, then one of\nthe key assumptions of the linear model does not hold, and tests and con\ufb01-\ndence intervals based on this assumption are invalid and cannot be trusted.\nFortunately, there is a solution to this problem: replace the assumption of\nindependent noise by the weaker assumption that the noise process is station-\nary but possibly correlated. One could, for example, assume that the noise is\nan ARMA process. This is the strategy we will discuss in this section; this\napproach is referred to as an ARMAX model, in which the X indicates the\ninclusion of exogenous regression variables.\n    The linear regression model with ARMA errors combines the linear regres-\nsion model (9.1) and the ARMA model (12.26) for the noise, so that\n\n                       Yt = \u03b20 + \u03b21 Xt,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xt,p + \u0017t ,               (13.13)\n\nwhere\n\n        (1 \u2212 \u03c61 B \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p B p ) \u0017t = (1 + \u03b81 B + \u00b7 \u00b7 \u00b7 + \u03b8q B q ) ut ,     (13.14)\n\nand u1 , . . . , un is white noise.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3 Linear Regression with ARMA Errors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8. Demand for ice cream\n\n    This example uses the data set Icecream in R\u2019s Ecdat package. The data\nare four-weekly observations from March 18, 1951, to July 11, 1953 on four\nvariables, cons = U.S. consumption of ice cream per head in pints; income =\naverage family income per week (in U.S. Dollars); price = price of ice cream\n(per pint); and temp = average temperature (in Fahrenheit). There is a total\nof 30 observations. Since there are 13 four-week periods per year, there are\nslightly over two years of data.\n    First, a linear model was \ufb01t with cons as the response and income, price,\nand temp as the predictor variables. One can see that income and temp are\nsigni\ufb01cant, especially temp (not surprisingly).\n    Call:\n    lm(formula = cons ~ income + price + temp, data = Icecream)\n\n    Residuals:\n         Min       1Q        Median          3Q        Max\n    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "Demand for ice cream",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06530 -0.01187       0.00274     0.01595    0.07899\n\n    Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n    (Intercept) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06530",
      "section_title": "-0.01187       0.00274     0.01595    0.07899",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.197315    0.270216    0.73    0.472\n    income       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.197315",
      "section_title": "0.270216    0.73    0.472",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.003308   0.001171    2.82    0.009 **\n    price       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.003308",
      "section_title": "0.001171    2.82    0.009 **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.044414   0.834357   -1.25    0.222\n\f378      13 Time Series Models: Further Topics\n\n      temp         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.044414",
      "section_title": "0.834357   -1.25    0.222",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.003458    0.000446      7.76   3.1e-08 ***\n      ---\n\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.003458",
      "section_title": "0.000446      7.76   3.1e-08 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0368 on 26 degrees of freedom\n      Multiple R-squared: 0.719,      Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0368",
      "section_title": "on 26 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.687\n      F-statistic: 22.2 on 3 and 26 DF, p-value: 2.45e-07\n\nA Durbin\u2013Watson test has a very small p-value, so we can reject the null\nhypothesis that the noise is uncorrelated.\n28 options(digits=3)\n29 library(\"car\")\n30 durbinWatsonTest(fit_ic_lm)\n\n\n      lag Autocorrelation D-W Statistic p-value\n         1            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.687",
      "section_title": "F-statistic: 22.2 on 3 and 26 DF, p-value: 2.45e-07",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.33          1.02       0\n       Alternative hypothesis: rho != 0\n\nNext, the linear regression model with AR(1) errors was \ufb01t and the AR(1)\ncoe\ufb03cient was over three times its standard error, indicating statistical signi\ufb01-\ncance. This was done using R\u2019s arima() function, which speci\ufb01es the regression\nmodel with the xreg argument. It is interesting to note that the coe\ufb03cient of\nincome is now nearly equal to 0 and no longer signi\ufb01cant. The e\ufb00ect of temp is\nsimilar to that of the linear model \ufb01t, though its standard error is now larger.\n      Series: cons\n      ARIMA(1,0,0) with non-zero mean\n\n      Coefficients:\n              ar1 intercept    income      price    temp\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.33",
      "section_title": "1.02       0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.732     0.538     0.000     -1.086   0.003\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.732",
      "section_title": "0.538     0.000     -1.086   0.003",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.237      0.325     0.003      0.734   0.001\n\n      sigma^2 estimated as 0.00091: log likelihood=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.237",
      "section_title": "0.325     0.003      0.734   0.001",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "62.1\n      AIC=-112   AICc=-109   BIC=-104\n\nFinally, the linear regression model with MA(1) errors was \ufb01t and the MA(1)\ncoe\ufb03cient was also over three times its standard error, again indicating statis-\ntical signi\ufb01cance. The model with AR(1) errors has a slightly better (smaller)\nAIC and BIC values than the model with MA(1), but there is not much of\na di\ufb00erence between the models in terms of AIC or BIC. However, the two\nmodels imply rather di\ufb00erent types of noise autocorrelation. The MA(1) model\nhas no correlation beyond lag 1. The AR(1) model with coe\ufb03cient ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "62.1",
      "section_title": "AIC=-112   AICc=-109   BIC=-104",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.732 has\nautocorrelation persisting much longer. For example, the autocorrelation is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.732",
      "section_title": "has",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7322 = 0.536 at lag 2, 0.7323 = 0.392 at lag 3, and still 0.7324 = 0.287 at\nlag 4.\n      Series: cons\n      ARIMA(0,0,1) with non-zero mean\n\f                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7322",
      "section_title": "= 0.536 at lag 2, 0.7323 = 0.392 at lag 3, and still 0.7324 = 0.287 at",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3 Time Series and Regression                            379\n\n\n      Coefficients:\n              ma1 intercept                 income              price         temp\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "Time Series and Regression                            379",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.503     0.332                  0.003             -1.398        0.003\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.503",
      "section_title": "0.332                  0.003             -1.398        0.003",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.160      0.270                  0.001              0.798        0.001\n\n      sigma^2 estimated as 0.000957: log likelihood=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.160",
      "section_title": "0.270                  0.001              0.798        0.001",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "61.6\n      AIC=-111   AICc=-107   BIC=-103\n\nInterestingly, the estimated e\ufb00ect of income is larger and signi\ufb01cant, much\nlike its e\ufb00ect as estimated by the linear model with independent errors but\nunlike the result for the linear model with AR(1) errors.\n\n        linear model/indep. noise              linear model/AR(1) noise                       linear model/MA(1) noise\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "61.6",
      "section_title": "AIC=-111   AICc=-107   BIC=-103",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                            0.8\n\n\n\n\n                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      0.4\n\n\n\n\n                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                           0.4\nACF\n\n\n\n\n                                      ACF\n\n\n\n\n                                                                                     ACF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                            0.0\n\n\n\n\n                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n      \u22120.4\n\n\n\n\n                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                           \u22120.4\n\n             0   2   4   6   8   12                0   2   4   6   8    12                        0   2   4   6   8   12\n                         lag                                    lag                                           lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9. Ice cream consumption example. Residual ACF plots for the linear model\nwith independent noise, the linear model with AR(1) noise, and the linear model with\nMA(1) noise.\n    The ACFs of the residuals from the linear model and from the linear\nmodels with AR(1) and MA(1) errors are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "Ice cream consumption example. Residual ACF plots for the linear model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9. The residuals\nfrom the linear model estimate \u00171 , . . . , \u0017n in (13.13), and show some autocor-\nrelation. The residuals from the linear models with either AR(1) or MA(1)\nerrors estimate u1 , . . . , un in (13.14), and show little autocorrelation. One con-\ncludes that the linear model with either AR(1) or MA(1) errors \ufb01ts well and\neither an AR(1) or MA(1) term is needed.\n    Why is the e\ufb00ect of income larger and signi\ufb01cant if the noise is assumed\nto be either independent or MA(1) but smaller and insigni\ufb01cant if the noise\nis AR(1)? To attempt an answer, time series plots of the four variables were\nexamined. The plots are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "The residuals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.10. The strong seasonal trend in\ntemp is obvious and cons follows this trend. There is a slightly increasing\ntrend in cons, which appears to have two possible explanations. The trend\nmight be explained by the increasing trend in income. However, with the\nstrong residual autocorrelation implied by the AR(1) model, the trend in\ncons could also be explained by noise autocorrelation. One problem here is\nthat we have a small sample size, only 30 observations. With more data it\nmight be possible to separate the e\ufb00ects on ice cream consumption of income\nand noise autocorrelation.\n\f380            13 Time Series Models: Further Topics\n\n      a                                                    b\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "The strong seasonal trend in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.55\n\n\n\n\n                                                                   70\n                                                           temp\n      cons\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.55",
      "section_title": "70",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.40\n\n\n\n\n                                                                   50\n                                                                   30\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.40",
      "section_title": "50",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25\n\n\n\n\n                       0   5   10   15      20   25   30                   0   5   10   15      20   25   30\n                                    index                                               index\n\n      c                                                    d\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "0   5   10   15      20   25   30                   0   5   10   15      20   25   30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.280\n                90\n      income\n\n\n\n\n                                                           price\n                80\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.280",
      "section_title": "90",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.260\n                       0   5   10   15      20   25   30                   0   5   10   15      20   25   30\n                                    index                                               index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.260",
      "section_title": "0   5   10   15      20   25   30                   0   5   10   15      20   25   30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.10. Time series plots for the ice cream consumption example and the vari-\nables used to predict consumption.\n\n\n    In summary, there is a strong seasonal component to ice cream consump-\ntion, with consumption increasing, as would be expected, with warmer tem-\nperatures. Ice cream consumption does not depend much, if at all, on price,\nthough it should be noted that price has not varied much in this study; see\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "Time series plots for the ice cream consumption example and the vari-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.10. Greater variation in price might cause cons to depend more on\nprice. Finally, it is uncertain whether ice cream consumption increases with\nfamily income.                                                            \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "Greater variation in price might cause cons to depend more on",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 Multivariate Time Series\nSuppose that for each t, Y t = (Y1,t , . . . , Yd,t )\u0003 is a d-dimensional random\nvector representing quantities that were measured at time t, e.g., returns on\nd equities. Then Y 1 , Y 2 . . . is called a d-dimensional multivariate time series.\n    The de\ufb01nition of stationarity for multivariate time series is the same as\ngiven before for univariate time series. A multivariate time series is said to be\nstationary if for every n and m, Y 1 , . . . , Y n and Y 1+m , . . . , Y n+m have the\nsame distributions.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Multivariate Time Series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.1 The Cross-Correlation Function\n\nSuppose that Yj and Yi are the two component series of a stationary multi-\nvariate time series. The cross-correlation function (CCF) between Yj and Yi\nis de\ufb01ned as\n                      \u03c1Yj ,Yi (h) = Corr{Yj (t), Yi (t \u2212 h)}         (13.15)\n\f                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "1 The Cross-Correlation Function",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 Multivariate Time Series     381\n\nand is the correlation between Yj at a time t and Yi at h time units earlier. As\nwith autocorrelation, h is called the lag. However, unlike the ACF, the CCF is\nnot symmetric in the lag variable h, that is, \u03c1Yj ,Yi (h) = \u03c1Yj ,Yi (\u2212h). Instead, as\na direct consequence of de\ufb01nition (13.15), we have that \u03c1Yj ,Yi (h) = \u03c1Yi ,Yj (\u2212h).\n    The CCF can be de\ufb01ned for multivariate time series that are not station-\nary, but only weakly stationary. A multivariate time series Y 1 , Y 2 , . . . is said\nto be weakly stationary if the mean and covariance matrix of Y t are \ufb01nite\nand do not depend on t, and if the right-hand side of (13.15) is independent\nof t for all j, i, and h.\n    Cross-correlations can suggest how the component series might be in\ufb02u-\nencing each other or might be in\ufb02uenced by a common factor. Like all corre-\nlations, cross-correlations only show statistical association, not causation, but\na causal relationship might be deduced from other knowledge.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Multivariate Time Series     381",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9. Cross-correlation between changes in CPI (consumer price in-\ndex) and IP (industrial production)\n\n\n      a                                                    b\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "Cross-correlation between changes in CPI (consumer price in-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1 4.3 4.5 4.7\n\n\n\n\n                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.1",
      "section_title": "4.3 4.5 4.7",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.45\n       log(CPI)\n\n\n\n\n                                                            log(IP)\n\n                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.45",
      "section_title": "log(CPI)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.30\n\n\n\n\n                                    1978   1982    1986                       1978   1982    1986\n                                            year                                      year\n\n      c                                                    d\n     \u0394 log(CPI)\n\n\n\n\n                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.30",
      "section_title": "1978   1982    1986                       1978   1982    1986",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n                                                          \u0394 log(IP)\n                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "\u0394 log(IP)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.005 0.005\n\n\n\n\n                                                                      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.005",
      "section_title": "0.005",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n\n\n\n\n                                    1978   1982    1986                       1978   1982    1986\n                                            year                                      year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "1978   1982    1986                       1978   1982    1986",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.11. (a) Time series plot of log(CPI) (b) Time series plot of log(IP) (c) Time\nseries plot of changes in log(CPI) (d) Time series plot of changes in log(IP).\n\n   Time series plots for the logarithm of CPI (cpi), the logarithm of IP (ip),\nand changes in cpi and ip, are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.11",
      "section_title": "(a) Time series plot of log(CPI) (b) Time series plot of log(IP) (c) Time",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.11 panels (a)\u2013(d), respectively.\nThe cross-correlation function between changes in the logarithm of CPI (\u0394cpi)\nand changes in the logarithm of IP (\u0394ip) is shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.11",
      "section_title": "panels (a)\u2013(d), respectively.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.12. It was created\nby the ccf() function in R.\n31   CPI.dat = read.csv(\"CPI.dat.csv\")\n32   CPI_diff1 = diff(log(as.matrix(CPI.dat$CPI)[769:900,])) # 1977--1987\n\f382    13 Time Series Models: Further Topics\n\n33 IP.dat = read.csv(\"IP.dat.csv\")\n34 IP_diff1 = diff(log(as.matrix(IP.dat$IP)[697:828,]))                # 1977--1987\n35 ccf(CPI_diff1, IP_diff1)\n\n\n\nThe largest absolute cross-correlations are at negative lags and these\ncorrelations are negative. This means that an above-average (below-average)\nchange in cpi predicts a future change in ip that is below (above) average. As\njust emphasized, correlation does not imply causation, so we cannot say that\nchanges in cpi cause opposite changes in future ip, but the two series behave\nas if this were happening. Correlation does imply predictive ability. There-\nfore, if we observe an above-average change in cpi, then we should predict\nfuture changes in ip that will be below average. In practice, we should use the\ncurrently observed changes in both cpi and ip, not just cpi, to predict future\nchanges in ip. We will discuss prediction using two or more related time series\nin Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.12",
      "section_title": "It was created",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.5.                                                             \u0002\n\n                                  corr{\u0394cpi(t),\u0394ip(t\u2212lag)}\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "5.                                                             \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n              0.0\n        CCF\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n              \u22120.2\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                     \u221215   \u221210     \u22125        0        5      10   15\n                                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "\u221215   \u221210     \u22125        0        5      10   15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.12. Sample CCF for \u0394cpi and \u0394ip. Note the negative correlation at negative\nlags, that is, between the cpi and future values of ip.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.12",
      "section_title": "Sample CCF for \u0394cpi and \u0394ip. Note the negative correlation at negative",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.2 Multivariate White Noise\n\nA d-dimensional multivariate time series Y 1 , Y 2 , . . . is a weak WN(\u03bc, \u03a3)\nprocess if\n 1. E(Y t ) = \u03bc (constant and \ufb01nite) for all t;\n 2. COV(Y t ) = \u03a3 (constant and \ufb01nite) for all t; and\n 3. for all t = s, all components of Y t are uncorrelated with all components\n    of Y s .\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "2 Multivariate White Noise",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 Multivariate Time Series      383\n\n    Notice that if \u03a3 is not diagonal, then there is cross-correlation between\nthe components of Y t because Corr(Yj,t , Yi,t ) = \u03a3j,i ; in other words, there\nmay be nonzero contemporaneous correlations. However, for all 1 \u2264 j, i \u2264 d,\nCorr(Yj,t , Yi,s ) = 0 if t = s.\n    Furthermore, Y 1 , Y 2 , . . . is an i.i.d. WN(\u03bc, \u03a3) process if, in addition to\nconditions 1\u20133, Y 1 , Y 2 , . . . are independent and identically distributed. If\nY 1 , Y 2 , . . . are also multivariate normally distributed, then they are a Gaus-\nsian WN(\u03bc, \u03a3) process.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Multivariate Time Series      383",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.3 Multivariate ACF Plots and the Multivariate\nLjung-Box Test\n\nThe ACF for multivariate time series includes the d marginal ACFs for each\nunivariate series {\u03c1Yi (h) : i = 1, . . . , d}, and the d(d \u2212 1)/2 CCFs for all\nunordered pairs of the univariate series {\u03c1Yj ,Yi (h) : 1 \u2264 j < i \u2264 d}. It is\nsu\ufb03cient to only consider the unordered pairs because \u03c1Yj ,Yi (h) = \u03c1Yi ,Yj (\u2212h).\n    In R, if two (or more) univariate time series with matching time indices are\nstored as n \u00d7 1 vectors, the cbind() function can by used to make an n \u00d7 2\nmatrix consisting of the joint time series. The acf() function may be applied\nto such multivariate time series. The sample ACF for (\u0394cpi, \u0394ip)\u0003 is shown\nin Fig. 13.13, and generated by the following commands in R.\n36   CPI_IP = cbind(CPI_diff1, IP_diff1)\n37   acf(CPI_IP)\nThe marginal sample ACF for \u0394cpi and \u0394ip are shown in the \ufb01rst and sec-\nond diagonal panels, respectively. Both show signi\ufb01cant serial correlation, but\nthere is much more persistence in the \ufb01rst. The sample CCF for \u0394cpi and\n\u0394ip has been split between the top right and bottom left panels by positive\nand negative lags, respectively. Notice that combining the o\ufb00-diagonal panels\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "3 Multivariate ACF Plots and the Multivariate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.13 reproduce the CCF shown in Fig. 13.12.\n    Each of the panels in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.13",
      "section_title": "reproduce the CCF shown in Fig. 13.12.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.13 include test bounds to test the null hy-\npothesis that an individual autocorrelation or lagged cross-correlation coe\ufb03-\ncient is 0. As in the univariate case, the usual level of the test is 0.05, and\none can expect to see about 1 out of 20 sample correlations outside the test\nbounds simply by chance. Also, as in the univariate case, a simultaneous test is\navailable.\n    Let \u03c1(h) denote the d\u00d7d lag-h cross-correlation matrix for a d-dimensional\nmultivariate time series. The null hypothesis of the multivariate Ljung\u2013Box\ntest is H0 : \u03c1(1) = \u03c1(2) = \u00b7 \u00b7 \u00b7 = \u03c1(K) = 0 for some K, say K = 5 or 10. If\nthe multivariate Ljung\u2013Box test rejects, then we conclude that one or more\nof \u03c1(1), \u00b7 \u00b7 \u00b7 , \u03c1(K) is nonzero. If, in fact, the lagged cross-correlation 1 to K\nare all zero, then there is only a 1 in 20 chance of falsely concluding that\nthey are not all zero, assuming a level ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.13",
      "section_title": "include test bounds to test the null hy-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 test. In contrast, if the lagged\ncross-correlation are tested one at time, then there is a much higher chance\nof concluding that one or more is nonzero.\n\f384          13 Time Series Models: Further Topics\n\n                            cpi_diff1                             cpi_diff1 & ip_diff1\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "test. In contrast, if the lagged",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                       1.0\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      0.6\nACF\n\n\n\n\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                       \u22120.2\n             0          5          10         15              0    5         10          15\n                               Lag                                        Lag\n\n\n\n                       ip_diff1 & cpi_diff1                             ip_diff1\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                       1.0\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      0.6\nACF\n\n\n\n\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                       \u22120.2\n                 \u221215         \u221210        \u22125         0          0    5         10          15\n                               Lag                                        Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.13. Sample ACF for (\u0394cpi, \u0394ip)\u0002 . The marginal sample ACF for \u0394cpi and\n\u0394ip are shown in the \ufb01rst and second diagonal panels, respectively; the sample CCF\nfor \u0394cpi and \u0394ip has been split between the top right and bottom left panels by\npositive and negative lags, respectively.\n\n\n    The following commands will conduct the multivariate Ljung\u2013Box test in\nR for the bivariate series (\u0394cpi, \u0394ip)\u0003 .\n38    source(\"SDAFE2.R\")\n39    mLjungBox(CPI_IP, lag = 10)\n         K   Q(K) d.f. p-value\n      1 10 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.13",
      "section_title": "Sample ACF for (\u0394cpi, \u0394ip)\u0002 . The marginal sample ACF for \u0394cpi and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "532.48   40       0\n\nThe multivariate Ljung\u2013Box test statistic was 532.48, and the approximate\np-value was 0, con\ufb01rming that there is signi\ufb01cant serial correlation in the \ufb01rst\nK = 10 lags.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "532.48",
      "section_title": "40       0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.4 Multivariate ARMA Processes\n\nA d-dimensional multivariate time series Y 1 , Y 2 , . . . is a multivariate ARMA\n(p, q) process with mean \u03bc if for d \u00d7 d matrices \u03a61 , . . . , \u03a6p and \u0398 1 , . . . , \u0398 q ,\n\nY t \u2212 \u03bc = \u03a61 (Y t\u22121 \u2212 \u03bc) + \u00b7 \u00b7 \u00b7 + \u03a6p (Y t\u2212p \u2212 \u03bc) + t + \u0398 1 t\u22121 + \u00b7 \u00b7 \u00b7 + \u0398 q t\u2212q ,\n                                                                           (13.16)\nwhere 1 , . . . , n is a multivariate weak WN(0, \u03a3) process. Multivariate AR\nprocesses (the case q = 0) are also called vector AR or VAR processes and are\nwidely used in practice.\n\f                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "4 Multivariate ARMA Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 Multivariate Time Series      385\n\n   As an example, a bivariate AR(1) process can be written as\n         \u0007           \b \u0007             \b\u0007             \b \u0007        \b\n           Y1,t \u2212 \u03bc1       \u03c61,1 \u03c61,2    Y1,t\u22121 \u2212 \u03bc1       \u00171,t\n                      =                               +          ,\n           Y2,t \u2212 \u03bc2       \u03c62,1 \u03c62,2    Y2,t\u22121 \u2212 \u03bc2       \u00172,t\n\nwhere\n                                         \u0007                 \b\n                                             \u03c61,1   \u03c61,2\n                             \u03a6 = \u03a61 =                          .\n                                             \u03c62,1   \u03c62,2\nTherefore,\n\n             Y1,t = \u03bc1 + \u03c61,1 (Y1,t\u22121 \u2212 \u03bc1 ) + \u03c61,2 (Y2,t\u22121 \u2212 \u03bc2 ) + \u00171,t\n\nand\n             Y2,t = \u03bc2 + \u03c62,1 (Y1,t\u22121 \u2212 \u03bc1 ) + \u03c62,2 (Y2,t\u22121 \u2212 \u03bc2 ) + \u00172,t ,\nso that \u03c6i,j is the amount of \u201cin\ufb02uence\u201d of Yj,t\u22121 on Yi,t . Similarly, for a\nbivariate AR(p) process, \u03c6ki,j (the (i, j)th component of \u03a6k ) is the in\ufb02uence\nof Yj,t\u2212k on Yi,t , k = 1, . . . , p.\n    For a d-dimensional AR(1), it follows from (13.16) with p = 1 and \u03a6 = \u03a61\nthat\n\n        E(Y t |Y t\u22121 , . . . , Y 1 ) = E(Y t |Y t\u22121 ) = \u03bc + \u03a6(Y t\u22121 \u2212 \u03bc).       (13.17)\n\nHow does E(Y t ) depend on the more distant past, say on Y t\u22122 ? To answer\nthis question, we can generalize (13.17). To keep notation simple, assume that\nthe mean has been subtracted from Y t so that \u03bc = 0. Then\n\n                   Y t = \u03a6Y t\u22121 +     t = \u03a6{\u03a6Y t\u22122 +           t\u22121 } +   t\n\nand, because E( t\u22121 |Y t\u22122 ) = 0 and E( t |Y t\u22122 ) = 0,\n\n                              E(Y t |Y t\u22122 ) = \u03a62 Y t\u22122 .\n\nBy similar calculations,\n\n                      E(Y t |Y t\u2212k ) = \u03a6k Y t\u2212k , for all k > 0.                (13.18)\n\n    It can be shown using (13.18), that the mean will explode if any of the\neigenvectors of \u03a6 are greater than 1 in magnitude. In fact, an AR(1) process is\nstationary if and only if all of the eigenvalues of \u03a6 are less than 1 in absolute\nvalue. The eigen() function in R can be used to \ufb01nd the eigenvalues.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Multivariate Time Series      385",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0.                (13.18)",
        "start": 1632,
        "end": 1661
      }
    ]
  },
  {
    "content": "13.10. A bivariate AR model for \u0394 cpi and \u0394 ip\n\n    This example uses the CPI and IP data sets discussed in earlier examples\n(cpi and ip denote the log transformed series). Bivariate AR processes were\n\ufb01t to (\u0394cpi, \u0394ip)\u0003 using R\u2019s function ar(). AIC as a function of p is shown\n\f386         13 Time Series Models: Further Topics\n\nbelow. The two best-\ufb01tting models are AR(1) and AR(5), with the latter being\nslightly better by AIC. Although BIC is not part of ar()\u2019s output, it can be\ncalculated easily since BIC = AIC + {log(n)\u22122}p. Because {log(n)\u22122} = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "A bivariate AR model for \u0394 cpi and \u0394 ip",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.9\nin this example, it is clear that BIC is much smaller for the AR(1) model than\nfor the AR(5) model. For this reason and because the AR(1) model is so much\nsimpler to analyze, we will use the AR(1) model.\n40 CPI_IP = cbind(CPI_diff1,IP_diff1)\n41 arFit = ar(CPI_IP,order.max=10)\n42 options(digits=2)\n\n43 arFit$aic\n\n\n\n      p                   0        1          2          3      4\n      AIC            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.9",
      "section_title": "in this example, it is clear that BIC is much smaller for the AR(1) model than",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "127.99     0.17       1.29       5.05   3.40\n\n                 5        6        7          8       9         10\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "127.99",
      "section_title": "0.17       1.29       5.05   3.40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00     6.87     9.33      10.83   13.19      14.11\n\nThe commands and results for \ufb01tting the bivariate AR(1) model are\n44   arFit1 = ar(CPI_IP, order.max = 1) ; arFit1\nwith\n                                         \u0007                      \b\n                                 \u0002=           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "6.87     9.33      10.83   13.19      14.11",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.767    0.0112\n                                 \u03a6\n                                             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.767",
      "section_title": "0.0112",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.330    0.3014\nand\n                                    \u0007                                \b\n                              \u0002 =       5.68e \u2212 06     3.33e \u2212 06\n                              \u03a3                                          .   (13.19)\n                                        3.33e \u2212 06     6.73e \u2212 05\nThe function ar() does not estimate \u03bc, but \u03bc can be estimated by the sample\nmean, which is (0.0052, 0.0021)\u0003 .\n45   colMeans(CPI_IP)\n                                                     \u0002 Since \u03a61,2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.330",
      "section_title": "0.3014",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01 \u2248 0,\n    It is useful to look at the two o\ufb00-diagonals of \u03a6.\nY2,t\u22121 (lagged ip) has little in\ufb02uence on Y1,t (cpi), and since \u03a62,1 = \u22120.330,\nY1,t\u22121 (lagged cpi) has a substantial negative e\ufb00ect on Y2,t (ip), given the\nother variables in the model. It should be emphasized that \u201ce\ufb00ect\u201d means\nstatistical association, not necessarily causation. This agrees with what we\nfound when looking at the CCF for these series in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "\u2248 0,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9.\n    How does ip depend on cpi further back in time? To answer this question\nwe look at the (1, 2) elements of the following powers of \u03a6:\n46 bPhi = arFit1$ar[,,] ; bPhi\n47 bPhi2 = bPhi %*% bPhi ; bPhi2\n48 bPhi3 = bPhi2 %*% bPhi ; bPhi3\n\n49 bPhi4 = bPhi3 %*% bPhi ; bPhi4\n\n50 bPhi5 = bPhi4 %*% bPhi ; bPhi5\n\f                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "How does ip depend on cpi further back in time? To answer this question",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4 Multivariate Time Series      387\n\n\n                     \u0007                \b         \u0007             \b\n             \u00022 =        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "Multivariate Time Series      387",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.58   0.012     \u00023 =     0.44 0.010\n             \u03a6                          , \u03a6                     ,\n                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.58",
      "section_title": "0.012     \u00023 =     0.44 0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35  0.087             \u22120.30 0.022\n               \u0007                  \b               \u0007               \b\n        \u00024 =       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "0.087             \u22120.30 0.022",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.34    0.0081           \u0002 5      0.26  0.0062\n        \u03a6                           , and \u03a6 =                       .\n                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.34",
      "section_title": "0.0081           \u0002 5      0.26  0.0062",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.24   0.0034                   \u22120.18 \u22120.0017\nWhat is interesting here is that the (1,2) elements, that is, \u22120.35, \u22120.30,\n\u22120.24, and \u22120.18, decay to zero slowly, much like the CCF. This helps\nexplain why the AR(1) model \ufb01ts the data well. This behavior where the\ncross-correlations are all negative and decay only slowly to zero is quite dif-\nferent from the behavior of the ACF of a univariate AR(1) process. For the\nlatter, the correlations either are all positive or else alternate in sign, and in\neither case, unless the lag-1 correlation is nearly equal to 1, the correlations\ndecay rapidly to 0.\n    In contrast to these negative correlations between \u0394 cpi and future \u0394 ip,\nit follows from (13.19)\n                     \u0011 that the white noise series has a positive, albeit small,\ncorrelation of 3.33/ (5.68)(67.3) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.24",
      "section_title": "0.0034                   \u22120.18 \u22120.0017",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.17. The white noise series represents\nunpredictable changes in the \u0394 cpi and \u0394 ip series, so we see that the unpre-\ndictable changes have positive correlation. In contrast, the negative correla-\ntions between \u0394 cpi and future \u0394 ip concern predictable changes.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.17",
      "section_title": "The white noise series represents",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.14 shows the ACF of the \u0394 cpi and \u0394 ip residuals and the CCF\nof these residuals. There is little auto- or cross-correlation in the residuals at\nnonzero lags, indicating that the AR(1) has a satisfactory \ufb01t. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.14",
      "section_title": "shows the ACF of the \u0394 cpi and \u0394 ip residuals and the CCF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.14\nwas produced by the acf() function in R. When applied to a multivariate\ntime series, acf() creates a matrix of plots. The univariate ACFs are on the\nmain diagonal, the CCFs at positive lags are above the main diagonal, and\nthe CCFs at negative values of lag are below the main diagonal.                 \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.14",
      "section_title": "was produced by the acf() function in R. When applied to a multivariate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.5 Prediction Using Multivariate AR Models\n\nForecasting with multivariate AR processes is much like forecasting with uni-\nvariate AR processes. Given a multivariate AR(p) time series Y 1 , . . . , Y n , the\nforecast of Y n+1 is\n\n             Y\u0002 n+1 = \u03bc  \u0002 1 (Y n \u2212 \u03bc\n                      \u0002 +\u03a6                       \u0002 p (Y n+1\u2212p \u2212 \u03bc\n                                    \u0002) + \u00b7 \u00b7 \u00b7 + \u03a6              \u0002 ),\n\nthe forecast of Y n+2 is\n\n            Y\u0002 n+2 = \u03bc  \u0002 1 (Y\u0002 n+1 \u2212 \u03bc\n                     \u0002 +\u03a6                          \u0002 p (Y n+2\u2212p \u2212 \u03bc\n                                      \u0002) + \u00b7 \u00b7 \u00b7 + \u03a6              \u0002 ),\n\nand so forth, so that for all h,\n\n       Y\u0002 n+h = \u03bc  \u0002 1 (Y\u0002 n+h\u22121 \u2212 \u03bc\n                \u0002 +\u03a6                            \u0002 p (Y\u0002 n+h\u2212p \u2212 \u03bc\n                                   \u0002) + \u00b7 \u00b7 \u00b7 + \u03a6               \u0002 ),         (13.20)\n\f388          13 Time Series Models: Further Topics\n\n                                     Resid Series 1                                     Resid Series 1 & Resid Series 2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "5 Prediction Using Multivariate AR Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                             1.0\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                             0.6\nACF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             0.2\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             \u22120.2\n             0                   5            10             15                     0         5          10            15\n                                          Lag                                                          Lag\n\n                        Resid Series 2 & Resid Series 1                                           Resid Series 2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                             1.0\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                             0.6\nACF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             0.2\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                             \u22120.2\n\n                        \u221215             \u221210             \u22125           0              0         5          10            15\n                                          Lag                                                          Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.14. The ACF and CCF for the residuals when \ufb01tting a bivariate AR(1)\nmodel to (\u0394 cpi, \u0394 ip)\u0002 . Top left: The ACF of \u0394 cpi residuals. Top right: The CCF\nof \u0394 cpi and \u0394 ip residuals with positive values of lag. Bottom left: The CCF of\n\u0394 cpi and \u0394 ip residuals with negative values of lag. Bottom right: The ACF of \u0394 ip\nresiduals.\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.14",
      "section_title": "The ACF and CCF for the residuals when \ufb01tting a bivariate AR(1)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.006\n\n\n\n\n                                 *\n                                                                                                  o      o         o\n                                                                                    o     o\n                                          *                              o\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.006",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.004\n\n\n\n\n                                                                     o\n                                                             o\n                                                   *o\n             forecast\n\n\n\n\n                                                             * *\n                                          o                      * * * *\n                                                                         * *\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.004",
      "section_title": "o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.002\n\n\n\n\n                                 o\n                                                         o    \u0394cpi\n                                                              \u0394ip\n                                                        *\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.002",
      "section_title": "o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                 0                 2                 4              6             8            10\n                                                                         h\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "0                 2                 4              6             8            10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.15. Forecasts of \u0394 cpi (solid) and \u0394 ip (dashed) using a bivariate AR(1)\nmodel. The number of time units ahead is h. At h = 0, the last observed values of the\ntime series are plotted. The two horizontal lines are at the means of the series, and\nthe forecasts will asymptote to these lines as h \u2192 \u221e since this model is stationary.\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.15",
      "section_title": "Forecasts of \u0394 cpi (solid) and \u0394 ip (dashed) using a bivariate AR(1)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5 Long-Memory Processes      389\n\nwhere we use the convention that Y\u0002 t = Y t if t \u2264 n. For an AR(1) model,\nrepeated application of (13.20) shows that\n\n                                    Y\u0002 n+h = \u03bc  \u0002 h (Y n \u2212 \u03bc\n                                             \u0002 +\u03a6          \u0002 ).                        (13.21)\n                                                  1\n\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Long-Memory Processes      389",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.11. Using a bivariate AR(1) model to predict CPI and IP\n\n    The \u0394CPI and \u0394IP series were forecast using (13.21) with estimates found\nin Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.11",
      "section_title": "Using a bivariate AR(1) model to predict CPI and IP",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.10. Figure 13.15 shows forecasts up to 10 months ahead for both\nCPI and IP. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "Figure 13.15 shows forecasts up to 10 months ahead for both",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.16 shows forecast limits computed by simulation using\nthe techniques described in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.16",
      "section_title": "shows forecast limits computed by simulation using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12.2 generalized to a multivariate time\nseries.                                                                    \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "2 generalized to a multivariate time",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5 Long-Memory Processes\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Long-Memory Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5.1 The Need for Long-Memory Stationary Models\n\nIn Chap. 12, ARMA processes were used to model stationary time series.\nStationary ARMA processes have only short memories in that their auto-\ncorrelation functions decay to zero exponentially fast. That is, there exist a\nD > 0 and r < 1 such that\n                                              \u03c1(k) < D|r|k\n\n                                                     \u0394cpi\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "1 The Need for Long-Memory Stationary Models",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and r < 1 such that\n                                              \u03c1(k) < D|r|k",
        "start": 274,
        "end": 358
      }
    ]
  },
  {
    "content": "0.010\n        forecast\n\n\n\n\n                                          *      *    *      *    *    *    *    *\n                                    *\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.010",
      "section_title": "forecast",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                           *    *\n\n\n                           0        2            4           6         8        10\n                                                      h\n\n\n                                                     \u0394ip\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "*    *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n        forecast\n\n\n\n\n                           o*   *   *     *      *    *      *    *    *    *    *\n                                o   o    o       o\n                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "forecast",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n\n\n\n\n                                                      o      o    o    o    o   o\n\n                           0        2            4           6         8        10\n                                                      h\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "o      o    o    o    o   o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.16. Forecast limits (dashed) for \u0394 cpi and \u0394 ip computed by simulation,\nand forecasts (solid). At h = 0, the last observed changes are plotted so the widths\nof the forecast intervals are zero.\n\f390    13 Time Series Models: Further Topics\n\nfor all k. In contrast, many \ufb01nancial time series appear to have long mem-\nory since their ACFs decay at a (slow) polynomial rate rather than a (fast)\ngeometric rate, that is,\n                                 \u03c1(k) \u223c Dk \u2212\u03b1\nfor some D and \u03b1 > 0. A polynomial rate of decay is sometimes called a hyp-\nerbolic rate. In this section, we will introduce the fractional ARIMA models,\nwhich include stationary processes with long memory.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.16",
      "section_title": "Forecast limits (dashed) for \u0394 cpi and \u0394 ip computed by simulation,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0. A polynomial rate of decay is sometimes called a hyp-\nerbolic rate. In this section, we will introduce the fractional ARIMA models,\nwhich include stationary processes with long memory.",
        "start": 482,
        "end": 673
      }
    ]
  },
  {
    "content": "13.5.2 Fractional Di\ufb00erencing\n\nThe most widely used models for stationary, long-memory processes use frac-\ntional di\ufb00erencing. For integer values of d we have\n                                            d   \u0007 \b\n                                                 d\n                     \u0394d = (1 \u2212 B)d =                (\u2212B)k .            (13.22)\n                                                 k\n                                          k=0\n\nIn this subsection, the de\ufb01nition of \u0394d will be extended to noninteger values\nof d. The only restriction on d will be that d > \u22121.\n    We de\ufb01ne\n                        \u0007 \b\n                          d     d(d \u2212 1) \u00b7 \u00b7 \u00b7 (d \u2212 k + 1)\n                              =                                       (13.23)\n                          k                 k!\nfor any d except negative integers and any integer k \u2265 0, except if d is an\ninteger and k > d, in which case d \u2212 k\u0007is a\bnegative integer\n                                                         \u0007 \band (d \u2212 k)! is not\n                                        d                 d\nde\ufb01ned. In the latter case, we de\ufb01ne         to be 0, so      is de\ufb01ned for all\n                                        k                 k\nd except negative integers and for all integer k \u2265 0. Only values of d greater\nthan \u22121 are needed for modeling long-memory processes, so we will restrict\nattention to this case.\n    The function f (x) = (1 \u2212 x)d has an in\ufb01nite Taylor series expansion\n                                      \u221e     \u0007 \b\n                                             d\n                         (1 \u2212 x)d =             (\u2212x)k .                (13.24)\n                                             k\n                                      k=0\n      \u0007 \b\n       d\nSince     = 0 if k > d and d > \u22121 is an integer, when d is an integer we\n       k\nhave\n                        \u221e \u0007 \b             d \u0007 \b\n                             d                 d\n             (1 \u2212 x)d =         (\u2212x)k =           (\u2212x)k .         (13.25)\n                             k                 k\n                           k=0                    k=0\n\nThe right-hand side of (13.25) is the usual \ufb01nite binomial expansion for d a\nnonnegative integer, so (13.24) extends the binomial expansion to all d > \u22121.\nSince (1 \u2212 x)d is de\ufb01ned for all d > \u22121, we can de\ufb01ne \u0394d = (1 \u2212 B)d for any\nd > \u22121. In summary, if d > \u22121, then\n\f                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "2 Fractional Di\ufb00erencing",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "\u22121.\n    We de\ufb01ne\n                        \u0007 \b\n                          d     d(d \u2212 1) \u00b7 \u00b7 \u00b7 (d \u2212 k + 1)\n                              =                                       (13.23)\n                          k                 k!\nfor any d except negative integers and any integer k \u2265 0, except if d is an\ninteger and k > d, in which case d \u2212 k\u0007is a\bnegative integer\n                                                         \u0007 \band (d \u2212 k)! is not\n                                        d                 d\nde\ufb01ned. In the latter case, we de\ufb01ne         to be 0, so      is de\ufb01ned for all\n                                        k                 k\nd except negative integers and for all integer k \u2265 0. Only values of d greater\nthan \u22121 are needed for modeling long-memory processes, so we will restrict\nattention to this case.\n    The function f (x) = (1 \u2212 x)d has an in\ufb01nite Taylor series expansion\n                                      \u221e     \u0007 \b\n                                             d\n                         (1 \u2212 x)d =             (\u2212x)k .                (13.24)\n                                             k\n                                      k=0\n      \u0007 \b\n       d\nSince     = 0 if k > d and d > \u22121 is an integer, when d is an integer we\n       k\nhave\n                        \u221e \u0007 \b             d \u0007 \b\n                             d                 d\n             (1 \u2212 x)d =         (\u2212x)k =           (\u2212x)k .         (13.25)\n                             k                 k\n                           k=0                    k=0",
        "start": 564,
        "end": 2107
      },
      {
        "language": "r",
        "code": "\u22121.\nSince (1 \u2212 x)d is de\ufb01ned for all d > \u22121, we can de\ufb01ne \u0394d = (1 \u2212 B)d for any\nd > \u22121. In summary, if d > \u22121, then",
        "start": 2256,
        "end": 2422
      }
    ]
  },
  {
    "content": "13.5 Long-Memory Processes         391\n                                    \u221e     \u0007 \b\n                             d             d\n                          \u0394 Yt =              (\u22121)k Yt\u2212k .                    (13.26)\n                                           k\n                                    k=0\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Long-Memory Processes         391",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5.3 FARIMA Processes\n\nA process Yt is a fractional ARIMA(p, d, q) process, also called an ARFIMA or\nFARIMA(p, d, q) process, if \u0394d Yt is an ARMA(p, q) process. We say that Yt is\na fractionally integrated process of order d or, simply, I(d) process. This is, of\ncourse, the previous de\ufb01nition of an ARIMA process extended to noninteger\nvalues of d. Usually, d \u2265 0, with d = 0 being the ordinary ARMA case, but\nd could be negative. If \u22121/2 < d < 1/2, then the process is stationary. If\n0 < d < 1/2, then it is a long-memory stationary process.\n    If d > 12 , then Yt can be di\ufb00erenced an integer number of times to become\na stationary process, though perhaps with long-memory. For example, if 12 <\nd < 1 12 , then \u0394Yt is fractionally integrated of order d \u2212 1 \u2208 (\u2212 12 , 12 ) and \u0394Yt .\nhas long-memory if 1 < d < 1 12 so that d \u2212 1 \u2208 (0, 12 ).\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "3 FARIMA Processes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "12 , then Yt can be di\ufb00erenced an integer number of times to become\na stationary process, though perhaps with long-memory. For example, if 12 <\nd < 1 12 , then \u0394Yt is fractionally integrated of order d \u2212 1 \u2208 (\u2212 12 , 12 ) and \u0394Yt .\nhas long-memory if 1 < d < 1 12 so that d \u2212 1 \u2208 (0, 12 ).\n    Figure",
        "start": 554,
        "end": 856
      }
    ]
  },
  {
    "content": "13.17 shows time series plots and sample ACFs for simulated\nFARIMA(0, d, 0) processes with n = 2,500 and d = \u22120.35, 0.35, and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.17",
      "section_title": "shows time series plots and sample ACFs for simulated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7.\nThe last case is nonstationary. The R function simARMA0() in the longmemo\npackage was used to simulate the stationary series. For the case d = 0.7,\nsimARMA0() was used to simulate a FARIMA(0, \u22120.3, 0) series and this was\nintegrated to create a FARIMA(0, d, 0) with d = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "The last case is nonstationary. The R function simARMA0() in the longmemo",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 + 1 = 0.7. As explained\nin Sect. 12.9, integration is implemented by taking partial sums, and this was\ndone with R\u2019s function cumsum().\n    The FARIMA(0, 0.35, 0) process has a sample ACF which drops below ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "+ 1 = 0.7. As explained",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\nalmost immediately but then persists well beyond 30 lags. This behavior is\ntypical of stationary processes with long memory. A short-memory stationary\nprocess would not have autocorrelations persisting that long, and a nonsta-\ntionary processes would not have a sample ACF that dropped below ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "almost immediately but then persists well beyond 30 lags. This behavior is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 so\nquickly.\n    Note that the case d = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "so",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35 in Fig. 13.17 has an ACF with a negative\nlag-1 autocorrelation and little additional autocorrelation. This type of ACF\nis often found when a time series is di\ufb00erenced once. After di\ufb00erencing, an\nMA term is needed to accommodate the negative lag-1 autocorrelated. A more\nparsimonious model can sometimes be used if the di\ufb00erencing is fractional.\nFor example, consider the third series in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "in Fig. 13.17 has an ACF with a negative",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.17. If it is di\ufb00erenced once,\nthen a series with d = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.17",
      "section_title": "If it is di\ufb00erenced once,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 is the result. However, if it is di\ufb00erenced with\nd = 0.7, then white noise is the result. This can be seen in the ACF plots in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "is the result. However, if it is di\ufb00erenced with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.18.\n\nExample 13.12. In\ufb02ation rates\u2014FARIMA modeling\n\n   This example uses the in\ufb02ation rates that have been studied already in\nChap. 12. From the analysis in that chapter it was unclear whether to model\nthe series as I(0) or I(1). Perhaps it would be better to have a compromise\n\f392              13 Time Series Models: Further Topics\n\n                                        d = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.18",
      "section_title": "Example 13.12. In\ufb02ation rates\u2014FARIMA modeling",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35                                             d = \u22120.35\n\n\n\n\n                                                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "d = \u22120.35",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.4 1.0\n      \u22122 0 2\n\n\n\n\n                                                            ACF\nx\n\n\n\n\n                     0         500 1000              2000                        0   5   10    15     20   25   30\n                                          Time                                                  Lag\n\n                                        d = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35                                              d = 0.35\n      0 2 4\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "d = 0.35",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.4 0.8\n                                                            ACF\nx\n      \u22124\n\n\n\n\n                     0         500 1000              2000                        0   5   10    15     20   25   30\n                                          Time                                                  Lag\n\n                                         d = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.4 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7                  0.0 0.4 0.8                  d = 0.7\n      \u22125 0\n\n\n\n\n                                                            ACF\nx\n      \u221215\n\n\n\n\n                     0         500 1000              2000                        0   5   10    15     20   25   30\n                                         Index                                                  Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "0.0 0.4 0.8                  d = 0.7",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.17. Time series plots (left) and sample ACFs (right) for simulated\nFARIMA(0, d, 0): the top series is stationary with short-term memory; the middle\nseries is stationary with long-term memory; the bottom series is nonstationary.\n\n                                             \u03940.7Y                                            \u0394Y\n                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.17",
      "section_title": "Time series plots (left) and sample ACFs (right) for simulated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                         0.8\n               ACF\n\n\n\n\n                                                            ACF\n                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                         0.4\n\n\n\n\n                                                                      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                         0.0\n\n\n\n\n                                0   5        15      25                          0   5    15          25\n                                             Lag                                              Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.18. Sample ACF plots for the simulated FARIMA(0, 0.7, 0) series in Figure\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.18",
      "section_title": "Sample ACF plots for the simulated FARIMA(0, 0.7, 0) series in Figure",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.17 after di\ufb00erencing using d = 0.7 and 1.\n\n\nbetween these alternatives. Now, with the new tool of fractional integration,\nwe can try di\ufb00erencing with d between 0 and 1. There is some reason to believe\nthat fractional di\ufb00erencing is suitable for this example, since the ACF plot in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.17",
      "section_title": "after di\ufb00erencing using d = 0.7 and 1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.3 is similar to that of the d = 0.35 plot in Fig. 13.17.\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.3",
      "section_title": "is similar to that of the d = 0.35 plot in Fig. 13.17.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.5 Long-Memory Processes               393\n\n    The function fracdiff() in R\u2019s fracdiff package will \ufb01t a FARIMA\n(p, d, q) process. The values of p, d, and q must be input; we are not aware of\nany R function that will chose p, d, and q automatically in the way this can be\ndone for an ARIMA process (that is, with d restricted to be an integer) using\nauto.arima(). First, a trial value of d was chosen by using fracdiff() with\np = q = 0, the default values. The estimate was d\u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.5",
      "section_title": "Long-Memory Processes               393",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.378. Then, the in\ufb02ation\nrates were fractionally di\ufb00erenced using this value of d and auto.arima() was\napplied to the fractionally di\ufb00erenced series. The result was that BIC selected\np = q = d = 0. The value d = 0 means that no further di\ufb00erencing is applied\nto the already fractionally di\ufb00erenced series. Fractional di\ufb00erencing was done\nwith the diffseries() function in R\u2019s fracdiff package.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.378",
      "section_title": "Then, the in\ufb02ation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.19 has sample ACF plots of the original series and the series\ndi\ufb00erenced with d = 0, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.19",
      "section_title": "has sample ACF plots of the original series and the series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 (from rounding 0.378), and 1. The \ufb01rst series has\na slowly decaying ACF typical of a long-memory process, the second series\nlooks like white noise, and the third series has negative autocorrelation at\nlag-1 which indicates overdi\ufb00erencing.\n\n                               d=0                                                   d=0\n         20\n\n\n\n\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "(from rounding 0.378), and 1. The \ufb01rst series has",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n         5 10\n\n\n\n\n                                                       ACF\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "5 10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\ny\n         \u22125 0\n\n\n\n\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "y",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                 1950   1960   1970      1980   1990                    0   5   10          15   20   25\n                                Time                                                  Lag\n\n\n                               d = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1950   1960   1970      1980   1990                    0   5   10          15   20   25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4                                               d = 0.4\n         15\n\n\n\n\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "d = 0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\nfdiffy\n\n\n\n\n                                                       ACF\n         0 5\n\n\n\n\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "fdiffy",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                             0.0\n         \u221210\n\n\n\n\n                 1950   1960   1970      1980   1990                    0   5   10          15   20   25\n                                Time                                                  Lag\n\n\n                               d=1                                                   d=1\n         10 20\n\n\n\n\n                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                                                             0.4\n                                                       ACF\ndiffy\n         0\n\n\n\n\n                                                             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 0.0\n         \u221220\n\n\n\n\n                 1950   1960   1970      1980   1990                    0   5   10          15   20   25\n                                Time                                                  Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.19. Time series plots (left) and sample ACF plots (right) for the in\ufb02ation\nrates series with di\ufb00erencing using d = 0, 0.4, and 1.\n\f394    13 Time Series Models: Further Topics\n\n    The conclusion is that a white noise process seems to be a suitable model\nfor the fractionally di\ufb00erenced series and the original series can be model as\nFARIMA(0,0.378,0), or, perhaps, more simply as FARIMA(0,0.4,0).\n    Di\ufb00erencing a stationary process creates another stationary process, but\nthe di\ufb00erenced process often has a more complex autocorrelation structure\nthan the original process. Therefore, one should not overdi\ufb00erence a time\nseries. However, if d is restricted to integer values, then often, as in this ex-\nample, overdi\ufb00erencing cannot be avoided.                                      \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.19",
      "section_title": "Time series plots (left) and sample ACF plots (right) for the in\ufb02ation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.6 Bootstrapping Time Series\n\n\nThe resampling methods introduced in Chap. 6 are designed for i.i.d. uni-\nvariate data but are easily extended to multivariate data. As discussed in\nSect. 7.11, if Y 1 , . . . , Y n is a sample of vectors, then one resamples the Y i\nthemselves, not their components, to maintain the covariance structure of the\ndata in the resamples.\n      It is not immediately obvious whether one can resample a time series\nY1 , . . . , Yn . A time series is essentially a sample of size 1 from a stochastic\nprocess. Resampling a sample of size 1 in the usual way is a futile exercise\u2014\neach resample is the original sample, so one learns nothing by resampling.\nTherefore, resampling of a time series requires new ideas.\n      Model-based resampling is easily adapted to time series. The resamples\nare obtained by simulating the time series model. For example, if the model\nis ARIMA(p, 1, q), then the resamples start with simulated samples of an\nARMA(p, q) model with MLEs (from the di\ufb00erenced series) of the autoregres-\nsive and moving average coe\ufb03cients and the noise variance. The resamples are\nthe sequences of partial sums of the simulated ARMA(p, q) process.\n      Model-free resampling of a time series is accomplished by block resampling,\nalso called the block bootstrap, which can be implemented using the tsboot()\nfunction in R\u2019s boot package. The idea is to break the time series into roughly\nequal-length blocks of consecutive observations, to resample the blocks with\nreplacement, and then to paste the blocks together. For example, if the time\nseries is of length 200 and one uses 10 blocks of length 20, then the blocks\nare the \ufb01rst 20 observations, the next 20, and so forth. A possible resample\nis the fourth block (observations 61 to 80), then the last block (observations\n181 to 200), then the second block (observations 21 to 40), then the fourth\nblock again, and so on until there are 10 blocks in the resample.\n      A major issue is how best to select the block length. The correlations in\nthe original sample are preserved only within blocks, so a large block size is\ndesirable. However, the number of possible resamples depends on the number\nof blocks, so a large number of blocks is also desirable. Obviously, there must\nbe a tradeo\ufb00 between the block size and the number of blocks. A full discussion\nof block bootstrapping is beyond the scope of this book, but see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.6",
      "section_title": "Bootstrapping Time Series",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.7 for\nfurther reading.\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.7",
      "section_title": "for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8 R Lab    395\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "R Lab    395",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.7 Bibliographic Notes\nBeran (1994) is a standard reference for long-memory processes, and Beran\n(1992) is a good introduction to this topic. Most of the time series text-\nbooks listed in the \u201cReferences\u201d section discuss seasonal ARIMA models. For\nmore details on HC and HAC covariance matrix estimators and the R package\nsandwich see Zeileis (2004). Enders (2004) has a section on bootstrapping\ntime series and a chapter on multivariate time series. Reinsel (2003) is an\nin-depth treatment of multivariate time series; see also Hamilton (1994) for\nthis topic. Transfer function models are another method for analyzing multi-\nvariate time series; see Box, Jenkins, and Reinsel (2008). Davison and Hinkley\n(1997) discuss both model-based and block resampling of time series and other\ntypes of dependent data. Lahiri (2003) provides an advanced and comprehen-\nsive account of block resampling. Bu\u0308hlmann (2002) is a review article about\nbootstrapping time series.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.7",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.1 Seasonal ARIMA Models\nThis section uses seasonally non-adjusted quarterly data on income and con-\nsumption in the UK. Run the following code to load the data and plot the\nvariable consumption.\n1 library(\"Ecdat\")\n2 library(\"forecast\")\n3 data(IncomeUK)\n\n4 consumption = IncomeUK[,2]\n\n5 plot(consumption)\n\n\n\nProblem 1 Describe the behavior of consumption. What types of di\ufb00erenc-\ning, seasonal, nonseasonal, or both, would you recommend? Do you recom-\nmend \ufb01tting a seasonal ARIMA model to the data with or without a log trans-\nformation? Consider also using ACF plots to help answer these questions.\n\n\nProblem 2 Regardless of your answers to Problem 1, \ufb01nd an ARIMA model\nthat provides a good \ufb01t to log(consumption). What order model did you\nselect? (Give the orders of the nonseasonal and seasonal components.)\n\n\nProblem 3 Check the ACF of the residuals from the model you selected in\nProblem 2. Do you see any residual autocorrelation?\n\n\nProblem 4 Apply auto.arima() to log(consumption) using BIC. Which\nmodel is selected?\n\f396    13 Time Series Models: Further Topics\n\nProblem 5 Forecast log(consumption) for the next eight quarters using the\nmodels you found in Problems 2 and 4. Plot the two sets of forecasts in side-by-\nside plots with the same limits on the x- and y-axes. Describe any di\ufb00erences\nbetween the two sets of forecasts.\nNote: To predict an arima object (an object returned by the arima()\nfunction), use the predict function. To learn how the predict() function works\non an arima object, use ?predict.Arima. To forecast an object returned by\nauto.arima(), use the forecast() function in the forecast package. For\nexample, the following code will forecast eight quarters ahead using the object\nreturned by auto.arima() and then plot the forecasts.\n6 logConsumption = log(consumption)\n7 fitAutoArima = auto.arima(logConsumption, ic=\"bic\")\n8 foreAutoArima = forecast(fitAutoArima, h=8)\n\n9 plot(foreAutoArima, xlim=c(1985.5,1987.5), ylim=c(10.7,11.2))\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "1 Seasonal ARIMA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.2 Regression with HAC Standard Errors\n\nRun the following commands in R to compute the OLS estimates of the regres-\nsion of the di\ufb00erenced one-month T-bill rates, tb1 diff, on the di\ufb00erenced\nthree-month T-bill rates, tb3 diff.\n1 data(Mishkin, package=\"Ecdat\")\n2 tb1_dif = diff(as.vector(Mishkin[,3]))\n3 tb3_dif = diff(as.vector(Mishkin[,4]))\n\n4 fit = lm(tb1_dif ~ tb3_dif )\n\n5 round(summary(fit)$coef, 4)\n\n6 acf(fit$resid)\n\n\n\nProblem 6 Is there evidence of signi\ufb01cant autocorrelation among the resid-\nuals? Why?\n\n\n  Now run the following commands to compute the HC standard error esti-\nmates and their associated t values.\n7 library(sandwich)\n8 sqrt(diag(NeweyWest(fit, lag = 0, prewhite = F)))\n9 coef(fit)/sqrt(diag(NeweyWest(fit, lag = 0, prewhite = F)))\n\n\n\nProblem 7 How do these t values compare to the t values from the OLS \ufb01t?\nDoes the HC adjustment change the conclusions of the hypothesis tests?\n\n\nProblem 8 Run the commands again, but with lag equal to 1,2, and 3 to\nobtain the corresponding HAC t values. How do the t values vary with lag?\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "2 Regression with HAC Standard Errors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8 R Lab     397\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "R Lab     397",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.3 Regression with ARMA Noise\n\nThis section uses the USMacroG data set used earlier in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "3 Regression with ARMA Noise",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.11.1. In the\nearlier analysis, we did not investigate residual correlation, but now we will.\nThe model will be the regression of changes in unemp = unemployment rate\non changes in government = real government expenditures and changes in\ninvest = real investment by the private sector. Run the following R code to\nread the data, compute di\ufb00erences, and then \ufb01t a linear regression model with\nAR(1) errors.\n1 library(AER)\n2 data(\"USMacroG\")\n3 MacroDiff = as.data.frame(apply(USMacroG, 2, diff))\n\n4 attach(MacroDiff)\n\n5 fit1 = arima(unemp, order=c(1,0,0), xreg=cbind(invest, government))\n\n\n\nProblem 9 Fit a linear regression model using lm(), which assumes uncor-\nrelated errors. Compare the two models by AIC and residual ACF plots. Which\nmodel \ufb01ts better?\n\n\nProblem 10 What are the values of BIC for the model with uncorrelated\nerrors and for the model with AR(1) errors? Does the conclusion in Problem 9\nabout which model \ufb01ts better change if one uses BIC instead of AIC?\n\n\nProblem 11 Does the model with AR(2) noise or the model with ARMA(1,1)\nnoise o\ufb00er a better \ufb01t than the model with AR(1) noise?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.11",
      "section_title": "1. In the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.4 VAR Models\n\nThis section uses data on the 91-day Treasury bill, the real GDP, and the in\ufb02a-\ntion rate. Run the following R code to read the data, \ufb01nd the best-\ufb01tting mul-\ntivariate AR to changes in the three series, and check the residual correlations.\n1 TbGdpPi = read.csv(\"TbGdpPi.csv\", header=TRUE)\n2 # r = the 91-day treasury bill rate\n3 #  y = the log of real GDP\n4 #  pi = the inflation rate\n5 TbGdpPi = ts(TbGdpPi, start = 1955, freq = 4)\n\n6 del_dat = diff(TbGdpPi)\n\n7 var1 = ar(del_dat, order.max=4, aic=T)\n\n8 var1\n\n9 acf(na.omit(var1$resid))\n\f398      13 Time Series Models: Further Topics\n\nProblem 12 For this problem, use the notation of Eq. (13.16) with q = 0.\n(a) What is p and what are the estimates \u03a61 , . . . , \u03a6p ?\n(b) What is the estimated covariance matrix of t ?\n(c) If the model \ufb01ts adequately, then there should be no residual auto- or\n    cross-correlation. Do you believe that the model does \ufb01t adequately?\n\n\nProblem 13 The last three changes in r, y, and pi are given next. What are\nthe predicted values of the next set of changes in these series?\n10   tail(TbGdpPi, n = 4)\n\n                r   y    pi\n      [233,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "4 VAR Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07 9.7 1.38\n      [234,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "9.7 1.38",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 9.7 0.31\n      [235,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "9.7 0.31",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02 9.7 0.28\n      [236,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "9.7 0.28",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07 9.7 -0.47\n\n\n      Now \ufb01t a VAR(1) using the following commands.\n11   var1 = ar(del_dat, order.max=1)\n\n  Suppose we observe changes in r, y, and pi that are each 10 % above the\nmean changes:\n12   yn = var1$x.mean * ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "9.7 -0.47",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1 ; yn\n\nProblem 14 Compute the h-step forecasts for h = 1, 2, and 5 using yn as\nthe most recent observation. How do these forecasts compare to the mean\nvar1$x.mean? For each h, compute ratios between the forecasts and the mean.\nHow do these values compare to the starting value, yn/var1$x.mean = 1.1?\nAre they closer to or farther from ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.1",
      "section_title": "; yn",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 = var1$x.mean/var1$x.mean? What\ndoes this suggest?\n      Using the \ufb01tted VAR(1) from above, examine the estimate of \u03a6\u0302:\n13   Phi_hat = var1$ar[1,,] ; Phi_hat\n\nProblem 15 What do the elements of Phi hat suggest about the relationships\namong the changes in r, y, and pi?\n   A VAR(1) process is stationary provided that the eigenvalues of \u03a6 are less\nthan one in magnitude. Compute the eigenvalues of \u03a6\u0302:\n14   eigen.values = eigen(Phi_hat)$values\n15   abs(eigen.values)\n\nProblem 16 Is the estimated process stationary? How does this result relate\nto the forecast calculations in Problem 14 above?\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "= var1$x.mean/var1$x.mean? What",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8 R Lab    399\n   The dataset MacroVars.csv contains three US macroeconomic indicators\nfrom Quarter 1 of 1959 to Quarter 4 of 1997: Real Gross Domestic Product (a\nmeasure of economic activity), Consumer Price Index (a measure of in\ufb02ation),\nand Federal Funds Rate (a proxy for monetary policy). Each series has been\ntransformed to stationary based on the procedures suggested by Stock and\nWatson (2005).\n16   MacroVars = read.csv(\"MacroVars.csv\", head=TRUE)\n\nProblem 17 Fit a V AR(p) model using the ar() function in R using AIC\n(the default) to select lag order.\n\n\nProblem 18\nBy modifying the output of the ar() function as discussed in Example 13.10,\nuse BIC to select the lag order. Comment on any di\ufb00erences.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "R Lab    399",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.5 Long-Memory Processes\n\nThis section uses changes in the square root of the Consumer Price Index.\nThe following code creates this time series.\n1 data(Mishkin, package=\"Ecdat\")\n2 cpi = as.vector(Mishkin[,5])\n3 DiffSqrtCpi = diff(sqrt(cpi))\n\n\n\nProblem 19 Plot DiffSqrtCpi and its ACF. Do you see any signs of long\nmemory? If so, describe them.\nRun the following code to estimate the amount of fractional di\ufb00erencing,\nfractionally di\ufb00erence DiffSqrtCpi appropriately, and check the ACF of the\nfractionally di\ufb00erenced series.\n4 library(\"fracdiff\")\n5 fit.frac = fracdiff(DiffSqrtCpi,nar=0,nma=0)\n6 fit.frac$d\n\n7 fdiff = diffseries(DiffSqrtCpi,fit.frac$d)\n\n8 acf(fdiff)\n\n\n\nProblem 20 Do you see any short- or long-term autocorrelation in the frac-\ntionally di\ufb00erenced series?\n\n\nProblem 21 Fit an ARIMA model to the fractionally di\ufb00erenced series using\nauto.arima(). Compare the models selected using AIC and BIC.\n\f400      13 Time Series Models: Further Topics\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "5 Long-Memory Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.8.6 Model-Based Bootstrapping of an ARIMA Process\n\nThis exercise uses the price of frozen orange juice. Run the following code to\n\ufb01t an ARIMA model.\n1 library(AER)\n2 library(forecast)\n3 data(\"FrozenJuice\")\n\n4 price = FrozenJuice[,1]\n\n5 plot(price)\n\n6 auto.arima(price, ic=\"bic\")\n\n\n\nThe output from auto.arima(), which is needed for model-based bootstrap-\nping, is\n      Series: price\n      ARIMA(2,1,0)\n\n      Coefficients:\n               ar1    ar2\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "6 Model-Based Bootstrapping of an ARIMA Process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2825 0.0570\n      s.e. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2825",
      "section_title": "0.0570",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0407 0.0408\n\n      sigma^2 estimated as 9.989: log likelihood = -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0407",
      "section_title": "0.0408",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1570.11\n      AIC = 3146.23   AICc = 3146.27  BIC = 3159.47\n\nNext, we will use the model-based bootstrap to investigate how well BIC\nselects the \u201ccorrect\u201d model, which is ARIMA(2,1,0). Since we will be looking\nat the output of each \ufb01tted model, only a small number of resamples will be\nused. Despite the small number of resamples, we will get some sense of how\nwell BIC works in this context. To simulate 10 model-based resamples from\nthe ARIMA(2,1,0) model, run the following commands.\n7  n = length(price)\n8  sink(\"priceBootstrap.txt\")\n 9 set.seed(1998852)\n\n10 for (iter in 1:10){\n\n11   eps = rnorm(n+20)\n12   y = rep(0,n+20)\n13   for (t in 3:(n+20)){\n14     y[t] = 0.2825*y[t-1] + 0.0570*y[t-2] + eps[t]\n15   }\n16   y = y[101:n+20]\n17   y = cumsum(y)\n18   y = ts(y, frequency=12)\n19   fit = auto.arima(y, d=1, D=0, ic=\"bic\")\n20   print(fit)\n21 }\n\n22 sink()\n\f                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1570.11",
      "section_title": "AIC = 3146.23   AICc = 3146.27  BIC = 3159.47",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9 Exercises    401\n\nThe results will be sent to the \ufb01le priceBootstrap.txt. The \ufb01rst two values\nof y are independent and are used to initialize the process. A burn-in period\nof 20 is used to remove the e\ufb00ect of initialization. Note the use of cumsum()\nto integrate the simulated AR(2) process and the use of ts() to convert a\nvector to a monthly time series.\n\nProblem 22 How often is the \u201ccorrect\u201d AR(2) model selected?\nNow we will perform a bootstrap where the correct model AR(2) is known and\nstudy the accuracy of the estimators. Since the correct model is known, it can\nbe \ufb01t by arima(). The estimates will be stored in a matrix called estimates.\nIn contrast to earlier when model-selection was investigated by resampling,\nnow a large number of bootstrap samples can be used, since arima() is fast\nand only the estimates are stored. Run the following:\n23 set.seed(1998852)\n24 niter = 1000\n25 estimates=matrix(0, nrow=niter, ncol=2)\n\n26 for (iter in 1:niter){\n\n27   eps = rnorm(n+20)\n28   y = rep(0, n+20)\n29   for (t in 3:(n+20)){\n30     y[t] = .2825 *y[t-1] + 0.0570*y[t-2] + eps[t]\n31   }\n32   y = y[101:n+20]\n33   y = cumsum(y)\n34   y = ts(y, frequency=12)\n35   fit=arima(y, order=c(2,1,0))\n36   estimates[iter,] = fit$coef\n37 }\n\n\n\nProblem 23 Find the biases, standard deviations, and MSEs of the estima-\ntors of the two coe\ufb03cients.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "Exercises    401",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.9 Exercises\n\n 1. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.9",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.20 contains ACF plots of 40 years of quarterly data, with all\n    possible combinations of \ufb01rst-order seasonal and nonseasonal di\ufb00erencing.\n    Which combination do you recommend in order to achieve stationarity?\n 2. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.20",
      "section_title": "contains ACF plots of 40 years of quarterly data, with all",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.21 contains ACF plots of 40 years of quarterly data, with all\n    possible combinations of \ufb01rst-order seasonal and nonseasonal di\ufb00erencing.\n    Which combination do you recommend in order to achieve stationarity?\n\f402          13 Time Series Models: Further Topics\n\na                none        b              non\u2212seasonal    c                  seasonal     d                  both\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.21",
      "section_title": "contains ACF plots of 40 years of quarterly data, with all",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                     1.0\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                    1.0\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                    0.6\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                     0.6\nACF\n\n\n\n\n                              ACF\n\n\n\n\n                                                             ACF\n\n\n\n\n                                                                                             ACF\n                                                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                    0.2\n                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n\n\n\n\n                                                                                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                    \u22120.2\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      \u22120.2\n\n\n\n\n             0   10     20                   0   10    20                  0     10    20                  0    10    20\n                 Lag                             Lag                             Lag                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.20. ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-\nencing, seasonal di\ufb00erencing, and both seasonal and nonseasonal di\ufb00erencing.\n\na                none        b             non\u2212seasonal     c              seasonal         d                  both\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.20",
      "section_title": "ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                    1.0\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                   1.0\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                    0.5\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n      0.6\nACF\n\n\n\n\n                             ACF\n\n\n\n\n                                                            ACF\n\n\n\n\n                                                                                            ACF\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                    0.0\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n\n\n\n\n                                                                                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                    \u22120.5\n\n\n\n\n                                                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      \u22120.2\n\n\n\n\n             0    10    20                   0   10    20                  0     10    20                  0   10     20\n                  Lag                            Lag                             Lag                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.21. ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-\nencing, seasonal di\ufb00erencing, and both seasonal and nonseasonal di\ufb00erencing.\n\na                none        b             non\u2212seasonal     c                  seasonal     d                  both\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.21",
      "section_title": "ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                    1.0\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                                                   1.0\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                                                    0.6\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                    0.6\nACF\n\n\n\n\n                             ACF\n\n\n\n\n                                                             ACF\n\n\n\n\n                                                                                            ACF\n                                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                                                    0.2\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n\n\n\n\n                                                                                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                    \u22120.2\n                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      \u22120.2\n\n\n\n\n             0    10    20                   0   10    20                  0      10   20                  0    10    20\n                  Lag                            Lag                             Lag                           Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.22. ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-\nencing, seasonal di\ufb00erencing, and both seasonal and nonseasonal di\ufb00erencing.\n\n\n3. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.22",
      "section_title": "ACF plots of quarterly data with no di\ufb00erencing, nonseasonal di\ufb00er-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.22 contains ACF plots of 40 years of quarterly data, with all\n   possible combinations of \ufb01rst-order seasonal and nonseasonal di\ufb00erencing.\n   Which combination do you recommend in order to achieve stationarity?\n4. In Example 13.10, a bivariate AR(1) model was \ufb01t to (\u0394cpi, \u0394ip)\u0003 and\n                                \u0007                 \b\n                           \u0002       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.22",
      "section_title": "contains ACF plots of 40 years of quarterly data, with all",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.767 0.0112\n                          \u03a6=                        .\n                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.767",
      "section_title": "0.0112",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.330 0.3014\n\f                                                              References    403\n\n    The mean of (\u0394cpi, \u0394ip)\u0003 is (0.0052, 0.0021)\u0003 and the last observation\n    of (\u0394cpi, \u0394ip)\u0003 is (0.0017, 0.0059)\u0003 . Forecast the next two values of \u0394ip.\n    (The forecasts are shown in Fig. 13.15, but you should compute numerical\n    values.)\n 5. Fit an ARIMA model to income, which is in the \ufb01rst column of the\n    IncomeUK data set in the Ecdat package. Explain why you selected the\n    model you did. Does your model exhibit any residual correlation?\n 6. (a) Find an ARIMA model that provides a good \ufb01t to the variable unemp\n        in the USMacroG data set in the AER package.\n    (b) Now perform a small model-based bootstrap to see how well auto.\n        arima() can select the true model. To do this, simulate eight data sets\n        from the ARIMA model selected in part (a) of this problem. Apply\n        auto.arima() with BIC to each of these data sets. How often is the\n        \u201ccorrect\u201d amount of di\ufb00erencing selected, that is, d and D are correctly\n        selected? How often is the \u201ccorrect\u201d model selected? \u201cCorrect\u201d means\n        in agreement with the simulation model. \u201cCorrect model\u201d means both\n        the correct amount of di\ufb00erencing and the correct orders for all the\n        seasonal and nonseasonal AR and MA components.\n 7. This exercise uses the TbGdpPi.csv data set. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.330",
      "section_title": "0.3014",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.15.1, nonseasonal\n    models were \ufb01t. Now use auto.arima() to \ufb01nd a seasonal model. Which\n    seasonal model is selected by AIC and by BIC? Do you feel that a seasonal\n    model is needed, or is a nonseasonal model su\ufb03cient?\n\n\nReferences\nBeran, J. (1992) Statistical methods for data with long-range dependence.\n  Statistical Science, 7, 404\u2013427.\nBeran, J. (1994) Statistics for Long-Memory Processes, Chapman & Hall,\n  Boca Raton, FL.\nBox, G. E. P., Jenkins, G. M., and Reinsel, G. C. (2008) Times Series Analysis:\n  Forecasting and Control, 4th ed., Wiley, Hoboken, NJ.\nBu\u0308hlmann, P. (2002) Bootstraps for time series. Statistical Science, 17, 52\u201372.\nDavison, A. C. and Hinkley, D. V. (1997) Bootstrap Methods and Their\n  Applications, Cambridge University Press, Cambridge.\nEnders, W. (2004) Applied Econometric Time Series, 2nd ed., Wiley,\n  New York.\nHamilton, J. D. (1994) Time Series Analysis, Princeton University Press,\n  Princeton, NJ.\nLahiri, S. N. (2003) Resampling Methods for Dependent Data, Springer,\n  New York.\nNewey, W. and West, K. (1987) A simple, positive semide\ufb01nite, heteroscedas-\n  ticity and autocorrelation consistent covariance matrix. Econometrica, 55,\n  703\u2013708.\n\f404    13 Time Series Models: Further Topics\n\nReinsel, G. C. (2003) Elements of Multivariate Time Series Analysis, 2nd ed.,\n  Springer, New York.\nStock, J. H. and Watson, M. W. (2005). An empirical comparison of methods\n  for forecasting using many predictors, manuscript http://www4.ncsu.edu/\n  ~arhall/beb_4.pdf\nWhite, H. (1980) A heteroscedasticity consistent covariance matrix estimator\n  and a direct test for heteroscedasticity. Econometrica, 48, 827\u2013838.\nZeileis, A. (2004) Econometric computing with HC and HAC covariance\n  matrix estimators. Journal of Statistical Software, 11(10), 1\u201317.\n\f14\nGARCH Models\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.15",
      "section_title": "1, nonseasonal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.1 Introduction\n\nAs seen in earlier chapters, \ufb01nancial market data often exhibits volatility\nclustering, where time series show periods of high volatility and periods of\nlow volatility; see, for example, Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.1. In fact, with economic and \ufb01nancial\ndata, time-varying volatility is more common than constant volatility, and\naccurate modeling of time-varying volatility is of great importance in \ufb01nancial\nengineering.\n    As we saw in Chap. 12, ARMA models are used to model the conditional\nexpectation of a process given the past, but in an ARMA model the con-\nditional variance given the past is constant. What does this mean for, say,\nmodeling stock returns? Suppose we have noticed that recent daily returns\nhave been unusually volatile. We might expect that tomorrow\u2019s return is also\nmore variable than usual. However, an ARMA model cannot capture this\ntype of behavior because its conditional variance is constant. So we need bet-\nter time series models if we want to model the nonconstant volatility. In this\nchapter we look at GARCH time series models that are becoming widely used\nin econometrics and \ufb01nance because they have randomly varying volatility.\n    ARCH is an acronym meaning Auto-Regressive Conditional Heteroskedas-\nticity. In ARCH models the conditional variance has a structure very similar to\nthe structure of the conditional expectation in an AR model. We \ufb01rst study\nthe \ufb01rst order ARCH(1) model, which is the simplest GARCH model, and\nanalogous to an AR(1) model. Then we look at ARCH(p) models, which are\nanalogous to AR(p) models, and GARCH (Generalized ARCH) models, which\nmodel conditional variances much as the conditional expectation is modeled by\nan ARMA model. Finally, we consider several multivariate GARCH processes.\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                             405\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 14\n\f406                             14 GARCH Models\n\na                                     S&P 500 daily return               b                                    BP/dollar exchange rate\n\n\n\n\n                                                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.1",
      "section_title": "In fact, with economic and \ufb01nancial",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n                        0.08\n\n\n\n\n                                                                          |change in rate|\n|log return|\n\n\n\n\n                                                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "0.08",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n                        0.04\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                                                                                                0.00\n                               1981    1983    1986     1988     1990                                  1980 1981 1983 1984 1986 1987\n                                               year                                                                    year\n\n\nc                                     Risk\u2212free interest rate            d                                         Inflation rate\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                15\n|change in log(rate)|\n\n\n\n\n                                                                          |rate \u2212 mean(rate)|\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                                10\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                5\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                0\n\n\n\n\n                               1960   1970    1980     1990     2000                                   1950    1960   1970     1980     1990\n                                               year                                                                    year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.1. Examples of \ufb01nancial markets and economic data with time-varying\nvolatility: (a) absolute values of S&P 500 log returns; (b) absolute values of changes\nin the BP/dollar exchange rate; (c) absolute values of changes in the log of the risk-\nfree interest rate; (d) absolute deviations of the in\ufb02ation rate from its mean. Loess\n(see Section 21.2) smooths have been added in red.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.1",
      "section_title": "Examples of \ufb01nancial markets and economic data with time-varying",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2 Estimating Conditional Means and Variances\nBefore looking at GARCH models, we study some general principles about\nmodeling nonconstant conditional variance. Consider regression modeling with\na constant conditional variance, Var(Yt | X1,t , . . . , Xp,t ) = \u03c3 2 . Then the gen-\neral form for the regression of Yt on X1,t , . . . , Xp,t is\n\n                                                      Yt = f (X1,t , . . . , Xp,t ) + \u0017t ,                                              (14.1)\n\nwhere \u0017t is independent of X1,t , . . . , Xp,t and has expectation equal to 0 and\na constant conditional variance \u03c3 2 . The function f (\u00b7) is the conditional exp-\nectation of Yt given X1,t , . . . , Xp,t . Moreover, the conditional variance of Yt\nis \u03c3 2 .\n    Equation (14.1) can be modi\ufb01ed to allow conditional heteroskedasticity.\nLet \u03c3 2 (X1,t , . . . , Xp,t ) be the conditional variance of Yt given X1,t , . . . , Xp,t .\nThen the model\n\n                                           Yt = f (X1,t , . . . , Xp,t ) + \u0017t \u03c3(X1,t , . . . , Xp,t ),                                  (14.2)\n\f                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "Estimating Conditional Means and Variances",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.3 ARCH(1) Processes     407\n\nwhere \u0017t has conditional (given X1,t , . . . , Xp,t ) mean equal to 0 and conditional\nvariance equal to 1, gives the correct conditional mean and variance of Yt .\n    The function \u03c3(X1,t , . . . , Xp,t ) should be nonnegative since it is a standard\ndeviation. If the function \u03c3(\u00b7) is linear, then its coe\ufb03cients must be constrained\nto ensure nonnegativity. Such constraints are cumbersome to implement, so\nnonlinear nonnegative functions are usually used instead. Models for con-\nditional variances are often called variance function models. The GARCH\nmodels of this chapter are an important class of variance function models.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.3",
      "section_title": "ARCH(1) Processes     407",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.3 ARCH(1) Processes\nSuppose for now that \u00171 , \u00172 , . . . is Gaussian white noise with unit variance.\nLater we will allow the noise to be i.i.d. white noise with a possibly non-\nnormal distribution, such as, a standardized t-distribution. Then\n\n                                E(\u0017t |\u0017t\u22121 , . . .) = 0,\n\nand\n                               Var(\u0017t |\u0017t\u22121 , . . .) = 1.                      (14.3)\nProperty (14.3) is called conditional homoskedasticity.\n   The process at is an ARCH(1) process under the model\n                                    #\n                             at = \u0017t \u03c9 + \u03b1a2t\u22121 ,                              (14.4)\n                                                                      #\nwhich is a special case of (14.2) with f equal to 0 and \u03c3 equal to     \u03c9 + \u03b1a2t\u22121 .\nWe require that \u03c9 > 0 and \u03b1 \u2265 0 so that \u03c9 + \u03b1a2t\u22121 > 0 for all t. It is also\nrequired that \u03b1 < 1 in order for {at } to be stationary with a \ufb01nite variance.\nEquation (14.4) can be written as\n\n                               a2t = \u00172t (\u03c9 + \u03b1a2t\u22121 ),\n\nwhich is similar to an AR(1), but in a2t , not at , and with multiplicative noise\nwith a mean of 1 rather than additive noise with a mean of 0. In fact, the\nARCH(1) model induces an ACF for a2t that is the same as an AR(1)\u2019s ACF,\nas we will see from the calculations below.\n   De\ufb01ne\n                              \u03c3t2 = Var(at |at\u22121 , . . .)\nto be the conditional variance of at given past values. Since \u0017t is independent\nof at\u22121 and E(\u00172t ) = Var(\u0017t ) = 1, we have\n\n                               E(at |at\u22121 , . . .) = 0,                        (14.5)\n\f408    14 GARCH Models\n\nand\n\n                    \u03c3t2 = E (\u03c9 + \u03b1a2t\u22121 ) \u00172t |at\u22121 , at\u22122 , . . .\n                       = (\u03c9 + \u03b1a2t\u22121 )E \u00172t |at\u22121 , at\u22122 , . . .\n                       = \u03c9 + \u03b1a2t\u22121 .                                     (14.6)\n\n    Equation (14.6) is crucial to understanding how GARCH processes work.\nIf at\u22121 has an unusually large absolute value, then \u03c3t is larger than usual and\nso at is also expected to have an unusually large magnitude. This volatility\n                                                                  2\npropagates since when at has a large magnitude that makes \u03c3t+1       large, then\n                                                                2\nat+1 tends to be large in magnitude, and so on. Similarly, if at\u22121 is unusually\nsmall, then \u03c3t2 is small, and a2t is also expected to be small, and so forth.\nBecause of this behavior, unusual volatility in at tends to persist, though not\nforever. The conditional variance tends to revert to the unconditional variance\nprovided that \u03b1 < 1, so that the process is stationary with a \ufb01nite variance.\n    The unconditional, that is, marginal, variance of at denoted by \u03b3a (0) is\nobtained by taking expectations in (14.6), which gives us\n\n                                \u03b3a (0) = \u03c9 + \u03b1\u03b3a (0)\n\nfor a stationary model. This equation has a positive solution if \u03b1 < 1:\n\n                                \u03b3a (0) = \u03c9/(1 \u2212 \u03b1).\n\nIf \u03b1 = 1, then \u03b3a (0) is in\ufb01nite, but at is stationary nonetheless and is called\nan integrated GARCH (I-GARCH) model.\n    Straightforward calculations using (14.5) show that the ACF of at is\n\n                               \u03c1a (h) = 0 if      h = 0.\n\nIn fact, any process in which the conditional expectation of the present\nobservation given the past is constant is an uncorrelated process.\n    In introductory statistics courses, it is often mentioned that independence\nimplies zero correlation but not vice versa. A process, such as a GARCH\nprocess, in which the conditional mean is constant but the conditional variance\nis nonconstant is an example of an uncorrelated but dependent process. The\ndependence of the conditional variance on the past causes the process to be\ndependent. The independence of the conditional mean on the past is the reason\nthat the process is uncorrelated.\n    Although at is an uncorrelated process, the process a2t has a more inter-\nesting ACF. If \u03b1 < 1, then\n\n                               \u03c1a2 (h) = \u03b1|h| ,     \u2200 h.\n\nIf \u03b1 \u2265 1, then a2t either is nonstationary or has an in\ufb01nite variance, so it does\nnot have an ACF. This geometric decay in the ACF of a2t for an ARCH(1)\n\f                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.3",
      "section_title": "ARCH(1) Processes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and \u03b1 \u2265 0 so that \u03c9 + \u03b1a2t\u22121 > 0 for all t. It is also\nrequired that \u03b1 < 1 in order for {at } to be stationary with a \ufb01nite variance.\nEquation (14.4) can be written as",
        "start": 805,
        "end": 978
      }
    ]
  },
  {
    "content": "14.4 The AR(1)+ARCH(1) Model             409\n\nprocess is analogous to the geometric decay in the ACF of an AR(1) process.\nTo complete the analogy, de\ufb01ne \u03b7t = a2t \u2212 \u03c3t2 , and note that {\u03b7t } is a mean\nzero weak white noise process, but not an i.i.d. white noise process. Adding\n\u03b7t to both sides of (14.6) and simplifying we have\n\n                       \u03c3t2 + \u03b7t = a2t = \u03c9 + \u03b1a2t\u22121 + \u03b7t ,                    (14.7)\n\nwhich is a direct representation of {a2t } as an AR(1) process.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "The AR(1)+ARCH(1) Model             409",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4 The AR(1)+ARCH(1) Model\n\nAs we have seen, an AR(1) process has a nonconstant conditional mean but a\nconstant conditional variance, while an ARCH(1) process is just the opposite.\nIf both the conditional mean and variance of the data depend on the past, then\nwe can combine the two models. In fact, we can combine any ARMA model\nwith any of the GARCH models in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "The AR(1)+ARCH(1) Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.6. In this section we combine an\nAR(1) model with an ARCH(1) model.                   #\n     Let at be an ARCH(1) process so that at = \u0017t           \u03c9 + \u03b1a2t\u22121 , where \u0017t is\ni.i.d. N (0, 1), and suppose that\n\n                          yt \u2212 \u03bc = \u03c6(yt\u22121 \u2212 \u03bc) + at .\n\nThe process yt is an AR(1) process, except that the noise term (at ) is not\ni.i.d. white noise, but rather an ARCH(1) process which is only weak white\nnoise.\n     Because at is an uncorrelated process, it has the same ACF as independent\nwhite noise, and therefore, yt has the same ACF as an AR(1) process with\nindependent white noise\n                               \u03c1y (h) = \u03c6|h| \u2200 h,\nin the stationary case. Moreover, a2t has the ARCH(1) ACF:\n\n                             \u03c1a2 (h) = \u03b1|h|   \u2200 h.\n\nThe ACF of yt2 also decays with |h| at a geometric rate in the stationary case,\nprovided some additional assumptions hold, however, the exact expressions\nare more complicated (see Palma and Zevallos, 2004). We need to assume\nthat both |\u03c6| < 1 and \u03b1 < 1 in order for yt to be stationary with a \ufb01nite\nvariance. Of course, \u03c9 > 0 and \u03b1 \u2265 0 are also assumed for positiveness of the\nconditional variance process \u03c3t2 . The process yt is such that its conditional\nmean and variance, given the past, are both nonconstant, so a wide variety of\ntime series can be modeled.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.6",
      "section_title": "In this section we combine an",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and \u03b1 \u2265 0 are also assumed for positiveness of the\nconditional variance process \u03c3t2 . The process yt is such that its conditional\nmean and variance, given the past, are both nonconstant, so a wide variety of\ntime series can be modeled.",
        "start": 1084,
        "end": 1325
      }
    ]
  },
  {
    "content": "14.1. A simulated ARCH(1) process and AR(1)+ARCH(1) process\n\f410                                      14 GARCH Models\n\n                 A simulated ARCH(1) process is shown in#Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.1",
      "section_title": "A simulated ARCH(1) process and AR(1)+ARCH(1) process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2. Panel (a) shows the\ni.i.d. white noise process \u0017t , (b) shows \u03c3t = 1 + 0.55a2t\u22121 , the conditional\nstandard deviation process, and (c) shows at = \u03c3t \u0017t , the ARCH(1) process.\nAs discussed in the previous section, an ARCH(1) process can be used as\nthe noise term of an AR(1) process. This process is shown in panel (d). The\nAR(1) parameters are \u03bc = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "Panel (a) shows the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 and \u03c6 = 0.8. The unconditional variance of\nat \u221ais \u03b3a (0) = 1/(1 \u2212 0.55) = 2.22, so the unconditional standard deviation\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "and \u03c6 = 0.8. The unconditional variance of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.22 = 1.49. Panels (e)\u2013(h) are sample ACF plots of the ARCH and\nAR+ARCH processes and squared processes. Notice that for the ARCH series,\nthe process is uncorrelated but the squared series has autocorrelation. Also\nnotice that for the AR(1)+ARCH(1) series the ACFs of the process and the\nsquared process, panels (g) and (h), both show autocorrelation. While the\ntrue ACFs have an exact geometric decay, this is only approximately true for\nthe sample ACFs in panels (f)\u2013(h); similarly, negative values are not present\nin the true ACFs, but the sample ACF has sampling error and may result in\nnegative values. The processes were all started at 0 and simulated for 10,200\nobservations. The \ufb01rst 10,000 observations were treated as a burn-in period\nand discarded.                                                              \u0002\n\na                                        white noise          b                               conditional std dev      c                                    ARCH            d                                      AR+ARCH\n                                                                                                                             4\n      2\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.22",
      "section_title": "= 1.49. Panels (e)\u2013(h) are sample ACF plots of the ARCH and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n      1\n\n\n\n\n                                                                                                                             2\n\n\n\n\n                                                                                                                                                                                  5\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n      0\n\n\n\n\n                                                                                                                             0\n                                                              \u03c3t\n\n\n\n\n                                                                                                                       a\n\n\n\n\n                                                                                                                                                                            y\n\u03b5\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                                                                                                                                  0\n      \u22121\n\n\n\n\n                                                                                                                             \u22122\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n      \u22122\n\n\n\n\n                                                                                                                                                                                  \u22125\n                                                                                                                             \u22124\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u22122",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n      \u22123\n\n\n\n\n                                     0   50   100 150 200                                     0   50   100 150 200                                 0   50   100 150 200                                     0     50   100 150 200\n\n                                               t                                                        t                                                    t                                                          t\n\n\n\ne                                             ARCH            f                                   ARCH squared         g                               AR+ARCH              h                                   AR+ARCH squared\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u22123",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0 0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\nACF\n\n\n\n\n                                                              ACF\n\n\n\n\n                                                                                                                       ACF\n\n\n\n\n                                                                                                                                                                            ACF\n                                                                                                                             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                     0   5    10    15   20                                   0   5    10    15   20                               0   5    10    15   20                                   0     5    10    15   20\n\n                                              Lag                                                      Lag                                                  Lag                                                        Lag\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0   5    10    15   20                                   0   5    10    15   20                               0   5    10    15   20                                   0     5    10    15   20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2. Simulation of 200 observations from an ARCH( 1) process and an\nAR( 1)+ARCH( 1) process. The parameters are \u03c9 = 1, \u03b1 = 0.55, \u03bc = 0.1, and\n\u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "Simulation of 200 observations from an ARCH( 1) process and an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8. Sample ACF plots of the ARCH and AR+ARCH processes and squared\nprocesses are shown in the bottom row.\n\f                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "Sample ACF plots of the ARCH and AR+ARCH processes and squared",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.6 ARIMA(pM , d, qM )+GARCH(pV , qV ) Models           411\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.6",
      "section_title": "ARIMA(pM , d, qM )+GARCH(pV , qV ) Models           411",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.5 ARCH(p) Models\nAs before, let \u0017t be Gaussian white noise with unit variance. Then at is an\nARCH(p) process if\n                                at = \u03c3 t \u0017t ,\nwhere\n                                   *\n                                   +\n                                   +          p\n                              \u03c3t = , \u03c9 +           \u03b1i a2t\u2212i\n                                             i=1\n\nis the conditional standard deviation of at given the past values at\u22121 , at\u22122 , . . .\nof this process. Like an ARCH(1) process, an ARCH(p) process is uncorrelated\nand has a constant mean (both conditional and unconditional) and a constant\nunconditional variance, but its conditional variance is nonconstant. In fact,\nthe ACF of a2t has the same structure as the ACF of an AR(p) process; see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.5",
      "section_title": "ARCH(p) Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.9.\n\n\n14.6 ARIMA(pM , d, qM )+GARCH(pV , qV ) Models\nA de\ufb01ciency of ARCH(p) models is that the conditional standard devia-\ntion process has high-frequency oscillations with high volatility coming in\nshort bursts. This behavior can be seen in Fig. 14.2b. GARCH models per-\nmit a wider range of behavior, in particular, more persistent volatility. The\nGARCH(p, q) model is\n                                     at = \u03c3t \u0017t ,\nin which\n                           *\n                           +\n                           +         p                 q\n                      \u03c3t = , \u03c9 +         \u03b1i a2t\u2212i +             2\n                                                            \u03b2j \u03c3t\u2212j .         (14.8)\n                                   i=1                j=1\n\nBecause past values of the \u03c3t process are fed back into the present value (with\nnonnegative coe\ufb03cients \u03b2j ), the conditional standard deviation can exhibit\nmore persistent periods of high or low volatility than seen in an ARCH pro-\ncess. In the stationary case, the process at is uncorrelated with a constant\nunconditional mean and variance and a2t has an ACF like an ARMA process\n(see Sect. 14.9). GARCH models include ARCH models as a special case, and\nwe use the term \u201cGARCH\u201d to refer to both ARCH and GARCH models.\n    A very general time series model lets at be GARCH(pV , qV ) and uses at\nas the noise term in an ARIMA(pM , d, qM ) model. The subscripts on p and q\ndistinguish between the conditional variance (V) or GARCH parameters and\nthe conditional mean (M) or ARIMA parameters. We will call such a process\nan ARIMA(pM , d, qM )+GARCH(pV , qV ) model.\n\f412                                 14 GARCH Models\n\n   Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.9",
      "section_title": "14.6 ARIMA(pM , d, qM )+GARCH(pV , qV ) Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.3 is a simulation of 500 observations from a GARCH(1,1) process\nand from a AR(1)+GARCH(1,1) process. The GARCH parameters are \u03c9 = 1,\n\u03b1 = 0.08, and \u03b2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.3",
      "section_title": "is a simulation of 500 observations from a GARCH(1,1) process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9. The large value of \u03b2 causes \u03c3t to be highly correlated\nwith \u03c3t\u22121 and gives the conditional standard deviation process a relatively\nlong-term persistence, at least compared to its behavior under an ARCH\nmodel. In particular, notice that the conditional standard deviation is less\n\u201cbursty\u201d than for the ARCH(1) process in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "The large value of \u03b2 causes \u03c3t to be highly correlated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2.\n\n\na                                   white noise           b                               conditional std dev        c                                       GARCH             d                                   AR+GARCH\n                                                                10 12 14 16\n\n\n\n\n                                                                                                                           10 20 30\n\n\n\n\n                                                                                                                                                                                     20 40\n      2\n      1\n\n\n\n\n                                                                                                                                                                                     \u221260 \u221240 \u221220 0\n      0\n\n\n\n\n                                                          \u03c3t\n\n\n\n\n                                                                                                                     a\n\n\n\n\n                                                                                                                                                                               y\n\u03b5\n\n\n\n\n                                                                                                                           \u221210 0\n      \u22123 \u22122 \u22121\n\n\n\n\n                                                                8\n                                                                6\n\n\n\n\n                                                                                                                           \u221230\n                                                                4\n\n\n\n\n                                0 100         300   500                                   0 100          300   500                                   0 100         300   500                                   0 100         300   500\n                                          t                                                          t                                                         t                                                         t\n\n\ne                                       GARCH             f                                   GARCH squared          g                                   AR+GARCH              h                               AR+GARCH squared\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "a                                   white noise           b                               conditional std dev        c                                       GARCH             d                                   AR+GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.2 0.4 0.6 0.8 1.0\nACF\n\n\n\n\n                                                          ACF\n\n\n\n\n                                                                                                                     ACF\n\n\n\n\n                                                                                                                                                                               ACF\n\n\n\n\n                                0   5   10 15 20 25                                       0    5   10 15 20 25                                       0   5   10 15 20 25                                       0   5   10 15 20 25\n                                         Lag                                                        Lag                                                       Lag                                                       Lag\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2 0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.3. Simulation of GARCH( 1, 1) and AR( 1)+GARCH( 1, 1) processes. The\nparameters are \u03c9 = 1, \u03b1 = 0.08, \u03b2 = 0.9, and \u03c6 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.3",
      "section_title": "Simulation of GARCH( 1, 1) and AR( 1)+GARCH( 1, 1) processes. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8.\n\n\n\n14.6.1 Residuals for ARIMA(pM , d, qM )+GARCH(pV , qV )\nModels\n\nWhen one \ufb01ts an ARIMA(pM , d, qM )+GARCH(pV , qV ) model to a time series\nYt , there are two types of residuals. The ordinary residual, denoted \u0002   at , is the\ndi\ufb00erence between Yt and its conditional expectation. As the notation implies,\nat estimates at . A standardized residual, denoted \u0002\n\u0002                                                    \u0017t , is an ordinary residual\n\u0002\nat divided by its estimated conditional standard deviation \u03c3   \u0002t . A standardized\nresidual estimates \u0017t . The standardized residuals should be used for model\nchecking. If the model \ufb01ts well, then neither \u0002 \u0017t nor \u0002 \u0017t2 should exhibit serial\ncorrelation. Moreover, if \u0017t has been assumed to have a normal distribution,\nthen this assumption can be checked by a normal plot of the standardized\nresiduals \u0002\u0017t . The \u0002\n                    at are the residuals of the ARIMA process and are used\nwhen forecasting via the methods in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "14.6.1 Residuals for ARIMA(pM , d, qM )+GARCH(pV , qV )",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.12.\n\f                                  14.8 Fitting ARMA+GARCH Models           413\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.12",
      "section_title": "14.8 Fitting ARMA+GARCH Models           413",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.7 GARCH Processes Have Heavy Tails\nResearchers have long noticed that stock returns have \u201cheavy-tailed\u201d or\n\u201coutlier-prone\u201d probability distributions, and we have seen this ourselves in\nearlier chapters. One reason for outliers may be that the conditional variance\nis not constant, and the outliers occur when the variance is large, as in the\nnormal mixture example of Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.7",
      "section_title": "GARCH Processes Have Heavy Tails",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5. In fact, GARCH processes exhibit heavy\ntails even if {\u0017t } is Gaussian. Therefore, when we use GARCH models, we can\nmodel both the conditional heteroskedasticity and the heavy-tailed distribu-\ntions of \ufb01nancial market data. Nonetheless, many \ufb01nancial time series have\ntails that are heavier than implied by a GARCH process with Gaussian {\u0017t }.\nTo handle such data, one can assume that, instead of being Gaussian white\nnoise, {\u0017t } is an i.i.d. white noise process with a heavy-tailed distribution.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "In fact, GARCH processes exhibit heavy",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.8 Fitting ARMA+GARCH Models\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "Fitting ARMA+GARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2. AR(1)+GARCH(1,1) model \ufb01t to daily BMW stock log returns\n\n\n    This example uses the daily BMW stock log returns. The ugarchfit()\nfunction from R\u2019s rugarch package is used to \ufb01t an AR(1)+GARCH(1,1)\nmodel to this series. Although ugarchfit() allows the white noise to have\na nonGaussian distribution, we begin this example using Gaussian white\nnoise (the default). First the model is speci\ufb01ed using the ugarchspec() func-\ntion; for an AR(1)+GARCH(1,1) model we specify armaOrder=c(1,0) and\ngarchOrder=c(1,1). The commands and abbreviated output are below.\n1 library(rugarch)\n2 data(bmw, package=\"evir\")\n3 arma.garch.norm = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n\n4                              variance.model=list(garchOrder=c(1,1)))\n5 bmw.garch.norm = ugarchfit(data=bmw, spec=arma.garch.norm)\n\n6 show(bmw.garch.norm)\n\n\n\n    GARCH Model : sGARCH(1,1)\n    Mean Model : ARFIMA(1,0,0)\n    Distribution : norm\n\n    Optimal Parameters\n    ------------------------------------\n            Estimate Std. Error t value Pr(>|t|)\n    mu      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "AR(1)+GARCH(1,1) model \ufb01t to daily BMW stock log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000453    0.000175   2.5938 0.009493\n    ar1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000453",
      "section_title": "0.000175   2.5938 0.009493",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.098135    0.014261   6.8813 0.000000\n    omega   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.098135",
      "section_title": "0.014261   6.8813 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000009    0.000000 23.0613 0.000000\n    alpha1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000009",
      "section_title": "0.000000 23.0613 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.099399     0.005593 17.7730 0.000000\n    beta1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.099399",
      "section_title": "0.005593 17.7730 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.863672    0.006283 137.4591 0.000000\n\f414      14 GARCH Models\n\n      LogLikelihood : 17752\n\n      Information Criteria\n      ------------------------------------\n      Akaike       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.863672",
      "section_title": "0.006283 137.4591 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7751\n      Bayes        -5.7696\n      Shibata      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.7751",
      "section_title": "Bayes        -5.7696",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.7751\n      Hannan-Quinn -5.7732\n\nIn the output, \u03c6\u00021 is denoted by ar1, the estimated mean \u03bc   \u0002 is mean, and \u03c9\u0002 is\ncalled omega. Note that \u03c6\u00021 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.7751",
      "section_title": "Hannan-Quinn -5.7732",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0981 and is statistically signi\ufb01cant, implying\nthat there is a small amount of positive autocorrelation. Both \u03b11 and \u03b21 are\nhighly signi\ufb01cant and \u03b2\u00021 = 0.8636, which implies rather persistent volatility\nclustering. There are two additional information criteria reported, Shibata\u2019s\ninformation criterion and Hannan\u2013Quinn information criterion (HQIC). These\nare less widely used than AIC and BIC and will not be discussed here.\n    In the output from ugarchfit(), the AIC and BIC values have been\nnormalized by dividing by n, so these values should be multiplied by n = 6146\nto have their usual values. In particular, AIC and BIC will not be so close\nto each other after multiplication by 6146. The daily BMW stock log return\nseries Yt , with two estimated conditional standard deviations superimposed,\nand the estimated conditional standard deviation series \u03c3   \u0002t (vs. the absolute\nvalue of the log return series |Yt |) are shown in the top row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0981",
      "section_title": "and is statistically signi\ufb01cant, implying",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4.\n    The output also includes the following tests applied to the standardized\nand squared standardized residuals.\n      Weighted Ljung-Box Test on Standardized Residuals\n      ------------------------------------\n                              statistic p-value\n      Lag[1]                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "The output also includes the following tests applied to the standardized",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7786 0.3776\n      Lag[2*(p+q)+(p+q)-1][2]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7786",
      "section_title": "0.3776",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9158 0.7892\n      Lag[4*(p+q)+(p+q)-1][5]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9158",
      "section_title": "0.7892",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.3270 0.3536\n      d.o.f=1\n      H0 : No serial correlation\n      Weighted Ljung-Box Test on Standardized Squared Residuals\n      ------------------------------------\n                              statistic p-value\n      Lag[1]                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.3270",
      "section_title": "0.3536",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.277 0.5987\n      Lag[2*(p+q)+(p+q)-1][5]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.277",
      "section_title": "0.5987",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.026 0.8537\n      Lag[4*(p+q)+(p+q)-1][9]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.026",
      "section_title": "0.8537",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.721 0.9356\n      d.o.f=2\n      Weighted ARCH LM Tests\n      ------------------------------------\n                  Statistic Shape Scale P-Value\n      ARCH Lag[3]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.721",
      "section_title": "0.9356",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1922 0.500 2.000 0.6611\n      ARCH Lag[5]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1922",
      "section_title": "0.500 2.000 0.6611",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1094 1.440 1.667 0.7008\n      ARCH Lag[7]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.1094",
      "section_title": "1.440 1.667 0.7008",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2290 2.315 1.543 0.8737\n      Adjusted Pearson Goodness-of-Fit Test:\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2290",
      "section_title": "2.315 1.543 0.8737",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.8 Fitting ARMA+GARCH Models            415\n\n     ------------------------------------\n       group statistic p-value(g-1)\n     1    20     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "Fitting ARMA+GARCH Models            415",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "493.1    1.563e-92\n     2    30     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "493.1",
      "section_title": "1.563e-92",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "513.4    5.068e-90\n     3    40     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "513.4",
      "section_title": "5.068e-90",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "559.3    2.545e-93\n     4    50     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "559.3",
      "section_title": "2.545e-93",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "585.6    5.446e-93\nWeighted versions of the Ljung-Box (and ARCH-LM) test statistics1 and their\napproximate p-values all indicate that the estimated model for the conditional\nmean and variance are adequate for removing serial correlation from the series\nand squared series, respectively. The sample ACF of the standardized resid-\n     \u0017t , and the squared standardized residuals \u0002\nuals \u0002                                            \u0017t2 are shown in the middle\n                                             2\nrow of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "585.6",
      "section_title": "5.446e-93",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4. The Goodness-of-Fit tests compare the empirical distribu-\ntion of the standardized residuals with the theoretical ones from the speci\ufb01ed\ndensity, which is Gaussian by default. The small p-values strongly reject the\nnull hypothesis that the white noise standardized innovation process {\u0017t } is\nGaussian. Empirical density estimates and a normal quantile plot of the stan-\ndardized residuals \u0002\u0017t are shown in the bottom row of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "The Goodness-of-Fit tests compare the empirical distribu-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4.\n    Figure 14.5 shows a t-plot with 4 df for the standardized residuals \u0002   \u0017t .\nUnlike the normal quantile plot in the last panel of Fig. 14.4, this plot is\nnearly a straight line except for four outliers in the left tail. The sample\nsize is 6146, so the outliers are a very small fraction of the data. Thus, it\nseems like a t-distribution would be suitable for the innovation process \u0017t .\nA t-distribution was \ufb01t to the standardized residuals by maximum likelihood\nusing the fitdistr() function from the Rpackage MASS.\n7 library(MASS)\n8 e = residuals(bmw.garch.norm, standardize=TRUE)\n9 fitdistr(e,\"t\")\n\n\n           m         s        df\n       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "Figure 14.5 shows a t-plot with 4 df for the standardized residuals \u0002   \u0017t .",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0243    0.7269    4.1096\n      ( 0.0109) ( 0.0121) ( 0.2359)\nThe MLE of the degrees-of-freedom parameter was ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0243",
      "section_title": "0.7269    4.1096",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1. This con\ufb01rms the\ngood \ufb01t by this distribution seen in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.1",
      "section_title": "This con\ufb01rms the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.5. The AR(1)+GARCH(1,1) model\nwas re\ufb01t assuming t-distributed errors, so distribution.model = \"std\" in\nugarchspec(). The commands and abbreviated results are below.\n10 arma.garch.t = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n11                           variance.model=list(garchOrder=c(1,1)),\n12                           distribution.model = \"std\")\n13 bmw.garch.t = ugarchfit(data=bmw,spec=arma.garch.t)\n\n14 show(bmw.garch.t)\n\n1\n  Weighted Ljung-Box and ARCH-LM statistics of Fisher and Gallagher (2012)\n  are provided by the ugarchfit() function to better account for the distribution\n  of the statistics when applied to residuals from a \ufb01tted model; their use and\n  interpretation remains unchanged.\n2\n  These Chi-squared tests are based on the tests of Palm (1996); group indicates\n  the number of bins used in the implementation.\n\f416                         14 GARCH Models\n                               Series with 2 Conditional SD Superimposed                                                                       Conditional SD (vs |returns|)\n\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.5",
      "section_title": "The AR(1)+GARCH(1,1) model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 0.10\n\n\n\n\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n                                                                                                                         0.08\n                                                                                                      Volatility\nReturns\n\n\n\n\n                                                                               GARCH model : sGARCH\n\n\n\n\n                                                                                                                                                                                        GARCH model : sGARCH\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "0.08",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                                                                                                                         0.04\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n              \u22120.15\n\n\n\n\n                          Jan 01    Jan 01        Jan 01    Jan 01    Jan 01                                                     Jan 01     Jan 01     Jan 01       Jan 01     Jan 01\n                           1970      1974          1978      1982      1986                                                       1970       1974       1978         1982       1986\n                                                    Time                                                                                                  Time\n\n\n                                      ACF of Standardized Residuals                                                                       ACF of Squared Standardized Residuals\n\n\n\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "\u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n              0.02\n\n\n\n\n                                                                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n              0.00\nACF\n\n\n\n\n                                                                                                      ACF\n                                                                               GARCH model : sGARCH\n\n\n\n\n                                                                                                                                                                                        GARCH model : sGARCH\n                                                                                                                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n              \u22120.02\n              \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "\u22120.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n\n\n\n\n                                                                                                                         \u22120.03\n\n\n\n\n                            1 4 7        11 15 19 23 27 31 35                                                                      1 4 7        11 15 19 23 27 31 35\n                                                     lag                                                                                                   lag\n\n\n                               Empirical Density of Standardized Residuals\n                           Median: \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "\u22120.03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 | Mean: \u22120.0094                                                                                        norm \u2212 QQ Plot\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "| Mean: \u22120.0094                                                                                        norm \u2212 QQ Plot",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                              normal Density\n                              norm (0,1) Fitted Density\n                                                                                                                         5\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "normal Density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                                                      Sample Quantiles\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "Sample Quantiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\nProbability\n\n\n\n\n                                                                                                                         0\n                                                                               GARCH model : sGARCH\n\n\n\n\n                                                                                                                                                                                        GARCH model : sGARCH\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "Probability",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                                                                                                                         \u22125\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "\u22125",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n              0.1\n\n\n\n\n                                                                                                                         \u221210\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                              \u221210            \u22125             0         5                                                          \u22124           \u22122            0             2         4\n                                                  zseries                                                                                       Theoretical Quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u221210            \u22125             0         5                                                          \u22124           \u22122            0             2         4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4. The daily BMW stock log return series Yt , with two estimated conditional\nstandard deviations superimposed; the estimated conditional standard deviation \u03c3   \u0002t\nseries (vs. the absolute value of the log return series |Yt |); the sample ACF of the\nstandardized residuals \u0002t and the squared standardized residuals \u0002t2 ; empirical den-\nsity estimates of the standardized residuals \u0002t ; and a normal quantile plot of the\nstandardized residuals \u0002t .\n\f                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "The daily BMW stock log return series Yt , with two estimated conditional",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.8 Fitting ARMA+GARCH Models   417\n\n                                               t\u2212plot, df=4\n\n\n\n\n                                 10\n                                 5\n                   t\u2212quantiles\n                                 0\n                                 \u22125\n                                 \u221210\n\n\n\n                                       \u221210      \u22125       0        5\n                                       Standardized residual quantiles\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "Fitting ARMA+GARCH Models   417",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.5. A t-plot with 4 df for the standardized residuals \u0002t from an\nAR(1)+GARCH(1,1) model \ufb01t to daily BMW stock log return; the reference lines\ngo through the \ufb01rst and third quartiles.\n\n\n   GARCH Model : sGARCH(1,1)\n   Mean Model : ARFIMA(1,0,0)\n   Distribution : std\n\n   Optimal Parameters\n   ------------------------------------\n           Estimate Std. Error t value Pr(>|t|)\n   mu      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.5",
      "section_title": "A t-plot with 4 df for the standardized residuals \u0002t from an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000135    0.000144 0.93978 0.347333\n   ar1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000135",
      "section_title": "0.000144 0.93978 0.347333",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.063911    0.012521 5.10436 0.000000\n   omega   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.063911",
      "section_title": "0.012521 5.10436 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000006    0.000003 1.69915 0.089291\n   alpha1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000006",
      "section_title": "0.000003 1.69915 0.089291",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.090592     0.012479 7.25936 0.000000\n   beta1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.090592",
      "section_title": "0.012479 7.25936 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.889887    0.014636 60.80228 0.000000\n   shape   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.889887",
      "section_title": "0.014636 60.80228 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.070078    0.301306 13.50813 0.000000\n\n   LogLikelihood : 18152\n\n   Information Criteria\n   ------------------------------------\n   Akaike       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.070078",
      "section_title": "0.301306 13.50813 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9048\n   Bayes        -5.8983\n   Shibata      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9048",
      "section_title": "Bayes        -5.8983",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9048\n   Hannan-Quinn -5.9026\n\n   Weighted Ljung-Box Test on Standardized Residuals\n   ------------------------------------\n                           statistic   p-value\n   Lag[1]                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9048",
      "section_title": "Hannan-Quinn -5.9026",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.640 1.904e-03\n   Lag[2*(p+q)+(p+q)-1][2]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.640",
      "section_title": "1.904e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.653 3.367e-09\n   Lag[4*(p+q)+(p+q)-1][5]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.653",
      "section_title": "3.367e-09",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.983 1.455e-04\n\f418      14 GARCH Models\n\n      d.o.f=1\n      H0 : No serial correlation\n\n      Weighted Ljung-Box Test on Standardized Squared Residuals\n      ------------------------------------\n                              statistic p-value\n      Lag[1]                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.983",
      "section_title": "1.455e-04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5641 0.4526\n      Lag[2*(p+q)+(p+q)-1][5]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5641",
      "section_title": "0.4526",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2964 0.7898\n      Lag[4*(p+q)+(p+q)-1][9]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2964",
      "section_title": "0.7898",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0148 0.9032\n      d.o.f=2\n\n      Adjusted Pearson Goodness-of-Fit Test:\n      ------------------------------------\n        group statistic p-value(g-1)\n      1    20     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0148",
      "section_title": "0.9032",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "229.0    5.460e-38\n      2    30     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "229.0",
      "section_title": "5.460e-38",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "279.6    8.428e-43\n      3    40     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "279.6",
      "section_title": "8.428e-43",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "313.8    1.230e-44\n      4    50     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "313.8",
      "section_title": "1.230e-44",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "374.6    1.037e-51\n\n    The weighted Ljung\u2013Box tests for the residuals have small p-values. These\nare due to small autocorrelations that should not be of practical importance.\nThe sample size here is 6146 so, not surprisingly, small autocorrelations are\nstatistically signi\ufb01cant. The goodness-of-\ufb01t test statistics are much smaller\nbut still signi\ufb01cant; the large sample size again makes rejection likely even\nwhen the discrepancies are negligible from a practical standpoint. However,\nboth AIC and BIC decreased substantially, and the re\ufb01t model with a t condi-\ntional distribution o\ufb00ers an improvement over the original \ufb01t with a Gaussian\nconditional distribution.                                                   \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "374.6",
      "section_title": "1.037e-51",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.9 GARCH Models as ARMA Models\nThe similarities seen in this chapter between GARCH and ARMA models are\nnot a coincidence. If at is a GARCH process, then a2t is an ARMA process,\nbut with weak white noise, not i.i.d. white noise. To show this, we will start\nwith the GARCH(1,1) model, where at = \u03c3t \u0017t . Here \u0017t is i.i.d. white noise\nand\n                    E(a2t |Ft\u22121 ) = \u03c3t2 = \u03c9 + \u03b1a2t\u22121 + \u03b2\u03c3t\u22121\n                                                         2\n                                                             ,           (14.9)\nwhere Ft\u22121 is the information set at time t \u2212 1. De\ufb01ne \u03b7t = a2t \u2212 \u03c3t2 . Since\nE(\u03b7t |Ft\u22121 ) = E(a2t |Ft\u22121 ) \u2212 \u03c3t2 = 0 by (A.33), \u03b7t is an uncorrelated process,\nthat is, a weak white noise process. The conditional heteroskedasticity of at\nis inherited by \u03b7t , so \u03b7t is not i.i.d. white noise.\n    Simple algebra shows that\n\n                         \u03c3t2 = \u03c9 + (\u03b1 + \u03b2)a2t\u22121 \u2212 \u03b2\u03b7t\u22121                 (14.10)\n\f                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.9",
      "section_title": "GARCH Models as ARMA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.10 GARCH(1,1) Processes          419\n\nand therefore\n\n                a2t = \u03c3t2 + \u03b7t = \u03c9 + (\u03b1 + \u03b2)a2t\u22121 \u2212 \u03b2\u03b7t\u22121 + \u03b7t .                     (14.11)\n\nAssume that \u03b1 + \u03b2 < 1. If \u03c5 = \u03c9/{1 \u2212 (\u03b1 + \u03b2)}, then\n\n                    a2t \u2212 \u03c5 = (\u03b1 + \u03b2)(a2t\u22121 \u2212 \u03c5) + \u03b2\u03b7t\u22121 + \u03b7t .                      (14.12)\n\nFrom (14.12) one sees that a2t is an ARMA(1,1). Using the notation of (12.25),\nthe mean is \u03bc = \u03c5, the AR(1) coe\ufb03cient is \u03c6 = \u03b1+\u03b2 and the MA(1) coe\ufb03cient\nis \u03b8 = \u2212\u03b2.\n    For the general case, assume that \u03c3t follows (14.8) such that\n                                       p                  q\n                        \u03c3t2 = \u03c9 +           \u03b1i a2t\u2212i +             2\n                                                               \u03b2j \u03c3t\u2212j .             (14.13)\n                                      i=1                j=1\n\nTo simplify notation, if q > p, then de\ufb01ne \u03b1i = 0 for i = p + 1, . . . , q.\nSimilarly, if p > q, then de\ufb01ne \u03b2j = 0 for j = q + 1, . . . , p. De\ufb01ne\n               \u0017max(p,q)\n\u03c5 = \u03c9/{1 \u2212 i=1           (\u03b1i + \u03b2i )}. Straightforward algebra similar to the\nGARCH(1,1) case shows that\n                    max(p,q)                                   q\n        a2t \u2212 \u03c5 =              (\u03b1i + \u03b2i )(a2t\u2212i \u2212 \u03c5) \u2212              \u03b2j \u03b7t\u2212j + \u03b7t ,   (14.14)\n                      i=1                                     j=1\n\nso that a2t is an ARMA(max(p, q), q) process with mean \u03bc = \u03c5, AR coe\ufb03-\ncients \u03c6i = \u03b1i + \u03b2i and MA coe\ufb03cients \u03b8j = \u2212\u03b2j . As a byproduct of these\ncalculations, we obtain a necessary condition for at to be stationary:\n                                   max(p,q)\n                                            (\u03b1i + \u03b2i ) < 1.                          (14.15)\n                                     i=1\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.10",
      "section_title": "GARCH(1,1) Processes          419",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "p, then de\ufb01ne \u03b1i = 0 for i = p + 1, . . . , q.\nSimilarly, if p > q, then de\ufb01ne \u03b2j = 0 for j = q + 1, . . . , p. De\ufb01ne\n               \u0017max(p,q)\n\u03c5 = \u03c9/{1 \u2212 i=1           (\u03b1i + \u03b2i )}. Straightforward algebra similar to the\nGARCH(1,1) case shows that\n                    max(p,q)                                   q\n        a2t \u2212 \u03c5 =              (\u03b1i + \u03b2i )(a2t\u2212i \u2212 \u03c5) \u2212              \u03b2j \u03b7t\u2212j + \u03b7t ,   (14.14)\n                      i=1                                     j=1",
        "start": 838,
        "end": 1312
      }
    ]
  },
  {
    "content": "14.10 GARCH(1,1) Processes\nThe GARCH(1,1) is the most widely used GARCH process, so it is worthwhile\nto study it in some detail. If at is GARCH(1,1), then as we have just seen, a2t is\nARMA(1,1). Therefore, the ACF of a2t can be obtained from formulas (12.31)\nand (12.32). After some algebra, one \ufb01nds that\n\n                                            \u03b1(1 \u2212 \u03b1\u03b2 \u2212 \u03b2 2 )\n                                \u03c1a2 (1) =                                            (14.16)\n                                             1 \u2212 2\u03b1\u03b2 \u2212 \u03b2 2\nand\n                       \u03c1a2 (h) = (\u03b1 + \u03b2)h\u22121 \u03c1a2 (1),                h \u2265 2.           (14.17)\n\f420    14 GARCH Models\n\nThese formulas also hold in an AR(1)+GARCH(1,1) model, and the ACF of\nyt2 also decays with h \u2265 2 at a geometric rate in the stationary case, provided\nsome additional assumptions hold, however, the exact expressions are more\ncomplicated (see Palma and Zevallos, 2004).\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.10",
      "section_title": "GARCH(1,1) Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n                                                      \u03b1 = 0.10, \u03b2 = 0.894\n                                                      \u03b1 = 0.30, \u03b2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u03b1 = 0.10, \u03b2 = 0.894",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.604\n                                                      \u03b1 = 0.50, \u03b2 = 0.000\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.604",
      "section_title": "\u03b1 = 0.50, \u03b2 = 0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                 0.6\n        \u03c1a2(h)\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                 0.2\n                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                       0    2         4          6          8          10\n                                           h\n\n       Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0    2         4          6          8          10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.6. ACFs of three GARCH(1,1) processes with \u03c1a2 (1) = 0.5.\n\n\n    By (14.16), there are in\ufb01nitely many values of (\u03b1, \u03b2) with the same value\nof \u03c1a2 (1). By (14.17), a higher value of \u03b1 + \u03b2 means a slower decay of \u03c1a2 (\u00b7)\nafter the \ufb01rst lag. This behavior is illustrated in Fig. 14.6, which contains the\nACF of a2t for three GARCH(1,1) processes with a lag-1 autocorrelation of\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.6",
      "section_title": "ACFs of three GARCH(1,1) processes with \u03c1a2 (1) = 0.5.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5. The solid curve has the highest value of \u03b1 + \u03b2 and the ACF decays very\nslowly. The dotted curve is a pure ARCH(1) process and has the most rapid\ndecay.\n    In Example 14.2, an AR(1)+GARCH(1,1) model was \ufb01t to the BMW\ndaily log returns. The GARCH parameters were estimated to be \u03b1          \u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "The solid curve has the highest value of \u03b1 + \u03b2 and the ACF decays very",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\nand \u03b2\u0002 = 0.86. By (14.16) the \u03c1\u0002a2 (1) = 0.197 for this process and the high\nvalue of \u03b2\u0002 suggests slow decay. The sample ACF of the squared residuals\n[from an AR(1) model] is plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "and \u03b2\u0002 = 0.86. By (14.16) the \u03c1\u0002a2 (1) = 0.197 for this process and the high",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.7. In that \ufb01gure, we see the lag-1\nautocorrelation is slightly below ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.7",
      "section_title": "In that \ufb01gure, we see the lag-1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 and after one lag the ACF decays slowly,\nexactly as expected.\n    The capability of the GARCH(1,1) model to \ufb01t the lag-1 autocorrelation\nand the subsequent rate of decay separately is important in practice. It appears\nto be the main reason that the GARCH(1,1) model \ufb01ts so many \ufb01nancial time\nseries.\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "and after one lag the ACF decays slowly,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.11 APARCH Models         421\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.11",
      "section_title": "APARCH Models         421",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.11 APARCH Models\nIn some \ufb01nancial time series, large negative returns appear to increase volatil-\nity more than do positive returns of the same magnitude. This is called the\n\n                                               Series res^2\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.11",
      "section_title": "APARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n               0.8\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n         ACF\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n               0.2\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                       0                10              20                 30\n                                                     Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0                10              20                 30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.7. ACF of the squared residuals from an AR(1) \ufb01t to the BMW log returns.\n\n\nleverage e\ufb00ect. Standard GARCH models, that is, the models given by (14.8),\ncannot model the leverage e\ufb00ect because they model \u03c3t as a function of past\nvalues of a2t \u2014whether the past values of at are positive or negative is not\ntaken into account. The problem here is that the square function x2 is sym-\nmetric in x. The solution is to replace the square function with a \ufb02exible class\nof nonnegative functions that include asymmetric functions. The APARCH\n(asymmetric power ARCH) models do this. They also o\ufb00er more \ufb02exibility\nthan GARCH models by modeling \u03c3t\u03b4 , where \u03b4 > 0 is another parameter.\n    The APARCH(p, q) model for the conditional standard deviation is\n                                  p                                  q\n                     \u03c3t\u03b4 = \u03c9 +         \u03b1i (|at\u2212i | \u2212 \u03b3i at\u2212i )\u03b4 +             \u03b4\n                                                                          \u03b2j \u03c3t\u2212j ,   (14.18)\n                                 i=1                                j=1\n\nwhere \u03b4 > 0 and \u22121 < \u03b3i < 1, i = 1, . . . , p. Note that \u03b4 = 2 and \u03b3 = \u00b7 \u00b7 \u00b7 =\n\u03b3p = 0 give a standard GARCH model.\n    The e\ufb00ect of at\u2212i upon \u03c3t is through the function g\u03b3i , where g\u03b3 (x) =\n|x|\u2212\u03b3x. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.7",
      "section_title": "ACF of the squared residuals from an AR(1) \ufb01t to the BMW log returns.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 is another parameter.\n    The APARCH(p, q) model for the conditional standard deviation is\n                                  p                                  q\n                     \u03c3t\u03b4 = \u03c9 +         \u03b1i (|at\u2212i | \u2212 \u03b3i at\u2212i )\u03b4 +             \u03b4\n                                                                          \u03b2j \u03c3t\u2212j ,   (14.18)\n                                 i=1                                j=1",
        "start": 650,
        "end": 1063
      },
      {
        "language": "r",
        "code": "0 and \u22121 < \u03b3i < 1, i = 1, . . . , p. Note that \u03b4 = 2 and \u03b3 = \u00b7 \u00b7 \u00b7 =\n\u03b3p = 0 give a standard GARCH model.\n    The e\ufb00ect of at\u2212i upon \u03c3t is through the function g\u03b3i , where g\u03b3 (x) =\n|x|\u2212\u03b3x. Figure",
        "start": 1071,
        "end": 1268
      }
    ]
  },
  {
    "content": "14.8 shows g\u03b3 (x) for several values of \u03b3. When \u03b3 > 0, g\u03b3 (\u2212x) >\ng\u03b3 (x) for any x > 0, so there is a leverage e\ufb00ect. If \u03b3 < 0, then there is a\nleverage e\ufb00ect in the opposite direction to what is expected\u2014positive past\nvalues of at increase volatility more than negative past values of the same\nmagnitude.\n\f422                 14 GARCH Models\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "shows g\u03b3 (x) for several values of \u03b3. When \u03b3 > 0, g\u03b3 (\u2212x) >",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, g\u03b3 (\u2212x) >\ng\u03b3 (x) for any x > 0, so there is a leverage e\ufb00ect. If \u03b3 < 0, then there is a\nleverage e\ufb00ect in the opposite direction to what is expected\u2014positive past\nvalues of at increase volatility more than negative past values of the same\nmagnitude.\n\f422                 14 GARCH Models",
        "start": 50,
        "end": 343
      }
    ]
  },
  {
    "content": "14.3. AR(1)+APARCH(1,1) \ufb01t to daily BMW stock log returns\n\n\n\n                         gamma = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.3",
      "section_title": "AR(1)+APARCH(1,1) \ufb01t to daily BMW stock log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5                              gamma = \u22120.2                              gamma = 0\n\n\n\n\n                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "gamma = \u22120.2                              gamma = 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n                                                        3.0\n        0 1 2 3 4\ng\u03b3(x)\n\n\n\n\n                                                g\u03b3(x)\n\n\n\n\n                                                                                          g\u03b3(x)\n\n                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                                        1.5\n                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                  0.0\n                    \u22123     \u22121       1   2   3                 \u22123     \u22121       1   2   3                 \u22123     \u22121       1   2   3\n                                x                                         x                                         x\n\n\n                         gamma = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12                              gamma = 0.3                               gamma = 0.9\n                                                        4\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "gamma = 0.3                               gamma = 0.9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                        3\n\n\n\n\n                                                                                                  4\ng\u03b3(x)\n\n\n\n\n                                                g\u03b3(x)\n\n\n\n\n                                                                                          g\u03b3(x)\n                                                        2\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n\n\n\n                                                                                                  2\n                                                        1\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                        0\n\n\n\n\n                                                                                                  0\n                    \u22123     \u22121       1   2   3                 \u22123     \u22121       1   2   3                 \u22123     \u22121       1   2   3\n                                x                                         x                                         x\n\n                                Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.8. Plots of g\u03b3 (x) for various values of \u03b3.\n\n\n    In this example, an AR(1)+APARCH(1,1) model with t-distributed errors\nis \ufb01t to the BMW log returns. The commands and abbreviated output from\nugarchfit() is below. The estimate of \u03b4 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "Plots of g\u03b3 (x) for various values of \u03b3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.48 with a standard error of\n0.14, so there is strong evidence that \u03b4 is not 2, the value under a standard\nGARCH model. Also, \u03b3     \u00021 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.48",
      "section_title": "with a standard error of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12 with a standard error of 0.045, so there is a\nstatistically signi\ufb01cant leverage e\ufb00ect, since we reject the null hypothesis that\n\u03b31 = 0. However, the leverage e\ufb00ect is small, as can be seen in the plot in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "with a standard error of 0.045, so there is a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.8 with \u03b3 = 0.12. The leverage might not be of practical importance.\n15 arma.aparch.t = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n16                 variance.model=list(model=\"apARCH\",\n17                                     garchOrder=c(1,1)),\n18                            distribution.model = \"std\")\n19 bmw.aparch.t = ugarchfit(data=bmw, spec=arma.aparch.t)\n\n20 show(bmw.aparch.t)\n\n\n\n        GARCH Model : apARCH(1,1)\n        Mean Model : ARFIMA(1,0,0)\n        Distribution : std\n        Optimal Parameters\n        ------------------------------------\n                Estimate Std. Error t value Pr(>|t|)\n        mu      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.8",
      "section_title": "with \u03b3 = 0.12. The leverage might not be of practical importance.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000048    0.000147   0.3255 0.744801\n        ar1     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000048",
      "section_title": "0.000147   0.3255 0.744801",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.063666    0.012352   5.1543 0.000000\n\f                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.063666",
      "section_title": "0.012352   5.1543 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.11 APARCH Models        423\n\n   omega    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.11",
      "section_title": "APARCH Models        423",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000050     0.000032    1.5541 0.120158\n   alpha1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000050",
      "section_title": "0.000032    1.5541 0.120158",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.098839     0.012741    7.7574 0.000000\n   beta1    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.098839",
      "section_title": "0.012741    7.7574 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.899506     0.013565   66.3105 0.000000\n   gamma1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.899506",
      "section_title": "0.013565   66.3105 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.121947     0.044664    2.7303 0.006327\n   delta    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.121947",
      "section_title": "0.044664    2.7303 0.006327",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.476643     0.142442   10.3666 0.000000\n   shape    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.476643",
      "section_title": "0.142442   10.3666 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.073809     0.234417   17.3784 0.000000\n\n   LogLikelihood : 18161\n\n   Information Criteria\n   ------------------------------------\n   Akaike       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.073809",
      "section_title": "0.234417   17.3784 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9073\n   Bayes        -5.8985\n   Shibata      -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9073",
      "section_title": "Bayes        -5.8985",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9073\n   Hannan-Quinn -5.9042\n\n   Weighted Ljung-Box Test on Standardized Residuals\n   ------------------------------------\n                           statistic   p-value\n   Lag[1]                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9073",
      "section_title": "Hannan-Quinn -5.9042",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.824 1.723e-03\n   Lag[2*(p+q)+(p+q)-1][2]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.824",
      "section_title": "1.723e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.849 2.003e-09\n   Lag[4*(p+q)+(p+q)-1][5]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.849",
      "section_title": "2.003e-09",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.253 1.100e-04\n   d.o.f=1\n   H0 : No serial correlation\n\n   Weighted Ljung-Box Test on Standardized Squared Residuals\n   ------------------------------------\n                           statistic p-value\n   Lag[1]                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.253",
      "section_title": "1.100e-04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.456 0.2276\n   Lag[2*(p+q)+(p+q)-1][5]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.456",
      "section_title": "0.2276",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.363 0.5354\n   Lag[4*(p+q)+(p+q)-1][9]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.363",
      "section_title": "0.5354",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.258 0.7157\n   d.o.f=2\n\n    As mentioned earlier, in the output from ugarchfit(), the Information\nCriteria values have been normalized by dividing by n, though this is not\nnoted in the output.\n    The normalized BIC for this model (\u22125.8985) is very nearly the same as the\nnormalized BIC for the GARCH model with t-distributed errors (\u22125.8983),\nbut after multiplying by n = 6146, the di\ufb00erence in the BIC values is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.258",
      "section_title": "0.7157",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.23.\nThe di\ufb00erence between the two normalized AIC values, \u22125.9073 and \u22125.9048,\nis even larger, 15.4, after multiplication by n. Therefore, AIC and BIC support\nusing the APARCH model instead of the GARCH model.\n    ACF plots (not shown) for the standardized residuals and their squares\nshowed little correlation, so the AR(1) model for the conditional mean and\nthe APARCH(1,1) model for the conditional variance \ufb01t well. Finally, shape\nis the estimated degrees of freedom of the t-distribution and is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.23",
      "section_title": "The di\ufb00erence between the two normalized AIC values, \u22125.9073 and \u22125.9048,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.07 with a\nsmall standard error, so there is very strong evidence that the conditional\ndistribution is heavy-tailed.                                                \u0002\n\f424      14 GARCH Models\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.07",
      "section_title": "with a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.12 Linear Regression with ARMA+GARCH\nErrors\n\nWhen using time series regression, one often observes autocorrelated residuals.\nFor this reason, linear regression with ARMA disturbances was introduced in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.12",
      "section_title": "Linear Regression with ARMA+GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3. The model considered was\n\n                        Yt = \u03b20 + \u03b21 Xt,1 + \u00b7 \u00b7 \u00b7 + \u03b2p Xt,p + et ,              (14.19)\n\nwhere\n\n      (1 \u2212 \u03c61 B \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03c6p B p )(et \u2212 \u03bc) = (1 + \u03b81 B + . . . + \u03b8q B q )at ,   (14.20)\n\nand {at } is i.i.d. white noise. This model is su\ufb03cient for serially correlated\nerrors, but it does not accommodate volatility clustering, which is often found\nin the residuals.\n    One solution is to model the noise as an ARMA+GARCH process. There-\nfore, we will now assume that, instead of being i.i.d. white noise, {at } is a\nGARCH process so that\n                                   at = \u03c3 t \u0017 t ,                       (14.21)\nwhere\n                             *\n                             +\n                             +          p                 q\n                        \u03c3t = , \u03c9 +          \u03b1i a2t\u2212i +             2 ,\n                                                               \u03b2j \u03c3t\u2212j          (14.22)\n                                      i=1                j=1\n\nand {\u0017t } is i.i.d. white noise. The model given by (14.19)\u2013(14.22) is a linear\nregression model with ARMA+GARCH disturbances.\n    Some software, including the ugarchfit() function from R\u2019s rugarch\npackage, can \ufb01t the linear regression model with ARMA+GARCH distur-\nbances in one step. Another solution is to adjust or correct the estimated\ncovariance matrix of the regression coe\ufb03cients, via the HAC estimator from\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3. The model considered was",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.2, by using the NeweyWest() function from the R package sandwich.\nHowever, if such software is not available, then a three-step estimation method\nis the following:\n1. estimate the parameters in (14.19) by ordinary least-squares;\n2. \ufb01t model (14.20)\u2013(14.22) to the ordinary least-squares residuals;\n3. reestimate the parameters in (14.19) by weighted least-squares with\n   weights equal to the reciprocals of the conditional variances from step 2.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "2, by using the NeweyWest() function from the R package sandwich.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.4. Regression analysis with ARMA+GARCH errors of the Nelson\u2013\nPlosser data\n\n    In Example 9.9, we saw that a parsimonious model for the yearly log ret-\nurns on the stock index diff(log(sp)) used diff(log(ip)) and diff(bnd)\nas predictors. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.4",
      "section_title": "Regression analysis with ARMA+GARCH errors of the Nelson\u2013",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.9 contains ACF plots of the residuals [panel (a)] and\n\f                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.9",
      "section_title": "contains ACF plots of the residuals [panel (a)] and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.12 Linear Regression with ARMA+GARCH Errors         425\n\nsquared residuals [panel (b)]. Externally studentized residuals were used, but\nthe plots for the raw residuals are similar. There is some autocorrelation in\nboth the residuals and squared residuals.\n21 nelsonplosser = read.csv(\"nelsonplosser.csv\", header = TRUE)\n22 new_np = na.omit(nelsonplosser)\n23 attach(new_np)\n\n24 fit.lm1 = lm(diff(log(sp)) ~ diff(log(ip)) + diff(bnd))\n\n25 summary(fit.lm1)\n\n     Coefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n     (Intercept)    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.12",
      "section_title": "Linear Regression with ARMA+GARCH Errors         425",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01657    0.02100   0.789 0.433316\n     diff(log(ip)) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01657",
      "section_title": "0.02100   0.789 0.433316",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.69748     0.16834   4.143 0.000113 ***\n     diff(bnd)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.69748",
      "section_title": "0.16834   4.143 0.000113 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.13224    0.06225 -2.124 0.037920 *\n     ---\n     Signif. codes: 0 *** ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.13224",
      "section_title": "0.06225 -2.124 0.037920 *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 ** 0.01 * 0.05 . 0.1    1\n\n     Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "** 0.01 * 0.05 . 0.1    1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1509 on 58 degrees of freedom\n     Multiple R-squared: 0.3087,Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1509",
      "section_title": "on 58 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2848\n     F-statistic: 12.95 on 2 and 58 DF, p-value: 2.244e-05\n    The auto.arima() function from R\u2019s forecast package selected an MA(1)\nmodel [i.e., ARIMA(0,0,1)] for the residuals. Next an MA(1)+ARCH(1) model\nwas \ufb01t to the regression model\u2019s raw residuals. Sample ACF plots of the stan-\ndardized residuals from the MA(1)+ARCH(1) model are in Fig. 14.9c and d.\nOne sees essentially no short-term autocorrelation in the ARMA+GARCH\nstandardized or squared standardized residuals, which indicates that the\nARMA+GARCH model accounts for the observed dependence in the regres-\nsion residuals satisfactorily. A normal plot showed that the standardized resid-\nuals are close to normally distributed, which is not unexpected for yearly log\nreturns.\n    Finally, the linear model was re\ufb01t with the reciprocals of the conditional\nvariances as weights. The estimated regression coe\ufb03cients are given below\nalong with their standard errors and p-values.\n26 fit.lm3 = lm(diff(log(sp)) ~ diff(log(ip)) + diff(bnd),\n27              weights = 1/sigma.arch^2)\n28 summary(fit.lm3)\n\n\n     Coefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n     (Intercept)    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2848",
      "section_title": "F-statistic: 12.95 on 2 and 58 DF, p-value: 2.244e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03216    0.02052   1.567 0.12263\n     diff(log(ip)) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03216",
      "section_title": "0.02052   1.567 0.12263",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.55464     0.16942   3.274 0.00181 **\n     diff(bnd)     -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.55464",
      "section_title": "0.16942   3.274 0.00181 **",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12215    0.05827 -2.096 0.04051 *\n     ---\n     Signif. codes: 0 *** ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12215",
      "section_title": "0.05827 -2.096 0.04051 *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 ** 0.01 * 0.05 . 0.1    1\n\n     Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "** 0.01 * 0.05 . 0.1    1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.071 on 57 degrees of freedom\n     Multiple R-squared: 0.2416,Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.071",
      "section_title": "on 57 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2149\n     F-statistic: 9.077 on 2 and 57 DF, p-value: 0.0003783\n\f426         14 GARCH Models\n\n    There are no striking di\ufb00erences between these results and the unweighted\n\ufb01t in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2149",
      "section_title": "F-statistic: 9.077 on 2 and 57 DF, p-value: 0.0003783",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.9. In this situation, the main reason for using the GARCH\n\n\n      a                regression: residuals   b            regression: squared residuals\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.9",
      "section_title": "In this situation, the main reason for using the GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                     1.0\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n      ACF\n\n\n\n\n                                               ACF\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n            \u22120.2\n\n\n\n\n                                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                   0        5     10     15                   0      5     10     15\n                                Lag                                      Lag\n\n\n      c                MA/ARCH: residuals      d            MA/ARCH: squared residuals\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0        5     10     15                   0      5     10     15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                     1.0\n      ACF\n\n\n\n\n                                               ACF\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                     0.4\n            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                     \u22120.2\n\n\n\n\n                   0        5     10     15                   0      5     10     15\n                                Lag                                      Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.9. (a) Sample ACF of the externally studentized residuals and (b) their\nsquared values, from a linear model; (c) Sample ACF of the standardized residu-\nals and (d) their squared values, from an MA(1)+ARCH(1) \ufb01t to the regression\nresiduals.\n\n\nmodel for the residuals would be in providing more accurate prediction inter-\nvals if the model were to be used for forecasting; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.9",
      "section_title": "(a) Sample ACF of the externally studentized residuals and (b) their",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13.        \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13 Forecasting ARMA+GARCH Processes\n\nForecasting ARMA+GARCH processes is in one way similar to forecasting\nARMA processes\u2014point estimates, e.g., forecasts of the conditional mean,\nare the same because a GARCH process is weak white noise. What di\ufb00ers\nbetween forecasting ARMA+GARCH and ARMA processes is the behavior\nof the prediction intervals. In times of high volatility, prediction intervals using\nan ARMA+GARCH model will widen to take into account the higher amount\nof uncertainty. Similarly, the prediction intervals will narrow in times of lower\nvolatility. Prediction intervals using an ARMA model without conditional\nheteroskedasticity cannot adapt in this way.\n    To illustrate, we will compare the prediction of a Gaussian white noise pro-\ncess and the prediction of a GARCH(1,1) process with Gaussian innovations.\n\f                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "Forecasting ARMA+GARCH Processes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13 Forecasting ARMA+GARCH Processes   427\n\nBoth have an ARMA(0,0) model for the conditional mean so their forecasts\nare equal to the marginal mean, which will be called \u03bc. For Gaussian white\nnoise, the prediction limits are \u03bc \u00b1 z\u03b1/2 \u03c3, where \u03c3 is the marginal standard\ndeviation. For a GARCH(1,1) process {Yt }, the prediction limits at time origin\nn for h-steps ahead forecasting are \u03bc \u00b1 z\u03b1/2 \u03c3n+h|n where \u03c3n+h|n is the condi-\ntional standard deviation of Yn+h given the information available at time n.\nAs h increases, \u03c3n+h|n converges to \u03c3, so for long lead times the prediction\nintervals for the two models are similar. For shorter lead times, however, the\nprediction limits can be quite di\ufb00erent.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "Forecasting ARMA+GARCH Processes   427",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.5. Forecasting BMW log returns\n\n    In this example, we will return to the daily BMW stock log returns\nused in several earlier examples. We have seen in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.5",
      "section_title": "Forecasting BMW log returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2 that an\nAR(1)+GARCH(1,1) model \ufb01ts the returns well. Also, the estimated AR(1)\ncoe\ufb03cient is small, less than ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "that an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. Therefore, it is reasonable to use a GARCH\n(1,1) model for forecasting.\n\n                                            Forecasting BMW returns\n                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "Therefore, it is reasonable to use a GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20\n\n\n\n\n                                      11\u221215\u221287\n                                      9\u221218\u221288\n                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.20",
      "section_title": "11\u221215\u221287",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n        log return\n                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "log return",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n                     \u22120.10\n\n\n\n\n                             Jan 01         Jan 03       Jan 01       Jan 01\n                              1986           1988         1990         1992\n                                                      year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "\u22120.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.10. Prediction limits for forecasting daily BMW stock log returns from two\ndi\ufb00erent time origins.\n   Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.10",
      "section_title": "Prediction limits for forecasting daily BMW stock log returns from two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.10 plots the returns from 1986 until 1992. Forecast limits are also\nshown for two time origins, November 15, 1987 and September 18, 1988. At\nthe \ufb01rst time origin, which is soon after Black Monday, the markets were very\nvolatile. The forecast limits are wide initially but narrow as the conditional\nstandard deviation converges downward to the marginal standard deviation.\nAt the second time origin, the markets were less volatile than usual and the\n\f428    14 GARCH Models\n\nprediction intervals are narrow initially but then widen. In theory, both sets\nof prediction limits should converge to the same values, \u03bc \u00b1 z\u03b1/2 \u03c3 where \u03c3\nis the marginal standard deviation for a stationary process. In this example,\nthey do not quite converge to each other because the estimates of \u03c3 di\ufb00er\nbetween the two time origins.                                               \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.10",
      "section_title": "plots the returns from 1986 until 1992. Forecast limits are also",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes\nFinancial asset returns tend to move together over time, as do their respective\nvolatilities, across both assets and markets. Modeling a time-varying condi-\ntional covariance matrix, or volatility matrix, is important in many \ufb01nan-\ncial applications, including asset pricing, hedging, portfolio selection, and risk\nmanagement.\n    Multivariate volatility modeling has major challenges to overcome. First,\nthe curse of dimensionality; there are d(d + 1)/2 variances and covariances for\na d-dimensional process, e.g., 45 for d = 9, all of which may vary over time.\nFurther, unlike returns, all of these variances and covariances are unobserved,\nor latent. Many parameterizations for the evolution of the volatility matrix\nuse such a large number of parameters that estimation becomes infeasible for\nd > 10. In addition to empirical adequacy (i.e., goodness of \ufb01t of the model\nto the data), ease and feasibility of estimation are important considerations.\n    Analogous to positivity constraints in univariate GARCH models, a well-\nde\ufb01ned multivariate volatility matrix process must be positive-de\ufb01nite at each\ntime point, and model-based forecasts should as well. From a practical per-\nspective, a well-de\ufb01ned inverse of a volatility matrix is frequently needed in\napplications. Additionally, a positive conditional variance estimate for a port-\nfolio\u2019s return, which are a linear combination of asset returns, is essential;\nfortunately, this is guaranteed by positive de\ufb01niteness.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "10. In addition to empirical adequacy (i.e., goodness of \ufb01t of the model\nto the data), ease and feasibility of estimation are important considerations.\n    Analogous to positivity constraints in univariate GARCH models, a well-\nde\ufb01ned multivariate volatility matrix process must be positive-de\ufb01nite at each\ntime point, and model-based forecasts should as well. From a practical per-\nspective, a well-de\ufb01ned inverse of a volatility matrix is frequently needed in\napplications. Additionally, a positive conditional variance estimate for a port-\nfolio\u2019s return, which are a linear combination of asset returns, is essential;\nfortunately, this is guaranteed by positive de\ufb01niteness.",
        "start": 834,
        "end": 1516
      }
    ]
  },
  {
    "content": "14.14.1 Multivariate Conditional Heteroscedasticity\nFigures 14.11a and b are time series plots of daily returns (in percentage)\nfor IBM stock and the Center for Research in Security Prices (CRSP) value-\nweighted index, including dividends, from January 3, 1989 to December 31,\n1998, respectively. The data are from the Ecdat package in R. Each series\nclearly exhibits volatility clustering. Let Y t denote the vector time series of\nthese returns.\n29 data(CRSPday, package=\"Ecdat\")\n30 CRSPday = ts(CRSPday, start = c(1989, 1), frequency = 253)\n31 ibm  = CRSPday[,5] * 100\n32 crsp = CRSPday[,7] * 100\n\n33 Y = cbind(ibm, crsp)\n\n34 par(mfrow = c(2,1))\n\n35 plot(Y[,1], type=\u2019l\u2019, xlab=\"year\", ylab=\"return (%)\", main=\"(a)\")\n\n36 plot(Y[,2], type=\u2019l\u2019, xlab=\"year\", ylab=\"return (%)\", main=\"(b)\")\n\f                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "1 Multivariate Conditional Heteroscedasticity",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes   429\n\n      a\n\n\n                   10\n      return (%)\n                   5\n                   0\n                   \u221210 \u22125\n\n\n\n\n                                1990   1992        1994      1996       1998\n                                                   year\n\n      b\n                   4\n                   2\n      return (%)\n                   \u22126 \u22124 \u22122 0\n\n\n\n\n                                1990   1992        1994      1996       1998\n                                                   year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes   429",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.11. Daily returns (in percentage) for (a) IBM stock and (b) the CRSP\nvalue-weighted index, including dividends.\n\n\n    Figures 14.12a and b are the sample ACF plots for the IBM stock and\nCRSP index returns, respectively. There is some evidence of minor serial cor-\nrelation at low lags. Next, we consider the lead-lag linear relationship between\npairs of returns. Figure 14.12c is the sample cross-correlation function (CCF)\nbetween IBM and CRSP. The lag zero estimate for contemporaneous cor-\nrelation is approximately ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.11",
      "section_title": "Daily returns (in percentage) for (a) IBM stock and (b) the CRSP",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.49. There is also some evidence of minor cross-\ncorrelation at low lags.\n37 layout(rbind(c(1,2), c(3,3)),widths=c(1,1,2),heights=c(1,1))\n38 acf(as.numeric(Y[,1]), ylim=c(-0.1,0.1), main=\"(a)\")\n39 acf(as.numeric(Y[,2]), ylim=c(-0.1,0.1), main=\"(b)\")\n\n40 ccf(as.numeric(Y[,1]),as.numeric(Y[,2]),\n\n41     type=c(\"correlation\"), main=\"(c)\", ylab=\"CCF\", lag=20)\n42 cor(ibm, crsp)\n\n\n     [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.49",
      "section_title": "There is also some evidence of minor cross-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4863639\n\n    The multivariate Ljung-Box test (see Sect. 13.4.3) is applied to simulta-\nneously test that the \ufb01rst K auto-correlations, as well as the lagged cross-\ncorrelations, are all zero. The multivariate Ljung-Box test statistic at lag \ufb01ve\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4863639",
      "section_title": "The multivariate Ljung-Box test (see Sect. 13.4.3) is applied to simulta-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "50.15. The associate p-value is very close to zero, which provides strong\nevidence to reject the null hypothesis and indicates there is signi\ufb01cant serial\ncorrelation in the vector process.\n\f430                 14 GARCH Models\n\n43    source(\"SDAFE2.R\")\n44    mLjungBox(Y, 5)\n\n        K Q(K) d.f. p-value\n      1 5 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "50.15",
      "section_title": "The associate p-value is very close to zero, which provides strong",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "50.15  20       0\n\n\na                                                              b\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "50.15",
      "section_title": "20       0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n\n\n\n\n                                                                     0.10\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                                                                     0.05\nACF\n\n\n\n\n                                                               ACF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                                                                     0.00\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10 \u22120.05\n\n\n\n\n                    0     5   10   15     20    25   30   35         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10 \u22120.05   0   5   10        15     20   25   30        35\n                                    Lag                                                               Lag\n\n\nc\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "\u22120.05   0   5   10        15     20   25   30        35",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n      0.4\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\nCCF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "CCF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.1\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                    \u221220                   \u221210                   0                               10                         20\n                                                               Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u221220                   \u221210                   0                               10                         20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.12. ACFs for (a) the IBM stock and (b) CRSP index returns; (c) CCF\nbetween IBM and CRSP returns.\n\n\n    For simplicity, we use ordinary least squares to \ufb01t a VAR(1) model (see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.12",
      "section_title": "ACFs for (a) the IBM stock and (b) CRSP index returns; (c) CCF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.4.4) to remove the minor serial correlation and focus on the condi-\ntional variance and covariance. Let a  \u0002 t denote the estimated residuals from\n                \u0002 t is an estimate of the innovation process at , which is des-\nthe regression; a\ncribed more fully below. The multivariate Ljung-Box test statistic at lag \ufb01ve\nis now 16.21, which has an approximate p-value of 0.704, indicating there is\nno signi\ufb01cant serial correlation in the vector residual process.\n45 fit.AR1 = ar(Y, aic = FALSE, order.max=1)\n46 A = fit.AR1$resid[-1,]\n47 mLjungBox(A, 5)\n\n\n        K Q(K) d.f. p-value\n      1 5 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.4",
      "section_title": "4) to remove the minor serial correlation and focus on the condi-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.21  20   0.704\n\f                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.21",
      "section_title": "20   0.704",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes         431\n\n     Although the residual series at is serially uncorrelated, Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes         431",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13 shows\nit is not an independent process. Figures 14.13a and b are sample ACF plots\nfor the squared residual series \u0002a2it . They both show substantial positive aut-\nocorrelation because of the volatility clustering. Figure 14.13c is the sample\nCCF for the squared series; this \ufb01gure shows there is a dynamic relationship\nbetween the squared series at low lags. Figure 14.13d is the sample ACF for\n                   a1t \u0002\nthe product series \u0002   a2t and shows that there is also evidence of positive aut-\nocorrelation in the conditional covariance series. The multivariate volatility\nmodels described below attempt to account for these forms of dependence\nexhibited in the vector residual series.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "shows",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14.2 Basic Setting\n\nLet Y t = (Y1,t , . . . , Yd,t )\u0003 denote a d-dimensional vector process and let Ft\ndenote the information set at time index t, generated by Y t , Y t\u22121 , . . .. We\nmay partition the process as\n\n                                 Y t = \u03bc t + at ,                           (14.23)\n\nin which \u03bct = E(Y t |Ft\u22121 ) is the conditional mean vector at time index t,\nand {at } is the mean zero weak white noise innovation vector process with\nunconditional covariance matrix \u03a3a = Cov(at ). Let\n\n                     \u03a3t = Cov(at |Ft\u22121 ) = Cov(Y t |Ft\u22121 )                  (14.24)\n\ndenote the conditional covariance or volatility matrix at time index t. Multi-\nvariate time series modeling is primarily concerned with the time evolutions of\n\u03bct and \u03a3 t , the conditional mean and conditional covariance matrix. For a sta-\ntionary process, the unconditional mean and unconditional covariance matrix\nare constant, even though the conditional mean and conditional covariance\nmatrix may be time-varying.\n                        \u0017p we assume that \u03bct follows a stationary VAR(p)\n    Throughout this section\nmodel with \u03bct = \u03bc + \u0002=1 \u03a6\u0002 (Y t\u2212\u0002 \u2212 \u03bc), where p is a non-negative integer,\n\u03bc is the d \u00d7 1 unconditional mean vector, and the \u03a6\u0002 are d \u00d7 d coe\ufb03cient\nmatrices, respectively. Recall, the residual series considered in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "2 Basic Setting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13 were\nfrom a VAR model with p = 1.\n    The relationship between the innovation process and the volatility process\nis de\ufb01ned by\n                                 1/2        iid\n                         at = \u03a3 t t ,     t \u223c F (0, I d ),                (14.25)\n           1/2                                                        1/2    1/2\nin which \u03a3 t is a symmetric matrix square-root of \u03a3 t , such that \u03a3 t \u03a3 t =\n\u03a3 t . The iid white noise t are standardized innovations from a multivariate\ndistribution F with mean zero and a covariance matrix equal to the iden-\ntity. The models detailed below describe dynamic evolutions for the volatility\nmatrix \u03a3 t .\n\f432           14 GARCH Models\n\na                                                                 b\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "were",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n\n\n\n                                                                        0.3\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "0.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                        0.2\nACF\n\n\n\n\n                                                                  ACF\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                                                        0.1\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                        0.0\n               0    5    10   15       20   25    30   35                     0   5   10   15     20   25   30   35\n                               Lag                                                          Lag\n\n\nc                                                                 d\n\n\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n      0.25\n\n\n\n\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.15\nCCF\n\n\n\n\n                                                                  ACF\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n      0.05\n\n\n\n\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n      \u22120.05\n\n\n\n\n              \u221220       \u221210        0         10        20                     0   5   10   15     20   25   30   35\n                               Lag                                                          Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.13. ACFs of squared residuals from a VAR(1) model for (a) IBM and (b)\nCRSP; (c) CCF between the squared residuals; (d) ACF for the product of the\nresiduals.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.13",
      "section_title": "ACFs of squared residuals from a VAR(1) model for (a) IBM and (b)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14.3 Exponentially Weighted Moving Average\n(EWMA) Model\nThe simplest matrix generalization of a univariate volatility model is the expo-\nnentially weighted moving average (EWMA) model. It is indexed by a single\nparameter \u03bb \u2208 (0, 1), and is de\ufb01ned by the recursion\n                                       \u03a3 t = (1 \u2212 \u03bb)at\u22121 a\u0003t\u22121 + \u03bb\u03a3 t\u22121                                     (14.26)\n                                                            \u221e\n                                            = (1 \u2212 \u03bb)             \u03bb\u0002\u22121 at\u2212\u0002 a\u0003t\u2212\u0002 .\n                                                            \u0002=1\n\nWhen the recursion in (14.26) is initialized with a positive-de\ufb01nite (p.d.)\nmatrix the sequence remains p.d. This single parameter model is simple to\nestimate regardless of the dimension, with large values of \u03bb indicating high\npersistence in the volatility process. However, the dynamics can be too re-\nstrictive in practice, since the component-wise evolutions all have the same\ndiscounting factor (i.e., persistence parameter) \u03bb.\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "3 Exponentially Weighted Moving Average",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes      433\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes      433",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 shows the in-sample \ufb01tted EWMA model for a      \u0002 t assuming a\nmultivariate standard normal distribution for t and using conditional maxi-\nmum likelihood estimation. The estimated conditional standard deviations are\nshown in (a) and (d), and the conditional covariances and implied conditional\ncorrelations are shown in (b) and (c), respectively. The persistence parameter\n\u03bb was estimated as ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "shows the in-sample \ufb01tted EWMA model for a      \u0002 t assuming a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.985. Estimation and Fig. 14.14 were calculated using the\nfollowing commands in R.\n48 source(\"SDAFE2.R\")\n49 EWMA.param = est.ewma(lambda.0=0.95, innov=A)\n50 EWMA.Sigma = sigma.ewma(lambda=EWMA.param$lambda.hat, innov=A)\n\n51 par(mfrow = c(2,2))\n\n52 plot(ts(EWMA.Sigma[1,1,]^.5, start = c(1989, 1), frequency = 253),\n\n53      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n54      main = expression(paste(\"(a) \", hat(sigma)[\"1,t\"])))\n55 plot(ts(EWMA.Sigma[1,2,], start = c(1989, 1), frequency = 253),\n\n56      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n57      main = expression(paste(\"(b) \", hat(sigma)[\"12,t\"])))\n58 plot(ts(EWMA.Sigma[1,2,]/(sqrt(EWMA.Sigma[1,1,]* EWMA.Sigma[2,2,])),\n\n59         start = c(1989, 1), frequency = 253),\n60      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n61      main = expression(paste(\"(c) \", hat(rho)[\"12,t\"])))\n62 points(ts(mvwindow.cor(A[,1],A[,2], win = 126)$correlation,\n\n63           start = c(1989, 1), frequency = 253),\n64        type = \u2019l\u2019, col = 2, lty = 2, lwd=2)\n65 plot(ts(EWMA.Sigma[2,2,]^.5, start = c(1989, 1), frequency = 253),\n\n66      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n67      main = expression(paste(\"(d) \", hat(sigma)[\"2,t\"])))\n68 EWMA.param$lambda.hat\n\n\n     [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.985",
      "section_title": "Estimation and Fig. 14.14 were calculated using the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.985046\n\n\n14.14.4 Orthogonal GARCH Models\n\nSeveral factor and orthogonal models have been proposed to reduce the num-\nber of parameters and parameter constraints by imposing a common dynamic\nstructure on the elements of the volatility matrix. The orthogonal GARCH\n(O-GARCH) model of Alexander (2001) is among the most popular because\nof its simplicity. It is assumed that the innovations at can be decomposed\ninto orthogonal components z t via a linear transformation U . This is done\nin conjunction with principal component analysis (PCA, see Sect. 18.2) as\nfollows. Let O be the matrix of eigenvectors and \u039b the diagonal matrix of the\ncorresponding eigenvalues of \u03a3a . Then, take U = \u039b\u22121/2 O \u0003 , and let\n\n                                  z t = U at .\n\nThe components are constructed such that Cov(z t ) = I d . The sample estimate\nof \u03a3a is typically used to estimate U.\n\f434                                14 GARCH Models\n\n                                                ^\n                                                \u03c3                                                                          ^\n                                                                                                                           \u03c3\na                                                 1,t                          b                                             12,t\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.985046",
      "section_title": "14.14.4 Orthogonal GARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n2.5\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "2.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n2.0\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n1.5\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                                                               0.5\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                  1990   1992   1994     1996   1998                                         1990   1992   1994     1996   1998\n                                                year                                                                       year\n\n\nc                                               \u03c1^12,t                         d                                           ^\n                                                                                                                           \u03c3 2,t\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1990   1992   1994     1996   1998                                         1990   1992   1994     1996   1998",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\n\n\n\n\n                                                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "0.2 0.3 0.4 0.5 0.6 0.7 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 0.6 0.8 1.0 1.2 1.4 1.6\n\n\n\n\n                                  1990   1992   1994     1996   1998                                         1990   1992   1994     1996   1998\n                                                year                                                                       year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.6 0.8 1.0 1.2 1.4 1.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14. A \ufb01tted EWMA model with \u03bb = 0.985. The red line in (c) is the sample\ncorrelation estimate over the previous six months for comparison.\n\n\n    Next, univariate GARCH(1,1) models are individually \ufb01t to each orthog-\nonal component to estimate the conditional covariance V t = Cov(z t |Ft\u22121 ).\nLet\n                                                          2             2           2\n                                                         vit = \u03c9i + \u03b1i zi,t\u22121 + \u03b2i vi,t\u22121\n                                                                     2\n                                                         V t = diag{v1,t            2\n                                                                         , . . . , vd,t }\n                                                                                       \u0003\n                                                         \u03a3t = U \u22121 V t U \u22121 .\n\nIn summary, a linear transformation U is estimated, using PCA, such that the\ncomponents of z t = U at have unconditional correlation approximately equal\nto zero. It is then also assumed that the conditional correlations of z t are\nalso zero; however, this is not at all assured to be true. Under this additional\nstronger assumption, V t , the conditional covariance matrix for z t , is diagonal.\nFor simplicity, univariate models are then \ufb01t to model the conditional variance\n 2\nvit for each component of z t .\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "A \ufb01tted EWMA model with \u03bb = 0.985. The red line in (c) is the sample",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes               435\n\na                    ^\n                     \u03c3 1,t\n                                                 b                    ^\n                                                                      \u03c3 12,t\n\n\n\n\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes               435",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n 2.5\n\n\n\n\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n 2.0\n\n\n\n\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n 1.5\n\n\n\n\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n 1.0\n\n\n\n\n       1990   1992   1994     1996   1998               1990   1992   1994     1996   1998\n                     year                                             year\n\n\nc                    ^\n                     \u03c1 12,t\n                                                 d                    ^\n                                                                      \u03c3 2,t\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n 0.8\n\n\n\n\n                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n 0.6\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                  1.0\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                  0.5\n\n\n\n\n       1990   1992   1994     1996   1998               1990   1992   1994     1996   1998\n                     year                                             year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.15. A \ufb01tted \ufb01rst order orthogonal GARCH model with (\u03c91 , \u03b11 , \u03b21 )\u0002 =\n(0.0038, 0.0212, 0.9758)\u0002 , (\u03c92 , \u03b12 , \u03b22 )\u0002 = (0.0375, 0.0711, 0.8913)\u0002 , and U \u22121 =\n((1.7278, 0.2706)\u0002 , (0.2706, 0.7241)\u0002 ). The red line in (c) is the sample correlation\nestimate over the previous six months for comparison.\n\n\n    The main drawback of this model is that the orthogonal components are\nuncorrelated unconditionally, but they may still be conditionally correlated.\nThe O-GARCH model implicitly assumes the conditional correlations for z t\nare zero. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.15",
      "section_title": "A \ufb01tted \ufb01rst order orthogonal GARCH model with (\u03c91 , \u03b11 , \u03b21 )\u0002 =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.15 shows a \ufb01tted O-GARCH model for a       \u0002 t using PCA\nfollowed by univariate conditional maximum likelihood estimation. The est-\nimated conditional standard deviations are shown in (a) and (d), and the\nconditional covariances and conditional correlations are shown in (b) and (c),\nrespectively. The implied conditional correlations do not appear adequate for\nthis \ufb01tted model compared to the sample correlation estimate over the previ-\nous six months (used as a proxy for the conditional correlation process).\n\f436     14 GARCH Models\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.15",
      "section_title": "shows a \ufb01tted O-GARCH model for a       \u0002 t using PCA",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14.5 Dynamic Orthogonal Component (DOC) Models\n\nTo properly apply univariate modeling after estimating a linear transforma-\ntion in the spirit of the O-GARCH model above, the resulting component\nprocesses must not only be orthogonal contemporaneously, the conditional\ncorrelations must also be zero. Additionally, the lagged cross-correlations for\nthe squared components must also be zero. In Matteson and Tsay (2011),\nif the components of a time series st satisfy these conditions, then they are\ncalled dynamic orthogonal components (DOCs) in volatility.\n     Let st = (s1,t , . . . , sd,t )\u0003 denote a vector time series of DOCs. Without\nloss of generality, st is assumed to be standardized such that E(si,t ) = 0\nand Var(si,t ) = 1 for i = 1, . . . , d. A Ljung-Box type statistic, de\ufb01ned below,\nis used to test for the existence of DOCs in volatility. Including lag zero in\nthe test implies that the pairwise product processes among stationary DOCs\nsi,t sj,t has zero serial correlation since the Cauchy-Schwarz inequality gives\n\n          |Cov(si,t sj,t , si,t\u2212h sj,t\u2212h )| \u2264 Var(si,t sj,t ) = E(s2i s2j ),            (14.27)\n\nand E(si,t sj,t ) = E(si,t\u2212h sj,t\u2212h ) = 0 by the assumption of a DOC model.\n   Let \u03c1s2i ,s2j (h) = Corr(s2i,t , s2j,t\u2212h ). The joint lag-K null and alternative hy-\npotheses to test for the existence of DOCs in volatility are\n\n                 H0 : \u03c1s2i ,s2j (h) = 0 for all i = j, h = 0, . . . , K\n                 HA : \u03c1s2i ,s2j (h) = 0 for some i = j, h = 0, . . . , K.\n\nThe corresponding Ljung-Box type test statistic is\n                                                       K\n  Q0d (s2 ; K) = n         \u03c1s2i ,s2j (0)2 + n(n + 2)             \u03c1s2i ,s2j (h)2 /(n \u2212 h). (14.28)\n                     i<j                               h=1 i=j\n\n\nUnder H0 , Q0d (s2 ; K) is asymptotically distributed as Chi-squared with d(d \u2212\n1)/2 + Kd(d \u2212 1) degrees of freedom. The null hypothesis is rejected for a\nlarge value of Q0d . When H0 is rejected, one must seek an alternative modeling\nprocedure.\n      As expected from Fig. 14.13, the DOCs in volatility hypothesis is rejected\nfor the VAR(1) residuals. The test statistic is Q02 (\u0002 a2 , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "5 Dynamic Orthogonal Component (DOC) Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "356.926 with a\np-value near zero. DOCs in volatility is also rejected for the principal compo-\nnents used in the O-GARCH model, the test statistic is Q02 (z 2 , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "356.926",
      "section_title": "with a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "135.492\nwith a p-value near zero. Starting with the uncorrelated principal components\nz t , Matteson and Tsay (2011) propose estimating an orthogonal matrix W\nsuch that the components st = W z t are as close to DOCs in volatility as\npossible. This is done by minimizing a reweighted version of the Ljung-Box\ntype test statistic (14.28), with respect to the separating matrix W . The null\nhypothesis of DOCs in volatility is accepted for the estimated components st ,\nwith Q02 (s2 , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "135.492",
      "section_title": "with a p-value near zero. Starting with the uncorrelated principal components",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.845 which has a p-value approximately equal to 0.727.\n\f                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.845",
      "section_title": "which has a p-value approximately equal to 0.727.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes   437\n\n   After DOCs are identi\ufb01ed, a univariate volatility model is considered for\n              2\neach process vi,t = Var(si,t |Ft\u22121 ). For example, the following model was \ufb01t\n                                           1/2\n                   a t = M st = M V t            t,\n                                                         iid\n                              2\n                  V t = diag{v1,t            2\n                                  , . . . , vd,t },   \u0017it \u223c t\u03bdi (0, 1)\n                   2\n                  vi,t = \u03c9i + \u03b1i s2i,t\u22121 + \u03b2i vi,t\u22121\n                                               2\n\n                  \u03a3t = M V t M \u0003 ,\n\nin which t\u03bdi (0, 1) denotes the standardized Student-t distribution with tail-\n                                            2\nindex \u03bdi . Each \u03a3t is positive-de\ufb01nite if vi,t > 0 for all components. The\nfundamental motivation is that empirically the dynamics of at can often be\nwell approximated by an invertible linear combination of DOCs at = M st ,\nin which M = U \u22121 W \u0003 by de\ufb01nition.\n    In summary, U is estimated by PCA to uncorrelate at , W is estimated to\nminimize a reweighted version of (14.28) de\ufb01ned above (giving more weight\nto lower lags). The matrices U and W are combined to estimate DOCs st , of\nwhich univariate volatility modeling may then be appropriately applied. This\napproach allows modeling of a d-dimensional multivariate volatility process\nwith d univariate volatility models, while greatly reducing both the number\nof parameters and the computational cost of estimation, and at the same time\nmaintaining adequate empirical performance.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes   437",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 for all components. The\nfundamental motivation is that empirically the dynamics of at can often be\nwell approximated by an invertible linear combination of DOCs at = M st ,\nin which M = U \u22121 W \u0003 by de\ufb01nition.\n    In summary, U is estimated by PCA to uncorrelate at , W is estimated to\nminimize a reweighted version of (14.28) de\ufb01ned above (giving more weight\nto lower lags). The matrices U and W are combined to estimate DOCs st , of\nwhich univariate volatility modeling may then be appropriately applied. This\napproach allows modeling of a d-dimensional multivariate volatility process\nwith d univariate volatility models, while greatly reducing both the number\nof parameters and the computational cost of estimation, and at the same time\nmaintaining adequate empirical performance.\n    Figure",
        "start": 855,
        "end": 1654
      }
    ]
  },
  {
    "content": "14.16 shows a \ufb01tted DOCs in volatility GARCH model for a  \u0002 t using\ngeneralized decorrelation followed by univariate conditional maximum likeli-\nhood estimation. The estimated conditional standard deviations are shown\nin (a) and (d), and the conditional covariances and implied correlations are\nshown in (b) and (c), respectively. Unlike the O-GARCH \ufb01t, the implied\nconditional correlations appear adequate compared to the rolling estimator.\nEstimation of the O-GARCH and DOC models and Figs. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "shows a \ufb01tted DOCs in volatility GARCH model for a  \u0002 t using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.15 and 14.16\nwere calculated using the following commands in R.\n69   source(\"SDAFE2.R\")\n70   DOC.fit = doc.garch(E = A, L = 4., c = 2.25, theta.ini = NULL)\n\n71 par(mfrow = c(2,2)) # O-GARCH\n72 plot(ts(DOC.fit$Sigma.pca[1,1,]^.5, start=c(1989,1), frequency=253),\n73      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n74      main = expression(paste(\"(a) \", hat(sigma)[\"1,t\"])))\n75 plot(ts(DOC.fit$Sigma.pca[2,1,], start=c(1989,1), frequency=253),\n\n76      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n77      main = expression(paste(\"(b) \", hat(sigma)[\"12,t\"])))\n78 plot(ts(DOC.fit$Sigma.pca[2,1,]/(sqrt(DOC.fit$Sigma.pca[1,1,]*\n\n79                                       DOC.fit$Sigma.pca[2,2,])),\n80         start=c(1989,1), frequency=253),\n81      type = \u2019l\u2019, xlab = \"year\", ylab = NULL, ylim = c(0.1,0.9),\n82      main = expression(paste(\"(c) \", hat(rho)[\"12,t\"])))\n83 points(ts(mvwindow.cor(A[,1],A[,2], win = 126)$correlation,\n\n84           start = c(1989, 1), frequency = 253),\n\f438     14 GARCH Models\n\n85        type = \u2019l\u2019, col = 2, lty = 2, lwd = 2)\n86 plot(ts(DOC.fit$Sigma.pca[2,2,]^.5, start=c(1989,1), frequency=253),\n87      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n88      main = expression(paste(\"(d) \", hat(sigma)[\"2,t\"])))\n\n\n\na                   ^                          b                   ^\n                                                                   \u03c3\n                    \u03c3 1,t                                            12,t\n\n\n\n\n                                               5\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.15",
      "section_title": "and 14.16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n\n\n\n\n                                               4\n                                               3\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                               2\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n\n\n\n                                               1\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n      1990   1992   1994     1996   1998             1990   1992   1994     1996   1998\n                    year                                           year\n\n\nc                   ^\n                    \u03c1                          d                   ^\n                                                                   \u03c3\n                      12,t                                           2,t\n                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1990   1992   1994     1996   1998             1990   1992   1994     1996   1998",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n0.8\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                               1.5\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                               1.0\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                               0.5\n\n\n\n\n      1990   1992   1994     1996   1998             1990   1992   1994     1996   1998\n                    year                                           year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16. A \ufb01tted \ufb01rst order DOCs in volatility GARCH model\nwith (\u03c91 , \u03b11 , \u03b21 , \u03bd1 )\u0002 = (0.0049, 0.0256, 0.9703, 4.3131)\u0002 , (\u03c92 , \u03b12 , \u03b22 , \u03bd2 )\u0002 =\n(0.0091, 0.0475, 0.9446, 5.0297)\u0002 , and M = ((1.5350, 0.838)\u0002 , (0.0103, 0.7730)\u0002 ). The\nred line in (c) is the sample correlation estimate over the previous six months for\ncomparison.\n\n89 par(mfrow = c(2,2)) # DOCs in volatility\n90 plot(ts(DOC.fit$Sigma.doc[1,1,]^.5, start=c(1989,1), frequency=253),\n91      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n92      main = expression(paste(\"(a) \", hat(sigma)[\"1,t\"])))\n93 plot(ts(DOC.fit$Sigma.doc[2,1,], start=c(1989,1), frequency=253),\n\n94      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n95      main = expression(paste(\"(b) \", hat(sigma)[\"12,t\"])))\n96 plot(ts(DOC.fit$Sigma.doc[2,1,]/(sqrt(DOC.fit$Sigma.doc[1,1,]*\n\f                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "A \ufb01tted \ufb01rst order DOCs in volatility GARCH model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes     439\n\n 97                                       DOC.fit$Sigma.doc[2,2,])),\n 98         start=c(1989,1), frequency=253),\n 99      type = \u2019l\u2019, xlab = \"year\", ylab = NULL, ylim = c(0.1,0.9),\n100      main = expression(paste(\"(c) \", hat(rho)[\"12,t\"])))\n101 points(ts(mvwindow.cor(A[,1],A[,2], win = 126)$correlation,\n\n102           start=c(1989,1), frequency=253),\n103        type = \u2019l\u2019, col = 2, lty = 2,lwd=2)\n104 plot(ts(DOC.fit$Sigma.doc[2,2,]^.5, start=c(1989,1), frequency=253),\n\n105      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n106      main = expression(paste(\"(d) \", hat(sigma)[\"2,t\"])))\n107   DOC.fit$coef.pca\n                 omega     alpha1     beta1\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes     439",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.003845283 0.02118369 0.9758129\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.003845283",
      "section_title": "0.02118369 0.9758129",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.037473820 0.07101731 0.8913321\n108   DOC.fit$coef.doc\n                 omega     alpha1     beta1    shape\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.037473820",
      "section_title": "0.07101731 0.8913321",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.004874403 0.02560464 0.9702966 4.313164\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.004874403",
      "section_title": "0.02560464 0.9702966 4.313164",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.009092705 0.04740792 0.9446408 5.030019\n109   DOC.fit$W.hat\n                [,1]       [,2]\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.009092705",
      "section_title": "0.04740792 0.9446408 5.030019",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9412834 -0.3376174\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9412834",
      "section_title": "-0.3376174",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3376174 0.9412834\n110   DOC.fit$U.hat\n                 [,1]      [,2]\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3376174",
      "section_title": "0.9412834",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6147515 -0.2297417\n      [2,] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6147515",
      "section_title": "-0.2297417",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2297417 1.4669516\n111   DOC.fit$M.hat\n                 [,1]      [,2]\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2297417",
      "section_title": "1.4669516",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.53499088 0.8380397\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.53499088",
      "section_title": "0.8380397",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01024854 0.7729063\n112   solve(DOC.fit$U.hat)\n                [,1]      [,2]\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01024854",
      "section_title": "0.7729063",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.7277983 0.2705934\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.7277983",
      "section_title": "0.2705934",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2705934 0.7240638\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2705934",
      "section_title": "0.7240638",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14.6 Dynamic Conditional Correlation (DCC) Models\n\nNonlinear combinations of univariate volatility models have been proposed\nto allow for time-varying correlations, a feature that is prevalent in many\n\ufb01nancial applications. Both Tse and Tsui (2002) and Engle (2002) generalize\nthe constant correlation model of Bollerslev (1990) to allow for such dynamic\nconditional correlations (DCC).\n\f440       14 GARCH Models\n\n  Analogously to the GARCH(1,1) model, the \ufb01rst order form of the DCC\nmodel in Engle (2002) may be represented by the following equations\n                         2\n                        \u03c3i,t = \u03c9i + \u03b1i a2i,t\u22121 + \u03b2i \u03c3i,t\u22121\n                                                        2\n                                                           ,\n                        D t = diag{\u03c31,t , . . . , \u03c3d,t },\n                        \u03b5t = D \u22121\n                               t at ,\n                        Qt = (1 \u2212 \u03bb)\u03b5t\u22121 \u03b5\u0003t\u22121 + \u03bbQt\u22121 ,\n                                            1                  1\n                        Rt = diag{Qt }\u2212 2 Qt diag{Qt }\u2212 2 ,\n                        \u03a3t = D t R t D t .\n\nThe main idea is to \ufb01rst model the conditional variance of each individual\n        2\nseries \u03c3it with a univariate volatility model, estimate the scaled innovations\n\u03b5t (not to be confused with standardized innovations t ) from these models,\nthen focus on modeling the conditional correlation matrix Rt . These are then\ncombined at each time point t to estimate the volatility matrix \u03a3t . The recur-\nsion Qt is an EWMA model applied to the scaled innovations \u03b5t . It is indexed\nby a single parameter \u03bb \u2208 (0, 1). The matrix Qt needs to be rescaled to form\na proper correlation matrix Rt with the value 1 for all elements on the main\ndiagonal.\n    The DCC model parameters can be estimated consistently in two stages\nusing quasi-maximum likelihood. First, a univariate GARCH(1, 1) model is \ufb01t\n                             2\nto each series to estimate \u03c3it . Then, given \u03b5t , \u03bb is estimated by maximizing\nthe components of the quasi-likelihood that only depend on the correlations.\nThis is justi\ufb01ed since the squared residuals do not depend on the correlation\nparameters.\n    In the form above, the variance components only condition on their own\nindividual lagged returns and not the joint returns. Also, the dynamics for\neach of the conditional correlations are constrained to have equal persistence\nparameters, similar to the EWMA model. An explicit parameterization of\nthe conditional correlation matrix Rt , with \ufb02exible dynamics, is just as di\ufb03-\ncult to estimate in high dimensions as \u03a3t itself. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "6 Dynamic Conditional Correlation (DCC) Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17 shows a \ufb01tted\nDCC model for a \u0002 t using quasi-maximum likelihood estimation. The estimated\nconditional standard deviations are shown in (a) and (d), and the conditional\ncovariances and conditional correlations are shown in (b) and (c), respectively.\nEstimation and Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "shows a \ufb01tted",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17 were calculated using the following commands in\nR.\n113 source(\"SDAFE2.R\")\n114 DCCe.fit = fit.DCCe(theta.0=0.95, innov=A)\n115 DCCe.fit$coef\n\n\n                omega     alpha1     beta1\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "were calculated using the following commands in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07435095 0.05528162 0.9231251\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07435095",
      "section_title": "0.05528162 0.9231251",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02064808 0.08341755 0.8822517\n\n116   DCCe.fit$lambda\n\f                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02064808",
      "section_title": "0.08341755 0.8822517",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14 Multivariate GARCH Processes          441\n\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "Multivariate GARCH Processes          441",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9876297\n\n117 par(mfrow = c(2,2))\n118 plot(ts(DCCe.fit$Sigma.t[1,1,]^.5, start=c(1989, 1), frequency=253),\n119      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n120      main = expression(paste(\"(a) \", hat(sigma)[\"1,t\"])))\n121 plot(ts(DCCe.fit$Sigma.t[2,1,], start=c(1989, 1), frequency=253),\n\n122      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n123      main = expression(paste(\"(b) \", hat(sigma)[\"12,t\"])))\n124 plot(ts(DCCe.fit$R.t[2,1,], start=c(1989, 1), frequency=253),\n\n125      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n126      main = expression(paste(\"(c) \", hat(rho)[\"12,t\"])))\n127 points(ts(mvwindow.cor(A[,1],A[,2], win = 126)$correlation,\n\n128           start=c(1989, 1), frequency=253),\n129        type = \u2019l\u2019, col = 2, lty = 2, lwd=2)\n130 plot(ts(DCCe.fit$Sigma.t[2,2,]^.5, start=c(1989, 1), frequency=253),\n\n131      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n132      main = expression(paste(\"(d) \", hat(sigma)[\"2,t\"])))\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9876297",
      "section_title": "117 par(mfrow = c(2,2))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.14.7 Model Checking\n                                \u0002 t , the standardized residuals are de\ufb01ned as\nFor a \ufb01tted volatility sequence \u03a3\n                                       \u0002\n                                  \u0002t = \u03a3\n                                          \u22121/2\n                                                 at ,                        (14.29)\n                                         t\n\nin which \u03a3   \u0002 \u22121/2 denotes the inverse of the matrix \u03a3\n                                                      \u0002 1/2 . To verify the adequacy\n              t                                         t\nof a \ufb01tted volatility model, lagged cross-correlations of the squared standard-\nized residuals should be zero. The product process \u0002      \u0017it \u0002\n                                                              \u0017jt should also have no\nserial correlation. Additional diagnostic checks for time series are considered\nin Li (2003). Since the standardized residuals are estimated and not observed,\nall p-values given in this section are only approximate.\n    To check the \ufb01rst condition we can apply a multivariate Ljung-Box test to\nthe squared standardized residuals. For the EWMA model, Q2 (\u00022t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.14",
      "section_title": "7 Model Checking",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "26.40\nwith a p-value of 0.153, implying no signi\ufb01cant serial correlation. For the O-\nGARCH model, Q2 (\u00022t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "26.40",
      "section_title": "with a p-value of 0.153, implying no signi\ufb01cant serial correlation. For the O-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "30.77 with a p-value 0.058. In this case, there is\nsome minor evidence of serial correlation. For the DOC in volatility model,\nQ2 (\u00022t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "30.77",
      "section_title": "with a p-value 0.058. In this case, there is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.68 with a p-value 0.543, implying no signi\ufb01cant serial correla-\ntion. For the DCC model, Q2 (\u00022t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.68",
      "section_title": "with a p-value 0.543, implying no signi\ufb01cant serial correla-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.54 with a p-value 0.957, implying no\nsigni\ufb01cant serial correlation.\n133 n = dim(A)[1] ; d = dim(A)[2]\n134 stdResid.EWMA = matrix(0,n,d)\n135 stdResid.PCA = matrix(0,n,d)\n\n136 stdResid.DOC = matrix(0,n,d)\n\n137 stdResid.DCCe = matrix(0,n,d)\n\n138 for(t in 1:n){\n\n139  stdResid.EWMA[t,] = A[t,] %*% matrix.sqrt.inv(EWMA.Sigma[,,t])\n140  stdResid.PCA[t,] = A[t,] %*% matrix.sqrt.inv(DOC.fit$Sigma.pca[,,t])\n141  stdResid.DOC[t,] = A[t,] %*% matrix.sqrt.inv(DOC.fit$Sigma.doc[,,t])\n\f442                             14 GARCH Models\n\n142   stdResid.DCCe[t,] = A[t,] %*% matrix.sqrt.inv(DCCe.fit$Sigma.t[,,t])\n143 }\n144 mLjungBox(stdResid.EWMA^2, lag=5)\n\n145 mLjungBox(stdResid.PCA^2, lag=5)\n\n146 mLjungBox(stdResid.DOC^2, lag=5)\n\n147 mLjungBox(stdResid.DCCe^2, lag=5)\n\n\n\n\n a                                           ^\n                                             \u03c3 1,t\n                                                                    b                   ^\n                                                                                        \u03c312,t\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.54",
      "section_title": "with a p-value 0.957, implying no",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.0\n\n\n\n\n                                                                    6\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.0",
      "section_title": "6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5\n\n\n\n\n                                                                    5\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.5",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                    4\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n\n\n\n\n                                                                    3\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                    2\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\n\n\n\n                                                                    1\n                                                                    0\n\n\n\n\n                               1990   1992   1994     1996   1998         1990   1992   1994    1996   1998\n                                             year                                       year\n\n\n c                                           ^                      d                   ^\n                                             \u03c1 12,t                                     \u03c3 2,t\n ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.3 0.4 0.5 0.6 0.7 0.8\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.3 0.4 0.5 0.6 0.7 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n                                                                    2.0\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                                                    1.0\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                               1990   1992   1994     1996   1998         1990   1992   1994    1996   1998\n                                             year                                       year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "1990   1992   1994     1996   1998         1990   1992   1994    1996   1998",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17. A \ufb01tted \ufb01rst order DCC model with (\u03c91 , \u03b11 , \u03b21 )\u0002                          =\n(0.0741, 0.0552, 0.9233)\u0002 , (\u03c92 , \u03b12 , \u03b22 )\u0002 = (0.0206, 0.0834, 0.8823)\u0002 , and \u03bb = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "A \ufb01tted \ufb01rst order DCC model with (\u03c91 , \u03b11 , \u03b21 )\u0002                          =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9876.\nThe red line in (c) is the sample correlation estimate over the previous six months\nfor comparison.\n\n\n    The multivariate Ljung-Box test for the squared standardized residu-\nals is not sensitive to misspeci\ufb01cation of the conditional correlation struc-\nture. To check this condition, we apply univariate Ljung-Box tests to the\nproduct of each pair of standardized residuals. For the EWMA model,\nQ(\u0002\u00171t \u0002\n       \u00172t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9876",
      "section_title": "The red line in (c) is the sample correlation estimate over the previous six months",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.45 with a p-value of 0.006. This model has not adequately\naccounted for the time-varying conditional correlation. For the O-GARCH\nmodel, Q(\u0002    \u00171t \u0002\n                  \u00172t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.45",
      "section_title": "with a p-value of 0.006. This model has not adequately",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "63.09 with a p-value near zero. This model also fails to\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "63.09",
      "section_title": "with a p-value near zero. This model also fails to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16 R Lab      443\n\naccount for the observed time-varying conditional correlation. For the DOC\nin volatility model, Q(\u0002\u00171t \u0002\n                            \u00172t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "R Lab      443",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.07 with a p-value of 0.106, implying no\nsigni\ufb01cant serial correlation. For the DCC model, Q(\u0002    \u00171t \u0002\n                                                             \u00172t , 5) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.07",
      "section_title": "with a p-value of 0.106, implying no",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.37 with a\np-value of 0.137, also implying no signi\ufb01cant serial correlation.\n148 mLjungBox(stdResid.EWMA[,1] * stdResid.EWMA[,2], lag=5)\n149 mLjungBox(stdResid.PCA[,1] * stdResid.PCA[,2], lag=5)\n150 mLjungBox(stdResid.DOC[,1] * stdResid.DOC[,2], lag=5)\n\n151 mLjungBox(stdResid.DCCe[,1] * stdResid.DCCe[,2], lag=5)\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.37",
      "section_title": "with a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.15 Bibliographic Notes\n\nModeling nonconstant conditional variances in regression is treated in depth\nin the book by Carroll and Ruppert (1988).\n    There is a vast literature on GARCH processes beginning with En-\ngle (1982), where ARCH models were introduced. Hamilton (1994), Enders\n(2004), Pindyck and Rubinfeld (1998), Gourieroux and Jasiak (2001), Alexan-\nder (2001), and Tsay (2005) have chapters on GARCH models. There are\nmany review articles, including Bollerslev (1986), Bera and Higgins (1993),\nBollerslev, Engle, and Nelson (1994), and Bollerslev, Chou, and Kroner (1992).\nJarrow (1998) and Rossi (1996) contain a number of papers on volatility in \ufb01-\nnancial markets. Duan (1995), Ritchken and Trevor (1999), Heston and Nandi\n(2000), Hsieh and Ritchken (2000), Duan and Simonato (2001), and many\nother authors study the e\ufb00ects of GARCH errors on options pricing, and\nBollerslev, Engle, and Wooldridge (1988) use GARCH models in the CAPM.\n    For a thorough review of multivariate GARCH modeling see Bauwens,\nLaurent, and Rombouts (2006), and Silvennoinen and Tera\u0308svirta (2009).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.15",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16.1 Fitting GARCH Models\n\nRun the following code to load the data set TbGdpPi.csv, which has three\nvariables: the 91-day T-bill rate, the log of real GDP, and the in\ufb02ation rate.\nIn this lab you will use only the T-bill rate.\n 1 TbGdpPi = read.csv(\"TbGdpPi.csv\", header=TRUE)\n 2 # r = the 91-day treasury bill rate\n 3 #  y = the log of real GDP\n 4 #  pi = the inflation rate\n 5 TbGdpPi = ts(TbGdpPi, start = 1955, freq = 4)\n\n 6 Tbill = TbGdpPi[,1]\n\n 7 Tbill.diff = diff(Tbill)\n\f444    14 GARCH Models\n\nProblem 1 Plot both Tbill and Tbill.diff. Use both time series and ACF\nplots. Also, perform ADF and KPSS tests on both series. Which series do you\nthink are stationary? Why? What types of heteroskedasticity can you see in\nthe Tbill.diff series?\n\nIn the following code, the variable Tbill can be used if you believe that series\nis stationary. Otherwise, replace Tbill by Tbill.diff. This code will \ufb01t an\nARMA+\nGARCH model to the series.\n 8 library(rugarch)\n 9 arma.garch.norm = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n10                              variance.model=list(garchOrder=c(1,1)))\n11 Tbill.arma.garch.norm = ugarchfit(data=Tbill, spec=arma.garch.norm)\n\n12 show(Tbill.arma.garch.norm)\n\n\n\nProblem 2 (a) Which ARMA+GARCH model is being \ufb01t? Write down the\n    model using the same parameter names as in the R output.\n(b) What are the estimates of each of the parameters in the model?\n\nNext, plot the residuals (ordinary or raw) and standardized residuals in various\nways using the code below. The standardized residuals are best for checking\nthe model, but the residuals are useful to see if there are GARCH e\ufb00ects in\nthe series.\n13 res = ts(residuals(Tbill.arma.garch.norm, standardize=FALSE),\n14           start = 1955, freq = 4)\n15 res.std = ts(residuals(Tbill.arma.garch.norm, standardize=TRUE),\n\n16               start = 1955, freq = 4)\n17 par(mfrow=c(2,3))\n\n18 plot(res)\n\n19 acf(res)\n\n20 acf(res^2)\n\n21 plot(res.std)\n\n22 acf(res.std)\n\n23 acf(res.std^2)\n\n\n\nProblem 3 (a) Describe what is plotted by acf(res). What, if anything,\n    does the plot tell you about the \ufb01t of the model?\n(b) Describe what is plotted by acf(res^2). What, if anything, does the plot\n    tell you about the \ufb01t of the model?\n(c) Describe what is plotted by acf(res_std^2). What, if anything, does the\n    plot tell you about the \ufb01t of the model?\n(d) Is there anything noteworthy in the \ufb01gure produced by the command\n    plot(res.std)?\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "1 Fitting GARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16 R Lab      445\n\nProblem 4 Now \ufb01nd an ARMA+GARCH model for the series diff.log.\nTbill, which we will de\ufb01ne as diff(log(Tbill)). Do you see any advantages\nof working with the di\ufb00erences of the logarithms of the T-bill rate, rather than\nwith the di\ufb00erence of Tbill as was done earlier?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "R Lab      445",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.16.2 The GARCH-in-Mean (GARCH-M) Model\nA GARCH-in-Mean or GARCH-M model takes the form\n\n\n                            Yt = \u03bc + \u03b4\u03c3t + at\n                            at = \u03c3 t \u0017t\n                            \u03c3t2 = \u03c9 + \u03b1a2t\u22121 + \u03b2\u03c3t\u22121\n                                                 2\n\n\n             iid\nin which \u0017t \u223c (0, 1). The GARCH-M model directly incorporates volatility\nas a regression variable. The parameter \u03b4 represents the risk premium, or\nreward for additional risk. Modern portfolio theory dictates that increased\nvolatility leads to increased risk, requiring larger expected returns. The pres-\nence of volatility as a statistically signi\ufb01cant predictor of returns is one of the\nprimary contributors to serial correlation in historic return series. The data\nset GPRO.csv() contains the adjusted daily closing price of GoPro stock from\nJune 26, 2014 to January 28, 2015.\n    Run the following R commands to \ufb01t a GARCH-M model to the GoPro\nstock returns.\n1 library(rugarch)\n2 GPRO = read.table(\"GPRO.csv\")\n3 garchm = ugarchspec(mean.model=list(armaOrder=c(0,0),\n\n4                                     archm=T,archpow=1),\n5                     variance.model=list(garchOrder=c(1,1)))\n6 GPRO.garchm = ugarchfit(garchm, data=GPRO)\n\n7 show(GPRO.garchm)\n\n\n\nProblem 5 Write out the \ufb01tted model. The parameter \u03b4 is equal to archm in\nthe R output.\n\n\nProblem 6 Test the one-sided hypothesis that \u03b4 > 0 verses the alternative\nthat \u03b4 = 0. Is the risk premium signi\ufb01cant?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "2 The GARCH-in-Mean (GARCH-M) Model",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 verses the alternative\nthat \u03b4 = 0. Is the risk premium signi\ufb01cant?",
        "start": 1397,
        "end": 1469
      }
    ]
  },
  {
    "content": "14.16.3 Fitting Multivariate GARCH Models\nRun the following code to again load the data set TbGdpPi.csv, which has\nthree variables: the 91-day T-bill rate, the log of real GDP, and the in\ufb02ation\nrate. In this lab you will now use the \ufb01rst and third series after taking \ufb01rst\ndi\ufb00erences.\n\f446      14 GARCH Models\n\n1 TbGdpPi = read.csv(\"TbGdpPi.csv\", header=TRUE)\n2 TbPi.diff = ts(apply(TbGdpPi[,-2],2,diff), start=c(1955,2), freq=4)\n3 plot(TbPi.diff)\n\n4 acf(TbPi.diff^2)\n\n5 source(\"SDAFE2.R\")\n\n6 mLjungBox(TbPi.diff^2, lag=8)\n\n\n\nProblem 7 Does the joint series exhibit conditional heteroskedasticity? Why?\n\n      Now \ufb01t and plot a EWMA model with the following R commands.\n7  EWMA.param = est.ewma(lambda.0=0.95, innov=TbPi.diff)\n8  EWMA.param$lambda.hat\n 9 EWMA.Sigma=sigma.ewma(lambda=EWMA.param$lambda.hat,innov=TbPi.diff)\n\n10 par(mfrow = c(2,2))\n\n11 plot(ts(EWMA.Sigma[1,1,]^.5, start = c(1955, 2), frequency = 4),\n\n12      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n13      main = expression(paste(\"(a) \", hat(sigma)[\"1,t\"])))\n14 plot(ts(EWMA.Sigma[1,2,], start = c(1955, 2), frequency = 4),\n\n15      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n16      main = expression(paste(\"(b) \", hat(sigma)[\"12,t\"])))\n17 plot(ts(EWMA.Sigma[1,2,]/(sqrt(EWMA.Sigma[1,1,]* EWMA.Sigma[2,2,])),\n\n18         start = c(1955, 2), frequency = 4),\n19      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n20      main = expression(paste(\"(c) \", hat(rho)[\"12,t\"])))\n21 plot(ts(EWMA.Sigma[2,2,]^.5, start = c(1955, 2), frequency = 4),\n\n22      type = \u2019l\u2019, xlab = \"year\", ylab = NULL,\n23      main = expression(paste(\"(d) \", hat(sigma)[\"2,t\"])))\n\nProblem 8 What is the estimated persistence parameter \u03bb?\n\n   Now estimate standardized residuals and check whether they exhibit any\nconditional heteroskedasticity\n24 n = dim(TbPi.diff)[1]\n25 d = dim(TbPi.diff)[2]\n26 stdResid.EWMA = matrix(0,n,d)\n\n27 for(t in 1:n){\n\n28   stdResid.EWMA[t,] = TbPi.diff[t,] %*% matrix.sqrt.inv\n29   (EWMA.Sigma[,,t])\n30 }\n\n31 mLjungBox(stdResid.EWMA^2, lag=8)\n\n\n\nProblem 9 Based on the output of the Ljung-Box test for the squared stan-\ndardized residuals, is the EWMA model adequate?\n   Run the following command in R to determine whether the joint series are\nDOCs in volatility.\n32   DOC.test(TbPi.diff^2, 8)\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.16",
      "section_title": "3 Fitting Multivariate GARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17 Exercises       447\n\nProblem 10 Is the null hypothesis rejected? Based on this conclusion, how\nshould the conditional heteroskedasticity in the bivariate series be modeled,\njointly or separately?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "Exercises       447",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17 Exercises\n1. Let Z have an N (0, 1) distribution. Show that\n                    \u0013 \u221e                       \u0013 \u221e                     \u0010\n                      1      2                       1    2               2\n         E(|Z|) =    \u221a |z|e\u2212z /2 dz = 2             \u221a ze\u2212z /2 dz =          .\n                  \u2212\u221e  2\u03c0                       0     2\u03c0                   \u03c0\n                2           2\n         d \u2212z /2\n   Hint: dz e     = \u2212ze\u2212z /2 .\n2. Suppose that fX (x) = 1/4 if |x| < 1 and fX (x) = 1/(4x2 ) if |x| \u2265 1. Show\n   that                       \u0013 \u221e\n                                    fX (x)dx = 1,\n                                    \u2212\u221e\n\n   so that fX really is a density, but that\n                             \u0013 0\n                                   xfX (x)dx = \u2212\u221e\n                                \u2212\u221e\n\n   and\n                                \u0013 \u221e\n                                      xfX (x)dx = \u221e,\n                                0\n   so that a random variable with this density does not have an expected\n   value.\n3. Suppose that \u0017t is an i.i.d. WN(0, 1) process, that\n                                     #\n                              at = \u0017t 1 + 0.35a2t\u22121 ,\n\n   and that\n                             yt = 3 + 0.72yt\u22121 + at .\n   (a) Find the mean of yt .\n   (b) Find the variance of yt .\n   (c) Find the autocorrelation function of yt .\n   (d) Find the autocorrelation function of a2t .\n4. Let yt be the AR(1)+ARCH(1) model\n                                        #\n                                 at = \u0017t \u03c9 + \u03b1a2t\u22121 ,\n                          (yt \u2212 \u03bc) = \u03c6(yt\u22121 \u2212 \u03bc) + at ,\n\n   where \u0017t is i.i.d. WN(0,1). Suppose that \u03bc = 0.4, \u03c6 = 0.45, \u03c9 = 1, and\n   \u03b11 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3.\n\f448     14 GARCH Models\n\n   (a) Find E(y2 |y1 = 1, y0 = 0.2).\n   (b) Find Var(y2 |y1 = 1, y0 = 0.2).\n   Suppose that \u0017t is white noise with mean 0 and variance 1, that at = \u0017t\n5. #\n     7 + a2t\u22121 /2, and that Yt = 2 + 0.67Yt\u22121 + at .\n   (a) What is the mean of Yt ?\n   (b) What is the ACF of Yt ?\n   (c) What is the ACF of at ?\n   (d) What is the ACF of a2t ?\n6. Let Yt be a stock\u2019s return in time period t and let Xt be the in\ufb02ation rate\n   during this time period. Assume the model\n\n                            Yt = \u03b20 + \u03b21 Xt + \u03b4\u03c3t + at ,                 (14.30)\n\n      where\n                                           #\n                                at = \u0017 t       1 + 0.5a2t\u22121 .            (14.31)\n\n   Here the \u0017t are independent N (0, 1) random variables. Model (14.30)\u2013\n   (14.31) is called a GARCH-in-mean model or a GARCH-M model.\n   Assume that \u03b20 = 0.06, \u03b21 = 0.35, and \u03b4 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "448     14 GARCH Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.22.\n   (a) What is E(Yt |Xt = 0.1 and at\u22121 = 0.6)?\n   (b) What is Var(Yt |Xt = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.22",
      "section_title": "(a) What is E(Yt |Xt = 0.1 and at\u22121 = 0.6)?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 and at\u22121 = 0.6)?\n   (c) Is the conditional distribution of Yt given Xt and at\u22121 normal? Why\n       or why not?\n   (d) Is the marginal distribution of Yt normal? Why or why not?\n7. Suppose that \u00171 , \u00172 , . . . is a Gaussian white noise process with mean 0 and\n   variance 1, and at and yt are stationary processes such that\n\n                      at = \u03c3 t \u0017 t   where       \u03c3t2 = 2 + 0.3a2t\u22121 ,\n\n      and\n                                yt = 2 + 0.6yt\u22121 + at .\n\n   (a) What type of process is at ?\n   (b) What type of process is yt ?\n   (c) Is at Gaussian? If not, does it have heavy or lighter tails than a Gaus-\n        sian distribution?\n   (d) What is the ACF of at ?\n   (e) What is the ACF of a2t ?\n   (f) What is the ACF of yt ?\n8. On Black Monday, the return on the S&P 500 was \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "and at\u22121 = 0.6)?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "22.8 %. Ouch! This\n   exercise attempts to answer the question, \u201cwhat was the conditional prob-\n   ability of a return this small or smaller on Black Monday?\u201d \u201cConditional\u201d\n   means given the information available the previous trading day. Run the\n   following R code:\n\f                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "22.8",
      "section_title": "%. Ouch! This",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.17 Exercises    449\n\n   1  library(rugarch)\n   2  library(Ecdat)\n    3 data(SP500,package=\"Ecdat\")\n\n    4 returnBlMon = SP500$r500[1805] ; returnBlMon\n\n    5 x = SP500$r500[(1804-2*253+1):1804]\n\n    6 ts.plot(c(x,returnBlMon))\n\n    7 spec = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n\n    8                   variance.model=list(garchOrder=c(1,1)),\n    9                   distribution.model = \"std\")\n   10 fit = ugarchfit(data=x, spec=spec)\n\n   11 dfhat = coef(fit)[6]\n\n   12 forecast = ugarchforecast(fit, data=x, n.ahead=1)\n\n   The S&P 500 returns are in the data set SP500 in the Ecdat package.\n   The returns are the variable r500 (this is the only variable in this data\n   set). Black Monday is the 1805th return in this data set. This code \ufb01ts\n   an AR(1)+GARCH(1,1) model to the last two years of data before Black\n   Monday, assuming 253 trading days/year. The conditional distribution of\n   the white noise is the t-distribution (called \"std\" in ugarchspec()). The\n   code also plots the returns during these two years and on Black Monday.\n   From the plot you can see that Black Monday was highly unusual. The pa-\n   rameter estimates are in coef(fit) and the sixth parameter is the degrees\n   of freedom of the t-distribution. The ugarchforecast() function is used\n   to predict one-step ahead, that is, to predict the return on Black Monday;\n   the input variable n.ahead speci\ufb01es how many days ahead to forecast, so\n   n.ahead=5 would forecast the next \ufb01ve days. The object forecast will\n   contain fitted(forecast), which is the conditional expected return on\n   Black Monday, and sigma(forecast), which is the conditional standard\n   deviation of the return on Black Monday.\n   (a) Use the information above to calculate the conditional probability of\n        a return less than or equal to \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.17",
      "section_title": "Exercises    449",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.228 on Black Monday.\n   (b) Compute and plot the standardized residuals. Also plot the ACF\n        of the standardized residuals and their squares. Include all three\n        plots with your work. Do the standardized residuals indicate that the\n        AR(1)+GARCH(1,1) model \ufb01ts adequately?\n   (c) Would an AR(1)+ARCH(1) model provide an adequate \ufb01t?\n   (d) Does an AR(1) model with a Gaussian conditional distribution provide\n        an adequate \ufb01t? Use the arima() function to \ufb01t the AR(1) model. This\n        function only allows a Gaussian conditional distribution.\n9. This problem uses monthly observations of the two-month yield, that is,\n   YT with T equal to two months, in the data set Irates in the Ecdat\n   package. The rates are log-transformed to stabilize the variance. To \ufb01t a\n   GARCH model to the changes in the log rates, run the following R code.\n   13 library(rugarch)\n   14 library(Ecdat)\n   15 data(Irates)\n\f450        14 GARCH Models\n\n      16 r = as.numeric(log(Irates[,2]))\n      17 n = length(r)\n      18 lagr = r[1:(n-1)]\n\n      19 diffr = r[2:n] - lagr\n\n      20 spec = ugarchspec(mean.model=list(armaOrder=c(1,0)),\n\n      21                   variance.model=list(garchOrder=c(1,1)),\n      22                   distribution.model = \"std\")\n      23 fit = ugarchfit(data=diffr, spec=spec)\n\n      24 plot(fit, which=\"all\")\n\n    (a) What model is being \ufb01t to the changes in r? Describe the model in\n        detail.\n    (b) What are the estimates of the parameters of the model?\n    (c) What is the estimated ACF of \u0394rt ?\n    (d) What is the estimated ACF of at ?\n    (e) What is the estimated ACF of a2t ?\n10. Consider the daily log returns on the S&P 500 index (GSPC). Begin by\n    running the following commands in R, then answer the questions below\n    for the series y.\n      25 library(rugarch)\n      26 library(quantmod)\n      27 getSymbols(\"^GSPC\", from=\"2005-01-01\", to=\"2014-12-31\")\n\n      28 head(GSPC)\n\n      29 sp500 = xts( diff( log( GSPC[,6] ) )[-1] )\n\n      30 plot(sp500)\n\n      31 y = as.numeric(sp500)\n\n      (a) Is there any serial correlation in the log returns of S&P 500 index?\n           Why?\n      (b) Is there any ARCH e\ufb00ect (evidence of conditional heteroskedasticity)\n           in the log returns of S&P 500 index? Why?\n      (c) Specify and \ufb01t an ARCH model to the log returns of S&P 500 index.\n           Write down the \ufb01tted model.\n      (d) Is your \ufb01tted ARCH model stationary? Why?\n      (e) Fit a GARCH(1,1) model for the log returns on the S&P 500 index\n           using the Gaussian distribution for the innovations. Write down the\n           \ufb01tted model.\n      (f) Perform model checking to ensure that the model is adequate using 20\n           lags in a Ljung-Box test of the standardized residuals and the squared\n           standardized residuals.\n      (g) Is the \ufb01tted GARCH model stationary? Why?\n      (h) Make a Normal quantile plot for the standardized residuals. Use\n           qqnorm() and qqline() in R. Is the Gaussian distribution appro-\n           priate for the standardized innovations?\n       (i) Plot the \ufb01tted conditional standard deviation process \u03c3\u0302t and com-\n           ment.\n\f                                                           References    451\n\n    (j) Calculate the 1\u201310 step ahead forecasts from the end of the se-\n        ries for both the process yt and the conditional variance using the\n        ugarchforecast() function.\n\n\nReferences\n\nAlexander, C. (2001) Market Models: A Guide to Financial Data Analysis,\n  Wiley, Chichester.\nBauwens, L., Laurent, S., and Rombouts, J. V. (2006) Multivariate GARCH\n  models: a survey. Journal of Applied Econometrics, 21(1), 79\u2013109.\nBera, A. K., and Higgins, M. L. (1993) A survey of Arch models. Journal of\n  Economic Surveys, 7, 305\u2013366. [Reprinted in Jarrow (1998).]\nBollerslev, T. (1986) Generalized autoregressive conditional heteroskedastic-\n  ity. Journal of Econometrics, 31, 307\u2013327.\nBollerslev, T. (1990) Modelling the coherence in short-run nominal exchange\n  rates: a multivariate generalized ARCH model. The Review of Economics\n  and Statistics, 72(3), 498\u2013505.\nBollerslev, T., Chou, R. Y., and Kroner, K. F. (1992) ARCH modelling in\n  \ufb01nance. Journal of Econometrics, 52, 5\u201359. [Reprinted in Jarrow (1998)]\nBollerslev, T., Engle, R. F., and Nelson, D. B. (1994) ARCH models, In Hand-\n  book of Econometrics, Vol IV, Engle, R.F., and McFadden, D.L., Elsevier,\n  Amsterdam.\nBollerslev, T., Engle, R. F., and Wooldridge, J. M. (1988) A capital asset\n  pricing model with time-varying covariances. Journal of Political Economy,\n  96, 116\u2013131.\nCarroll, R. J., and Ruppert, D. (1988) Transformation and Weighting in Re-\n  gression, Chapman & Hall, New York.\nDuan, J.-C. (1995) The GARCH option pricing model. Mathematical Finance,\n  5, 13\u201332. [Reprinted in Jarrow (1998).]\nDuan, J-C., and Simonato, J. G. (2001) American option pricing under\n  GARCH by a Markov chain approximation. Journal of Economic Dynamics\n  and Control, 25, 1689\u20131718.\nEnders, W. (2004) Applied Econometric Time Series, 2nd ed., Wiley, New\n  York.\nEngle, R. F. (1982) Autoregressive conditional heteroskedasticity with esti-\n  mates of variance of U.K. in\ufb02ation. Econometrica, 50, 987\u20131008.\nEngle, R. F. (2002) Dynamic conditional correlation: A simple class of mul-\n  tivariate generalized autoregressive conditional heteroskedasticity models.\n  Journal of Business & Economic Statistics, 20(3), 339\u2013350.\nFisher, T.J., and Gallagher, C.M. (2012) New weighted portmanteau statistics\n  for time series goodness of \ufb01t testing. Journal of the American Statistical\n  Association, 107(498), 777\u2013787.\nGourieroux, C. and Jasiak, J. (2001) Financial Econometrics, Princeton Uni-\n  versity Press, Princeton, NJ.\n\f452    14 GARCH Models\n\nHamilton, J. D. (1994) Time Series Analysis, Princeton University Press,\n   Princeton, NJ.\nHeston, S. and Nandi, S. (2000) A closed form GARCH option pricing model.\n   The Review of Financial Studies, 13, 585\u2013625.\nHsieh, K. C. and Ritchken, P. (2000) An empirical comparison of GARCH\n   option pricing models. working paper.\nJarrow, R. (1998) Volatility: New Estimation Techniques for Pricing Deriva-\n   tives, Risk Books, London. (This is a collection of articles, many on GARCH\n   models or on stochastic volatility models, which are related to GARCH\n   models.)\nLi, W. K. (2003) Diagnostic checks in time series, CRC Press.\nMatteson, D. S. and Tsay, R. S. (2011) Dynamic orthogonal components for\n   multivariate time series. Journal of the American Statistical Association,\n   106(496), 1450\u20131463.\nPalm, F.C. (1996) GARCH models of volatility. Handbook of Statistics, 14,\n   209\u2013240.\nPalma, W. and Zevallos, M. (2004). Analysis of the correlation structure of\n   square time series. Journal of Time Series Analysis, 25(4), 529\u2013550.\nPindyck, R. S. and Rubinfeld, D. L. (1998) Econometric Models and Economic\n   Forecasts, Irwin/McGraw Hill, Boston.\nRitchken, P. and Trevor, R. (1999) Pricing options under generalized GARCH\n   and stochastic volatility processes. Journal of Finance, 54, 377\u2013402.\nRossi, P. E. (1996) Modelling Stock Market Volatility, Academic Press, San\n   Diego.\nSilvennoinen, A. and Tera\u0308svirta, T (2009) Multivariate GARCH models. In\n   Handbook of Financial Time Series, 201\u2013229, Springer, Berlin.\nTsay, R. S. (2005) Analysis of Financial Time Series, 2nd ed., Wiley, New\n   York.\nTse, Y. K. and Tsui, A. K. C. (2002) A multivariate generalized autoregressive\n   conditional heteroscedasticity model with time-varying correlations. Jour-\n   nal of Business & Economic Statistics, 20(3), 351\u2013362.\n\f15\nCointegration\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.228",
      "section_title": "on Black Monday.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1 Introduction\n\nCointegration analysis is a technique that is frequently applied in econometrics.\nIn \ufb01nance it can be used to \ufb01nd trading strategies based on mean-reversion.\n    Suppose one could \ufb01nd a stock whose price (or log-price) series was sta-\ntionary and therefore mean-reverting. This would be a wonderful investment\nopportunity. Whenever the price was below the mean, one could buy the stock\nand realize a pro\ufb01t when the price returned to the mean. Similarly, one could\nrealize pro\ufb01ts by selling short whenever the price was above the mean. Alas,\nreturns are stationary but not prices. We have seen that log-prices are inte-\ngrated. However, not all is lost. Sometimes one can \ufb01nd two or more assets\nwith prices so closely connected that a linear combination of their prices is\nstationary. Then, a portfolio with weights assigned by the cointegrating vec-\ntor, which is the vector of coe\ufb03cients of this linear combination, will have a\nstationary price. Cointegration analysis is a means for \ufb01nding cointegration\nvectors.\n    Two time series, Y1,t and Y2,t , are cointegrated if each is I(1) but there\nexists a \u03bb such that Y1,t \u2212\u03bbY2,t is stationary. For example, the common trends\nmodel is that\n                              Y1,t = \u03b21 Wt + \u00171,t ,\n                              Y2,t = \u03b22 Wt + \u00172,t ,\n\nwhere \u03b21 and \u03b22 are nonzero, the trend Wt common to both series is I(1),\nand the noise processes \u00171,t and \u00172,t are I(0). Because of the common trend,\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                              453\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 15\n\f454        15 Cointegration\n\nY1,t and Y2,t are nonstationary but there is a linear combination of these two\nseries that is free of the trend, so they are cointegrated. To see this, note that\nif \u03bb = \u03b21 /\u03b22 , then\n                \u03b22 (Y1,t \u2212 \u03bbY2,t ) = \u03b22 Y1,t \u2212 \u03b21 Y2,t = \u03b22 \u00171,t \u2212 \u03b21 \u00172,t  (15.1)\nis free of the trend Wt , and therefore is I(0).\n    The de\ufb01nition of cointegration extends to more than two time series.\nA d-dimensional multivariate time series is cointegrated of order r if the\ncomponent series are I(1) but r independent linear combinations of the com-\nponents are I(0) for some r, 0 < r \u2264 d. Somewhat di\ufb00erent de\ufb01nitions of\ncointegration exist, but this one is best for our purposes.\n    In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.1 we saw the danger of spurious regression when the residuals\nare integrated. This problem should make one cautious about regression with\nnonstationary time series. However, if Yt is regressed on Xt and the two series\nare cointegrated, then the residuals will be I(0) so that the least-squares\nestimator will be consistent.\n    The Phillips\u2013Ouliaris cointegration test regresses one integrated series on\nothers and applies the Phillips\u2013Perron unit root test to the residuals. The\nnull hypothesis is that the residuals are unit root nonstationary, which implies\nthat the series are not cointegrated. Therefore, a small p-value implies that\nthe series are cointegrated and therefore suitable for regression analysis. The\nresiduals will still be correlated and so they should be modeled as such; see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "1 we saw the danger of spurious regression when the residuals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.3.3.\n\n         3\u2212month                            6\u2212month                             1\u2212year                     2\u2212year\n8\n\n\n\n\n                           8\n\n\n\n\n                                                                    8\n\n\n\n\n                                                                                                 8\n6\n\n\n\n\n                           6\n\n\n\n\n                                                                    6\n\n\n\n\n                                                                                                 6\n4\n\n\n\n\n                           4\n\n\n\n\n                                                                    4\n\n\n\n\n                                                                                                 4\n2\n\n\n\n\n                           2\n\n\n\n\n                                                                    2\n\n\n\n\n                                                                                                 2\n0\n\n\n\n\n    Jan 02 Jan 04 Jan 02               Jan 02 Jan 04 Jan 02               Jan 02 Jan 04 Jan 02       Jan 02 Jan 04 Jan 02\n     1990   1999   2008                 1990   1999   2008                 1990   1999   2008         1990   1999   2008\n\n\n\n\n          3\u2212year                            residuals                      ACF of residuals\n8\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.3",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n                           0.1\n6\n\n\n\n\n                                                              ACF\n                           \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 \u22120.1\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "\u22120.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n4\n2\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n    Jan 02 Jan 04 Jan 02               Jan 02 Jan 04 Jan 02                 0   10    20   30\n     1990   1999   2008                 1990   1999   2008\n                                                                                     lag\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "Jan 02 Jan 04 Jan 02               Jan 02 Jan 04 Jan 02                 0   10    20   30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1. Time series plots of the \ufb01ve yields and the residuals from a regression of\nthe 1-year yields on the other four yields. Also, a sample ACF plot of the residuals.\n\f                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.1",
      "section_title": "Time series plots of the \ufb01ve yields and the residuals from a regression of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2 Vector Error Correction Models     455\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Vector Error Correction Models     455",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1. Phillips\u2013Ouliaris test on bond yields\n\n    This example uses three-month, six-month, one-year, two-year, and three-\nyear bond yields recorded daily from January 2, 1990 to October 31, 2008, for\na total of 4,714 observations. The \ufb01ve yield series are plotted in Fig. 15.1, and\none can see that they track each other somewhat closely. This suggests that\nthe \ufb01ve series may be cointegrated. The one-year yields were regressed on the\nfour others and the residuals and their ACF are also plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.1",
      "section_title": "Phillips\u2013Ouliaris test on bond yields",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1. The\ntwo residual plots are ambiguous about whether the residuals are stationary,\nso a test of cointegration would be helpful.\n1 library(forecast)\n2 library(tseries)\n3 library(urca)\n\n4 library(xts)\n\n5 yieldDat = read.table(\"treasury_yields.txt\", header=T)\n\n6 date = as.Date(yieldDat[,1], format = \"%m/%d/%y\")\n\n7 dat = as.xts(yieldDat[,3:7], date)\n\n8 res = residuals(lm(dat[,3]~dat[,1]+dat[,2]+dat[,4]+dat[,5]))\n\n\n    Next, the Phillips\u2013Ouliaris test was run using the R function po.test()\nin the tseries package.\n9   po.test(dat[,c(3,1,2,4,5)])\n            Phillips-Ouliaris Cointegration Test\n\n    data: dat[, c(3, 1, 2, 4, 5)]\n    Phillips-Ouliaris demeaned = -323.546, Truncation lag\n    parameter = 47, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.1",
      "section_title": "The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01\n\n    Warning message:\n    In po.test(dat[, c(3, 1, 2, 4, 5)]) : p-value smaller\n    than printed p-value\n   The p-value is computed by interpolation if it is within the range of a\ntable in Phillips and Ouliaris (1990). In this example, the p-value is outside\nthe range and we know only that it is below 0.01, the lower limit of the table.\nThe small p-value leads to the conclusion that the residuals are stationary\nand so the \ufb01ve series are cointegrated.\n   Though stationary, the residuals have a large amount of autocorrelation\nand may have long-term memory. They take a long time to revert to their\nmean of zero. Devising a pro\ufb01table trading strategy from these yields seems\nproblematic.                                                                 \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "Warning message:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2 Vector Error Correction Models\nThe regression approach to cointegration is somewhat unsatisfactory, since\none series must be chosen as the dependent variable, and this choice must be\n\f456       15 Cointegration\n\nsomewhat arbitrary. In Example 15.1, the middle yield, ordered by maturity,\nwas used but for no compelling reason. Moreover, regression will \ufb01nd only one\ncointegration vector, but there could be more than one.\n    An alternative approach to cointegration that treats the series symmet-\nrically uses a vector error correction model (VECM). In these models, the\ndeviation from the mean is called the \u201cerror\u201d and whenever the stationary\nlinear combination deviates from its mean, it is subsequently pushed back\ntoward its mean (the error is \u201ccorrected\u201d).\n    The idea behind error correction is simplest when there are only two series,\nY1,t and Y2,t . In this case, the error correction model is\n\n                        \u0394Y1,t = \u03c61 (Y1,t\u22121 \u2212 \u03bbY2,t\u22121 ) + \u00171,t ,               (15.2)\n                        \u0394Y2,t = \u03c62 (Y1,t\u22121 \u2212 \u03bbY2,t\u22121 ) + \u00172,t ,               (15.3)\n\nwhere \u00171,t and \u00172,t are white noise. Subtracting \u03bb times (15.3) from (15.2)\ngives\n\n       \u0394(Y1,t \u2212 \u03bbY2,t ) = (\u03c61 \u2212 \u03bb\u03c62 )(Y1,t\u22121 \u2212 \u03bbY2,t\u22121 ) + (\u00171,t \u2212 \u03bb\u00172,t ).   (15.4)\n\nLet Ft denote the information set at time t. If (\u03c61 \u2212 \u03bb\u03c62 ) < 0, then\nE {\u0394(Y1,t \u2212 \u03bbY2,t )|Ft\u22121 } is opposite in sign to Y1,t\u22121 \u2212 \u03bbY2,t\u22121 . This causes\nerror correction because whenever Y1,t\u22121 \u2212 \u03bbY2,t\u22121 is positive, its expected\nchange is negative and vice versa.\n    A rearrangement of (15.4) shows that Y1,t\u22121 \u2212 \u03bbY2,t\u22121 is an AR(1) process\nwith coe\ufb03cient 1+\u03c61 \u2212\u03bb\u03c62 . Therefore, the series Y1,t \u2212\u03bbY2,t is I(0), unit-root\nnonstationary, or an explosive series in the cases where |1 + \u03c61 \u2212 \u03bb\u03c62 | is less\nthan 1, equal to 1, and greater than 1, respectively.\n\n\u2022     If \u03c61 \u2212 \u03bb\u03c62 > 0, then 1 + \u03c61 \u2212 \u03bb\u03c62 > 1 and Y1,t \u2212 \u03bbY2,t is explosive.\n\u2022     If \u03c61 \u2212 \u03bb\u03c62 = 0, then 1 + \u03c61 \u2212 \u03bb\u03c62 = 1 and Y1,t \u2212 \u03bbY2,t is a random walk.\n\u2022     If \u03c61 \u2212 \u03bb\u03c62 < 0, then 1 + \u03c61 \u2212 \u03bb\u03c62 < 1 and Y1,t \u2212 \u03bbY2,t is stationary, unless\n      \u03c61 \u2212 \u03bb\u03c62 \u2264 \u22122, so that 1 + \u03c61 \u2212 \u03bb\u03c62 \u2264 \u22121.\n\n    The case \u03c61 \u2212 \u03bb\u03c62 \u2264 \u22122 is \u201cover-correction.\u201d The change in Y1,t \u2212 \u03bbY2,t\nis in the correct direction but too large, so the series oscillates in sign but\ndiverges to \u221e in magnitude.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Vector Error Correction Models",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, then 1 + \u03c61 \u2212 \u03bb\u03c62 > 1 and Y1,t \u2212 \u03bbY2,t is explosive.\n\u2022     If \u03c61 \u2212 \u03bb\u03c62 = 0, then 1 + \u03c61 \u2212 \u03bb\u03c62 = 1 and Y1,t \u2212 \u03bbY2,t is a random walk.\n\u2022     If \u03c61 \u2212 \u03bb\u03c62 < 0, then 1 + \u03c61 \u2212 \u03bb\u03c62 < 1 and Y1,t \u2212 \u03bbY2,t is stationary, unless\n      \u03c61 \u2212 \u03bb\u03c62 \u2264 \u22122, so that 1 + \u03c61 \u2212 \u03bb\u03c62 \u2264 \u22121.",
        "start": 1826,
        "end": 2097
      }
    ]
  },
  {
    "content": "15.2. Simulation of an error correction model\n\n    Model (15.2)\u2013(15.3) was simulated with \u03c61 = 0.5, \u03c62 = 0.55, and \u03bb = 1.\nA total of 5,000 observations were simulated, but, for visual clarity, only every\n10th observation is plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Simulation of an error correction model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2. Neither Y1,t nor Y2,t is stationary, but\nY1,t \u2212 \u03bbY2,t is stationary. Notice how closely Y1,t and Y2,t track one another.\n10 n = 5000\n11 set.seed(12345)\n12 a1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Neither Y1,t nor Y2,t is stationary, but",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\f                                                 15.2 Vector Error Correction Models                          457\n\n13 a2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "15.2 Vector Error Correction Models                          457",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.55\n14 lambda = 1\n15 y1 = rep(0,n)\n\n16 y2 = y1\n\n17 e1 = rnorm(n)\n\n18 e2 = rnorm(n)\n\n19 for (i in 2:n){\n\n20   y1[i] = y1[i-1] + a1 * (y1[i-1] - lambda*y2[i-1]) + e1[i]\n21   y2[i] = y2[i-1] + a2 * (y1[i-1] - lambda*y2[i-1]) + e2[i]\n22 }\n\n\n                                                                                                               \u0002\n   To see how to generalize error correction to more than two series, it is\nuseful to rewrite Eqs. (15.2) and (15.3) in vector form. Let Y t = (Y1,t , Y2,t )\u0003\nand t = (\u00171,t , \u00172,t )\u0003 . Then\n\n                                  \u0394Y t = \u03b1\u03b2 \u0003 Y t\u22121 +               t,                                    (15.5)\n\nwhere                             \u0007          \b                    \u0007        \b\n                                      \u03c61                               1\n                            \u03b1=                   and         \u03b2=                    ,                      (15.6)\n                                      \u03c62                              \u2212\u03bb\nso that \u03b2 is the cointegration vector, and \u03b1 speci\ufb01es the speed of mean-\nreversion and is called the loading matrix or adjustment matrix .\n   Model (15.5) also applies when there are d series such that Y t and t\nare d-dimensional. In this case \u03b2 and \u03b1 are each full-rank d \u00d7 r matrices for\n\n                     Y1                               Y2                                       Y1\u2212\u03bbY2\n                                                                           10 15\n     1000\n\n\n\n\n                                      1000\n\n\n\n\n                                                                           5\n                                                                           0\n     500\n\n\n\n\n                                      500\n\n\n\n\n                                                                           \u221210\n                                      0\n     0\n\n\n\n\n            0 100     300   500              0 100     300    500                      0 100      300   500\n                    Index                            Index                                      Index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.55",
      "section_title": "14 lambda = 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2. Simulation of an error correction model. 5,000 observations were simu-\nlated but only every 10th is plotted.\n\n\nsome r \u2264 d which is the number of linearly independent cointegration vectors.\nThe columns of \u03b2 are the cointegration vectors.\n   Model (15.5) is a vector AR(1) [that is, VAR(1)] model but, for added\n\ufb02exibility, can be extended to a VAR(p) model, and there are several ways to\ndo this. We will use the notation and the second of two forms of the VECM\nfrom the function ca.jo() in R\u2019s urca package. This VECM is\n\f458      15 Cointegration\n\n \u0394Y t = \u0393 1 \u0394Y t\u22121 + \u00b7 \u00b7 \u00b7 + \u0393 p\u22121 \u0394Y t\u2212p+1 + \u03a0 Y t\u22121 + \u03bc + \u03a6D t + t , (15.7)\n\nwhere \u03bc is a mean vector, D t is a vector of nonstochastic regressors, and\n\n                                    \u03a0 = \u03b1\u03b2 \u0003 .                                (15.8)\n\nAs before, \u03b2 and \u03b1 are each full-rank d\u00d7r matrices and \u03b1 is called the loading\nmatrix.\n   It is easy to show that the columns of \u03b2 are the cointegration vectors.\nSince Y t is I(1), \u0394Y t on the left-hand side of (15.7) is I(0) and therefore\n\u03a0 Y t\u22121 = \u03b1\u03b2 \u0003 Y t\u22121 on the right-hand side of (15.7) is also I(0). It follows\nthat each of the r components of \u03b2 \u0003 Y t\u22121 is I(0).\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Simulation of an error correction model. 5,000 observations were simu-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.3. VECM test on bond yields\n\n    A VECM was \ufb01t to the bond yields using R\u2019s ca.jo() function. The output\nis below. The eigenvalues are used to test null hypotheses of the form H0 :\nr \u2264 r0 . The values of the test statistics and critical values (for 1 %, 5 %,\nand 10 % level tests) are listed below the eigenvalues. The null hypothesis is\nrejected when the test statistic exceeds the critical level. In this case, regardless\nof whether one uses a 1 %, 5 %, or 10 % level test, one accepts that r is less\nthan or equal to 3 but rejects that r is less than or equal to 2, so one concludes\nthat r = 3. Although \ufb01ve cointegration vectors are printed, only the \ufb01rst three\nwould be meaningful. The cointegration vectors are the columns of the matrix\nlabeled \u201cEigenvectors, normalized to \ufb01rst column.\u201d The cointegration vectors\nare determined only up to multiplication by a nonzero scalar and so can be\nnormalized so that their \ufb01rst element is 1.\n      ######################\n      # Johansen-Procedure #\n      ######################\n\n      Test type: maximal eigenvalue statistic (lambda max),\n      with linear trend\n\n      Eigenvalues (lambda):\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.3",
      "section_title": "VECM test on bond yields",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03436 0.02377 0.01470 0.00140 0.00055\n\n      Values of test statistic and critical values of test:\n\n                 test 10pct 5pct 1pct\n      r <= 4 |   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03436",
      "section_title": "0.02377 0.01470 0.00140 0.00055",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.59   6.5 8.18 11.6\n      r <= 3 |   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.59",
      "section_title": "6.5 8.18 11.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.62 12.9 14.90 19.2\n      r <= 2 | ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.62",
      "section_title": "12.9 14.90 19.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "69.77 18.9 21.07 25.8\n      r <= 1 | ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "69.77",
      "section_title": "18.9 21.07 25.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "113.36 24.8 27.14 32.1\n      r = 0 | ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "113.36",
      "section_title": "24.8 27.14 32.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "164.75 30.8 33.32 38.8\n\n      Eigenvectors, normalised to first column:\n\f                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "164.75",
      "section_title": "30.8 33.32 38.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.3 Trading Strategies     459\n\n   (These are the cointegration relations)\n\n             X3mo.l2 X6mo.l2 X1yr.l2 X2yr.l2 X3yr.l2\n   X3mo.l2     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.3",
      "section_title": "Trading Strategies     459",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.000    1.00    1.00 1.0000    1.000\n   X6mo.l2    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.000",
      "section_title": "1.00    1.00 1.0000    1.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.951    2.46    1.07 0.0592    0.897\n   X1yr.l2     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.951",
      "section_title": "2.46    1.07 0.0592    0.897",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.056   14.25   -3.95 -2.5433 -1.585\n   X2yr.l2     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.056",
      "section_title": "14.25   -3.95 -2.5433 -1.585",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.304 -46.53     3.51 -3.4774 -0.118\n   X3yr.l2    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.304",
      "section_title": "-46.53     3.51 -3.4774 -0.118",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.412   30.12   -1.71 5.2322    1.938\n\n   Weights W:\n   (This is the loading matrix)\n\n           X3mo.l2   X6mo.l2   X1yr.l2   X2yr.l2   X3yr.l2\n   X3mo.d -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.412",
      "section_title": "30.12   -1.71 5.2322    1.938",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03441 -0.002440 -0.011528 -0.000178 -0.000104\n   X6mo.d ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03441",
      "section_title": "-0.002440 -0.011528 -0.000178 -0.000104",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01596 -0.002090 -0.007066 0.000267 -0.000170\n   X1yr.d -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01596",
      "section_title": "-0.002090 -0.007066 0.000267 -0.000170",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00585 -0.001661 -0.001255 0.000358 -0.000289\n   X2yr.d ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00585",
      "section_title": "-0.001661 -0.001255 0.000358 -0.000289",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00585 -0.000579 -0.003673 -0.000072 -0.000412\n   X3yr.d ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00585",
      "section_title": "-0.000579 -0.003673 -0.000072 -0.000412",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01208 -0.000985 -0.000217 -0.000431 -0.000407\n\n                                                                                 \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01208",
      "section_title": "-0.000985 -0.000217 -0.000431 -0.000407",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.3 Trading Strategies\n\nAs discussed previously, price series that are cointegrated can be used in sta-\ntistical arbitrage. Unlike pure arbitrage, statistical arbitrage means an opp-\nortunity where a pro\ufb01t is only likely, not guaranteed. Pairs trading uses pairs\nof cointegrated asset prices and has been a popular statistical arbitrage tech-\nnique. Pairs trading requires the trader to \ufb01nd cointegrated pairs of assets,\nto select from these the pairs that can be traded pro\ufb01tably after accounting\nfor transaction costs, and \ufb01nally to design a trading strategy which includes\nthe buy and sell signals. A full discussion of statistical arbitrage is outside the\nscope of this book, but see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.3",
      "section_title": "Trading Strategies",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.4 for further reading.\n    Although many \ufb01rms have been very successful using statistical arbitrage,\none should be mindful of the risks. One is model risk; the error-correction\nmodel may be incorrect. Even if the model is correct, one must use estimates\nbased on past data and the parameters might change, perhaps rapidly. If sta-\ntistical arbitrage opportunities exist, then it is possible that other traders have\ndiscovered them and their trading activity is one reason to expect parameters\nto change. Another risk is that one can go bankrupt before a stationary pro-\ncess reverts to its mean. This risk is especially large because \ufb01rms engaging\nin statistical arbitrage are likely to be heavily leveraged. High leverage will\nmagnify a small loss caused when a process diverges even farther from its\nmean before reverting. See Sects. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.4",
      "section_title": "for further reading.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.2 and 15.5.4.\n\f460    15 Cointegration\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "2 and 15.5.4.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.4 Bibliographic Notes\nAlexander (2001), Enders (2004), and Hamilton (1994) contain useful discus-\nsions of cointegration. Pfa\ufb00 (2006) is a good introduction to the analysis of\ncointegrated time series using R.\n    The MLEs and likelihood ratio tests of the parameters in (15.7) were\ndeveloped by Johansen (1991), Johansen (1995) and Johansen and Juselius\n(1990).\n    The applications of cointegration theory in statistical arbitrage are dis-\ncussed by Vidyamurthy (2004) and Alexander, Giblin, and Weddington (2001).\nPole (2007) is a less technical introduction to statistical arbitrage.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.4",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5.1 Cointegration Analysis of Midcap Prices\nThe data set midcapD.ts.csv has daily log returns on 20 midcap stocks\nin columns 2\u201321. Columns 1 and 22 contain the date and market returns,\nrespectively. In this section, we will use returns on the \ufb01rst 10 stocks. To \ufb01nd\nthe stock prices from the returns, we use the relationship\n                          Pt = P0 exp(r1 + \u00b7 \u00b7 \u00b7 + rt ),\nwhere Pt and rt are the price and log return at time t. The returns will be used\nas approximations to the log returns. The prices at time 0 are unknown, so we\nwill use P0 = 1 for each stock. This means that the price series we use will be\no\ufb00 by multiplicative factors. This does not a\ufb00ect the number of cointegration\nvectors. If we \ufb01nd that there are cointegration relationships, then it would be\nnecessary to get the price data to investigate trading strategies.\n    Johansen\u2019s cointegration analysis will be applied to the prices with the\nca.jo() function in the urca package. Run\n1 library(urca)\n2 midcapD.ts = read.csv(\"midcapD.ts.csv\",header=T)\n3 x = midcapD.ts[,2:11]\n\n4 prices= exp(apply(x,2,cumsum))\n\n5 options(digits=3)\n\n6 summary(ca.jo(prices))\n\n\n\nProblem 1 How many cointegration vectors were found?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "1 Cointegration Analysis of Midcap Prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5.2 Cointegration Analysis of Yields\nThis example is similar to Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "2 Cointegration Analysis of Yields",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.3 but uses di\ufb00erent yield data. The\ndata are in the mk.zero2.csv data set. There are 55 maturities and they\nare in the vector mk.maturity. We will use only the \ufb01rst 10 yields. Run the\nfollowing commands in R.\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.3",
      "section_title": "but uses di\ufb00erent yield data. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5 R Lab      461\n\n1 library(urca)\n2 mk.maturity = read.csv(\"mk.zero2.csv\", header=T)\n3 summary(ca.jo(mk.maturity[,2:11]))\n\n\n\nProblem 2 What maturities are being used? Are they short-, medium-, or\nlong-term, or a mixture of short- and long-term maturities?\n\n\nProblem 3 How many cointegration vectors were found? Use 1 % level tests.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "R Lab      461",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5.3 Cointegration Analysis of Daily Stock Prices\nThe CokePepsi.csv data set contains the adjusted daily closing prices of\nCoke and Pepsi stock from January 2007 to November 2012. Run the following\ncommands in R.\n1    CokePepsi = read.table(\"CokePepsi.csv\", header=T)\n2    ts.plot(CokePepsi)\n\nProblem 4 Do these two series appear cointegrated from the time series plot?\nWhy?\n\n     Now make a time series plot of the di\ufb00erence between the two prices.\n3    ts.plot(CokePepsi[,2] - CokePepsi[,1])\n\nProblem 5 Does this di\ufb00erence series appear stationary? Why?\n\n     Run the following commands to conduct Johansen\u2019s cointegration test.\n4    library(urca)\n5    summary(ca.jo(CokePepsi))\n\nProblem 6 Are these two series cointegrated? Why?\n   Now consider the daily adjusted closing prices for 10 company stocks from\nJanuary 2, 1987 to September 1, 2006 from the Stock FX Bond.csv dataset.\n6 Stock_FX_Bond = read.csv(\"Stock_FX_Bond.csv\", header=T)\n7 adjClose = Stock_FX_Bond[,seq(from=3, to=21, by=2)]\n8 ts.plot(adjClose)\n\n9 summary(ca.jo(adjClose))\n\n\n\nProblem 7 Are these 10 stock price series cointegrated? If so, what is the\nrank of the cointegrating matrix, and what are the cointegrating vectors?\n     Rerun the Johansen\u2019s cointegration test with lag K = 8.\n10   summary(ca.jo(adjClose, K=8))\n\nProblem 8 Are these 10 stock price series cointegrated if Johansen\u2019s cointe-\ngration test is conducted with lag K = 8? If so, has the estimated rank of the\ncointegrating matrix or the cointegrating vectors changed?\n\f462    15 Cointegration\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "3 Cointegration Analysis of Daily Stock Prices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.5.4 Simulation\n\nIn this section, you will run simulations similar to those in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.5",
      "section_title": "4 Simulation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.2. The\ndi\ufb00erence is that now the price process is mean-reverting.\n    Suppose a hedge fund owns a $1,000,000 position in a portfolio and used\n$50,000 of its own capital and $950,000 in borrowed money for the purchase.\nIf the value of the portfolio falls below $950,000 at the end of any trading day,\nthen the hedge fund must liquidate and repay the loan.\n    The portfolio was selected by cointegration analysis and its price is an\nAR(1) process,\n                          (Pt \u2212 \u03bc) = \u03c6(Pt\u22121 \u2212 \u03bc) + \u0017t ,\nwhere Pt is the price of the portfolio at the end of trading day t, \u03bc =\n$1,030,000, \u03c6 = 0.99, and the standard deviation of \u0017t is $5000. The hedge\nfund knows that the price will eventually revert to $1,030,000 (assuming that\nthe model is correct and, of course, this is a big assumption). It has decided\nto liquidate its position on day t if Pt \u2265 $1,020,000. This will yield a pro\ufb01t\nof at least $20,000. However, if the price falls below $950,000, then it must\nliquidate and lose its entire $50,000 investment plus the di\ufb00erence between\n$950,000 and the price at liquidation.\n    In summary, the hedge fund will liquidate at the end of the \ufb01rst day such\nthat the price is either above $1,020,000 or below $950,000. In the \ufb01rst case,\nit will achieve a pro\ufb01t of at least $20,000 and in the second case it will su\ufb00er\na loss of at least $50,000. Presumably, the probability of a loss is small, and\nwe will see how small by simulation.\n    Run a simulation experiment similar to the one in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "2. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.2 to answer\nthe following questions. Use 10,000 simulations.\n\nProblem 9 What is the expected pro\ufb01t?\n\n\nProblem 10 What is the probability that the hedge fund will need to liquidate\nfor a loss?\n\n\nProblem 11 What is the expected waiting time until the portfolio is liqui-\ndated?\n\n\nProblem 12 What is the expected yearly return on the $50,000 investment?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "2 to answer",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.6 Exercises\n 1. Show that (15.4) implies that Y1,t\u22121 \u2212 \u03bbY2,t\u22121 is an AR(1) process with\n    coe\ufb03cient 1 + \u03c61 \u2212 \u03bb\u03c62 .\n\f                                                             References    463\n\n2. In (15.2) and (15.3) there are no constants, so that Y1,t \u2212 \u03bbY2,t is a sta-\n   tionary process with mean zero. Introduce constants into (15.2) and (15.3)\n   and show how they determine the mean of Y1,t \u2212 \u03bbY2,t .\n3. Verify that in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.6",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2 Y1,t \u2212 \u03bbY2,t is stationary.\n4. Suppose that Y t = (Y1,t , Y2,t )\u0003 is the bivariate AR(1) process in Example\n   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Y1,t \u2212 \u03bbY2,t is stationary.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.2. Is Y t stationary? (Hint: See Sect. 13.4.4.)\n\n\nReferences\n\nAlexander, C. (2001) Market Models: A Guide to Financial Data Analysis,\n  Wiley, Chichester.\nAlexander, C., Giblin, I., and Weddington, W. III (2001) Cointegration and\n  Asset Allocation: A New Hedge Fund, ISMA Discussion Centre Discussion\n  Papers in Finance 2001\u20132003.\nEnders, W. (2004) Applied Econometric Time Series, 2nd ed., Wiley,\n  New York.\nHamilton, J. D. (1994) Time Series Analysis, Princeton University Press,\n  Princeton, NJ.\nJohansen, S. (1991) Estimation and hypothesis testing of cointegration vectors\n  in gaussian vector autoregressive models. Econometrica, 59, 1551\u20131580.\nJohansen, S. (1995) Likelihood-Based Inference in Cointegrated Vector\n  Autoregressive Models, Oxford University Press, New York.\nJohansen, S., and Juselius, K. (1990) Maximum likelihood estimation and\n  inference on cointegration \u2014 With applications to the demand for money.\n  Oxford Bulletin of Economics and Statistics, 52, 2, 169\u2013210.\nPfa\ufb00, B. (2006) Analysis of Integrated and Cointegrated Time Series with R,\n  Springer, New York.\nPhillips, P. C. B., and Ouliaris, S. (1990) Asymptotic properties of residual\n  based tests for cointegration. Econometrica, 58, 165\u2013193.\nPole, A. (2007) Statistical Arbitrage, Wiley, Hoboken, NJ.\nVidyamurthy, G. (2004) Pairs Trading, Wiley, Hoboken, NJ.\n\f16\nPortfolio Selection\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.2",
      "section_title": "Is Y t stationary? (Hint: See Sect. 13.4.4.)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 Trading O\ufb00 Expected Return and Risk\nHow should we invest our wealth? Portfolio theory provides an answer to this\nquestion based upon two principles:\n\u2022   we want to maximize the expected return; and\n\u2022   we want to minimize the risk, which we de\ufb01ne in this chapter to be the\n    standard deviation of the return, though we may ultimately be concerned\n    with the probabilities of large losses.\nThese goals are somewhat at odds because riskier assets generally have a\nhigher expected return, since investors demand a reward for bearing risk. The\ndi\ufb00erence between the expected return of a risky asset and the risk-free rate\nof return is called the risk premium. Without risk premiums, few investors\nwould invest in risky assets.\n    Nonetheless, there are optimal compromises between expected return and\nrisk. In this chapter we show how to maximize expected return subject to an\nupper bound on the risk, or to minimize the risk subject to a lower bound on\nthe expected return. One key concept that we discuss is reduction of risk by\ndiversifying the portfolio.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "Trading O\ufb00 Expected Return and Risk",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2 One Risky Asset and One Risk-Free Asset\nWe start with a simple example with one risky asset, which could be a portfo-\nlio, for example, a mutual fund. Assume that the expected return is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "One Risky Asset and One Risk-Free Asset",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 and\nthe standard deviation of the return is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25. Assume that there is a risk-free\nasset, such as, a 90-day T-bill, and the risk-free rate is 6 %, so the return on\nthe risk-free asset is 6 %, or ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "Assume that there is a risk-free",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06. The standard deviation of the return on the\n\n\n\u00a9 Springer Science+Business Media New York 2015                             465\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 16\n\f466    16 Portfolio Selection\n\nrisk-free asset is 0 by de\ufb01nition of \u201crisk-free.\u201d The rates and returns here are\nannual, though all that is necessary is that they be in the same time units.\n    We are faced with the problem of constructing an investment portfolio that\nwe will hold for one time period, which is called the holding period and which\ncould be a day, a month, a quarter, a year, 10 years, and so forth. At the end\nof the holding period we might want to readjust the portfolio, so for now we\nare only looking at returns over one time period. Suppose that a fraction w\nof our wealth is invested in the risky asset and the remaining fraction 1 \u2212 w\nis invested in the risk-free asset. Then the expected return is\n              E(R) = w(0.15) + (1 \u2212 w)(0.06) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "The standard deviation of the return on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06 + 0.09w,              (16.1)\nthe variance of the return is\n                   2\n                  \u03c3R = w2 (0.25)2 + (1 \u2212 w)2 (0)2 = w2 (0.25)2 ,\nand the standard deviation of the return is\n                                 \u03c3R = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "+ 0.09w,              (16.1)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 |w|.                           (16.2)\nAs will be discussed later, w is negative if the risky asset is sold short, so we\nhave |w| rather than w in (16.2).\n    To decide what proportion w of one\u2019s wealth to invest in the risky asset,\none chooses either the expected return E(R) one wants or the amount of risk\n\u03c3R with which one is willing to live. Once either E(R) or \u03c3R is chosen, w can\nbe determined.\n    Although \u03c3 is a measure of risk, a more direct measure of risk is actual\nmonetary loss. In the next example, w is chosen to control the maximum size\nof the loss.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "|w|.                           (16.2)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1. Finding w to achieve a targeted value-at-risk\n\n   Suppose that a \ufb01rm is planning to invest $1,000,000 and has capital res-\nerves that could cover a loss of $150,000 but no more. Therefore, the \ufb01rm\nwould like to be certain that, if there is a loss, then it is no more than 15 %,\nthat is, that R is greater than \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "Finding w to achieve a targeted value-at-risk",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15. Suppose that R is normally distributed.\nThen the only way to guarantee that R is greater than \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "Suppose that R is normally distributed.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 with probability\nequal to 1 is to invest entirely in the risk-free asset. The \ufb01rm might instead\nbe more modest and require only that P (R < \u22120.15) be small, for example,\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "with probability",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01. Therefore, the \ufb01rm should \ufb01nd the value of w such that\n                                \u0007                            \b\n                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "Therefore, the \ufb01rm should \ufb01nd the value of w such that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 \u2212 (0.06 + 0.09 w)\n            P (R < \u22120.15) = \u03a6                                  = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "\u2212 (0.06 + 0.09 w)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01.\n                                            0.25 w\nThe solution is\n                                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "0.25 w",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.21\n                      w=                           = 0.4264.\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.21",
      "section_title": "w=                           = 0.4264.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 \u03a6\u22121 (0.01) + 0.9\nThe value of \u03a6\u22121 (0.01) is calculated by qnorm(0.01) and is \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "\u03a6\u22121 (0.01) + 0.9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.33.\n\f                            16.2 One Risky Asset and One Risk-Free Asset          467\n\n   In Chap. 19, $150,000 is called the value-at-risk (= VaR) and 1 \u2212 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.33",
      "section_title": "16.2 One Risky Asset and One Risk-Free Asset          467",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01 =\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "=",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99 is called the con\ufb01dence coe\ufb03cient. What was done in this example is to\n\ufb01nd the portfolio that has a VaR of $150,000 with ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.99",
      "section_title": "is called the con\ufb01dence coe\ufb03cient. What was done in this example is to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99 con\ufb01dence.\n   We saw in Chap. 5 that the distributions of stock returns usually have\nmuch heavier tails than a normal distribution. In Chap. 19, VaR is estimated\nunder more realistic assumptions, e.g., that the returns are t-distributed. \u0002\n\n\n   More generally, if the expected returns on the risky and risk-free assets\nare \u03bc1 and \u03bcf and if the standard deviation of the risky asset is \u03c31 , then\nthe expected return on the portfolio is w\u03bc1 + (1 \u2212 w)\u03bcf while the standard\ndeviation of the portfolio\u2019s return is |w| \u03c31 .\n   This model is simple but not as useless as it might seem at \ufb01rst. As dis-\ncussed later, \ufb01nding an optimal portfolio can be achieved in two steps:\n 1. \ufb01nding the \u201coptimal\u201d portfolio of risky assets, called the \u201ctangency port-\n    folio,\u201d and\n 2. \ufb01nding the appropriate mix of the risk-free asset and the tangency\n    portfolio.\n    So we now know how to do the second step. What we still need to learn\nis how \ufb01nd the tangency portfolio.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.99",
      "section_title": "con\ufb01dence.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2.1 Estimating E(R) and \u03c3R\n\nThe value of the risk-free rate, \u03bcf , will be known since Treasury bill rates are\npublished in sources providing \ufb01nancial information.\n    What should we use as the values of E(R) and \u03c3R ? If returns on the asset\nare assumed to be stationary, then we can take a time series of past returns\nand use the sample mean and standard deviation. Whether the stationarity\nassumption is realistic is always debatable. If we think that E(R) and \u03c3R\nwill be di\ufb00erent from the past, we could subjectively adjust these estimates\nupward or downward according to our opinions, but we must live with the\nconsequences if our opinions prove to be incorrect. Also, the sample mean\nand standard deviation are not particularly accurate and could be replaced\nby estimates from a factor model such as the CAPM or the Fama-French\nmodel; see Chaps. 17, 18, and 20.\n    Another question is how long a time series to use, that is, how far back in\ntime one should gather data. A long series, say 10 or 20 years, will give much\nless variable estimates. However, if the series is not stationary but rather has\nslowly drifting parameters, then a shorter series (maybe 1 or 2 years) will be\nmore representative of the future. Almost every time series of returns is nearly\nstationary over short enough time periods.\n    Even if the time series is stationary, it is likely to exhibit volatility cluster-\ning. In that case one might use a GARCH estimate of the conditional standard\ndeviation of the return over the holding period. See Chap. 14.\n\f468    16 Portfolio Selection\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "1 Estimating E(R) and \u03c3R",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3 Two Risky Assets\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "Two Risky Assets",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3.1 Risk Versus Expected Return\n\nThe mathematics of mixing risky assets is most easily understood when there\nare only two risky assets. This is where we start.\n    Suppose the two risky assets have returns R1 and R2 and that we mix\nthem in proportions w and 1 \u2212 w, respectively. The return on the portfolio\nis Rp = wR1 + (1 \u2212 w)R2 . The expected return on the portfolio is E(RP ) =\nw\u03bc1 + (1 \u2212 w)\u03bc2 . Let \u03c112 be the correlation between the returns on the two\nrisky assets. The variance of the return on the portfolio is\n                 2\n                \u03c3R = w2 \u03c312 + (1 \u2212 w)2 \u03c322 + 2w(1 \u2212 w)\u03c112 \u03c31 \u03c32 .              (16.3)\n\nNote that \u03c112 \u03c31 \u03c32 = \u03c3R1 ,R2 .\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "1 Risk Versus Expected Return",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2. The expectation and variance of the return on a portfolio with\ntwo risky assets\n\n   Suppose that \u03bc1 = 0.14, \u03bc2 = 0.08, \u03c31 = 0.2, \u03c32 = 0.15, and \u03c112 = 0.\nThen\n                        E(RP ) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "The expectation and variance of the return on a portfolio with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08 + 0.06w,\nand because \u03c112 = 0 in this example,\n                       2\n                      \u03c3R P\n                           = (0.2)2 w2 + (0.15)2 (1 \u2212 w)2 .\n\nUsing di\ufb00erential calculus, one can easily show that the portfolio with the\nminimum risk is w = 0.045/0.125\u0011    = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "+ 0.06w,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.36. For this portfolio E(RP ) = 0.08 +\n(0.06)(0.36) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.36",
      "section_title": "For this portfolio E(RP ) = 0.08 +",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1016 and \u03c3RP = (0.2)2 (0.36)2 + (0.15)2 (0.64)2 = 0.12.\n    The somewhat parabolic curve1 in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1016",
      "section_title": "and \u03c3RP = (0.2)2 (0.36)2 + (0.15)2 (0.64)2 = 0.12.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 is the locus of values of\n(\u03c3R , E(R)) when 0 \u2264 w \u2264 1. The leftmost point on this locus achieves\nthe minimum value of the risk and is called the minimum variance portfo-\nlio. The points on this locus that have an expected return at least as large\nas the minimum variance portfolio are called the e\ufb03cient frontier. Portfolios\non the e\ufb03cient frontier are called e\ufb03cient portfolios or, more precisely, mean-\nvariance e\ufb03cient portfolios.2 The points labeled R1 and R2 correspond to\nw = 1 and w = 0, respectively. The other features of this \ufb01gure are explained\nin Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "is the locus of values of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4.                                                               \u0002\n\n\n\n1                                             2\n  In fact, the curve would be parabolic if \u03c3R   were plotted on the x-axis instead of\n  \u03c3R .\n2\n  When a risk-free asset is available, then the e\ufb03cient portfolios are no longer those\n  on the e\ufb03cient frontier but rather are characterized by Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 ahead.\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "ahead.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4 Combining Two Risky Assets with a Risk-Free Asset   469\n\n\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "Combining Two Risky Assets with a Risk-Free Asset   469",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14\n                                                                      R1\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.14",
      "section_title": "R1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n                                                           T\n\n                                                       P\n        means\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "T",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n\n\n                                                      MV\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "MV",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n\n\n\n\n                                                           R2\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "R2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n\n\n\n\n                        F\n\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "F",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00       0.05        0.10         0.15      0.20\n                                               risk\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.05        0.10         0.15      0.20",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1. Expected return versus risk for Example 16.2. F = risk-free asset. T =\ntangency portfolio. R1 is the \ufb01rst risky asset. R2 is the second risky asset. MV is\nthe minimum variance portfolio. The e\ufb03cient frontier is the red curve. All points\non the curve connecting R2 and R1 are attainable with 0 \u2264 w \u2264 1, but the ones on\nthe black curve are suboptimal. P is a typical portfolio on the e\ufb03cient frontier.\n\n\n    In practice, the mean and standard deviations of the returns can be est-\nimated as discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "Expected return versus risk for Example 16.2. F = risk-free asset. T =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2.1 and the correlation coe\ufb03cient can be esti-\nmated by the sample correlation coe\ufb03cient. Alternatively, in Chaps. 18 and 20\nfactor models are used to estimate expected returns and the covariance matrix\nof returns.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "1 and the correlation coe\ufb03cient can be esti-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4 Combining Two Risky Assets\nwith a Risk-Free Asset\n\nOur ultimate goal is to \ufb01nd optimal portfolios combining many risky assets\nwith a risk-free asset. However, many of the concepts needed for this task can\nbe \ufb01rst understood most easily when there are only two risky assets.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "Combining Two Risky Assets",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4.1 Tangency Portfolio with Two Risky Assets\n\nAs mentioned in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "1 Tangency Portfolio with Two Risky Assets",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3.1, each point on the e\ufb03cient frontier in Fig. 16.1\nis (\u03c3RP , E(Rp )) for some value of w between 0 and 1. If we \ufb01x w, then we\nhave a \ufb01xed portfolio of the two risky assets. Now let us mix that portfo-\nlio of risky assets with the risk-free asset. The point F in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "1, each point on the e\ufb03cient frontier in Fig. 16.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 gives\n(\u03c3RP , E(R)) for the risk-free asset; of course, \u03c3RP = 0 at F. The possible\nvalues of (\u03c3RP , E(Rp )) for a portfolio consisting of the \ufb01xed portfolio of two\n\f470      16 Portfolio Selection\n\nrisky assets and the risk-free asset is a line connecting the point F with a\npoint on the e\ufb03cient frontier, for example, the dashed purple line.\n    Notice that the solid blue line connecting F with the point labeled T lies\nabove the dashed purple line connecting F and the typical portfolio. This\nmeans that for any value of \u03c3RP , the solid blue line gives a higher expected\nreturn than the dashed purple line. The slope of each line is called its Sharpe\u2019s\nratio, named after William Sharpe, whom we will meet again in Chap. 17.\nIf E(RP ) and \u03c3RP are the expected return and standard deviation of the\nreturn on a portfolio and \u03bcf is the risk-free rate, then\n\n                                    E(RP ) \u2212 \u03bcf\n                                                                           (16.4)\n                                       \u03c3 RP\n\nis Sharpe\u2019s ratio of the portfolio. Sharpe\u2019s ratio can be thought of as a \u201creward-\nto-risk\u201d ratio. It is the ratio of the reward quanti\ufb01ed by the excess expected\nreturn3 to the risk as measured by the standard deviation.\n    A line with a larger slope gives a higher expected return for a given level\nof risk, so the larger Sharpe\u2019s ratio, the better regardless of what level of risk\none is willing to accept. The point T on the e\ufb03cient frontier is the portfolio\nwith the highest Sharpe\u2019s ratio. It is the optimal portfolio for the purpose of\nmixing with the risk-free asset. This portfolio is called the tangency portfolio\nsince its line is tangent to the e\ufb03cient frontier.\n\n\nResult ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "gives",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 The optimal or e\ufb03cient portfolios mix the tangency portfolio\nwith the risk-free asset. Each e\ufb03cient portfolio has two properties:\n\u2022     it has a higher expected return than any other portfolio with the same or\n      smaller risk, and\n\u2022     it has a smaller risk than any other portfolio with the same or higher\n      expected return.\nThus we can only improve (reduce) the risk of an e\ufb03cient portfolio by accept-\ning a worse (smaller) expected return, and we can only improve (increase) the\nexpected return of an e\ufb03cient portfolio by accepting worse (higher) risk.\n\n\n   Note that all e\ufb03cient portfolios use the same mix of the two risky assets,\nnamely, the tangency portfolio. Only the proportion allocated to the tangency\nportfolio and the proportion allocated to the risk-free asset vary.\n   Given the importance of the tangency portfolio, you may be wondering\n\u201chow do we \ufb01nd it?\u201d Again, let \u03bc1 , \u03bc2 , and \u03bcf be the expected returns on\nthe two risky assets and the return on the risk-free asset. Let \u03c31 and \u03c32 be\nthe standard deviations of the returns on the two risky assets and let \u03c112 be\nthe correlation between the returns on the risky assets.\n\n3\n    Here \u201cexcess\u201d means in excess of the risk-free rate.\n\f               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "The optimal or e\ufb03cient portfolios mix the tangency portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4 Combining Two Risky Assets with a Risk-Free Asset      471\n\n   De\ufb01ne V1 = \u03bc1 \u2212 \u03bcf and V2 = \u03bc2 \u2212 \u03bcf , the excess expected returns. Then\nthe tangency portfolio uses weight\n\n                                    V1 \u03c322 \u2212 V2 \u03c112 \u03c31 \u03c32\n                   wT =                                                  (16.5)\n                            V1 \u03c322 + V2 \u03c312 \u2212 (V1 + V2 )\u03c112 \u03c31 \u03c32\n\nfor the \ufb01rst risky asset and weight (1 \u2212 wT ) for the second.\n    Let RT , E(RT ), and \u03c3T be the return, expected return, and standard\ndeviation of the return on the tangency portfolio. Then E(RT ) and \u03c3T can\nbe found by \ufb01rst \ufb01nding wT using (16.5) and then using the formulas\n\n                          E(RT ) = wT \u03bc1 + (1 \u2212 wT )\u03bc2\n\nand               #\n           \u03c3T =    wT2 \u03c312 + (1 \u2212 wT )2 \u03c322 + 2wT (1 \u2212 wT )\u03c112 \u03c31 \u03c32 .\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "Combining Two Risky Assets with a Risk-Free Asset      471",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3. The tangency portfolio with two risky assets\n\n   Suppose as before that \u03bc1 = 0.14, \u03bc2 = 0.08, \u03c31 = 0.2, \u03c32 = 0.15, and\n\u03c112 = 0. Suppose as well that \u03bcf = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "The tangency portfolio with two risky assets",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06. Then V1 = 0.14 \u2212 0.06 = 0.08 and\nV2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "Then V1 = 0.14 \u2212 0.06 = 0.08 and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08 \u2212 0.06 = 0.02. Plugging these values into formula (16.5), we get\nwT = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "\u2212 0.06 = 0.02. Plugging these values into formula (16.5), we get",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.693 and 1 \u2212 wt = 0.307. Therefore,\n\n              E(RT ) = (0.693)(0.14) + (0.307)(0.08) = 0.122,\n\nand                  \u0011\n              \u03c3T =       (0.693)2 (0.2)2 + (0.307)2 (0.15)2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.693",
      "section_title": "and 1 \u2212 wt = 0.307. Therefore,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.146.\n                                                                             \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.146",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4.2 Combining the Tangency Portfolio\nwith the Risk-Free Asset\n\nLet Rp be the return on the portfolio that allocates a fraction \u03c9 of the in-\nvestment to the tangency portfolio and 1 \u2212 \u03c9 to the risk-free asset. Then\nRp = \u03c9RT + (1 \u2212 \u03c9)\u03bcf = \u03bcf + \u03c9(RT \u2212 Rf ), so that\n\n            E(Rp ) = \u03bcf + \u03c9{E(RT ) \u2212 \u03bcf } and             \u03c3Rp = \u03c9\u03c3T .\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "2 Combining the Tangency Portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4. (Continuation of Example 16.2 and 16.3)\n\n   In this example, we will \ufb01nd the optimal investment with \u03c3Rp = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "(Continuation of Example 16.2 and 16.3)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05.\n\f472       16 Portfolio Selection\n\n    The maximum expected return with \u03c3Rp = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "472       16 Portfolio Selection",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 mixes the tangency port-\nfolio and the risk-free asset such that \u03c3Rp = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "mixes the tangency port-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05. Since \u03c3T = 0.146, we have\nthat ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "Since \u03c3T = 0.146, we have",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 = \u03c3Rp = \u03c9 \u03c3T = 0.146 \u03c9, so that \u03c9 = 0.05/0.146 = 0.343 and\n1 \u2212 \u03c9 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "= \u03c3Rp = \u03c9 \u03c3T = 0.146 \u03c9, so that \u03c9 = 0.05/0.146 = 0.343 and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.657.\n    So 65.7 % of the portfolio should be in the risk-free asset, and 34.3 % should\nbe in the tangency portfolio. Thus (0.343)(",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.657",
      "section_title": "So 65.7 % of the portfolio should be in the risk-free asset, and 34.3 % should",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "69.3 %) = 23.7 % should be in the\n\ufb01rst risky asset and (0.343)(",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "69.3",
      "section_title": "%) = 23.7 % should be in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "30.7 %) = 10.5 % should be in the second risky\nasset. The total is not quite 100 % because of rounding.                        \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "30.7",
      "section_title": "%) = 10.5 % should be in the second risky",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.5. (Continuation of Examples 16.2\u201316.4)\n\n    Now suppose that you want a 10 % expected return. In this example we\nwill compare\n\u2022     the best portfolio of only risky assets, and\n\u2022     The best portfolio of the risky assets and the risk-free asset.\n    The best portfolio of only risky assets uses w solving ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.5",
      "section_title": "(Continuation of Examples 16.2\u201316.4)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 = w(0.14)+(1\u2212w)\n(0.08), which implies that w = 1/3. This is the only portfolio of risky assets\nwith E(Rp ) = 0.1, so by default it is best. Then\n         \u0011                                  \u0011\n \u03c3RP = w2 (0.2)2 + (1 \u2212 w)2 (0.15)2 = (1/9)(0.2)2 + 4/9(0.15)2 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "= w(0.14)+(1\u2212w)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.120.\n\nThe best portfolio of the two risky assets and the risk-free asset can be found\nas follows. First, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.120",
      "section_title": "The best portfolio of the two risky assets and the risk-free asset can be found",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 = E(R) = \u03bcf + \u03c9{E(RT ) \u2212 \u03bcf } = 0.06 + 0.062 \u03c9 =\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "= E(R) = \u03bcf + \u03c9{E(RT ) \u2212 \u03bcf } = 0.06 + 0.062 \u03c9 =",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06 + 0.425 \u03c3R , since \u03c3RP = \u03c9 \u03c3T or \u03c9 = \u03c3RP /\u03c3T = \u03c3RP /0.146. This implies\nthat \u03c3RP = 0.04/",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "+ 0.425 \u03c3R , since \u03c3RP = \u03c9 \u03c3T or \u03c9 = \u03c3RP /\u03c3T = \u03c3RP /0.146. This implies",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.425 = 0.094 and \u03c9 = 0.04/0.062 = 0.645. So combining\nthe risk-free asset with the two risky assets reduces \u03c3RP from ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.425",
      "section_title": "= 0.094 and \u03c9 = 0.04/0.062 = 0.645. So combining",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.120 to 0.094\nwhile maintaining E(Rp ) at ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.120",
      "section_title": "to 0.094",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. The reduction in risk is (0.120 \u2212 0.094)/0.094\n= 28 %, which is substantial.                                                 \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "The reduction in risk is (0.120 \u2212 0.094)/0.094",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4.3 E\ufb00ect of \u03c112\n\nPositive correlation between the two risky assets increases risk. With positive\ncorrelation, the two assets tend to move together which increases the volatility\nof the portfolio. Conversely, negative correlation is bene\ufb01cial since it decreases\nrisk. If the assets are negatively correlated, a negative return of one tends\nto occur with a positive return of the other so the volatility of the portfolio\ndecreases. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "3 E\ufb00ect of \u03c112",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2 shows the e\ufb03cient frontier and tangency portfolio when\n\u03bc1 = 0.14, \u03bc2 = 0.09, \u03c31 = 0.2, \u03c32 = 0.15, and \u03bcf = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "shows the e\ufb03cient frontier and tangency portfolio when",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03. The value of \u03c112 is\nvaried from ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "The value of \u03c112 is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 to \u22120.7. Notice that Sharpe\u2019s ratio of the tangency portfolio\nreturns increases as \u03c112 decreases. This means that when \u03c112 is small, then\ne\ufb03cient portfolios have less risk for a given expected return compared to when\n\u03c112 is large.\n\f                                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "to \u22120.7. Notice that Sharpe\u2019s ratio of the tangency portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.5 Selling Short              473\n\n                            rho = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.5",
      "section_title": "Selling Short              473",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5                                                 rho = 0.3\n\n\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "rho = 0.3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.18\n\n\n\n\n                                                                   0.18\n     means                                      T\n\n\n\n\n                                                           means\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.18",
      "section_title": "0.18",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12                                                                                  T\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "T",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n             0.06\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n                    0.00   0.10          0.20       0.30                  0.00       0.10          0.20   0.30\n                                  risk                                                      risk\n\n\n                             rho = 0                                                 rho = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "0.00   0.10          0.20       0.30                  0.00       0.10          0.20   0.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7\n             0.18\n\n\n\n\n                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "0.18",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.18\n     means\n\n\n\n\n                                                           means\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.18",
      "section_title": "means",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n\n\n\n\n                                                                   0.12\n                                  T\n                                                                                 T\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "0.12",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06\n\n\n\n\n                                                                   0.06\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00   0.10          0.20       0.30                  0.00       0.10          0.20   0.30\n                                  risk                                                      risk\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.10          0.20       0.30                  0.00       0.10          0.20   0.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2. E\ufb03cient frontier (red) and tangency portfolio (T) when \u03bc1 = 0.14,\n\u03bc2 = 0.09, \u03c31 = 0.2, \u03c32 = 0.15, and \u03bcf = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "E\ufb03cient frontier (red) and tangency portfolio (T) when \u03bc1 = 0.14,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03. The value of \u03c112 is varied from 0.5\nto \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "The value of \u03c112 is varied from 0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7.\n\n\n16.5 Selling Short\nOften some of the weights in an e\ufb03cient portfolio are negative. A negative\nweight on an asset means that this asset is sold short. Selling short is a way\nto pro\ufb01t if a stock price goes down. To sell a stock short, one sells the stock\nwithout owning it. The stock must be borrowed from a broker or another\ncustomer of the broker. At a later point in time, one buys the stock and gives\nit back to the lender. This closes the short position.\n    Suppose a stock is selling at $25/share and you sell 100 shares short. This\ngives you $2500. If the stock goes down to $17/share, you can buy the 100\nshares for $1700 and close out your short position. You made a pro\ufb01t of $800\n(ignoring transaction costs) because the stock went down 8 points. If the stock\nhad gone up, then you would have had a loss.\n    Suppose now that you have $100 and there are two risky assets. With your\nmoney you could buy $150 worth of risky asset 1 and sell $50 short of risky\nasset 2. The net cost would be exactly $100. If R1 and R2 are the returns on\nrisky assets 1 and 2, then the return on your portfolio would be\n                                      \u0007     \b\n                               3          1\n                                 R1 + \u2212       R2 .\n                               2          2\nYour portfolio weights are w1 = 3/2 and w2 = \u22121/2. Thus, you hope that\nrisky asset 1 rises in price and risky asset 2 falls in price. Here, again, we have\nignored transaction costs.\n\f474    16 Portfolio Selection\n\n   If one sells a stock short, one is said to have a short position in that stock,\nand owning the stock is called a long position.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "16.5 Selling Short",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 Risk-E\ufb03cient Portfolios with N Risky Assets\n\nIn this section, we use quadratic programming to \ufb01nd e\ufb03cient portfolios\nwith an arbitrary number of assets. An advantage of quadratic program-\nming is that it allows one to impose constraints such as limiting short sales.\nWith no constraints on the allocation vector w, analytic formulas for the tan-\ngency portfolio can be derived using Lagrange multipliers, but this approach\ndoes not generalize to constrained w.\n   Assume that we have N risky assets and that the return on the ith risky\nasset is Ri and has expected value \u03bci . De\ufb01ne\n                                         \u239b\u239e\n                                      R1\n                                        .\n                                 R = \u239d .. \u23a0\n                                             RN\n\nto be the random vector of returns,\n                                             \u239b\n                                             \u239e\n                                         \u03bc1\n                                        \u239c . \u239f\n                             E(R) = \u03bc = \u239d .. \u23a0 ,\n                                                  \u03bcN\n\nand \u03a3 to be the covariance matrix of R.\n   Let                             \u239b     \u239e\n                                      w1\n                                       .\n                              w = \u239d .. \u23a0\n                                     wN\nbe a vector of portfolio weights so that w1 + \u00b7 \u00b7 \u00b7 + wN = 1T w = 1, where\n                                       \u239b \u239e\n                                         1\n                                         .\n                                  1 = .. \u23a0\n                                       \u239d\n                                         1\n\nis a column of N ones. The expected return on the portfolio is\n                                 N\n                                      wi \u03bci = \u03c9 T \u03bc.                       (16.6)\n                                i=1\n\n   Suppose there is a target value, \u03bcP , of the expected return on the portfolio.\nWhen N = 2, the target expected returns is achieved by only one portfolio\nand its w1 -value solves \u03bcP = w1 \u03bc1 + w2 \u03bc2 = \u03bc2 + w1 (\u03bc1 \u2212 \u03bc2 ). For N \u2265 3,\n\f                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "Risk-E\ufb03cient Portfolios with N Risky Assets",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 Risk-E\ufb03cient Portfolios with N Risky Assets      475\n\nthere will be an in\ufb01nite number of portfolios achieving the target \u03bcP . The\none with the smallest variance is called the \u201ce\ufb03cient\u201d portfolio. Our goal is\nto \ufb01nd the e\ufb03cient portfolio.\n    The variance of the return on the portfolio with weights w is\n\n                                    wT \u03a3w.                                  (16.7)\n\nThus, given a target \u03bcP , the e\ufb03cient portfolio minimizes (16.7) subject to\n\n                                   w T \u03bc = \u03bcP                               (16.8)\n\nand\n                                   wT 1 = 1.                                (16.9)\n    Quadratic programming is used to minimize a quadratic objective function\nsubject to linear constraints. In applications to portfolio optimization, the obj-\nective function is the variance of the portfolio return. The objective function\nis a function of N variables, such as the weights of N assets, that are denoted\nby an N \u00d7 1 vector x. Suppose that the quadratic objective function to be\nminimized is\n                                 1 T\n                                   x Dx \u2212 dT x,                            (16.10)\n                                 2\nwhere D is an N \u00d7 N matrix and d is an N \u00d7 1 vector. The factor of 1/2 is\nnot essential but is used here to keep our notation consistent with R. There\nare two types of linear constraints on x, inequality and equality constraints.\nThe linear inequality constraints are\n\n                                 AT\n                                  neq x \u2265 bneq ,                           (16.11)\n\nwhere Aneq is an m \u00d7 N matrix, bneq is an m \u00d7 1 vector, and m is the number\nof inequality constraints. The equality constraints are\n\n                                  AT\n                                   eq x = beq ,                            (16.12)\n\nwhere Aeq is an n \u00d7 N matrix, beq is an n \u00d7 1 vector, and n is the number of\nequality constraints. Quadratic programming minimizes the quadratic objec-\ntive function (16.10) subject to linear inequality constraints (16.11) and linear\nequality constraints (16.12).\n    To apply quadratic programming to \ufb01nd an e\ufb03cient portfolio, we use x =\nw, D = 2\u03a3, and d equal to an N \u00d71 vector of zeros so that (16.10) is wT \u03a3w,\nthe return variance of the portfolio. There are two equality constraints, one\nthat the weights sum to 1 and the other that the portfolio return is a speci\ufb01ed\ntarget \u03bcP . Therefore, we de\ufb01ne\n                                         \u0007 T\b\n                                           1\n                                 AT    =\n                                    eq\n                                           \u03bcT\n\f476    16 Portfolio Selection\n\nand\n                                               \u0007        \b\n                                                    1\n                                    beq =                   ,\n                                                   \u03bcP\nso that (16.12) becomes\n                                \u0007          \b       \u0007            \b\n                                    1T w                 1\n                                               =                    ,\n                                    \u03bcT w                \u03bcP\n\nwhich is the same as constraints (16.8) and (16.9). So far, inequality con-\nstraints have not been used.\n    Investors often wish to impose additional inequality constraints. If an\ninvestor cannot or does not wish to sell short, then the constraint\n\n                                           w\u22650\n\ncan be used. Here 0 is a vector of N zeros. In this case Aneq is the N \u00d7 N\nidentical matrix and bneq = 0.\n    To avoid concentrating the portfolio in just one or a few stocks, an investor\nmay wish to constrain the portfolio so that no wi exceeds a bound \u03bb, for\nexample, \u03bb = 1/4 means that no more than 1/4 of the portfolio can be in any\nsingle stock. In this case, w \u2264 \u03bb1 or equivalently \u2212w \u2265 \u2212\u03bb1, so that Aneq\nis minus the N \u00d7 N identity matrix and bneq = \u2212\u03bb1. One can combine these\nconstraints with those that prohibit short selling.\n    To \ufb01nd the e\ufb03cient frontier, one uses a grid of values of \u03bcP and \ufb01nds\nthe corresponding e\ufb03cient portfolios. For each portfolio, \u03c3P2 , which is the\nminimized value of the objective function, can be calculated. Then one can\n\ufb01nd the minimum variance portfolio by \ufb01nding the portfolio with the smallest\nvalue of the \u03c3P2 . The e\ufb03cient frontier is the set of e\ufb03cient portfolios with\nexpected return above the expected return of the minimum variance portfolio.\nOne can also compute Sharpe\u2019s ratio for each portfolio on the e\ufb03cient frontier\nand the tangency portfolio is the one maximizing Sharpe\u2019s ratio.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "Risk-E\ufb03cient Portfolios with N Risky Assets      475",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6. Finding the e\ufb03cient frontier, tangency portfolio, and minimum\nvariance portfolio using quadratic programming\n\n    The following R program uses the returns on three stocks, GE, IBM,\nand Mobil, in the CRSPday data set in the Ecdat package. The function\nsolve.QP() in the quadprog package is used for quadratic programming.\nsolve.QP() combines AT              T\n                         eq and Aneq into a single matrix Amat by stacking\nAT               T                                                      T\n  eq on top of Aneq . The parameter meq is the number of rows of Aeq . beq\nand bneq are handled analogously. In this example, there are no inequality\nconstraints, so AT\n                 neq and bneq are not needed, but they are used in the next\nexample.\n    The e\ufb03cient portfolio is found for each of 300 target values of \u03bcP between\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "Finding the e\ufb03cient frontier, tangency portfolio, and minimum",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 and 0.14. For each portfolio, Sharpe\u2019s ratio is found at line 28 and the\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "and 0.14. For each portfolio, Sharpe\u2019s ratio is found at line 28 and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 Risk-E\ufb03cient Portfolios with N Risky Assets      477\n\nlogical vector ind at line 29 indicates which portfolio is the tangency portfolio\nmaximizing Sharpe\u2019s ratio. Similarly, ind2 at line 34 indicates the minimum\nvariance portfolio. Also, ind3 at line 36 indicates the points on the e\ufb03cient\nfrontier. It is assumed that the risk-free rate is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "Risk-E\ufb03cient Portfolios with N Risky Assets      477",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.3 %/year; see line 26.\n1  llibrary(Ecdat)\n2  library(quadprog)\n 3 data(CRSPday)\n\n 4 R = 100*CRSPday[ ,4:6]   # convert to percentages\n 5 mean_vect = apply(R, 2 ,mean)\n\n 6 cov_mat = cov(R)\n\n 7 sd_vect = sqrt(diag(cov_mat))\n\n 8 Amat = cbind(rep(1, 3), mean_vect)   # set the constraints matrix\n 9 muP = seq(0.05, 0.14, length = 300)   # target portfolio means\n10 # for the expect portfolio return\n\n11 sdP = muP # set up storage for std dev\u2019s of portfolio returns\n\n12 weights = matrix(0, nrow = 300, ncol = 3) # storage for weights\n\n13 for (i in 1:length(muP))   # find the optimal portfolios\n14 {\n\n15   bvec = c(1, muP[i]) # constraint vector\n16   result =\n17      solve.QP(Dmat = 2 * cov_mat, dvec = rep(0, 3),\n18      Amat = Amat, bvec = bvec, meq = 2)\n19   sdP[i] = sqrt(result$value)\n20   weights[i,] = result$solution\n21 }\n\n22 pdf(\"quad_prog_plot.pdf\", width = 6, height = 5)\n\n23 plot(sdP, muP, type = \"l\", xlim = c(0, 2.5),\n\n24    ylim = c(0, 0.15), lty = 3) # plot efficient frontier (and\n25                # inefficient portfolios below the min var portfolio)\n26 mufree = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.3",
      "section_title": "%/year; see line 26.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.3 / 253 # input value of risk-free interest rate\n\n27 points(0, mufree, cex = 4, pch = \"*\")   # show risk-free asset\n28 sharpe = (muP - mufree) / sdP # compute Sharpe\u2019s ratios\n\n29 ind = (sharpe == max(sharpe)) # Find maximum Sharpe\u2019s ratio\n\n30 weights[ind, ] #   print the weights of the tangency portfolio\n31 lines(c(0, 2), mufree + c(0, 2) * (muP[ind] - mufree) / sdP[ind],\n\n32    lwd = 4, lty = 1, col = \"blue\") # show line of optimal portfolios\n33 points(sdP[ind], muP[ind], cex = 4, pch = \"*\") #   tangency portfolio\n34 ind2 = (sdP == min(sdP)) # find minimum variance portfolio\n\n35 points(sdP[ind2], muP[ind2], cex = 2, pch = \"+\") # min var portfolio\n\n36 ind3 = (muP > muP[ind2])\n\n37 lines(sdP[ind3], muP[ind3], type = \"l\", xlim = c(0, 0.25),\n\n38    ylim = c(0, 0.3), lwd = 3, col = \"red\") # plot efficient frontier\n39 text(sd_vect[1], mean_vect[1], \"GE\", cex = 1.15)\n\n40 text(sd_vect[2], mean_vect[2], \"IBM\", cex = 1.15)\n\n41 text(sd_vect[3], mean_vect[3], \"Mobil\", cex = 1.15)\n\n42 graphics.off()\n\f478      16 Portfolio Selection\n\n\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.3",
      "section_title": "/ 253 # input value of risk-free interest rate",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "muP[ind2])",
        "start": 674,
        "end": 688
      }
    ]
  },
  {
    "content": "0.15\n                0.10                          GE\n\n                                       *\n                                      + Mobil\n          muP\n\n\n\n\n                                                        IBM\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "0.10                          GE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                       *\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                       0.0   0.5      1.0         1.5         2.0     2.5\n                                            sdP\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.0   0.5      1.0         1.5         2.0     2.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3. E\ufb03cient frontier (solid), line of e\ufb03cient portfolios (dashed) connecting\nthe risk-free asset and tangency portfolio (asterisks), and the minimum variance\nportfolio (plus) with three stocks (GE, IBM, and Mobil). The three stocks are also\nshown on reward-risk space.\n\n\nThe plot produced by this program is Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "E\ufb03cient frontier (solid), line of e\ufb03cient portfolios (dashed) connecting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3. The program prints the\nweights of the tangency portfolio, which are\n\n      > weights[ind,] # Find tangency portfolio\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "The program prints the",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "weights[ind,] # Find tangency portfolio\n      [1]",
        "start": 81,
        "end": 133
      }
    ]
  },
  {
    "content": "0.5512 0.0844 0.3645\n\n                                                                                 \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5512",
      "section_title": "0.0844 0.3645",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7. Finding the e\ufb03cient frontier, tangency portfolio, and minimum\nvariance portfolio with no short selling using quadratic programming\n\n    In this example, Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "Finding the e\ufb03cient frontier, tangency portfolio, and minimum",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 is modi\ufb01ed so that short sales are not\nallowed. Only three lines of code need to be changed. When short sales are\nprohibited, the target expected return on the portfolio must lie between the\nsmallest and largest expected returns on the stocks. To prevent numerical err-\nors, the target expected returns will start ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "is modi\ufb01ed so that short sales are not",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0001 above the smallest expected\nstock return and end ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0001",
      "section_title": "above the smallest expected",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0001 below the largest expected stock return. This is\nenforced by the following change:\n\n      muP = seq(min(mean_vect) + 0.0001, max(mean_vect) - 0.0001,\n         length = 300)\n\f                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0001",
      "section_title": "below the largest expected stock return. This is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7 Resampling and E\ufb03cient Portfolios       479\n\nTo enforce no short sales, an Aneq matrix is needed and is set equal to a 3 \u00d7 3\nidentity matrix:\n   Amat = cbind(rep(1, 3), mean_vect, diag(1, nrow = 3))\n\nAlso, bneq is set equal to a three-dimensional vector of zeros:\n   bvec = c(1, muP[i], rep(0, 3))\n\nThe new plot is shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "Resampling and E\ufb03cient Portfolios       479",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4. Since the tangency portfolio in Example\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "Since the tangency portfolio in Example",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 had all weights positive, the tangency portfolio is unchanged by the\nprohibition of short sales. The e\ufb03cient frontier is changed since without short\nsales, it is impossible to have expected returns greater than the expected\nreturn of GE, the stock with the highest expected return. In contrast, when\nshort sales are allowed, there is no upper bound on the expected return (or\non the risk). In Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "had all weights positive, the tangency portfolio is unchanged by the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4 the red curve is the entire e\ufb03cient frontier, but in\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "the red curve is the entire e\ufb03cient frontier, but in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3 the e\ufb03cient frontier is the red curve extended to (+\u221e, +\u221e).        \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "the e\ufb03cient frontier is the red curve extended to (+\u221e, +\u221e).        \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7 Resampling and E\ufb03cient Portfolios\nThe theory of portfolio optimization assumes that the expected returns and\nthe covariance matrix of the returns is known. In practice, one must replace\nthese quantities with estimates as in the previous examples. However, the\ne\ufb00ects of estimation error, especially with smaller values of N , can result in\nportfolios that only appear e\ufb03cient. This problem will be investigated in this\nsection using the bootstrap to quantify the e\ufb00ects of estimation error.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "Resampling and E\ufb03cient Portfolios",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8. The global asset allocation problem\n\n    One application of optimal portfolio selection is allocation of capital to dif-\nferent market segments. For example, Michaud (1998) discusses a global asset\nallocation problem where capital must be allocated to \u201cU.S. stocks and govern-\nment/corporate bonds, euros, and the Canadian, French, German, Japanese,\nand U.K. equity markets.\u201d Here we look at a similar example where we allo-\ncate capital to the equity markets of 10 di\ufb00erent countries. Monthly returns for\nthese markets were calculated from MSCI Hong Kong, MSCI Singapore, MSCI\nBrazil, MSCI Argentina, MSCI UK, MSCI Germany, MSCI Canada, MSCI\nFrance, MSCI Japan, and the S&P 500. \u201cMSCI\u201d means \u201cMorgan Stanley\nCapital Index.\u201d The data are from January 1988 to January 2002, inclusive,\nso there are 169 months of data.\n    Assume that we want to \ufb01nd the tangency portfolio that maximizes\nSharpe\u2019s ratio. The tangency portfolio was estimated using sample means and\nthe sample covariance as in Example 16.6, and its Sharpe\u2019s ratio is estimated\nto be ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "The global asset allocation problem",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3681. However, we should suspect that 0.3681 must be an overestimate\nsince this portfolio only maximizes Sharpe\u2019s ratio using estimated parameters,\n\f480     16 Portfolio Selection\n\n\n\n\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3681",
      "section_title": "However, we should suspect that 0.3681 must be an overestimate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n               0.10                           GE\n\n                                        *\n                                       + Mobil\n         muP\n\n\n\n\n                                                        IBM\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "0.10                           GE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                      *\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "*",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n\n\n\n\n                      0.0   0.5       1.0         1.5         2.0      2.5\n                                            sdP\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.0   0.5       1.0         1.5         2.0      2.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.4. E\ufb03cient frontier (solid), line of e\ufb03cient portfolios (dashed) connecting\nthe risk-free asset and tangency portfolio (asterisks), and the minimum variance\nportfolio (plus) with three stocks (GE, IBM, and Mobil) with short sales prohibited.\n\n\nnot the true means and covariance matrix. To evaluate the possible amount\nof overestimation, one can use the bootstrap. As discussed in Chap. 6, in the\nbootstrap simulation experiment, the sample is the \u201ctrue population\u201d so that\nthe sample mean and covariance matrix are the \u201ctrue parameters,\u201d and the\nresamples mimic the sampling process. Actual Sharpe\u2019s ratios are calculated\nwith the sample means and covariance matrix, while estimated Sharpe\u2019s ratio\nuse the means and covariance matrix of the resamples.\n    First, 250 resamples were taken and for each the tangency portfolio was\nestimated. Resampling was done by sampling rows of the data matrix as\ndiscussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.4",
      "section_title": "E\ufb03cient frontier (solid), line of e\ufb03cient portfolios (dashed) connecting",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.11. For each of the 250 tangency portfolios estimated\nfrom the resamples, the actual and estimated Sharpe\u2019s ratios were calculated.\nBoxplots of the 250 actual and 250 estimated Sharpe\u2019s ratios of the estimated\ntangency portfolios are in Fig. 16.5a. \u201cEstimated\u201d means calculated from the\nresample and \u201ctrue\u201d means calculated from the sample. In this \ufb01gure, there is\na dashed horizontal line at height 0.3681, the actual Sharpe\u2019s ratio of the true\ntangency portfolio. One can see that all 250 estimated tangency portfolios\nhave actual Sharpe\u2019s ratios below this value, as they must since the actual\nSharpe\u2019s ratio is maximized by the true tangency portfolio, not the estimated\ntangency portfolios.\n    From the boxplot on the right-hand side of (a), one can see that the\nestimated Sharpe\u2019s ratios overestimate not only the actual Sharpe\u2019s ratios\nof the estimated tangency portfolios but also the somewhat larger (and\nunattainable) actual Sharpe\u2019s ratio of the true (but unknowable) tangency\nportfolio.                                                                    \u0002\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.11",
      "section_title": "For each of the 250 tangency portfolios estimated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7 Resampling and E\ufb03cient Portfolios         481\n\n     a        Short Sales Allowed            b          No Short Sales\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "Resampling and E\ufb03cient Portfolios         481",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                             0.6\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                             0.4\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                             0.2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                             0.0\n               actual      estimated                   actual      estimated\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.5. Bootstrapping estimation of the tangency portfolio and its Sharpe\u2019s ratio.\n(a) Short sales allowed. The left-hand boxplot is of the actual Sharpe\u2019s ratios of the\nestimated tangency portfolios for 250 resamples. The right-hand boxplot contains the\nestimated Sharpe\u2019s ratios for these portfolios. The horizontal dashed line indicates\nSharpe\u2019s ratio of the true tangency portfolio. (b) Same as (a) but with short sales\nnot allowed.\n\n\n    There are several ways to alleviate the problems caused by estimation\nerror when attempting to \ufb01nd a tangency portfolio. One can try to \ufb01nd more\naccurate estimators; the factor models of Chap. 18 and Bayes estimators of\nChap. 20 (see especially Example 20.12) do this. Another possibility is to\nrestrict short sales.\n    Portfolios with short sales aggressively attempt to maximize Sharpe\u2019s ratio\nby selling short those stocks with the smallest estimated mean returns and\nhaving large long positions in those stocks with the highest estimated mean\nreturns. The weakness with this approach is that it is particularly sensitive\nto estimation error. Unfortunately, expected returns are estimated with rel-\natively large uncertainty. This problem can be seen in Table 16.1, which has\n95 % con\ufb01dence intervals for the mean returns. The percentile method is used\nfor the con\ufb01dence intervals, so the endpoints are the ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.5",
      "section_title": "Bootstrapping estimation of the tangency portfolio and its Sharpe\u2019s ratio.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 and 97.5 bootstrap\npercentiles. Notice for Singapore and Japan, the con\ufb01dence intervals include\nboth positive and negative values. In the table, the returns are expressed as\npercentage returns.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "and 97.5 bootstrap",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.9. The global asset allocation problem: short sales prohibited\n\n   This example repeats the bootstrap experimentation of Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.9",
      "section_title": "The global asset allocation problem: short sales prohibited",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8 with\nshort sales prohibited by using inequality constraints such as in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7.\nWith short sales not allowed, the actual Sharpe\u2019s ratio of the true tangency\nportfolio is 0.3503, which is only slightly less than when short sales are allowed.\n\f482    16 Portfolio Selection\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "With short sales not allowed, the actual Sharpe\u2019s ratio of the true tangency",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1. 95 % percentile-method bootstrap con\ufb01dence intervals for the mean re-\nturns of the 10 countries.\n                           Country    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "95 % percentile-method bootstrap con\ufb01dence intervals for the mean re-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5 % 97.5 %\n                           Hong Kong ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "% 97.5 %",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.186 2.709\n                           Singapore \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.186",
      "section_title": "2.709",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.229 2.003\n                           Brazil      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.229",
      "section_title": "2.003",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.232 5.136\n                           Argentina   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.232",
      "section_title": "5.136",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.196 6.548\n                           UK          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.196",
      "section_title": "6.548",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.071 1.530\n                           Germany     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.071",
      "section_title": "1.530",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.120 1.769\n                           Canada      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.120",
      "section_title": "1.769",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.062 1.580\n                           France      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.062",
      "section_title": "1.580",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.243 2.028\n                           Japan     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.243",
      "section_title": "2.028",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.884 0.874\n                           U.S.        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.884",
      "section_title": "0.874",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.636 1.690\n\n\n\n    Boxplots of actual and apparent Sharpe\u2019s ratios are in Fig. 16.5b. Compar-\ning Fig. 16.5a and b, one sees that prohibiting short sales has two bene\ufb01cial\ne\ufb00ects\u2014Sharpe\u2019s ratios actually achieved are slightly higher with no short\nsales allowed compared to having no constraints on short sales. In fact, the\nmean of the 250 actual Sharpe\u2019s ratios is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.636",
      "section_title": "1.690",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3060 with short sales allowed and\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3060",
      "section_title": "with short sales allowed and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3169 with short sales prohibited. Moreover, the overestimation of Sharpe\u2019s\nratio is reduced by prohibiting short sales\u2014the mean apparent Sharpe\u2019s ratio\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3169",
      "section_title": "with short sales prohibited. Moreover, the overestimation of Sharpe\u2019s",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4524 [with estimation error (0.4524 \u2212 0.3681) = 0.0843] with short sales\nallowed but only ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4524",
      "section_title": "[with estimation error (0.4524 \u2212 0.3681) = 0.0843] with short sales",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4038 [with estimation error (0.4038 \u2212 0.3503) = 0.0535]\nwith short sales prohibited. However, these e\ufb00ects, though positive, are only\nmodest and do not entirely solve the problem of overestimation of Sharpe\u2019s\nratio.                                                                      \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4038",
      "section_title": "[with estimation error (0.4038 \u2212 0.3503) = 0.0535]",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10. The global asset allocation problem: Shrinkage estimation and\nshort sales prohibited\n\n    In Example 16.9, we saw that prohibiting short sales can increase Sharpe\u2019s\nratio of the estimated tangency portfolio, but the improvement is only modest.\nFurther improvement requires more accurate estimation of the mean vector\nor the covariance matrix of the returns.\n    This example investigates possible improvements from shrinking the 10\nestimated means toward    each other. Speci\ufb01cally, if Y i is the sample mean of\n                        \u001710\nthe ith country, Y = ( i=1 Y i )/10 is the grand mean (mean of the means),\nand \u03b1 is a tuning parameter between 0 and 1, then the estimated mean return\nfor the ith country is\n                                \u0002i = \u03b1Y i + (1 \u2212 \u03b1)Y .\n                                \u03bc                                        (16.13)\nThe purpose of shrinkage is to reduce the variance of the estimator, though\nthe reduced variance comes at the expense of some bias. Since it is the mean of\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "The global asset allocation problem: Shrinkage estimation and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7 Resampling and E\ufb03cient Portfolios         483\n\n10 means, Y is much less variable than any of Y 1 , . . . , Y 10 . Therefore, Var(\u0002\n                                                                                  \u03bci )\ndecreases as \u03b1 is decreased toward 0. However,\n                                                    10\n                                             1\u2212\u03b1\n                             \u03bci ) = \u03b1\u03bci +\n                           E(\u0002                       \u03bci                        (16.14)\n                                              10 i=1\n\nso that, for any \u03b1 = 1, \u03bc\u0002i is biased, except under the very likely circumstance\nthat \u03bc1 = \u00b7 \u00b7 \u00b7 = \u03bc10 . The parameter \u03b1 controls the bias\u2013variance tradeo\ufb00. In\nthis example, \u03b1 = 1/2 will be used for illustration and short sales will not be\nallowed.\n\n     a           No Shrinkage                b               Shrinkage\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "Resampling and E\ufb03cient Portfolios         483",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                             0.6\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                             0.4\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                             0.2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                             0.0\n\n\n\n\n               actual      estimated                     actual    estimated\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6. Bootstrapping estimation of the tangency portfolio and its Sharpe\u2019s ra-\ntio. Short sales not allowed. (a) No shrinkage. The left-hand boxplot is of the actual\nSharpe\u2019s ratios of the estimated tangency portfolios for 250 resamples. The right-\nhand boxplot contains the estimated Sharpe\u2019s ratios for these portfolios. The hori-\nzontal dashed line indicates Sharpe\u2019s ratio of the true tangency portfolio. (b) Same\nas (a) but with shrinkage.\n\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "Bootstrapping estimation of the tangency portfolio and its Sharpe\u2019s ra-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 compares the performance of shrinkage versus no shrinkage.\nPanel (a) contains the boxplots that we saw in panel (b) of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "compares the performance of shrinkage versus no shrinkage.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.5 where\n\u03b1 = 1. Panel (b) has the boxplots when the tangency portfolio is estimated\nusing \u03b1 = 1/2. Compared to panel (a), in panel (b) the actual Sharpe\u2019s ratios\nare somewhat closer to the dashed line indicating Sharpe\u2019s ratio of the true\ntangency portfolio; the means of the actual Sharpe\u2019s ratios are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.5",
      "section_title": "where",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.317 and 0.318\nwith and without shrinkage, respectively. These values should be compared\nwith the Sharpe ratio of the true (but unknown) tangency portfolio of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.317",
      "section_title": "and 0.318",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.34.\n    Moreover, the estimated Sharpe\u2019s ratios in (b) are smaller and closer to the\ntrue Sharpe\u2019s ratios, so there is less overoptimization\u2014shrinkage has helped\nin two ways. The mean estimated Sharpe\u2019s ratios are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.34",
      "section_title": "Moreover, the estimated Sharpe\u2019s ratios in (b) are smaller and closer to the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.390 and 0.404 with\nand without shrinkage.\n\f484    16 Portfolio Selection\n\n    The next step might be selection of \u03b1 to optimize performance of shrinkage\nestimation. Doing this need not be di\ufb03cult, since di\ufb00erent values of \u03b1 can be\ncompared by bootstrapping.                                                  \u0002\n\n    There are other methods for improving the estimation of the mean vector\nand estimation of the covariance matrix can be improved as well, for example,\nby using the factor models in Chap. 18 or Bayesian estimation as in Chap. 20.\nMoreover, one need not focus on the tangency portfolio but could, for example,\nestimate the minimum variance portfolio. Whatever the focus of estimation,\nthe bootstrap can be used to compare various strategies for improving the\nestimation of the optimal portfolio.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.390",
      "section_title": "and 0.404 with",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8 Utility\nEconomists generally do not model economic decisions in terms of the mean\nand variance of the return but rather by using a utility function. The utility\nof an amount X of money is said to be U (X) where the utility function U\ngenerally has the properties:\n1. U (0) = 0;\n2. U is strictly increasing;\n3. the \ufb01rst derivative U \u0003 (X) is strictly decreasing.\nAssumption 1 is not necessary but is reasonable and states that the utility of\n0 dollars is 0. Assumption 2 merely states that more money is better than less.\nAssumption 3 implies that the more money we have the less we value an extra\ndollar and is called risk aversion. Assumption 3 implies that we would decline\na bet that pays \u00b1\u0394 with equal probabilities. In fact, Assumption 3 implies\nthat we would decline any bet with a payo\ufb00 that is symmetrically distributed\nabout 0, because the expected utility of our wealth would be reduced if we\naccepted the bet. Mathematically, Assumption 3 implies that U is strictly\nconcave. If the second derivative U \u0003\u0003 exists then Assumption 3 is equivalent\nto the assumption that U \u0003\u0003 (X) < 0 for all X.\n    It is assumed that a rational person will make investment decisions so as\nto maximize\n                        E{U (X)} = E[U {X0 (1 + R)}]                   (16.15)\nwhere X is that person\u2019s \ufb01nal wealth, X0 is the person\u2019s initial wealth, and\nR is the return from the investments. In economics this is almost a part of\nthe de\ufb01nition of a rational person, with another component of the de\ufb01nition\nbeing that a rational person will update probabilities using Bayes\u2019 law (see\nChap. 20). Each individual is assumed to have his or her own utility function\nand two di\ufb00erent rational people may make di\ufb00erent decisions because they\nhave di\ufb00erent utility functions.\n\f                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "Utility",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8 Utility      485\n\n    How di\ufb00erent are mean-variance e\ufb03cient portfolios and portfolios that\nmaximize expected utility? In the case that returns are normally distributed,\nthis question can be answered.\n\nResult ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "Utility      485",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2 If returns on all portfolios are normally distributed and if U\nsatis\ufb01es Assumptions 3, then the portfolio that maximizes expected utility is\non the e\ufb03cient frontier.\n\n\nSo, if one chose a portfolio to maximize expected utility, then a mean-variance\ne\ufb03cient portfolio would be selected. Exactly which portfolio on the e\ufb03cient\nfrontier one chooses would depend on one\u2019s utility function.\n\nProof of Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "If returns on all portfolios are normally distributed and if U",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2\n    This result can be proven by proving the following fact: if R1 and R2 are\nnormally distributed with the same means and with standard deviations \u03c31\nand \u03c32 such that \u03c31 < \u03c32 , then E{U (R1 )} > E{U (R2 )}. We will show that\nthis follows from Jensen\u2019s inequality which states that if U is concave function\nand X is any random variable, then E{U (X)} \u2264 U {E(X)}. The inequality is\nstrict if U is strictly convex and X is nondegenerate.4\n    Let X = R1 + e where e is independent of R1 and normally distributed\nwith mean 0 and variance \u03c322 \u2212 \u03c312 . Then X has the same distribution as\nR2 , e is nondegenerate, and, using the law of iterated expectations and then\nJensen\u2019s inequality and Assumption 3, we have\n\nE{U (R2 )} = E{U (X)} = E[E{U (X)|R1 }] < E[{U {E(X|R1 )}] = E{U (R1 )},\n                                                                (16.16)\nsince E(X|R1 ) = R1 .                                                             \u0002\n    The assumption in Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "This result can be proven by proving the following fact: if R1 and R2 are",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "E{U (R2 )}. We will show that\nthis follows from Jensen\u2019s inequality which states that if U is concave function\nand X is any random variable, then E{U (X)} \u2264 U {E(X)}. The inequality is\nstrict if U is strictly convex and X is nondegenerate.4\n    Let X = R1 + e where e is independent of R1 and normally distributed\nwith mean 0 and variance \u03c322 \u2212 \u03c312 . Then X has the same distribution as\nR2 , e is nondegenerate, and, using the law of iterated expectations and then\nJensen\u2019s inequality and Assumption 3, we have",
        "start": 199,
        "end": 713
      }
    ]
  },
  {
    "content": "16.2 that the returns are normally distributed\ncan be weakened to the more realistic assumption that the vector of returns on\nthe assets is a multivariate scale mixture, e.g., has a multivariate t-distribution.\nTo prove this extension, one conditions on the mixing variable so that the ret-\nurns have a conditional multivariate normal distribution. Then (16.16) holds\nconditionally for all values of the mixing variable and therefore holds uncon-\nditionally.\n    A common class of utility functions is\n\n                             U (x; \u03bb) = 1 \u2212 exp(\u2212\u03bbx),                           (16.17)\n\nwhere \u03bb > 0 determines the amount of risk aversion. Note that U \u0003 (x; \u03bb) =\n\u03bb exp(\u03bbx) and U \u0003\u0003 (x; \u03bb) = \u2212\u03bb2 exp(\u2212\u03bbx) (di\ufb00erentiation is with respect to x).\n4\n    Jensen\u2019s inequality is usually stated for convex functions with the inequality re-\n    versed. If U is concave then \u2212U is convex so that the two forms of Jensen\u2019s\n    inequality are equivalent. A random variable X is degenerate if there is a con-\n    stant a such that P (X = a) = 1. Otherwise, it is nondegenerate.\n\f486    16 Portfolio Selection\n\n                              \u03bb= ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "that the returns are normally distributed",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 determines the amount of risk aversion. Note that U \u0003 (x; \u03bb) =\n\u03bb exp(\u03bbx) and U \u0003\u0003 (x; \u03bb) = \u2212\u03bb2 exp(\u2212\u03bbx) (di\ufb00erentiation is with respect to x).\n4\n    Jensen\u2019s inequality is usually stated for convex functions with the inequality re-\n    versed. If U is concave then \u2212U is convex so that the two forms of Jensen\u2019s\n    inequality are equivalent. A random variable X is degenerate if there is a con-\n    stant a such that P (X = a) = 1. Otherwise, it is nondegenerate.\n\f486    16 Portfolio Selection",
        "start": 600,
        "end": 1101
      }
    ]
  },
  {
    "content": "0.25\n                              \u03bb= 0.5\n\n\n\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "\u03bb= 0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                              \u03bb= 1\n                              \u03bb= 2\n                              \u03bb= 5\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u03bb= 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n         U(x;\u03bb)\n                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "U(x;\u03bb)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n                  0.0\n\n\n\n\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0             0.5    1.0             1.5   2.0\n                                                x\n                                 \u0013 (x; \u03bb) = U (x; \u03bb)/U (0, \u03bb) where U (x; \u03bb) = 1 \u2212\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5    1.0             1.5   2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.7. The utility functions U\nexp(\u2212\u03bbx).\n\n\nThus, U \u0003\u0003 (x; \u03bb) is negative for all x so Assumption 3 is met; it is easy to see\nthat Assumptions 1 and 2 also hold. As x \u2192 \u221e, U (x : \u03bb) \u2192 1.\n    Multiplying a utility function by a positive constant will not a\ufb00ect which\ndecision maximizes utility and can standardize utility functions to make them\nmore comparable. In Fig. 16.7, U     0 (x; \u03bb) := U (x; \u03bb)/U (0; \u03bb) is plotted for \u03bb\n                                   0\n= 0.25, 0.5, 1, 2 and 5. Since U (1; \u03bb) = 1 for all \u03bb, these utility functions\nhave been standardized so that the utility corresponding to a return of 0 is\nalways 1. Stated di\ufb00erently, a return equal to 0 has the same utility for all\ndegrees of risk aversion.\n    Adding a constant to the utility function does not e\ufb00ect the optimal deci-\nsion, so one could work with the slightly simpler utility function \u2212 exp(\u2212\u03bbx)\ninstead of U (x; \u03bb) or U0 (x; \u03bb).\n    When the utility function is given by (16.17) and R is normally distributed,\nthen (16.15) becomes\n                           !                                     \"\n                                                          var(R)\n                  1 \u2212 exp \u2212\u03bbX0 {1 + E(R)} + (\u03bbX0 )2                         (16.18)\n                                                             2\n\nby properties of the lognormal distribution; see Appendix A.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.7",
      "section_title": "The utility functions U",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4. For given\nvalues of \u03bb and X0 , the expected utility is maximized by maximizing\n\n                                                    var(R)\n                                    E(R) \u2212 (\u03bbX0 )          .               (16.19)\n                                                       2\nTherefore, using the notation of Sect. 16.6, one selects the allocation vector\nw of the portfolio to maximize\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "For given",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8 Utility      487\n\n                                            wT \u03a3w\n                            wT \u03bc \u2212 (\u03bbX0 )                                  (16.20)\n                                              2\nsubject to wT 1 = 1.\n     Maximizing (16.20) subject to linear constraints is a quadratic program-\nming problem. As \u03bb \u2192 0, the expected return and standard deviation of the\nreturn converge to \u221e. Conversely, as \u03bb \u2192 \u221e, the solution converges to the\nminimum variance portfolio. Therefore, as \u03bb is varied from \u221e to 0, one \ufb01nds\nall of the portfolios on the e\ufb03cient frontier from left to right. This behavior\nis illustrated in the next example; see Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "Utility      487",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8.\n\n\nExample 16.11. Finding portfolios the maximize expected utility\n\n    We will use stock price data in the \ufb01le Stock Bond.csv. This data set was\ndiscussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "Example 16.11. Finding portfolios the maximize expected utility",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4.1. For simplicity of notation, we subsume X0 into \u03bb.\nThe R code below solves the quadratic program (16.20) for 250 values of\nlog(\u03bb) equally spaced from 2 to 8; this range was selected by trial-and-error.\n1  library(quadprog)\n2  dat = read.csv(\"Stock_Bond.csv\")\n 3 y = dat[, c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)]\n\n 4 n = dim(y)[1]\n\n 5 m = dim(y)[2] - 1\n\n 6 r = y[-1,] / y[-n,] - 1\n\n 7 mean_vect = as.matrix(colMeans(r))\n\n 8 cov_mat = cov(r)\n\n 9 nlambda = 250\n\n10 loglambda_vect = seq(2, 8, length = nlambda)\n\n11 w_matrix = matrix(nrow = nlambda, ncol = 10)\n\n12 mu_vect = matrix(nrow = nlambda, ncol = 1)\n\n13 sd_vect = mu_vect\n\n14 ExUtil_vect = mu_vect\n\n15 conv_vect = mu_vect\n\n16 for (i in 1:nlambda)\n\n17 {\n\n18    lambda = exp(loglambda_vect[i])\n19    opt = solve.QP(Dmat = as.matrix(lambda^2 * cov_mat),\n20        dvec = lambda * mean_vect, Amat = as.matrix(rep(1,10)),\n21        bvec = 1, meq = 1)\n22    w = opt$solution\n23    mu_vect[i] = w %*% mean_vect\n24    sd_vect[i] = sqrt(w %*% cov_mat %*% w)\n25    w_matrix[i,] = w\n26    ExUtil_vect[i] = opt$value\n27 }\n\n\nNext, the expected return and the standard deviation of the return are plotted\nagainst \u03bb and then the e\ufb03cient frontier is drawn by plotting the expect return\nagain the standard deviation of the return. The plots are in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "1. For simplicity of notation, we subsume X0 into \u03bb.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8.     \u0002\n\f488                   16 Portfolio Selection\n\n                                                                                                                                    Efficient Frontier\n\n\n\n\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0125\n            0.00085\n\n\n\n\n                                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0125",
      "section_title": "0.00085",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00085\n                                                       SD(return)\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00085",
      "section_title": "SD(return)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0120\nE(return)\n\n\n\n\n                                                                                                              E(return)\n            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0120",
      "section_title": "E(return)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00075\n\n\n\n\n                                                                                                                          0.00075\n                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00075",
      "section_title": "0.00075",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0115\n            0.00065\n\n\n\n\n                                                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0115",
      "section_title": "0.00065",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00065\n                      2   3   4     5      6   7   8                         2   3   4     5      6   7   8                         0.0115           0.0125\n                                  log(\u03bb)                                                 log(\u03bb)                                         SD(return)\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00065",
      "section_title": "2   3   4     5      6   7   8                         2   3   4     5      6   7   8                         0.0115           0.0125",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.8. The expected portfolio return versus log(\u03bb (left), the standard deviation\nof the return versus log(\u03bb) (center) and the e\ufb03cient frontier (right).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.8",
      "section_title": "The expected portfolio return versus log(\u03bb (left), the standard deviation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.9 Bibliographic Notes\n\nMarkowitz (1952) was the original paper on portfolio theory and was expanded\ninto the book Markowitz (1959). Bodie and Merton (2000) provide an ele-\nmentary introduction to portfolio selection theory. Bodie, Kane, and Marcus\n(1999) and Sharpe, Alexander, and Bailey (1999) give a more comprehensive\ntreatment. See also Merton (1972). Formula (16.5) is derived in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.9",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.10\nof Ruppert (2004).\n    Jobson and Korkie (1980) and Britten-Jones (1999) discuss the statistical\nissue of estimating the e\ufb03cient frontier; see the latter for additional recent\nreferences. Britten-Jones (1999) shows that the tangency portfolio can be\nestimated by regression analysis and hypotheses about the tangency portfolio\ncan be tested by regression F -tests. Jagannathan and Ma (2003) discuss how\nimposing constraints such as no short sales can reduce risk.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.10",
      "section_title": "of Ruppert (2004).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10.1 E\ufb03cient Equity Portfolios\n\nThis section uses daily stock prices in the data set Stock Bond.csv that is\nposted on the book\u2019s website and in which any variable whose name ends\nwith \u201cAC\u201d is an adjusted closing price. As the name suggests, these prices\nhave been adjusted for dividends and stock splits, so that returns can be\ncalculated without further adjustments. Run the following code which will\nread the data, compute the returns for six stocks, create a scatterplot matrix\nof these returns, and compute the mean vector, covariance matrix, and vector\nof standard deviations of the returns. Note that returns will be percentages.\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "1 E\ufb03cient Equity Portfolios",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10 R Lab     489\n\n   dat = read.csv(\"Stock_Bond.csv\", header = T)\n   prices = cbind(dat$GM_AC, dat$F_AC, dat$CAT_AC, dat$UTX_AC,\n      dat$MRK_AC, dat$IBM_AC)\n   n = dim(prices)[1]\n   returns = 100 * (prices[2:n, ] / prices[1:(n-1), ] - 1)\n   pairs(returns)\n   mean_vect = colMeans(returns)\n   cov_mat = cov(returns)\n   sd_vect = sqrt(diag(cov_mat))\n\nProblem 1 Write an R program to \ufb01nd the e\ufb03cient frontier, the tangency\nportfolio, and the minimum variance portfolio, and plot on \u201creward-risk\nspace\u201d the location of each of the six stocks, the e\ufb03cient frontier, the tan-\ngency portfolio, and the line of e\ufb03cient portfolios. Use the constraints that\n\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "R Lab     489",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 \u2264 wj \u2264 0.5 for each stock. The \ufb01rst constraint limits short sales but\ndoes not rule them out completely. The second constraint prohibits more than\n50 % of the investment in any single stock. Assume that the annual risk-free\nrate is 3 % and convert this to a daily rate by dividing by 365, since interest\nis earned on trading as well as nontrading days.\n\n\nProblem 2 If an investor wants an e\ufb03cient portfolio with an expected daily\nreturn of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u2264 wj \u2264 0.5 for each stock. The \ufb01rst constraint limits short sales but",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07 %, how should the investor allocate his or her capital to the six\nstocks and to the risk-free asset? Assume that the investor wishes to use the\ntangency portfolio computed with the constraints \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "%, how should the investor allocate his or her capital to the six",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 \u2264 wj \u2264 0.5, not the\nunconstrained tangency portfolio.\n\n\nProblem 3 Does this data set include Black Monday?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u2264 wj \u2264 0.5, not the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10.2 E\ufb03cient Portfolios with Apple, Exxon-Mobil, Target,\nand McDonald\u2019s Stock\n\nThis section constructs portfolios with stocks from four companies: Apple Inc.\n(AAPL), Exxon-Mobil (XOM), Target Corp. (TGT), and McDonalds (MCD).\nRun the following code to get 2013 returns in terms of percentage for each of\nthe 4 companies:\n   dat = read.csv(\"FourStocks_Daily2013.csv\", header = TRUE)\n   head(dat)\n   prices = dat[,-1]\n   n = dim(prices)[1]\n   returns = 100*(prices[-1,] / prices[-n,] - 1)\n\f490      16 Portfolio Selection\n\nProblem 4 Write an R program to plot the e\ufb03cient frontier and to \ufb01nd the\nallocation weight vector w corresponding to the tangency portfolio. Use the\nsample mean vector and sample covariance matrix of the returns to estimate\n\u03bc and \u03a3. Assume that the annual risk free rate is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "2 E\ufb03cient Portfolios with Apple, Exxon-Mobil, Target,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.3 %. Use the constraints\nthat no wj can be less than \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.3",
      "section_title": "%. Use the constraints",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 or greater than 0.5. Let \u03bcP range from 0.045\nto ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "or greater than 0.5. Let \u03bcP range from 0.045",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06 %. Report both the Sharpe\u2019s Ratio and w for the tangency portfolio.\n\n\nProblem 5 Write an R program to minimize\n                                             wT \u03a3w\n                                  wT \u03bc \u2212 \u03bb                                 (16.21)\n                                               2\nover w, subject to wT 1 = 1, for each \u03bb on a log-spaced grid. Plot the expected\nreturn and standard deviation of the return for the portfolios found this way\nand show that the curve coincides with the e\ufb03cient frontier found in Prob-\nlem 4. Select the range of the grid of log-\u03bb values by trial and error to cover\nan interesting range of the e\ufb03cient frontier. What value of \u03bb yields a portfo-\nlio with \u03bcP = 0.046? What value of \u03bb yields to the tangency portfolio? What\nvalue of \u03bb yields to the minimum variance portfolio?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06",
      "section_title": "%. Report both the Sharpe\u2019s Ratio and w for the tangency portfolio.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10.3 Finding the Set of Possible Expected Returns\n\nIn Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "3 Finding the Set of Possible Expected Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.6 when we found the e\ufb03cient frontier by quadratic programming,\nit was necessary to set up a grid of possible values of the expected returns\non the portfolios. When there are no constraints on the allocation vector w\nexcept that its elements sum to 1, any expected return is feasible.5 We saw in\nExample 16.7, that if short sales are prohibited by the constraints 0 \u2264 wi \u2264 1\nfor all i, the the feasible expected portfolio returns lie between the smallest\nand largest expected returns on the individual assets.\n    When more complex constraints are placed on the wi , the set of feasible\nexpected portfolio returns can be found by linear programming. In this section,\nwe use the same data as used in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.6",
      "section_title": "when we found the e\ufb03cient frontier by quadratic programming,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10.1. We will impose the constraints\nthat wi \u2264 B1 and \u2212B2 \u2264 wi for all i.\n    The function solveLP() in the linprog package minimizes (or maximizes)\nover N -dimensional variable x the objection function cT x subject to Ax \u2264 b\nand x \u2265 0. Here c is an N \u00d7 1 constant vector, A is an N \u00d7 k constant\nmatrix, and b is a k \u00d7 1 constant vector for some integers N and k. Also, 0 is\na k-dimensional zero vector.\n    Since x \u2265 1 we cannot let w be x unless we are prohibiting short sale.\nWhen short sales are allowed, we can instead let w equal x1 \u2212 x2 and xT =\n(xT     T\n   1 , x2 ). Then the constraints are that each element of x1 is at most B1 and\neach element of x2 is at most B2. The objective function wT \u03bc is equal to\n(\u03bcT , \u2212\u03bcT )x.\n5\n    \u201cFeasible\u201d means that there exists a vector w achieving that expected return.\n\f                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "1. We will impose the constraints",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.11 Exercises    491\n\n    The constraints in Ax \u2264 b can be a mixture of equality and inequality con-\nstraints. The argument const.dir speci\ufb01es the directions of the constraints;\nsee line 17. In the program below, there are 2M + 1 constraints where M is\nthe number of assets. The \ufb01rst M constraints are that wi \u2264 B1 for all i, the\nnext M constraints are that \u2212B2 \u2264 wi for all i, and the last constraint is that\nwT 1 = 1.\n    The function solveLP() is used twice, once at lines 18 and 19 to \ufb01nd the\nsmallest feasible expected portfolio return and then at lines 20 and 21 to \ufb01nd\nthe largest possible expected return.\n1  dat = read.csv(\"Stock_Bond.csv\", header = T)\n2  prices = cbind(dat$GM_AC, dat$F_AC, dat$CAT_AC, dat$UTX_AC,\n 3    dat$MRK_AC, dat$IBM_AC)\n 4 n = dim(prices)[1]\n\n 5 returns =  100 * (prices[2:n, ] / prices[1:(n-1), ] - 1)\n 6 mean_vect = colMeans(returns)\n\n 7 M = length(mean_vect)\n\n 8 B1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.11",
      "section_title": "Exercises    491",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n\n 9 B2 = 0.1\n\n10 library(linprog)\n\n11 AmatLP1 = cbind(diag(1, nrow = M), matrix(0, nrow = M, ncol = M))\n\n12 AmatLP2 = cbind(matrix(0, nrow = M, ncol = M), diag(1, nrow = M))\n\n13 AmatLP3 = c(rep(1, M), rep(-1, M))\n\n14 AmatLP = rbind(AmatLP1, AmatLP2, AmatLP3)\n\n15 bvecLP = c(rep(B1, M), rep(B2, M), 1)\n\n16 cLP =  c(mean_vect, -mean_vect)\n17 const.dir = c(rep(\"<=\", 2 * M), \"=\")\n\n18 resultLP_min = solveLP(cvec = cLP, bvec = bvecLP, Amat = AmatLP,\n\n19    lpSolve=T, const.dir = const.dir, maximum = FALSE)\n20 resultLP_max = solveLP(cvec = cLP, bvec = bvecLP,\n\n21    Amat = AmatLP, lpSolve = TRUE, maximum = TRUE)\n\nProblem 6 What is the set of feasible expected portfolio returns when \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "9 B2 = 0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 \u2264\nwi \u2264 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u2264",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 for all i? What allocation vector w achieve the smallest possible\nexpected portfolio return? What allocation vector w achieve the largest possible\nexpected portfolio return?\n\n\nProblem 7 Would it be possible to use B1= ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "for all i? What allocation vector w achieve the smallest possible",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15 and B2 = 0.15? Explain\nyour answer.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "and B2 = 0.15? Explain",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.11 Exercises\n 1. Suppose that there are two risky assets, A and B, with expected returns\n    equal to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.11",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.3 % and 4.5 %, \u221arespectively.\n                                         \u221a Suppose that the standard devi-\n    ations of the returns are 6 % and 11 % and that the returns on the\n    assets have a correlation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.3",
      "section_title": "% and 4.5 %, \u221arespectively.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.17.\n\f492   16 Portfolio Selection\n\n   (a) What portfolio of A and B achieves a 3\u221a    % rate of expected return?\n   (b) What portfolios of A and B achieve a ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.17",
      "section_title": "492   16 Portfolio Selection",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 % standard deviation of\n       return? Among these, which has the largest expected return?\n2. Suppose there are two risky assets, C and D, the tangency portfolio is\n   65 % C and 35 % D, and the expected return and standard deviation\n   of the return on the tangency portfolio are 5 % and 7 %, respectively.\n   Suppose also that the risk-free rate of return is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "% standard deviation of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 %. If you want the\n   standard deviation of your return to be 5 %, what proportions of your\n   capital should be in the risk-free asset, asset C, and asset D?\n3. (a) Suppose that stock A shares sell at $75 and stock B shares at $115.\n       A portfolio has 300 shares of stock A and 100 of stock B. What are\n       the weights w and 1 \u2212 w of stocks A and B in this portfolio?\n   (b) More generally, if a portfolio has N stocks, if the price per share of\n       the jth stock is Pj , and if the portfolio has nj shares of stock j, then\n       \ufb01nd a formula for wj as a function of n1 , . . . , nN and P1 , . . . , PN .\n4. Let RP be a return of some type on a portfolio and let R1 , . . . , RN be\n   the same type of returns on the assets in this portfolio. Is\n\n                           RP = w 1 R 1 + \u00b7 \u00b7 \u00b7 + w N R N\n\n   true if RP is a net return? Is this equation true if RP is a gross return?\n   Is it true if RP is a log return? Justify your answers.\n5. Suppose one has a sample of monthly log returns on two stocks with\n   sample means of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "%. If you want the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0032 and 0.0074, sample variances of 0.017 and 0.025,\n   and a sample covariance of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0032",
      "section_title": "and 0.0074, sample variances of 0.017 and 0.025,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0059. For purposes of resampling, consider\n   these to be the \u201ctrue population values.\u201d A bootstrap resample has sample\n   means of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0059",
      "section_title": "For purposes of resampling, consider",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0047 and 0.0065, sample variances of 0.0125 and 0.023, and a\n   sample covariance of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0047",
      "section_title": "and 0.0065, sample variances of 0.0125 and 0.023, and a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0058.\n   (a) Using the resample, estimate the e\ufb03cient portfolio of these two stocks\n        that has an expected return of 0.005; that is, give the two portfolio\n        weights.\n   (b) What is the estimated variance of the return of the portfolio in part\n        (a) using the resample variances and covariances?\n   (c) What are the actual expected return and variance of return for the\n        portfolio in (a) when calculated with the true population values (e.g.,\n        with using the original sample means, variances, and covariance)?\n6. Stocks 1 and 2 are selling for $100 and $125, respectively. You own 200\n   shares of stock 1 and 100 shares of stock 2. The weekly returns on these\n   stocks have means of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0058",
      "section_title": "(a) Using the resample, estimate the e\ufb03cient portfolio of these two stocks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.001 and 0.0015, respectively, and standard devia-\n   tions of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.001",
      "section_title": "and 0.0015, respectively, and standard devia-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 and 0.04, respectively. Their weekly returns have a correla-\n   tion of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "and 0.04, respectively. Their weekly returns have a correla-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35. Find the correlation matrix of the weekly returns on the two\n   stocks and the mean and standard deviation of the weekly returns on the\n   portfolio.\n\f                                                            References    493\n\nReferences\nBodie, Z., and Merton, R. C. (2000) Finance, Prentice-Hall, Upper Saddle\n  River, NJ.\nBodie, Z., Kane, A., and Marcus, A. (1999) Investments, 4th ed., Irwin/\n  McGraw-Hill, Boston.\nBritten-Jones, M. (1999) The sampling error in estimates of mean-variance\n  e\ufb03cient portfolio weights. Journal of Finance, 54, 655\u2013671.\nJagannathan, R. and Ma, T. (2003) Risk reduction in large portfolios: Why\n  imposing the wrong constraints helps. Journal of Finance, 58, 1651\u20131683.\nJobson, J. D., and Korkie, B. (1980) Estimation for Markowitz e\ufb03cient port-\n  folios. Journal of the American Statistical Association, 75, 544\u2013554.\nMarkowitz, H. (1952) Portfolio Selection. Journal of Finance, 7, 77\u201391.\nMarkowitz, H. (1959) Portfolio Selection: E\ufb03cient Diversi\ufb01cation of Invest-\n  ment, Wiley, New York.\nMerton, R. C. (1972) An analytic derivation of the e\ufb03cient portfolio frontier.\n  Journal of Financial and Quantitative Analysis, 7, 1851\u20131872.\nMichaud, R. O. (1998) E\ufb03cient Asset Management: A Practical Guide to\n  Stock Portfolio Optimization and Asset Allocation, Harvard Business School\n  Press, Boston.\nRuppert, D. (2004) Statistics and Finance: An Introduction, Springer,\n  New York.\nSharpe, W. F., Alexander, G. J., and Bailey, J. V. (1999) Investments, 6th\n  ed., Prentice-Hall, Upper Saddle River, NJ.\n\f17\nThe Capital Asset Pricing Model\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "Find the correlation matrix of the weekly returns on the two",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1 Introduction to the CAPM\n\nThe CAPM (capital asset pricing model) has a variety of uses. It provides\na theoretical justi\ufb01cation for the widespread practice of passive investing by\nholding index funds.1 The CAPM can provide estimates of expected rates of\nreturn on individual investments and can establish \u201cfair\u201d rates of return on\ninvested capital in regulated \ufb01rms or in \ufb01rms working on a cost-plus basis.2\n   The CAPM starts with the question, what would be the risk premiums on\nsecurities if the following assumptions were true?\n 1. The market prices are \u201cin equilibrium.\u201d In particular, for each asset, sup-\n    ply equals demand.\n 2. Everyone has the same forecasts of expected returns and risks.\n 3. All investors choose portfolios optimally according to the principles of\n    e\ufb03cient diversi\ufb01cation discussed in Chap. 16. This implies that everyone\n    holds a tangency portfolio of risky assets as well as the risk-free asset.\n 4. The market rewards people for assuming unavoidable risk, but there is no\n    reward for needless risks due to ine\ufb03cient portfolio selection. Therefore,\n    the risk premium on a single security is not due to its \u201cstandalone\u201d risk,\n    but rather to its contribution to the risk of the tangency portfolio. The\n    various components of risk are discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "Introduction to the CAPM",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4.\n\n\n1\n  An index fund holds the same portfolio as some index. For example, an S&P 500\n  index fund holds all 500 stocks on the S&P 500 in the same proportions as in the\n  index. Some funds do not replicate an index exactly, but are designed to track\n  the index, for instance, by being cointegrated with the index.\n2\n  See Bodie and Merton (2000).\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                               495\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 17\n\f496    17 The Capital Asset Pricing Model\n\nAssumption 3 implies that the market portfolio is equal to the tangency port-\nfolio. Therefore, a broad index fund that mimics the market portfolio can be\nused as an approximation to the tangency portfolio.\n    The validity of the CAPM can only be guaranteed if all of these assump-\ntions are true, and certainly no one believes that any of them are exactly\ntrue. Assumption 3 is at best an idealization. Moreover, some of the conclu-\nsions of the CAPM are contradicted by the behavior of \ufb01nancial markets; see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4.1 for an example. Despite its shortcomings, the CAPM is widely\nused in \ufb01nance and it is essential for a student of \ufb01nance to understand the\nCAPM. Many of its concepts such as the beta of an asset and systematic and\ndiversi\ufb01able risks are of great importance, and the CAPM has been general-\nized to the widely used factor models introduced in Chap. 18.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "1 for an example. Despite its shortcomings, the CAPM is widely",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.2 The Capital Market Line (CML)\nThe capital market line (CML) relates the excess expected return on an e\ufb03-\ncient portfolio to its risk. Excess expected return is the expected return minus\nthe risk-free rate and is also called the risk premium. The CML is\n                                        \u03bcM \u2212 \u03bcf\n                           \u03bc R = \u03bcf +           \u03c3R ,                     (17.1)\n                                          \u03c3M\nwhere R is the return on a given e\ufb03cient portfolio (mixture of the market\nportfolio [= tangency portfolio] and the risk-free asset), \u03bcR = E(R), \u03bcf is the\nrisk-free rate, RM is the return on the market portfolio, \u03bcM = E(RM ), \u03c3M\nis the standard deviation of RM , and \u03c3R is the standard deviation of R. The\nrisk premium of R is \u03bcR \u2212 \u03bcf and the risk premium of the market portfolio\nis \u03bcM \u2212 \u03bcf .\n    In (17.1) \u03bcf , \u03bcM , and \u03c3M are constant. What varies are \u03c3R and \u03bcR . These\nvary as we change the e\ufb03cient portfolio R. Think of the CML as showing how\n\u03bcR depends on \u03c3R .\n    The slope of the CML is, of course,\n                                   \u03bcM \u2212 \u03bcf\n                                           ,\n                                     \u03c3M\nwhich can be interpreted as the ratio of the risk premium to the standard\ndeviation of the market portfolio. This is Sharpe\u2019s famous \u201creward-to-risk\nratio,\u201d which is widely used in \ufb01nance. Equation (17.1) can be rewritten as\n                            \u03bcR \u2212 \u03bcf   \u03bcM \u2212 \u03bcf\n                                    =         ,\n                              \u03c3R        \u03c3M\nwhich says that the reward-to-risk ratio for any e\ufb03cient portfolio equals that\nratio for the market portfolio\u2014all e\ufb03cient portfolios have the same Sharpe\u2019s\nratio as the market portfolio.\n\f                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "The Capital Market Line (CML)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.2 The Capital Market Line (CML)               497\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "The Capital Market Line (CML)               497",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1. The CML\n\n    Suppose that the risk-free rate of interest is \u03bcf = 0.06, the expected return\non the market portfolio is \u03bcM = 0.15, and the risk of the market portfolio\nis \u03c3M = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "The CML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.22. Then the slope of the CML is (0.15 \u2212 0.06)/0.22 = 9/22. The\nCML of this example is illustrated in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.22",
      "section_title": "Then the slope of the CML is (0.15 \u2212 0.06)/0.22 = 9/22. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1.                               \u0002\n\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.16\n\n         0.14                                  CML\n\n                         risk                                                  M\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.16",
      "section_title": "0.14                                  CML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n\n          0.1\n                     reward\n                                               an efficient portfolio\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.08\n\n         0.06\n\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08",
      "section_title": "0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n\n         0.02\n                     F\n            0\n                0        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "0.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05         0.1        0.15             0.2          0.25\n                                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.1        0.15             0.2          0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.22\nFig. 17.1. CML when \u03bcf = 0.06, \u03bcM = 0.15, and \u03c3M = 0.22. All e\ufb03cient portfolios\nare on the line connecting the risk-free asset (F) and the market portfolio (M).\nTherefore, the reward-to-risk ratio is the same for all e\ufb03cient portfolios, including\nthe market portfolio. This fact is illustrated by the thick lines, whose lengths are the\nrisk and reward for a typical e\ufb03cient portfolio.\n\n\n   The CML is easy to derive. Consider an e\ufb03cient portfolio that allocates a\nproportion w of its assets to the market portfolio and (1 \u2212 w) to the risk-free\nasset. Then\n                R = wRM + (1 \u2212 w)\u03bcf = \u03bcf + w(RM \u2212 \u03bcf ).                  (17.2)\nTherefore, taking expectations in (17.2),\n                                \u03bcR = \u03bcf + w(\u03bcM \u2212 \u03bcf ).                                (17.3)\nAlso, from (17.2),\n                                     \u03c3R = w\u03c3M ,                                       (17.4)\nor\n                                        \u03c3R\n                                      w=   .                                          (17.5)\n                                        \u03c3M\nSubstituting (17.5) into (17.3) gives the CML.\n\f498    17 The Capital Asset Pricing Model\n\n    The CAPM says that the optimal way to invest is to\n 1. decide on the risk \u03c3R that you can tolerate, 0 \u2264 \u03c3R \u2264 \u03c3M 3 ;\n 2. calculate w = \u03c3R /\u03c3M ;\n 3. invest w proportion of your investment in a market index fund, that is, a\n    fund that tracks the market as a whole;\n 4. invest 1 \u2212 w proportion of your investment in risk-free Treasury bills, or\n    a money-market fund.\nAlternatively,\n 1. choose the reward \u03bcR \u2212 \u03bcf that you want; the only constraint is that\n    \u03bcf \u2264 \u03bcR \u2264 \u03bcM so that 0 \u2264 w \u2264 14 ;\n 2. calculate\n                                      \u03bcR \u2212 \u03bcf\n                                 w=            ;\n                                      \u03bcM \u2212 \u03bcf\n3. do steps 3 and 4 as above.\n    Instead of specifying the expected return or standard deviation of return,\nas in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.22",
      "section_title": "Fig. 17.1. CML when \u03bcf = 0.06, \u03bcM = 0.15, and \u03c3M = 0.22. All e\ufb03cient portfolios",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 one can \ufb01nd the portfolio with the highest expected return\nsubject to a guarantee that with con\ufb01dence 1 \u2212 \u03b1 the maximum loss is below\na prescribed bound M determined, say, by a \ufb01rm\u2019s capital reserves. If the \ufb01rm\ninvests an amount C, then for the loss to be greater than M the return must\nbe less than \u2212M/C. If we assume that the return is normally distributed,\nthen by (A.11), (17.3), and (17.4),\n           \u0007           \b     \u0007                               \b\n                    M          \u2212M/C \u2212 {\u03bcf + w(\u03bcM \u2212 \u03bcf )}\n         P R<\u2212           =\u03a6                                    .        (17.6)\n                    C                      w\u03c3M\n\nThus, we solve the following equation for w:\n\n                              \u2212M/C \u2212 {\u03bcf + w(\u03bcM \u2212 \u03bcf )}\n                  \u03a6\u22121 (\u03b1) =                             .\n                                       w\u03c3M\n\n   One can view w = \u03c3R /\u03c3M as an index of the risk aversion of the investor.\nThe smaller the value of w the more risk-averse the investor. If an investor\nhas w equal to 0, then that investor is 100 % in risk-free assets. Similarly,\nan investor with w = 1 is totally invested in the tangency portfolio of risky\nassets.5\n\n\n\n\n3\n  In fact, \u03c3R > \u03c3M is possible by borrowing money to buy risky assets on margin.\n4\n  This constraint can be relaxed if one is permitted to buy assets on margin.\n5\n  An investor with w > 1 is buying the market portfolio on margin, that is, bor-\n  rowing money to buy the market portfolio.\n\f                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "one can \ufb01nd the portfolio with the highest expected return",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "\u03c3M is possible by borrowing money to buy risky assets on margin.\n4\n  This constraint can be relaxed if one is permitted to buy assets on margin.\n5\n  An investor with w > 1 is buying the market portfolio on margin, that is, bor-\n  rowing money to buy the market portfolio.",
        "start": 1192,
        "end": 1500
      }
    ]
  },
  {
    "content": "17.3 Betas and the Security Market Line       499\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "Betas and the Security Market Line       499",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3 Betas and the Security Market Line\nThe security market line (SML) relates the excess return on an asset to the\nslope of its regression on the market portfolio. The SML di\ufb00ers from the CML\nin that the SML applies to all assets while the CML applies only to e\ufb03cient\nportfolios.\n    Suppose that there are many securities indexed by j. De\ufb01ne\n\n         \u03c3jM = covariance between the returns on the jth security\n                    and the market portfolio.\n\nAlso, de\ufb01ne\n                                          \u03c3jM\n                                   \u03b2j =     2 .                             (17.7)\n                                          \u03c3M\nIt follows from the theory of best linear prediction in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "Betas and the Security Market Line",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.1 that \u03b2j is the\nslope of the best linear predictor of the jth security\u2019s returns using returns of\nthe market portfolio as the predictor variable. This fact follows from equation\n(11.37) for the slope of a best linear prediction equation. In fact, the best\nlinear predictor of Rj based on RM is\n\n                               \u0002j = \u03b20,j + \u03b2j RM ,\n                               R                                            (17.8)\n\nwhere \u03b2j in (17.8) is the same as in (17.7). Also, \u03b20,j is the intercept that can\nbe calculated by taking expectations in (17.8) and solving to obtain \u03b20,j =\nE(Rj ) \u2212 \u03b2j E(RM ).\n    Another way to appreciate the signi\ufb01cance of \u03b2j uses linear regression. As\ndiscussed in Sect. 11.9, linear regression is a method for estimating the coe\ufb03-\ncients of the best linear predictor based upon data. To apply linear regression,\nsuppose that we have a bivariate time series (Rj,t , RM,t )nt=1 of returns on the\njth asset and the market portfolio. Then, the estimated slope of the linear\nregression of Rj,t on RM,t is\n                            \u0017n\n                                  (Rj,t \u2212 Rj )(RM,t \u2212 RM )\n                      \u03b2\u0302j = t=1\u0017n                           ,              (17.9)\n                                    t=1 (RM,t \u2212 RM )\n                                                     2\n\n\nwhich, after multiplying the numerator and denominator by the same factor\nn\u22121 , becomes an estimate of \u03c3jM divided by an estimate of \u03c3M2\n                                                                and therefore\nby (17.7) an estimate of \u03b2j .\n    Let \u03bcj be the expected return on the jth security. Then \u03bcj \u2212 \u03bcf is the\nrisk premium (or reward for risk or excess expected return) for that security.\nUsing the CAPM, it can be shown that\n\n                            \u03bcj \u2212 \u03bcf = \u03b2j (\u03bcM \u2212 \u03bcf ).                       (17.10)\n\nThis equation, which is called the security market line (SML), is derived in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "1 that \u03b2j is the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5.2. In (17.10) \u03b2j is a variable in the linear equation, not the slope;\n\f500    17 The Capital Asset Pricing Model\n\nmore precisely, \u03bcj is a linear function of \u03b2j with slope \u03bcM \u2212 \u03bcf . This point\nis worth remembering. Otherwise, there could be some confusion since \u03b2j\nwas de\ufb01ned earlier as a slope of a regression model. In other words, \u03b2j is a\nslope in one context but is the independent variable in the di\ufb00erent context of\nthe SML. One can estimate \u03b2j using (17.9) and then plug this estimate into\n(17.10).\n    The SML says that the risk premium of the jth asset is the product of its\nbeta (\u03b2j ) and the risk premium of the market portfolio (\u03bcM \u2212 \u03bcf ). Therefore,\n\u03b2j measures both the riskiness of the jth asset and the reward for assuming\nthat riskiness. Consequently, \u03b2j is a measure of how \u201caggressive\u201d the jth\nasset is. By de\ufb01nition, the beta for the market portfolio is 1; i.e., \u03b2M = 1.\nThis suggest the rules-of-thumb\n\n                         \u03b2j > 1 \u21d2 \u201caggressive,\u201d\n                         \u03b2j = 1 \u21d2 \u201caverage risk,\u201d\n                         \u03b2j < 1 \u21d2 \u201cnot aggressive.\u201d\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "2. In (17.10) \u03b2j is a variable in the linear equation, not the slope;",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1 \u21d2 \u201caggressive,\u201d\n                         \u03b2j = 1 \u21d2 \u201caverage risk,\u201d\n                         \u03b2j < 1 \u21d2 \u201cnot aggressive.\u201d",
        "start": 956,
        "end": 1079
      }
    ]
  },
  {
    "content": "17.2 illustrates the SML and an asset J that is not on the SML.\nThis asset contradicts the CAPM, because according to the CAPM all assets\nare on the SML so no such asset exists.\n    Consider what would happen if an asset like J did exist. Investors would\nnot want to buy it because, since it is below the SML, its risk premium is too\nlow for the risk given by its beta. They would invest less in J and more in\nother securities. Therefore, the price of J would decline and after this decline\nits expected return would increase. After that increase, the asset J would be\non the SML, or so the theory predicts.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "illustrates the SML and an asset J that is not on the SML.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3.1 Examples of Betas\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "1 Examples of Betas",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1 has some \u201c\ufb01ve-year betas\u201d taken from the Salomon, Smith, Barney\nwebsite between February 27 and March 5, 2001. The beta for the S&P 500\nis given as 1.00; why?\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "has some \u201c\ufb01ve-year betas\u201d taken from the Salomon, Smith, Barney",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3.2 Comparison of the CML with the SML\n\nThe CML applies only to the return R of an e\ufb03cient portfolio. It can be\narranged so as to relate the excess expected return of that portfolio to the\nexcess expected return of the market portfolio:\n                                  \u0007    \b\n                                    \u03c3R\n                       \u03bcR \u2212 \u03bcf =          (\u03bcM \u2212 \u03bcf ).                (17.11)\n                                    \u03c3M\nThe SML applies to any asset and like the CML relates its excess expected\nreturn to the excess expected return of the market portfolio:\n\n                           \u03bcj \u2212 \u03bcf = \u03b2j (\u03bcM \u2212 \u03bcf ).                     (17.12)\n\f                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "2 Comparison of the CML with the SML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4 The Security Characteristic Line      501\n\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "The Security Characteristic Line      501",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4\n\n                                                                          SML\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.4",
      "section_title": "SML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2\n\n\n                         1\n         risk premium\n\n\n\n\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n                        0.6\n                                                                       Market\n                                                                       portfolio\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n                                                     *J\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "*J",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n                                        non\u2212\n                                        aggressive                    aggressive\n                         0\n                              0              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "non\u2212",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5                  1                 1.5\n                                                          beta\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "1                 1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.2. Security market line (SML) showing that the risk premium of an asset is\na linear function of the asset\u2019s beta. J is a security not on the line and a contradiction\nto the CAPM. Theory predicts that the price of J decreases until J is on the SML.\nThe vertical dotted line separates the nonaggressive and aggressive regions.\n\n\nIf we take an e\ufb03cient portfolio and consider it as an asset, then \u03bcR and \u03bcj\nboth denote the expected return on that portfolio/asset. Both (17.11) and\n(17.12) hold so that\n                                 \u03c3R\n                                     = \u03b2R .\n                                 \u03c3M\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "Security market line (SML) showing that the risk premium of an asset is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4 The Security Characteristic Line\nLet Rjt be the return at time t on the jth asset. Similarly, let RM,t and \u03bcf,t\nbe the return on the market portfolio and the risk-free return at time t. The\nsecurity characteristic line (sometimes shortened to the characteristic line) is\na regression model:\n\n                                  Rj,t = \u03bcf,t + \u03b2j (RM,t \u2212 \u03bcf,t ) + \u0017j,t ,                 (17.13)\n\nwhere \u0017j,t is N (0, \u03c3 2,j ). It is often assumed that the \u0017j,t s are uncorrelated\nacross assets, that is, that \u0017j,t is uncorrelated with \u0017j \u0002 ,t for j = j \u0003 . This\nassumption has important rami\ufb01cations for risk reduction by diversi\ufb01cation;\nsee Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "The Security Characteristic Line",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4.1.\n\f502    17 The Capital Asset Pricing Model\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1. Selected stocks and in which industries they are. Betas are given for\neach stock (Stock\u2019s \u03b2) and its industry (Ind\u2019s \u03b2). Betas taken from the Salomon,\nSmith, Barney website between February 27 and March 5, 2001.\n      Stock (symbol)          Industry                   Stock\u2019s \u03b2   Ind\u2019s \u03b2\n      Celanese (CZ)         Synthetics                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "Selected stocks and in which industries they are. Betas are given for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.13       0.86\n      General Mills (GIS)   Food\u2014major diversif            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.13",
      "section_title": "0.86",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.29       0.39\n      Kellogg (K)           Food\u2014major, diversif           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.29",
      "section_title": "0.39",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.30       0.39\n      Proctor & Gamble (PG) Cleaning prod                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.30",
      "section_title": "0.39",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.35       0.40\n      Exxon-Mobil (XOM)     Oil/gas                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.35",
      "section_title": "0.40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.39       0.56\n      7-Eleven (SE)         Grocery stores                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.39",
      "section_title": "0.56",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.55       0.38\n      Merck (Mrk)           Major drug manuf               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.55",
      "section_title": "0.38",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.56       0.62\n      McDonalds (MCD)       Restaurants                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.56",
      "section_title": "0.62",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.71       0.63\n      McGraw-Hill (MHP)     Pub\u2014books                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.71",
      "section_title": "0.63",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.87       0.77\n      Ford (F)              Auto                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.87",
      "section_title": "0.77",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.89       1.00\n      Aetna (AET)           Health care plans              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.89",
      "section_title": "1.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.11       0.98\n      General Motors (GM)   Major auto manuf               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.11",
      "section_title": "0.98",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.11       1.09\n      AT&T (T)              Long dist carrier              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.11",
      "section_title": "1.09",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.19       1.34\n      General Electric (GE) Conglomerates                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.19",
      "section_title": "1.34",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.22       0.99\n      Genentech (DNA)       Biotech                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.22",
      "section_title": "0.99",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.43       0.69\n      Microsoft (MSFT)      Software applic.               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.43",
      "section_title": "0.69",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.77       1.72\n      Cree (Cree)           Semicond equip                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.77",
      "section_title": "1.72",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.16       2.30\n      Amazon (AMZN)         Net soft & serv                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.16",
      "section_title": "2.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.99       2.46\n      Doubleclick (Dclk)    Net soft & serv                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.99",
      "section_title": "2.46",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.06       2.46\n\n\n\n   Let \u03bcj,t = E(Rj,t ) and \u03bcM,t = E(RM,t ). Taking expectations in (17.13)\nwe get,\n                        \u03bcj,t = \u03bcf,t + \u03b2j (\u03bcM,t \u2212 \u03bcf,t ),\nwhich is equation (17.10), the SML, though in (17.10) it is not shown explicitly\nthat the expected returns can depend on t. The SML gives us information\nabout expected returns, but not about the variance of the returns. For the\nlatter we need the characteristic line. The characteristic line is said to be a\nreturn-generating process since it gives us a probability model of the returns,\nnot just a model of their expected values.\n    An analogy to the distinction between the SML and characteristic line is\nthis. The regression line E(Y |X) = \u03b20 + \u03b21 X gives the expected value of Y\ngiven X but not the conditional probability distribution of Y given X. The\nregression model\n\n                  Y t = \u03b20 + \u03b2 1 Xt + \u0017 t   and   \u0017t \u223c N (0, \u03c3 2 )\n\ndoes give us this conditional probability distribution.\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.06",
      "section_title": "2.46",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4 The Security Characteristic Line        503\n\n   The characteristic line implies that\n                                \u03c3j2 = \u03b2j2 \u03c3M\n                                           2\n                                             + \u03c3 2,j ,\nthat\n                                                    2\n                                   \u03c3jj \u0002 = \u03b2j \u03b2j \u0002 \u03c3M                             (17.14)\n         \u0003\nfor j = j , and that\n                                                  2\n                                     \u03c3 M j = \u03b2j \u03c3 M .\nFor (17.14) to hold, \u0017j,t and \u0017j \u0002 ,t must be uncorrelated. The total risk of the\njth asset is                          #\n                               \u03c3j =      \u03b2j2 \u03c3M\n                                              2 + \u03c32 .\n                                                    ,j\n\nThe squared risk has two components: \u03b2j2 \u03c3M2\n                                             is called the market or systematic\ncomponent of risk and \u03c3 2,j is called the unique, nonmarket, or unsystematic\ncomponent of risk.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "The Security Characteristic Line        503",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4.1 Reducing Unique Risk by Diversi\ufb01cation\nThe market component of risk cannot be reduced by diversi\ufb01cation, but\nthe unique component can be reduced or even eliminated by su\ufb03cient\ndiversi\ufb01cation.\n   Suppose that there are N assets with returns R1,t , . . . , RN,t for holding\nperiod t. If we form a portfolio with weights w1 , . . . , wN , then the return of\nthe portfolio is\n                       RP,t = w1 R1,t + \u00b7 \u00b7 \u00b7 + wN RN,t .\nLet RM,t be the return on the market portfolio. According to the characteristic\nline model Rj,t = \u03bcf,t + \u03b2j (RM,t \u2212 \u03bcf,t ) + \u0017j,t , so that\n                          \u239b          \u239e\n                               N                                N\n             RP,t = \u03bcf,t + \u239d         \u03b2j wj \u23a0 (RM,t \u2212 \u03bcf,t ) +         wj \u0017j,t .\n                               j=1                              j=1\n\nTherefore, the portfolio beta is\n                                            N\n                                   \u03b2P =          wj \u03b2j ,\n                                           j=1\n\nand the \u201cepsilon\u201d for the portfolio is\n                                           N\n                                 \u0017P,t =          wj \u0017j,t .\n                                           j=1\n\nWe now assume that \u00171,t , . . . , \u0017N,t are uncorrelated. Therefore, by equation\n(7.11),\n                                           N\n                                \u03c3 2,P =          wj2 \u03c3 2,j .\n                                           j=1\n\f504      17 The Capital Asset Pricing Model\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "1 Reducing Unique Risk by Diversi\ufb01cation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.2. Reduction in risk by diversi\ufb01cation\n\n    Suppose the assets in the portfolio are equally weighted; that is, wj = 1/N\nfor all j. Then\n                                        \u0017N\n                                          j=1 \u03b2j\n                                 \u03b2P =            ,\n                                           N\nand                                     \u0017N\n                           2\n                                  N \u22121 j=1 \u03c3 2,j     \u03c32\n                          \u03c3 ,P =                   =    ,\n                                         N           N\nwhere \u03c3 2 is the average of the \u03c3 2,j .\n    As an illustration, if we assume the simple case where \u03c3 2,j is a constant,\nsay \u03c3 2 , for all j, then\n                                           \u03c3\n                                  \u03c3 ,P = \u221a .                              (17.15)\n                                            N\nFor example, suppose that \u03c3 is 5 %. If N = 20, then by (17.15) \u03c3 ,P is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "Reduction in risk by diversi\ufb01cation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.12 %.\nIf N = 100, then \u03c3 ,P is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.12",
      "section_title": "%.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 %. There are approximately 1600 stocks on the\nNYSE; if N = 1600, then \u03c3 ,P = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "%. There are approximately 1600 stocks on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.125 %, a remarkable reduction from 5 %.\n                                                                               \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.125",
      "section_title": "%, a remarkable reduction from 5 %.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.4.2 Are the Assumptions Sensible?\n\nA key assumption that allows nonmarket risk to be removed by diversi\ufb01cation\nis that \u00171,t , . . . , \u0017N,t are uncorrelated. This assumption implies that all corre-\nlation among the cross-section6 of asset returns is due to a single cause and\nthat cause is measured by the market index. For this reason, the characteristic\nline is a \u201csingle-factor\u201d or \u201csingle-index\u201d model with RM,t being the \u201cfactor.\u201d\n    This assumption of uncorrelated \u0017jt would not be valid if, for example,\ntwo energy stocks are correlated over and beyond their correlation due to the\nmarket index. In this case, unique risk could not be eliminated by holding a\nlarge portfolio of all energy stocks. However, if there are many market sectors\nand the sectors are uncorrelated, then one could eliminate nonmarket risk\nby diversifying across all sectors. All that is needed is to treat the sectors\nthemselves as the underlying assets and then apply the CAPM theory.\n    Correlation among the stocks in a market sector can be modeled using a\nfactor model; see Chap. 18.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.4",
      "section_title": "2 Are the Assumptions Sensible?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5 Some More Portfolio Theory\nIn this section we use portfolio theory to show that \u03c3j,M quanti\ufb01es the con-\ntribution of the jth asset to the risk of the market portfolio. Also, we derive\nthe SML.\n6\n    \u201cCross-section\u201d of returns means returns across assets within a single holding\n    period.\n\f                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "Some More Portfolio Theory",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5 Some More Portfolio Theory                  505\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "Some More Portfolio Theory                  505",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5.1 Contributions to the Market Portfolio\u2019s Risk\n\nSuppose that the market consists of N risky assets and that w1,M , . . . , wN,M\nare the weights of these assets in the market portfolio. Then\n                                            N\n                               RM,t =             wi,M Ri,t ,\n                                            i=1\n\nwhich implies that the covariance between the return on the jth asset and the\nreturn on the market portfolio is\n                                      N                         N\n              \u03c3j,M = Cov Rj,t ,             wi,M Ri,t       =         wi,M \u03c3i,j .          (17.16)\n                                      i=1                       i=1\n\nTherefore,\n          N   N                       N                N                       N\n   2\n  \u03c3M =             wj,M wi,M \u03c3i,j =         wj,M            wi,M \u03c3i,j      =         wj,M \u03c3j,M .\n         j=1 i=1                      j=1             i=1                      j=1\n                                                                                           (17.17)\nEquation (17.17) shows that the contribution of the jth asset to the risk of\nthe market portfolio is wj,M \u03c3j,M , where wj,M is the weight of the jth asset\nin the market portfolio and \u03c3j,M is the covariance between the return on the\njth asset and the return on the market portfolio.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "1 Contributions to the Market Portfolio\u2019s Risk",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5.2 Derivation of the SML\n\nThe derivation of the SML is a nice application of portfolio theory, calculus,\nand geometric reasoning. It is based on a clever idea of putting together a\nportfolio with two assets, the market portfolio and the ith risky asset, and\nthen looking at the locus in reward-risk space as the portfolio weight assigned\nto the ith risky asset varies.\n    Consider a portfolio P with weight wi given to the ith risky asset and\nweight (1 \u2212 wi ) given to the market (tangency) portfolio. The return on this\nportfolio is\n                         RP,t = wi Ri,t + (1 \u2212 wi )RM,t .\nThe expected return is\n                             \u03bcP = wi \u03bci + (1 \u2212 wi )\u03bcM ,                                    (17.18)\nand the risk is\n                      #\n              \u03c3P =        wi2 \u03c3i2 + (1 \u2212 wi )2 \u03c3M\n                                                2 + 2w (1 \u2212 w )\u03c3\n                                                      i      i i,M .                       (17.19)\n\nAs we vary wi , we get the locus of points on (\u03c3, \u03bc) space that is shown as a\ndashed curve in Fig. 17.3, which uses the same returns as in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "2 Derivation of the SML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.3 and\nMobil stock as asset i.\n\f506     17 The Capital Asset Pricing Model\n\n    It is easy to see geometrically that the derivative of this locus of points\nevaluated at the tangency portfolio (which is the point where wi = 0) is equal\nto the slope of the CML. We can calculate this derivative and equate it to the\nslope of the CML to see what we get. We will see that the result is the SML.\n    We have from (17.18)\n                               d \u03bcP\n                                     = \u03bci \u2212 \u03bcM ,\n                               d wi\nand from (17.19) that\n\n               d \u03c3P  1\n                    = \u03c3P\u22121 2wi \u03c3i2 \u2212 2(1 \u2212 wi )\u03c3M\n                                                2\n                                                  + 2(1 \u2212 2wi )\u03c3i,M .\n               d wi  2\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.3",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n\n\n\n\n                             CML\n                             efficient frontier\n                             portfolios of tangency and Mobil\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "CML",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.11\n\n\n\n\n                      *      tangency portfolio\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.11",
      "section_title": "*      tangency portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n         muP\n\n\n\n\n                                  *\n               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "muP",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.09\n               0.08\n\n\n\n\n                                                                          Mobil\n\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.09",
      "section_title": "0.08",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.00      1.05     1.10     1.15      1.20   1.25    1.30\n                                                     sdP\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.00",
      "section_title": "1.05     1.10     1.15      1.20   1.25    1.30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3. Derivation of the SML. The purple curve is the locus of portfolios\ncombining Mobil stock and the tangency portfolio (asterisk). The purple curve is\nto the right of the e\ufb03cient frontier (red) and intersects the e\ufb03cient frontier at the\ntangency portfolio. Therefore, the derivative of the purple curve at the tangency\nportfolio is equal to the slope of the CML (blue), since the purple curve is tangent\nto the CML at the tangency portfolio.\n\n\nTherefore,\n\n         d \u03bcP   d \u03bcP /d wi                (\u03bci \u2212 \u03bcM )\u03c3P\n              =            =                                        .\n         d \u03c3P   d \u03c3P /d wi   wi \u03c3i2 \u2212 \u03c3M\n                                       2 + w \u03c32 + \u03c3\n                                             i M     i,M \u2212 2wi \u03c3i,M\n\nNext,\n                                  d \u03bcP \u0015\u0015      (\u03bci \u2212 \u03bcM )\u03c3M\n                                        \u0015    =              .\n                                  d \u03c3P wi =0    \u03c3i,M \u2212 \u03c3M2\n\f                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "Derivation of the SML. The purple curve is the locus of portfolios",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6 Estimation of Beta and Testing the CAPM            507\n\n   Recall that wi = 0 is the tangency portfolio, the point in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "Estimation of Beta and Testing the CAPM            507",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3 where\nthe dashed locus is tangent to the CML. Therefore,\n                                  d \u03bcP \u0015\u0015\n                                        \u0015\n                                  d \u03c3P wi =0\nmust equal the slope of the CML, which is (\u03bcM \u2212 \u03bcf )/\u03c3M . Therefore,\n\n                            (\u03bci \u2212 \u03bcM )\u03c3M   \u03bcM \u2212 \u03bcf\n                                         =         ,\n                             \u03c3i,M \u2212 \u03c3M2      \u03c3M\n\nwhich, after some algebra, gives us\n                                \u03c3i,M\n                   \u03bci \u2212 \u03bcf =      2 (\u03bcM \u2212 \u03bcf ) = \u03b2i (\u03bcM \u2212 \u03bcf ),\n                                \u03c3M\n\nwhich is the SML given in equation (17.10).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "where",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6 Estimation of Beta and Testing the CAPM\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "Estimation of Beta and Testing the CAPM",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6.1 Estimation Using Regression\n\nRecall the security characteristic line\n\n                       Rj,t = \u03bcf,t + \u03b2j (RM,t \u2212 \u03bcf,t ) + \u0017j,t .                (17.20)\n     \u2217                                                                   \u2217\nLet Rj,t = Rj,t \u2212 \u03bcf,t be the excess return on the jth security and let RM,t =\nRM,t \u2212 \u03bcf,t, be the excess return on the market portfolio. Then (17.20) can\nbe written as\n                               \u2217         \u2217\n                             Rj,t = \u03b2j RM,t + \u0017j,t .                    (17.21)\nEquation (17.21) is a regression model without an intercept and with \u03b2j as\nthe slope. A more elaborate model is\n                              \u2217              \u2217\n                             Rj,t = \u03b1j + \u03b2j RM,t + \u0017j,t ,                      (17.22)\n\nwhich includes an intercept. The CAPM says that \u03b1j = 0 but by allowing\n\u03b1j = 0, we recognize the possibility of mispricing.\n                                                                                     \u2217\n    Given time series Rj,t , RM,t , and \u03bcf,t for t = 1, . . . , n, we can calculate Rj,t\n      \u2217                  \u2217        \u2217\nand RM,t  and regress Rj,t   on RM,t  to estimate \u03b1j , \u03b2j , and \u03c3 2,j . By testing the\nnull hypothesis that \u03b1j = 0, we are testing whether the jth asset is mispriced\naccording to the CAPM.\n    As discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "1 Estimation Using Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2.2, when \ufb01tting model (17.21) or (17.22) one should\nuse daily data if available, rather than weekly or monthly data. A more dif-\n\ufb01cult question to answer is how long a time series to use. Longer time series\ngive more data, of course, but models (17.21) and (17.22) assume that \u03b2j is\nconstant and this might not be true over a long time period.\n\f508      17 The Capital Asset Pricing Model\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "2, when \ufb01tting model (17.21) or (17.22) one should",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3. Estimation of \u03b1 and \u03b2 for Microsoft\n\n   As an example, daily closing prices on Microsoft and the S&P 500 index\nfrom November 1, 1993, to April 3, 2003, were used. The S&P 500 was taken as\nthe market price. Three-month T-bill rates were used as the risk-free returns.7\nThe excess returns are the returns minus the T-bill rates. The code is\n      dat = read.csv(\"capm.csv\", header = TRUE)\n      attach(dat)\n      n = dim(dat)[1]\n      EX_R_sp500 = Close.sp500[2:n] / Close.sp500[1:(n-1)]\n         - 1 - Close.tbill[2:n] / (100 * 253)\n      EX_R_msft = Close.msft[2:n] / Close.msft[1:(n-1)]\n         - 1 - Close.tbill[2:n] / (100 * 253)\n      fit = lm(EX_R_msft ~ EX_R_sp500)\n      options(digits = 3)\n      summary(fit)\n\nand the output is\n      Call:\n      lm(formula = EX_R_msft ~ EX_R_sp500)\n\n      Coefficients:\n                  Estimate Std. Error t value Pr(>|t|)\n      (Intercept) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "Estimation of \u03b1 and \u03b2 for Microsoft",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000914   0.000409    2.23    0.026 *\n      EX_R_sp500 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000914",
      "section_title": "0.000409    2.23    0.026 *",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.247978    0.035425   35.23   <2e-16 ***\n      ---\n      Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.247978",
      "section_title": "0.035425   35.23   <2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0199 on 2360 degrees of freedom\n      Multiple R-squared: 0.345,Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0199",
      "section_title": "on 2360 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.344\n      F-statistic: 1.24e+03 on 1 and 2360 DF, p-value: <2e-16\n\n    For Microsoft, we \ufb01nd that \u03b2\u0302 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.344",
      "section_title": "F-statistic: 1.24e+03 on 1 and 2360 DF, p-value: <2e-16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.25 and \u03b1\u0302 = 0.0009. The estimate of\n\u03b1 is very small and, although the p-value for \u03b1 is 0.026, we can conclude\nthat for practical purposes, \u03b1 is essentially 0. This is another example of an\ne\ufb00ect being statistically signi\ufb01cant according to a test of the hypothesis of no\ne\ufb00ect but not practically signi\ufb01cant. Very small e\ufb00ects are often statistically\nsigni\ufb01cant when the sample size is large. In this example, we have nearly 10\nyears of daily data and the sample size is quite large for a hypothesis testing\nproblem, 2363.\n    The estimate of \u03c3 is the root MSE which equals ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.25",
      "section_title": "and \u03b1\u0302 = 0.0009. The estimate of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0199. Notice that the\nR2 (R-sq) value for the regression is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0199",
      "section_title": "Notice that the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "34.5 %. The interpretation of R2 is the\npercent of the variance in the excess returns on Microsoft that is due to excess\nreturns on the market. In other words, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "34.5",
      "section_title": "%. The interpretation of R2 is the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "34.5 % of the squared risk is due to\n7\n    Interest rates are return rates. Thus, we use the T-bill rates themselves as the\n    risk-free returns. One does not take logs and di\ufb00erence the T-bill rates as if they\n    were prices. However, the T-bill rates were divided by 100 to convert from a\n    percentage and then by 253 to convert to a daily rate.\n\f                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "34.5",
      "section_title": "% of the squared risk is due to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6 Estimation of Beta and Testing the CAPM          509\n\nsystematic or market risk (\u03b2j2 \u03c3M\n                                2\n                                  ). The remaining ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "Estimation of Beta and Testing the CAPM          509",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "65.5 % is due to unique or\n                 2\nnonmarket risk (\u03c3 ).\n   If we assume that \u03b1 = 0, then we can re\ufb01t the model using a no-intercept\nmodel. The code for \ufb01tting the model is changed to\n\n     fit_NoInt = lm(EX_R_msft ~ EX_R_sp500 - 1)\n     options(digits = 3)\n     summary(fit_NoInt)\n\nNotice the \u201c\u22121\u201d in the formula. The \u201c1\u201d represents the intercept so \u201c\u22121\u201d\nindicates that the intercept is removed. The output changes to\n\n     Call:\n     lm(formula = EX_R_msft ~ EX_R_sp500 - 1)\n\n     Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n     EX_R_sp500   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "65.5",
      "section_title": "% is due to unique or",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2491     0.0355    35.2   <2e-16 ***\n     ---\n     Residual standard error: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2491",
      "section_title": "0.0355    35.2   <2e-16 ***",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0199 on 2361 degrees of freedom\n     Multiple R-squared: 0.345,Adjusted R-squared: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0199",
      "section_title": "on 2361 degrees of freedom",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.344\n     F-statistic: 1.24e+03 on 1 and 2361 DF, p-value: <2e-16\n\nWith no intercept \u03b2\u0302, \u03c3\u0302 and R2 are nearly the same as before\u2014forcing a nearly\nzero intercept to be exactly zero has little e\ufb00ect.                         \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.344",
      "section_title": "F-statistic: 1.24e+03 on 1 and 2361 DF, p-value: <2e-16",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6.2 Testing the CAPM\n\nTesting that \u03b1 equals 0 tests only one of the conclusions of the CAPM. Ac-\ncepting this null hypothesis only means that the CAPM has passed one test,\nnot that we should now accept it as true.8 To fully test the CAPM, its other\nconclusions should also be tested. The factor models in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "2 Testing the CAPM",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 have been\nused to test the CAPM and fairly strong evidence against the CAPM has\nbeen found. Fortunately, these factor models do provide a generalization of\nthe CAPM that is likely to be useful for \ufb01nancial decision making.\n    Often, as an alternative to regression using excess returns, the returns on\nthe asset are regressed on the returns on the market. When this is done, an\nintercept model should be used. In the Microsoft data when using returns\ninstead of excess returns, the estimate of beta changed hardly at all.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "have been",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.6.3 Interpretation of Alpha\n\nIf \u03b1 is nonzero, then the security is mispriced, at least according to the CAPM.\nIf \u03b1 > 0 then the security is underpriced; the returns are too large on average.\n8\n    In fact, acceptance of a null hypothesis should never be interpreted as proof that\n    the null hypothesis is true.\n\f510    17 The Capital Asset Pricing Model\n\nThis is an indication of an asset worth purchasing. Of course, one must be\ncareful. If we reject the null hypothesis that \u03b1 = 0, all we have done is to\nshow that the security was mispriced in the past.\n    Warning: If we use returns rather than excess returns, then the intercept\nof the regression equation does not estimate \u03b1, so one cannot test whether \u03b1\nis zero by testing the intercept.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.6",
      "section_title": "3 Interpretation of Alpha",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 then the security is underpriced; the returns are too large on average.\n8\n    In fact, acceptance of a null hypothesis should never be interpreted as proof that\n    the null hypothesis is true.\n\f510    17 The Capital Asset Pricing Model",
        "start": 118,
        "end": 360
      }
    ]
  },
  {
    "content": "17.7 Using the CAPM in Portfolio Analysis\nSuppose we have estimated beta and \u03c3 2 for each asset in a portfolio and also\n           2\nestimated \u03c3M and \u03bcM for the market. Then, since \u03bcf is also known, we can\ncompute the expectations, variances, and covariances of all asset returns by\nthe formulas\n\n                          \u03bcj = \u03bcf + \u03b2j (\u03bcM \u2212 \u03bcf ),\n                          \u03c3j2 = \u03b2j2 \u03c3M\n                                     2\n                                       + \u03c3 2,j ,\n                                          2\n                         \u03c3jj \u0002 = \u03b2j \u03b2j \u0002 \u03c3M for j = j \u0003 .\n\nThere is a noteworthy danger here: These estimates depend heavily on the\nvalidity of the CAPM assumptions. Any or all of the quantities beta, \u03c3 2 ,\n  2\n\u03c3M  , \u03bcM , and \u03bcf could depend on time t. However, it is generally assumed\nthat the betas and \u03c3 2 s of the assets as well as \u03c3M\n                                                   2\n                                                     and \u03bcM of the market\nare independent of t so that these parameters can be estimated assuming\nstationarity of the time series of returns.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.7",
      "section_title": "Using the CAPM in Portfolio Analysis",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.8 Bibliographic Notes\nThe CAPM was developed by Sharpe (1964), Lintner (1965a,b), and Mossin\n(1966). Introductions to the CAPM can be found in Bodie, Kane, and Marcus\n(1999), Bodie and Merton (2000), and Sharpe, Alexander, and Bailey (1999).\nI \ufb01rst learned about the CAPM from these three textbooks. Campbell, Lo,\nand MacKinlay (1997) discuss empirical testing of the CAPM. The derivation\nof the SML in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.8",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.5.2 was adapted from Sharpe, Alexander, and Bailey\n(1999). Discussion of factor models can be found in Sharpe, Alexander, and\nBailey (1999), Bodie, Kane, and Marcus (1999), and Campbell, Lo, and\nMacKinlay (1997).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.5",
      "section_title": "2 was adapted from Sharpe, Alexander, and Bailey",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.9 R Lab\nIn this lab, you will \ufb01t model (17.20). The S&P 500 index will be a proxy for\nthe market portfolio and the 90-day Treasury rate will serve as the risk-free\nrate.\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.9",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.9 R Lab     511\n\n   This lab uses the data set Stock_Bond_2004_to_2006.csv, which is\navailable on the book\u2019s website. This data set contains a subset of the data in\nthe data set Stock_Bond.csv used elsewhere.\n   The R commands needed to \ufb01t model (17.20) will be given in small groups\nso that they can be explained better. First run the following commands to\nread the data, extract the prices, and \ufb01nd the number of observations:\n\n   dat = read.csv(\"Stock_Bond_2004_to_2006.csv\", header = TRUE)\n   prices = dat[ , c(5, 7, 9, 11, 13, 15, 17, 24)]\n   n = dim(prices)[1]\n\nNext, run these commands to convert the risk-free rate to a daily rate, compute\nnet returns, extract the Treasury rate, and compute excess returns for the\nmarket and for seven stocks. The risk-free rate is given as a percentage so the\nreturns are also computed as percentages.\n\n   dat2 = as.matrix(cbind(dat[(2:n), 3] / 365,\n      100 * (prices[2:n,] / prices[1:(n-1), ] - 1)))\n   names(dat2)[1] = \"treasury\"\n   risk_free = dat2[,1]\n   ExRet = dat2[ ,2:9] - risk_free\n   market = ExRet[ ,8]\n   stockExRet = ExRet[ ,1:7]\n\nNow \ufb01t model (17.20) to each stock, compute the residuals, look at a scatter-\nplot matrix of the residuals, and extract the estimated betas.\n\n   fit_reg = lm(stockExRet ~ market)\n   summary(fit_reg)\n   res = residuals(fit_reg)\n   pairs(res)\n   options(digits = 3)\n   betas = fit_reg$coeff[2, ]\n\nProblem 1 Would you reject the null hypothesis that alpha is zero for any\nof the seven stocks? Why or why not?\n\n\nProblem 2 Use model (17.20) to estimate the expected excess return for all\nseven stocks. Compare these results to using the sample means of the excess re-\nturns to estimate these parameters. Assume for the remainder of this lab that\nall alphas are zero. (Note: Because of this assumption, one might consider\nreestimating the betas and the residuals with a no-intercept model. However,\nsince the estimated alphas were close to zero, forcing the alphas to be ex-\nactly zero will not change the estimates of the betas or the residuals by much.\nTherefore, for simplicity, do not reestimate.)\n\f512    17 The Capital Asset Pricing Model\n\nProblem 3 Compute the correlation matrix of the residuals. Do any of the\nresidual correlations seem large? Could you suggest a reason why the large\ncorrelations might be large? (Information about the companies in this data set\nis available at Yahoo Finance and other Internet sites.)\n\n\nProblem 4 Use model (17.20) to estimate the covariance matrix of the excess\nreturns for the seven companies.\n\n\nProblem 5 What percentage of the excess return variance for UTX is due to\nthe market?\n\n\nProblem 6 An analyst predicts that the expected excess return on the market\nnext year will be 4 %. Assume that the betas estimated here using data from\n2004\u20132006 are suitable as estimates of next year\u2019s betas. Estimate the expected\nexcess returns for the seven stocks for next year.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.9",
      "section_title": "R Lab     511",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.9.1 Zero-beta Portfolios\n\nA portfolio with beta = 0 is neutral to market risk and bounding the abso-\nlute weights of the portfolio reduces the portfolio\u2019s unique risk. In the next\nproblem, you will \ufb01nd a low-risk portfolio with a large alpha. The data in this\nsection have been simulated and are only for illustration. Estimation of the\nalphas of stock is di\ufb03cult, especially the prediction of future values of alphas.\n\nProblem 7 The \ufb01le AlphaBeta.csv contains alphas and betas on 50 stocks.\nUse linear programming to \ufb01nd the portfolio containing these stocks that has\nthe maximum possible alpha subject to the portfolio\u2019s beta being equal to zero\nand weights satisfying \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.9",
      "section_title": "1 Zero-beta Portfolios",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 \u2264 wi \u2264 0.25 for all i = 1, . . . , 50. What are the\n50 weights of your portfolio? What is its alpha?\n    Hint: This is a linear programming problem. Use the function solveLP().\nSee Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "\u2264 wi \u2264 0.25 for all i = 1, . . . , 50. What are the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10.3.\n\n\nProblem 8 If you attempt to \ufb01nd a zero-beta portfolio with \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 \u2264 wi \u2264\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "\u2264 wi \u2264",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 with a smaller number of stock, you will \ufb01nd that there is no solution. (If\nyou like, try this with the \ufb01rst 20 stocks.) Discuss why is there no solution.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "with a smaller number of stock, you will \ufb01nd that there is no solution. (If",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.10 Exercises\n\n 1. What is the beta of a portfolio if E(RP ) = 16 %, \u03bcf = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.10",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5 %, and\n    E(RM ) = 11 %?\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "%, and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.10 Exercises     513\n\n2. Suppose that the risk-free rate of interest is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.10",
      "section_title": "Exercises     513",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03 and the expected rate\n   of return on the market portfolio is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "and the expected rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14. The standard deviation of the\n   market portfolio is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.14",
      "section_title": "The standard deviation of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12.\n   (a) According to the CAPM, what is the e\ufb03cient way to invest with an\n       expected rate of return of 0.11?\n   (b) What is the risk (standard deviation) of the portfolio in part (a)?\n3. Suppose that the risk-free interest rate is 0.023, that the expected return\n   on the market portfolio is \u03bcM = 0.10, and that the volatility of the market\n   portfolio is \u03c3M = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "(a) According to the CAPM, what is the e\ufb03cient way to invest with an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12.\n   (a) What is the expected return on an e\ufb03cient portfolio with \u03c3R = 0.05?\n   (b) Stock A returns have a covariance of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "(a) What is the expected return on an e\ufb03cient portfolio with \u03c3R = 0.05?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.004 with market returns. What\n       is the beta of Stock A?\n   (c) Stock B has beta equal to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.004",
      "section_title": "with market returns. What",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 and \u03c3 = 0.08. Stock C has beta equal\n       to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "and \u03c3 = 0.08. Stock C has beta equal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.8 and \u03c3 = 0.10.\n         i. What is the expected return of a portfolio that is one-half Stock B\n            and one-half Stock C?\n        ii. What is the volatility of a portfolio that is one-half Stock B and\n            one-half Stock C? Assume that the \u0017s of Stocks B and C are in-\n            dependent.\n4. Show that equation (17.16) follows from equation (7.8).\n5. True or false: The CAPM implies that investors demand a higher return\n   to hold more volatile securities. Explain your answer.\n6. Suppose that the riskless rate of return is 4 % and the expected market\n   return is 12 %. The standard deviation of the market return is 11 %. Sup-\n   pose as well that the covariance of the return on Stock A with the market\n   return is 165 %2 .9\n   (a) What is the beta of Stock A?\n   (b) What is the expected return on Stock A?\n   (c) If the variance of the return on Stock A is 220 %2 , what percentage of\n       this variance is due to market risk?\n7. Suppose there are three risky assets with the following betas and \u03c3 2j .\n                                     j \u03b2j   \u03c3 2j\n                                     1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.8",
      "section_title": "and \u03c3 = 0.10.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 0.010\n                                     2 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1 0.015\n                                     3 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.1",
      "section_title": "0.015",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 0.011\n     Suppose also that the variance of RM t \u2212 \u03bcf t is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.011",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.014.\n     (a) What is the beta of an equally weighted portfolio of these three assets?\n     (b) What is the variance of the excess return on the equally weighted\n         portfolio?\n     (c) What proportion of the total risk of asset 1 is due to market risk?\n\n9\n    If returns are expressed in units of percent, then the units of variances and co-\n    variances are percent-squared. A variance of 165 %2 equals 165/10,000.\n\f514    17 The Capital Asset Pricing Model\n\n 8. Suppose there are two risky assets, call them C and D. The tangency\n    portfolio is 60 % C and 40 % D. The expected yearly returns are 4 % and\n    6 % for assets C and D. The standard deviations of the yearly returns are\n    10 % and 18 % for C and D and the correlation between the returns on C\n    and D is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.014",
      "section_title": "(a) What is the beta of an equally weighted portfolio of these three assets?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5. The risk-free yearly rate is 1.2 %.\n    (a) What is the expected yearly return on the tangency portfolio?\n    (b) What is the standard deviation of the yearly return on the tangency\n        portfolio?\n    (c) If you want an e\ufb03cient portfolio with a standard deviation of the\n        yearly return equal to 3 %, what proportion of your equity should\n        be in the risk-free asset? If there is more than one solution, use the\n        portfolio with the higher expected yearly return.\n    (d) If you want an e\ufb03cient portfolio with an expected yearly return equal\n        to 7 %, what proportions of your equity should be in asset C, asset D,\n        and the risk-free asset?\n 9. What is the beta of a portfolio if the expected return on the portfolio is\n    E(RP ) = 15 %, the risk-free rate is \u03bcf = 6 %, and the expected return\n    on the market is E(RM ) = 12 %? Make the usual CAPM assumptions\n    including that the portfolio alpha is zero.\n10. Suppose that the risk-free rate of interest is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "The risk-free yearly rate is 1.2 %.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07 and the expected rate\n    of return on the market portfolio is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "and the expected rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14. The standard deviation of the\n    market portfolio is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.14",
      "section_title": "The standard deviation of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12.\n    (a) According to the CAPM, what is the e\ufb03cient way to invest with an\n        expected rate of return of 0.11?\n    (b) What is the risk (standard deviation) of the portfolio in part (a)?\n11. Suppose there are three risky assets with the following betas and \u03c3 2j when\n    regressed on the market portfolio.\n                                  j \u03b2j   \u03c3 2j\n                                  1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "(a) According to the CAPM, what is the e\ufb03cient way to invest with an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7 0.010\n                                  2 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8 0.025\n                                  3 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.025",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 0.012\n    Assume \u00171 , \u00172 , and \u00173 are uncorrelated. Suppose also that the variance of\n    RM \u2212 \u03bcf is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.012",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02.\n    (a) What is the beta of an equally weighted portfolio of these three assets?\n    (b) What is the variance of the excess return on the equally weighted\n        portfolio?\n    (c) What proportion of the total risk of asset 1 is due to market risk?\n12. As an analyst, you have constructed 2 possible portfolios. Both portfolios\n    have the same beta and expected return, but portfolio 1 was constructed\n    with only technology companies whereas portfolio 2 was constructed using\n    technology, healthcare, energy, consumer products, and metals and mining\n    companies. Should you be impartial to which portfolio you invest in?\n    Explain why or why not.\n\f                                                           References    515\n\nReferences\nBodie, Z., and Merton, R. C. (2000) Finance, Prentice-Hall, Upper Saddle\n  River, NJ.\nBodie, Z., Kane, A., and Marcus, A. (1999) Investments, 4th ed., Irwin/\n  McGraw-Hill, Boston.\nCampbell, J. Y., Lo, A. W., and MacKinlay, A. C. (1997) The Econometrics\n  of Financial Markets, Princeton University Press, Princeton, NJ.\nLintner, J. (1965a) The valuation of risky assets and the selection of risky\n  investments in stock portfolios and capital budgets. Review of Economics\n  and Statistics, 47, 13\u201337.\nLintner, J. (1965b) Security prices, risk, and maximal gains from diversi\ufb01ca-\n  tion. Journal of Finance, 20, 587\u2013615.\nMossin, J. (1966) Equilibrium in capital markets. Econometrica, 34, 768\u2013783.\nSharpe, W. F. (1964) Capital asset prices: A theory of market equilibrium\n  under conditions of risk. Journal of Finance, 19, 425\u2013442.\nSharpe, W. F., Alexander, G. J., and Bailey, J. V. (1999) Investments, 6th\n  ed., Prentice-Hall, Upper Saddle River, NJ.\n\f18\nFactor Models and Principal Components\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "(a) What is the beta of an equally weighted portfolio of these three assets?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.1 Dimension Reduction\nHigh-dimensional data can be challenging to analyze. They are di\ufb03cult to vi-\nsualize, need extensive computer resources, and often require special statistical\nmethodology. Fortunately, in many practical applications, high-dimensional\ndata have most of their variation in a lower-dimensional space that can be\nfound using dimension reduction techniques. There are many methods de-\nsigned for dimension reduction, and in this chapter we will study two closely\nrelated techniques, factor analysis and principal components analysis, often\ncalled PCA.\n    PCA \ufb01nds structure in the covariance or correlation matrix and uses this\nstructure to locate low-dimensional subspaces containing most of the variation\nin the data.\n    Factor analysis explains returns with a smaller number of fundamental\nvariables called factors or risk factors. Factor analysis models can be classi\ufb01ed\nby the types of variables used as factors, macroeconomic or fundamental, and\nby the estimation technique, time series regression, cross-sectional regression,\nor statistical factor analysis.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.1",
      "section_title": "Dimension Reduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 Principal Components Analysis\nPCA starts with a sample Y i = (Yi,1 , . . . , Yi,d ), i = 1, . . . , n, of d-dimensional\nrandom vectors with mean vector \u03bc and covariance matrix \u03a3. One goal of\nPCA is \ufb01nding \u201cstructure\u201d in \u03a3.\n   We will start with a simple example that illustrates the main idea. Suppose\nthat Y i = \u03bc + Wi o, where W1 , . . . , Wn are i.i.d. mean-zero random variables\nand o is some \ufb01xed vector, which can be taken to have norm 1. The Y i lie on\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                                      517\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 18\n\f518     18 Factor Models and Principal Components\n\nthe line that passes through \u03bc and is in the direction given by o, so that all\nvariation among the mean-centered vectors Y i \u2212 \u03bc is in the one-dimensional\nspace spanned by o. Also, the covariance matrix of Y i is\n\n                            \u03a3 = E{Wi2 ooT } = \u03c3W\n                                               2\n                                                 ooT .\n\nThe vector o is called the \ufb01rst principal axis of \u03a3 and is the only eigenvector of\n\u03a3 with a nonzero eigenvalue, so o can be estimated by an eigen-decomposition\n(Appendix A.20) of the estimated covariance matrix.\n    A slightly more realistic situation is where Y i = \u03bc+Wi o+ i , where i is a\nrandom vector uncorrelated with Wi and having a \u201csmall\u201d covariance matrix.\nThen most of the variation among the Y i \u2212 \u03bc vectors is in the space spanned\nby o, but there is small variation in other directions due to i . Having looked\nat some simple special cases, we now turn to the general case.\n    PCA can be applied to either the sample covariance matrix or the corre-\nlation matrix. We will use \u03a3 to represent whichever matrix is chosen. The\ncorrelation matrix is, of course, the covariance matrix of the standardized\nvariables, so the choice between the two matrices is really a decision whether\nor not to standardize the variables before PCA. This issue will be addressed\nlater. Even if the data have not been standardized, to keep notation simple,\nwe assume that the mean Y has been subtracted from each Y i . By (A.50),\n\n                           \u03a3 = O diag(\u03bb1 , . . . , \u03bbd ) O T ,                    (18.1)\n\nwhere O is an orthogonal matrix whose columns o1 , . . . , od are the eigen-\nvectors of \u03a3 and \u03bb1 > . . . > \u03bbd are the corresponding eigenvalues. The\ncolumns of O have been arranged so that the eigenvalues are ordered from\nlargest to smallest. This is not essential, but it is convenient. We also as-\nsume no ties among the eigenvalues, which almost certainly will be true in\nactual applications.\n    A normed linear combination of Y i (either\n                                           #\u0017 standardized or not) is of the\n        T\n               \u0017p                              p\nform \u03b1 Y i = j=1 \u03b1j Yi,j , where \u0012\u03b1\u0012 =               2\n                                               j=1 \u03b1i = 1. The \ufb01rst principal\ncomponent is the normed linear combination with the greatest variance. The\nvariation in the direction \u03b1, where \u03b1 is any \ufb01xed vector with norm 1, is\n\n                               Var(\u03b1T Y i ) = \u03b1T \u03a3\u03b1.                             (18.2)\n\nThe \ufb01rst principal component maximizes (18.2) over \u03b1. The maximizer is\n\u03b1 = o1 , the eigenvector corresponding to the largest eigenvalue, and is called\nthe \ufb01rst principal axis. The projections oT\n                                          1 Y i , i = 1, . . . , n, onto this vector are\ncalled the \ufb01rst principal component or principal component scores. Requiring\nthat the norm of \u03b1 be \ufb01xed is essential, because otherwise (18.2) is unbounded\nand there is no maximizer.\n    After the \ufb01rst principal component has been found, one searches for the\ndirection of maximum variation perpendicular to the \ufb01rst principal axis (eigen-\nvector). This means maximizing (18.2) subject to \u0012\u03b1\u0012 = 1 and \u03b1T o1 = 0.\n\f                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal Components Analysis",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": ". . . > \u03bbd are the corresponding eigenvalues. The\ncolumns of O have been arranged so that the eigenvalues are ordered from\nlargest to smallest. This is not essential, but it is convenient. We also as-\nsume no ties among the eigenvalues, which almost certainly will be true in\nactual applications.\n    A normed linear combination of Y i (either\n                                           #\u0017 standardized or not) is of the\n        T\n               \u0017p                              p\nform \u03b1 Y i = j=1 \u03b1j Yi,j , where \u0012\u03b1\u0012 =               2\n                                               j=1 \u03b1i = 1. The \ufb01rst principal\ncomponent is the normed linear combination with the greatest variance. The\nvariation in the direction \u03b1, where \u03b1 is any \ufb01xed vector with norm 1, is",
        "start": 2429,
        "end": 3193
      }
    ]
  },
  {
    "content": "18.2 Principal Components Analysis             519\n\nThe maximizer, called the second principal axis, is o2 , and the second prin-\ncipal component is the set of projections oT         2 Y i , i = 1, . . . , n, onto this axis.\nThe reader can probably see where we are going. The third principal compo-\nnent maximizes (18.2) subject to \u0012\u03b1\u0012 = 1, \u03b1T o1 = 0, and \u03b1T o2 = 0 and is\noT3 Y i , and so forth, so that o1 , . . . , od are the principal axes and the set of\nprojections oT  j Y i , i = 1, . . . , n, onto the jth eigenvector is the jth principal\ncomponent. Moreover,\n                                          \u03bbi = o T\n                                                 i \u03a3oi\n\nis the variance of the ith principal component, \u03bbi /(\u03bb1 +\u00b7 \u00b7 \u00b7+\u03bbd ) is the propor-\ntion of the variance due to this principal component, and (\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbi )/(\u03bb1 +\n\u00b7 \u00b7 \u00b7 + \u03bbd ) is the proportion of the variance due to the \ufb01rst i principal compo-\nnents. The principal components are mutually uncorrelated since for j = k\nwe have\n                          Cov(oT         T          T\n                                j Y i , ok Y i ) = oj \u03a3ok = 0\n\nby (A.52).\n   Let                                       \u239b    \u239e\n                                             YT 1\n                                           \u239c      \u239f\n                                       Y = \u239d ... \u23a0\n                                                 YT\n                                                  n\n\nbe the original data and let\n                                   \u239b                          \u239e\n                                oT\n                                 1Y 1            \u00b7\u00b7\u00b7    oT\n                                                         dY 1\n                               \u239c ..              ..        .. \u239f\n                             S=\u239d .                  .       . \u23a0\n                                       oT\n                                        1Y n     \u00b7\u00b7\u00b7    oT\n                                                         dY n\n\nbe the matrix of principal components. Then\n\n                                         S = Y O.\n\nPostmultiplication of Y by O to obtain S is an orthogonal rotation of the\ndata. For this reason, the eigenvectors are sometimes called the rotations, e.g.,\nin output from R\u2019s pca() function.\n    In many applications, the \ufb01rst few principal components, such as, the \ufb01rst\nthree to \ufb01ve, account for almost all of the variation, and, for most purposes,\none can work solely with these principal components and discard the rest. This\ncan be a sizable reduction in dimension. See Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal Components Analysis             519",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 for an illustration.\n    So far, we have left unanswered the question of how one should decide\nbetween working with the original or the standardized variables. If the com-\nponents of Y i are comparable, e.g., are all daily returns on equities or all\nare yields on bonds, then working with the original variables should cause no\nproblems. However, if the variables are not comparable, e.g., one is an unem-\nployment rate and another is the GDP in dollars, then some variables may\nbe many orders of magnitude larger than the others. In such cases, the large\n\f520      18 Factor Models and Principal Components\n\nvariables could completely dominate the PCA, so that the \ufb01rst principal com-\nponent is in the direction of the variable with the largest standard deviation.\nTo eliminate this problem, one should standardize the variables.\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "for an illustration.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.1. PCA with unstandardized and standardized variables\n\n   As a simple illustration of the di\ufb00erence between using standardized and\nunstandardized variables, suppose there are two variables (d = 2) with a\ncorrelation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.1",
      "section_title": "PCA with unstandardized and standardized variables",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9. Then the correlation matrix is\n                                 \u0007         \b\n                                    1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "Then the correlation matrix is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9\n                                   0.9 1\n\nwith normalized eigenvectors (0.71, 0.71) and (\u22120.71, 0.71)1 and eigenvalues\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "0.9 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.9 and 0.1. Most of the variation is in the direction (1, 1), which is consistent\nwith the high correlation between the two variables.\n    However, suppose that the \ufb01rst variable has variance 1,000,000 and the\nsecond has variance 1. The covariance matrix is\n                             \u0007                   \b\n                                1,000,000 900\n                                                   ,\n                                   900        1\n\nwhich has eigenvectors, after rounding, equal to (1.0000, 0.0009) and\n(\u22120.0009,1) and eigenvalues 1,000,000 and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.9",
      "section_title": "and 0.1. Most of the variation is in the direction (1, 1), which is consistent",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.19. The \ufb01rst variable dominates\nthe principal components analysis based on the covariance matrix. This prin-\ncipal components analysis does correctly show that almost all of the variation\nis in the \ufb01rst variable, but this is true only with the original units. Suppose\nthat variable 1 had been in dollars and is now converted to millions of dollars.\nThen its variance is equal to 10\u22126 , so that the principal components analysis\nusing the covariance matrix will now show most of the variation to be due to\nvariable 2. In contrast, principal components analysis based on the correlation\nmatrix does not change as the variables\u2019 units change.                        \u0002\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.19",
      "section_title": "The \ufb01rst variable dominates",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2. Principal components analysis of yield curves\n    This example uses yields on Treasury bonds at 11 maturities, T = 1, 3,\nand 6 months and 1, 2, 3, 5, 7, 10, 20, and 30 years. Daily yields were taken\nfrom a U.S. Treasury website for the time period January 2, 1990, to October\n31, 2008, A subset of these data was used in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal components analysis of yield curves",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15.1. The yield curves\nare shown in Fig. 18.1a for three di\ufb00erent dates. Notice that the yield curves\ncan have a variety of shapes. In this example, we will use PCA to study how\nthe curves change from day to day.\n    To analyze daily changes in yields, all 11 time series were di\ufb00erenced. Daily\nyields were missing from some values of T because, for example to quote the\n1\n    The normalized eigenvalues are determined only up to sign so they could multi-\n    plied by \u22121 to become (\u22120.71, \u22120.71) and (0.71, \u22120.71).\n\f                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15.1",
      "section_title": "The yield curves",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 Principal Components Analysis                          521\n\na                                                    b\n        6\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal Components Analysis                          521",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n        5\n        4\n\n\n\n\n                                                      Variances\nYield\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n        3\n\n\n\n\n                                      07/31/01\n        2\n\n\n\n\n                                      07/02/07\n        1\n\n\n\n\n                                      10/31/08\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00\n        0\n\n\n\n\n               0   5   10   15   20    25    30\n                            T\n\nc                                                    d\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                  0.5\nPC\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                      PC\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "PC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                         PC 1                                                            PC 1\n        \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "PC 1                                                            PC 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                         PC 2                     \u22120.5                                   PC 2\n                                         PC 3                                                            PC 3\n\n               0   5   10   15   20    25    30                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "PC 2                     \u22120.5                                   PC 2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0   0.5   1.0   1.5   2.0   2.5   3.0\n                            T                                                              T\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5   1.0   1.5   2.0   2.5   3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.1. (a) Treasury yields on three dates. (b) Scree plot for the changes in\nTreasury yields. Note that the \ufb01rst three principal components have most of the\nvariation, and the \ufb01rst \ufb01ve have virtually all of it. (c) The \ufb01rst three eigenvectors\nfor changes in the Treasury yields. (d) The \ufb01rst three eigenvectors for changes in\nthe Treasury yields in the range 0 \u2264 T \u2264 3.\n\n\nwebsite, \u201cTreasury discontinued the 20-year constant maturity series at the\nend of calendar year 1986 and reinstated that series on October 1, 1993.\u201d Dif-\nferencing caused a few additional days to have missing values. In the analysis,\nall days with missing values of the di\ufb00erenced data were omitted. This left\n819 days of data starting on July 31, 2001, when the one-month series started\nand ending on October 31, 2008, with the exclusion of the period February\n19, 2002 to February 2, 2006 when the 30-year Treasury was discontinued.\nOne could use much longer series by not including the one-month and 30-year\nseries.\n    The covariance matrix, not the correlation matrix, was used, because in\nthis example the variables are comparable and in the same units.\n    First, we will look at the 11 eigenvalues using R\u2019s function prcomp(). The\ncode is:\n        datNoOmit = read.table(\"treasury_yields.txt\", header = TRUE)\n        diffdatNoOmit = diff(as.matrix(datNoOmit[ , 2:12]))\n        dat = na.omit(datNoOmit)\n        diffdat = na.omit(diffdatNoOmit)\n        n = dim(diffdat)[1]\n\f522      18 Factor Models and Principal Components\n\n      options(digits = 5)\n      pca = prcomp(diffdat)\n      summary(pca)\n\nThe results are:\n      Importance of components:\n                              PC1 PC2    PC3   PC4   PC5    PC6\n      Standard deviation     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.1",
      "section_title": "(a) Treasury yields on three dates. (b) Scree plot for the changes in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.21 0.14 0.071 0.045 0.033 0.0173\n      Proportion of Variance ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.21",
      "section_title": "0.14 0.071 0.045 0.033 0.0173",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.62 0.25 0.070 0.028 0.015 0.0041\n      Cumulative Proportion ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.62",
      "section_title": "0.25 0.070 0.028 0.015 0.0041",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.62 0.88 0.946 0.974 0.989 0.9932\n\n      PC7       PC8     PC9     PC10      PC11\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.62",
      "section_title": "0.88 0.946 0.974 0.989 0.9932",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0140 0.0108 0.0092 0.00789 0.00610\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0140",
      "section_title": "0.0108 0.0092 0.00789 0.00610",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0027 0.0016 0.0012 0.00085 0.00051\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0027",
      "section_title": "0.0016 0.0012 0.00085 0.00051",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9959 0.9975 0.9986 0.99949 1.00000\n                                       \u221a\n      The \ufb01rst row gives the values of \u03bbi , the second row the values of \u03bbi /(\u03bb1 +\n\u00b7 \u00b7 \u00b7 + \u03bbd ), and the third row the values of (\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbi )/(\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbd ) for\ni = 1, . . . , 11. One can see, for example, that the standard deviation of the\n\ufb01rst principal component is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9959",
      "section_title": "0.9975 0.9986 0.99949 1.00000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.21 and represents 62 % of the total variance.\nAlso, the \ufb01rst three principal components have ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.21",
      "section_title": "and represents 62 % of the total variance.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "94.6 % of the variation, and\nthis increases to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "94.6",
      "section_title": "% of the variation, and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "97.4 % for the \ufb01rst four principal components and to 98.9 %\nfor the \ufb01rst \ufb01ve. The variances (the squares of the \ufb01rst row) are plotted in\nFig. 18.1b. This type of plot is called a \u201cscree plot\u201d since it looks like scree,\nfallen rocks that have accumulated at the base of a mountain.\n      We will concentrate on the \ufb01rst three principal components since approxi-\nmately 95 % of the variation in the changes in yields is in the space they span.\nThe eigenvectors, labeled \u201cPC,\u201d are plotted in Fig. 18.1c and d, the latter\nshowing detail in the range T \u2264 3. The eigenvectors have interesting interpre-\ntations. The \ufb01rst, o1 , has all positive values.2 A change in this direction either\nincreases all yields or decreases all yields, and by roughly the same amounts.\nOne could call such changes \u201cparallel shifts\u201d of the yield curve, though they\nare only approximately parallel. These shifts are shown in Fig. 18.2a, where\nthe mean yield curve is shown as a solid black line, the mean plus o1 is a\ndashed red line, and the mean minus o1 is a dashed blue line. Only the range\nT \u2264 7 is shown, since the curves change less after this point. Since the stan-\ndard deviation of the \ufb01rst principal component is only 0.21, a \u00b11 shift in a\nsingle day is huge and is used only for better graphical presentation.\n\n\n\n\n2\n    As mentioned previously, the eigenvectors are determined only up to a sign re-\n    versal, since multiplication by \u22121 would not change the spanned space or the\n    norm. Thus, we could instead say the eigenvector has only negative values, but\n    this would not change the interpretation.\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "97.4",
      "section_title": "% for the \ufb01rst four principal components and to 98.9 %",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 Principal Components Analysis                     523\n\na                                                        b\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal Components Analysis                     523",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5\n\n\n\n\n                                                                 4.5\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.5",
      "section_title": "4.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.0\n\n\n\n\n                                                                 4.0\nYield\n\n\n\n\n                                                         Yield\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.0",
      "section_title": "4.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5\n\n\n\n\n                                                                 3.5\n                                        mean                                                      mean\n                                        mean + PC1                                                mean + PC2\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.5",
      "section_title": "3.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                 3.0\n                                        mean \u2212 PC1                                                mean \u2212 PC2\n\n                0   1   2   3       4    5   6   7                      0   1   2    3        4    5    6   7\n                                T                                                        T\n\nc                                                        d\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5\n\n\n\n\n                                                                                                        PC 4\n\n\n\n\n                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.5",
      "section_title": "PC 4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                                                        PC 5\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "PC 5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.0\nYield\n\n\n\n\n                                                         PC\n\n                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.0",
      "section_title": "Yield",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n          3.5\n\n\n\n\n                                        mean\n                                        mean + PC3\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "3.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                        mean \u2212 PC3               \u22120.6\n\n\n                0   1   2   3       4    5   6   7                      0   5   10       15       20   25   30\n                                T                                                        T\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "mean \u2212 PC3               \u22120.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2. (a) The mean yield curve plus and minus the \ufb01rst eigenvector. (b)\nThe mean yield curve plus and minus the second eigenvector. (c) The mean yield\ncurve plus and minus the third eigenvector. (d) The fourth and \ufb01fth eigenvectors for\nchanges in the Treasury yields.\n\n\n    The graph of o2 is everywhere decreasing3 and changes in this direction\neither increase or decrease the slope of the yield curve. The result is that a\ngraph of the mean plus or minus PC2 will cross the graph of the mean curve\nat approximately T = 1, where o2 equals zero; see Fig. 18.2b.\n    The graph of o3 is \ufb01rst decreasing and then increasing, and the changes\nin this direction either increase or decrease the convexity of the yield curve.\nThe result is that a graph of the mean plus or minus PC3 will cross the\ngraph of the mean curve twice; see Fig. 18.2c. It is worth repeating a point\njust made in connection with PC1, since it is even more important here. The\nstandard deviations in the directions of PC2 and PC3 are only ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "(a) The mean yield curve plus and minus the \ufb01rst eigenvector. (b)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14 and 0.071,\nrespectively, so observed changes in these directions will be much smaller than\nthose shown in Fig. 18.2b and c. Moreover, parallel shifts will be larger than\nchanges in slope, which will be larger than changes in convexity.\n    Figure 18.2d plots the fourth and \ufb01fth eigenvectors. The patterns in their\ngraphs are complex and do not have easy interpretations. Fortunately, the\nvariation in the space they span is too small to be of much importance.\n\n\n\n3\n        The graph would, of course, be everywhere increasing if o2 were multiplied by \u22121.\n\f524            18 Factor Models and Principal Components\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.14",
      "section_title": "and 0.071,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5                  PC 1                                PC 2                                     PC 3\n\n\n\n\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "PC 1                                PC 2                                     PC 3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                        0.6\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                            0.4\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                        0.2\n\n\n\n\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\u22120.5 0.0\n\n\n\n\n                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.5 0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                                        \u22120.2\n\n\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 \u22120.2\n                                        \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\u22121.5\n\n\n\n\n           0   200   400    600   800          0   200   400    600   800               0   200   400    600   800\n                     day                                 day                                      day\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22121.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3. Time series plots of the \ufb01rst three principal components of the Treasury\nyields. There are 819 days of data, but they are not consecutive because of missing\ndata; see text.\n\n\n    A bond portfolio manager would be interested in the behavior of the yield\nchanges over time. Time series analysis based on the changes in the 11 yields\ncould be useful, but a better approach would be to use the \ufb01rst three principal\ncomponents. Their time series and auto- and cross-correlation plots are shown\nin Figs. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "Time series plots of the \ufb01rst three principal components of the Treasury",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 and 18.4, respectively. The latter shows moderate short-term\nauto-correlations which could be modeled with an ARMA process, though\nthe correlation is small enough that it might be ignored. Notice that the lag-0\ncross-correlations are zero; this is not a coincidence but rather is due to the\nway the principal components are de\ufb01ned. They are de\ufb01ned to be uncorrelated\nwith each other, so their lag-0 correlations are exactly zero. Cross-correlations\nat nonzero lags are not zero, but in this example they are small. The practical\nimplication is that parallel shifts, changes in slopes, and changes in convexity\nare nearly uncorrelated and could be analyzed separately. The time series\nplots show substantial volatility clustering which could be modeled using the\nGARCH models of Chap. 14.                                                      \u0002\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "and 18.4, respectively. The latter shows moderate short-term",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3. Principal components analysis of equity funds\n\n    This example uses the data set equityFunds.csv. The variables are daily\nreturns from January 1, 2002 to May 31, 2007 on eight equity funds: EASTEU,\nLATAM, CHINA, INDIA, ENERGY, MINING, GOLD, and WATER. The\nfollowing code was run:\n           equityFunds = read.csv(\"equityFunds.csv\")\n           pcaEq = prcomp(equityFunds[ , 2:9])\n           summary(pcaEq)\n\nThe results in this example are below and are di\ufb00erent than those for the\nchanges in yields, because in this example the variation is less concentrated\nin the \ufb01rst few principal components. For example, the \ufb01rst three principal\n\f                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "Principal components analysis of equity funds",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 Principal Components Analysis               525\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "Principal Components Analysis               525",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0                   PC1                                PC1 & PC2                           PC1 & PC3\n\n\n\n\n                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "PC1                                PC1 & PC2                           PC1 & PC3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                          1.0\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                     0.6\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\u22120.2 0.2\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22120.2 0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n                                                                          \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n           0    5     10 15 20                  0    5     10 15 20                  0   5     10 15 20\n                       lag                                  lag                                 lag\n\n                    PC2 & PC1                              PC2                               PC2 & PC3\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                     1.0\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n0.6\n\n\n\n\n                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                          0.6\n\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n                                                                          \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n               \u221220      \u221210      0              0    5     10 15 20                  0   5     10 15 20\n                       lag                                  lag                                 lag\n\n                    PC3 & PC1                            PC3 & PC2                             PC3\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                     1.0\n\n\n\n\n                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n0.6\n\n\n\n\n                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                          0.6\n\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n                                                                          \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.2\n\n\n\n\n               \u221220      \u221210      0                  \u221220      \u221210      0              0   5     10 15 20\n                       lag                                  lag                                 lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4. Sample auto- and cross-correlations of the \ufb01rst three principal compo-\nnents of the Treasury yields.\n\n\ncomponents have only 75 % of the variance, compared to 95 % for the yield\nchanges. For the equity funds, one needs six principal components to get 95 %.\nA scree plot is shown in Fig. 18.5a.\n\f526                18 Factor Models and Principal Components\n\n       a                                          b\n\n\n\n\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Sample auto- and cross-correlations of the \ufb01rst three principal compo-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n                    6e\u221204\n                                                                                       PC1\n                                                                                       PC2\n                                                                                       PC3\n\n\n\n\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "6e\u221204",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n       Variances\n                    3e\u221204\n\n\n\n\n                                                  PC\n                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "Variances",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                    0e+00\n\n\n\n\n                                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0e+00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                                              EASTEU LATAM CHINA INDIA ENERGY MINING GOLD WATER\n\n\n\n\n                                                              1     2      3      4     5      6      7      8\n                                                                                  Index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "EASTEU LATAM CHINA INDIA ENERGY MINING GOLD WATER",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5. (a) Scree plot for the Equity Funds example. (b) The \ufb01rst three eigen-\nvectors for the Equity Funds example.\n\n\n\n      Importance of components:\n                               PC1   PC2   PC3   PC4    PC5\n      Standard deviation     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "(a) Scree plot for the Equity Funds example. (b) The \ufb01rst three eigen-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.026 0.016 0.013 0.012 0.0097\n      Proportion of Variance ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.026",
      "section_title": "0.016 0.013 0.012 0.0097",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.467 0.168 0.117 0.097 0.0627\n      Cumulative Proportion ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.467",
      "section_title": "0.168 0.117 0.097 0.0627",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.467 0.635 0.751 0.848 0.9107\n\n        PC6    PC7    PC8\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.467",
      "section_title": "0.635 0.751 0.848 0.9107",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0079 0.0065 0.0055\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0079",
      "section_title": "0.0065 0.0055",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0413 0.0280 0.0201\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0413",
      "section_title": "0.0280 0.0201",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9520 0.9799 1.0000\n\n    The \ufb01rst three eigenvectors are plotted in Fig. 18.5b. The \ufb01rst eigenvector\nhas only positive values, and returns in this direction are either positive for all\nof the funds or negative for all of them. The second eigenvector is negative for\nmining and gold (funds 6 and 7) and positive for the other funds. Variation\nalong this eigenvector has mining and gold moving in the opposite direction of\nthe other funds. Gold and mining stock moving counter to the rest of the stock\nmarket is a common occurrence and, in fact, these types of stock often have\nnegative betas, so it is not surprising that the second principal component has\n17 % of the variation. The third principal component is less easy to interpret,\nbut its loading on India (fund 4) is higher than on the other funds, which\nmight indicate that there is something di\ufb00erent about Indian equities.           \u0002\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9520",
      "section_title": "0.9799 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4. Principal components analysis of the Dow Jones 30\n\n   As a further example, we will use returns on the 30 stocks on the Dow Jones\naverage. The data are in the data set DowJone30.csv and cover the period\nfrom January 2, 1991 to January 2, 2002 The \ufb01rst \ufb01ve principal components\nhave over 97 % of the variation:\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Principal components analysis of the Dow Jones 30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 Factor Models      527\n\n   Importance of components:\n                           PC1    PC2   PC3    PC4    PC5\n   Standard deviation    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "Factor Models      527",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "88.53 24.967 13.44 10.602 8.2165\n   Proportion of Variance ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "88.53",
      "section_title": "24.967 13.44 10.602 8.2165",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.87 0.069 0.02 0.012 0.0075\n   Cumulative Proportion  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.87",
      "section_title": "0.069 0.02 0.012 0.0075",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.87 0.934 0.95 0.967 0.9743\n\n   In contrast to the analysis of the equity funds where six principal compo-\nnents were needed to obtain 95 % of the variance, here the \ufb01rst three principal\ncomponents have over 95 % of the variance. Why are the Dow Jones stocks\nbehaving di\ufb00erently compared to the equity funds? The Dow Jones stocks are\nsimilar to each other since they are all large companies in the United States.\nThus, we can expect that their returns will be highly correlated with each\nother and a few principal components will explain most of the variation. \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.87",
      "section_title": "0.934 0.95 0.967 0.9743",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 Factor Models\nA factor model for excess equity returns is\n\n                   Rj,t = \u03b20,j + \u03b21,j F1,t + \u00b7 \u00b7 \u00b7 + \u03b2p,j Fp,t + \u0017j,t ,          (18.3)\n\nwhere Rj,t is either the return or the excess return on the jth asset at time\nt, F1,t , . . . , Fp,t are variables, called factors or risk factors, that represent the\n\u201cstate of the \ufb01nancial markets and world economy\u201d at time t, and \u00171,t , . . . , \u0017n,t\nare uncorrelated, mean-zero random variables called the unique risks of the\nindividual stocks. The assumption that unique risks are uncorrelated means\nthat all cross-correlation between the returns is due to the factors. Notice\nthat the factors do not depend on j since they are common to all returns. The\nparameter \u03b2i,j is called a factor loading and speci\ufb01es the sensitivity of the jth\nreturn to the ith factor. Depending on the type of factor model, either the\nloadings, the factors, or both the factors and the loadings are unknown and\nmust be estimated.\n    The CAPM is a factor model where p = 1 and F1,t is the excess return on\nthe market portfolio. In the CAPM, the market risk factor is the only source of\nrisk besides the unique risk of each asset. Because the market risk factor is the\nonly risk that any two assets share, it is the sole source of correlation between\nasset returns. Factor models generalize the CAPM by allowing more factors\nthan simply the market risk and the unique risk of each asset. A factor can\nbe any variable thought to a\ufb00ect asset returns. Examples of factors include:\n 1. returns on the market portfolio;\n 2. growth rate of the GDP;\n 3. interest rate on short term Treasury bills or changes in this rate;\n 4. in\ufb02ation rate or changes in this rate;\n 5. interest rate spreads, for example, the di\ufb00erence between long-term Trea-\n    sury bonds and long-term corporate bonds;\n\f528    18 Factor Models and Principal Components\n\n 6. return on some portfolio of stocks, for example, all U.S. stocks or all stocks\n    with a high ratio of book equity to market equity \u2014 this ratio is called\n    BE/ME in Fama and French (1992, 1995, 1996);\n 7. the di\ufb00erence between the returns on two portfolios, for example, the\n    di\ufb00erence between returns on stocks with high BE/ME values and stocks\n    with low BE/ME values.\nWith enough factors, most, and perhaps all, commonalities between assets\nshould be accounted for in the model. Then the \u0017j,t should represent factors\ntruly unique to the individual assets and therefore should be uncorrelated\nacross j (across assets), as is being assumed.\n    Factor models that use macroeconomic variables such as 1\u20135 as factors\nare called macroeconomic factor models. Fundamental factor models use ob-\nservable asset characteristics (fundamentals) such as 6 and 7 as factors. Both\ntypes of factor models can be \ufb01t by time series regression, the topic of the\nnext section. Fundamental factor models can also be \ufb01t by cross-sectional\nregression, as explained in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "Factor Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5.\n\n\n18.4 Fitting Factor Models by Time Series Regression\nEquation (18.3) is a regression model. If j is \ufb01xed, then it is a univariate\nmultiple regression model, \u201cunivariate\u201d because there is one response (the\nreturn on the jth asset) and \u201cmultiple\u201d since there can be several predictor\nvariables (the factors). If we combine these models across j, then we have\na multivariate regression model, that is, a regression model with more than\none response. Multivariate regression is used when \ufb01tting a set of returns to\nfactors.\n    As discussed in Sect. 17.6, when \ufb01tting time series regression models, one\nshould use data at the highest sampling frequency available, which is often\ndaily or weekly, though only monthly data were available for the next example.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "18.4 Fitting Factor Models by Time Series Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5. A macroeconomic factor model\n\n    The e\ufb03cient market hypothesis implies that stock prices change because\nof new information. Although there is considerable debate about the extent to\nwhich markets are e\ufb03cient, one still can expect that stock returns will be in\ufb02u-\nenced by unpredictable changes in macroeconomic variables. Accordingly, the\nfactors in a macroeconomic model are not the macroeconomic variables them-\nselves, but rather the residuals when changes in the macroeconomic variables\nare predicted from past data by a time series model, such as, a multivariate\nAR model.\n    In this example, we look at a subset of a case study that has been presented\nby other authors; see the bibliographical notes in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "A macroeconomic factor model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.7. The macroeco-\nnomic variables in this example are changes in the logs of CPI (Consumer\n\f                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.7",
      "section_title": "The macroeco-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Fitting Factor Models by Time Series Regression        529\n\nPrice Index) and IP (Industrial Production). The changes in these series have\nbeen analyzed before in Examples 12.10, 12.11, and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Fitting Factor Models by Time Series Regression        529",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.10 and in that last\nexample a bivariate AR model was \ufb01t. It was found that the AR(5) model\nminimized AIC, but the AR(1) had an AIC value nearly as small as the AR(5)\nmodel.\n    In this example, we will use the residuals from the AR(5) model as the fac-\ntors. Monthly returns on nine stocks were taken from the berndtInvest.csv\ndata set. The returns are from January 1978 to December 1987. The CPI and\nIP series from July 1977 to December 1987 were used, but the month of July\n1977 was lost through di\ufb00erencing. This left enough data (the \ufb01ve months Au-\ngust 1977 to December 1977) for forecasting CPI and IP beginning January\n1978 when the return series started.\n    R2 and the slopes for the regressions of the stock returns on the CPI resid-\nuals and the IP residuals are plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.10",
      "section_title": "and in that last",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6 for each of the 9 stocks. Note\nthat the R2 -values are very small, so the macroeconomic factors have little\nexplanatory power. The problem of low explanatory power is common with\nmacroeconomic factor models and has been noticed by other authors. For this\nreason, fundamental factor models are more widely used than macroeconomic\nmodels.                                                                        \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "for each of the 9 stocks. Note",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4.1 Fama and French Three-Factor Model\n\nFama and French (1995) have developed a fundamental factor model with\nthree risk factors, the \ufb01rst being the excess return of the market portfolio,\nwhich is the sole factor in the CAPM. The second risk factor, which is called\nsmall minus big (SMB), is the di\ufb00erence in returns on a portfolio of small\nstocks and a portfolio of large stocks. Here \u201csmall\u201d and \u201cbig\u201d refer to the\nsize of the market value, which is the share price times the number of shares\noutstanding. The third factor, HML (high minus low), is the di\ufb00erence in\nreturns on a portfolio of high book-to-market value (BE/ME) stocks and\na portfolio of low BE/ME stocks. Book value is the net worth of the \ufb01rm\naccording to its accounting balance sheet. Fama and French argue that most\npricing anomalies that are inconsistent with the CAPM disappear in the three-\nfactor model. Their model of the return on the jth asset for the tth holding\nperiod is\n\n    Rj,t \u2212 \u03bcf,t = \u03b20,j + \u03b21,j (RM,t \u2212 \u03bcf,t ) + \u03b22,j SMBt + \u03b23,j HMLt + \u0017j,t ,\n\nwhere SMBt and HMLt are the values of SMB and HML and \u03bcf,t is the\nrisk-free rate for the tth holding period. Returns on portfolios have little au-\ntocorrelation, so the returns themselves, rather than residuals from a time\nseries model, can be used.\n    Notice that this model does not use the size or the BE/ME ratio of the\njth asset to explain returns. The coe\ufb03cients \u03b22,j and \u03b23,j are the loading\nof the jth asset on SMB and HML. These loadings may, but need not, be\n\f530                   18 Factor Models and Principal Components\n\n                             R squared                            beta CPI                            beta IP\n\n      MARKET\n\n\n\n\n                                             MARKET\n\n\n\n\n                                                                               MARKET\n      IBM\n\n\n\n\n                                             IBM\n\n\n\n\n                                                                               IBM\n      GENMIL GERBER\n\n\n\n\n                                             GENMIL GERBER\n\n\n\n\n                                                                               GENMIL GERBER\n      DELTA\n\n\n\n\n                                             DELTA\n\n\n\n\n                                                                               DELTA\n      DEC\n\n\n\n\n                                             DEC\n\n\n\n\n                                                                               DEC\n      DATGEN\n\n\n\n\n                                             DATGEN\n\n\n\n\n                                                                               DATGEN\n      CONTIL\n\n\n\n\n                                             CONTIL\n\n\n\n\n                                                                               CONTIL\n      CONED\n\n\n\n\n                                             CONED\n\n\n\n\n                                                                               CONED\n\n\n\n\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "1 Fama and French Three-Factor Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00     0.02   0.04                   \u22126     \u22122 0 2 4                   \u22120.5      0.5    1.5\n                              2\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.02   0.04                   \u22126     \u22122 0 2 4                   \u22120.5      0.5    1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6. R and slopes of regressions of stock returns on CPI residuals and IP\nresiduals.\n\n\nrelated to the size and to the BE/ME ratio of the jth asset. In any event, the\nloadings are estimated by regression, not by measuring the size or BE/ME of\nthe jth asset. If the loading \u03b22,j of the jth asset on SMB is high, that might\nbe because the jth asset is small or it might be because that asset is large\nbut, in terms of returns, behaves similarly to small assets.\n    For emphasis, it is mentioned again that the factors SMBt and HMLt\ndo not depend on j since they are di\ufb00erences between returns on two \ufb01xed\nportfolios, not variables that are measured on the jth asset. This is true in\ngeneral of the factors and loadings in model (18.3), not just the Fama\u2013French\nmodel\u2014only the loadings, that is, the parameters \u03b2k,j , depend on the asset\nj. The factors are macroeconomic variables, linear combinations of returns on\nportfolios, or other variables that depend only on the \ufb01nancial markets and\nthe economy as a whole.\n\f                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "R and slopes of regressions of stock returns on CPI residuals and IP",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Fitting Factor Models by Time Series Regression    531\n\n    There are many reasons why book and market values may di\ufb00er. Book\nvalue is determined by accounting methods that do not necessarily re\ufb02ect\nmarket values. Also, a stock might have a low book-to-market value because\ninvestors expect a high return on equity, which increases its market value rela-\ntive to its book value. Conversely, a high book-to-market value could indicate\na \ufb01rm that is in trouble, which decreases its market value. A low market value\nrelative to the book value is an indication of a stock\u2019s \u201ccheapness,\u201d and stocks\nwith a high market-to-book value are considered growth stocks for which in-\nvestors are willing to pay a premium because of the promise of higher future\nearnings. Stocks with a low market-to-book value are called value stocks and\ninvesting in them is called value investing.\n    SMB and HML are the returns on portfolio that are long on one group of\nstocks and short on another. Such portfolios are called hedge portfolios since\nthey are hedged, though perhaps not perfectly, against changes in the overall\nmarket.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Fitting Factor Models by Time Series Regression    531",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6. Fitting the Fama\u2013French model to GE, IBM, and Mobil\n\n    This example uses two data sets. The \ufb01rst is CRSPmon in R\u2019s Ecdat package.\nThis is similar to the CRSPday data set used in previous examples except that\nthe returns are now monthly rather than daily. There are returns on three\nequities, GE, IBM, and Mobil, as well as on the CRSP average, though we\nwill not use the last one here. The returns are from January 1969 to December\n1998. The second data set is the Fama\u2013French factors and was taken from the\nwebsite of Prof. Kenneth French.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Fitting the Fama\u2013French model to GE, IBM, and Mobil",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.7 is a scatterplot matrix of the GE, IBM, and Mobil excess re-\nturns and the factors. Focusing on GE, we see that, as would be expected,\nGE excess returns are highly correlated with the excess market returns. The\nGE returns are negatively related with the factor HML which would indi-\ncate that GE behaves as a growth stock, since it moves in the same direction\nas low BE/ME stocks and in the opposite direction of high BE/ME stocks.\nHowever, this is a false impression caused by the lack of adjustment for asso-\nciations between GE excess returns and the other factors. Regression analysis\nwill be used soon to address this problem. The two Fama\u2013French factors are\nnot quite hedge portfolios since SMB is positively and HML negatively re-\nlated to the excess market return. However, these associations are far weaker\nthan that between the excess returns on the stocks and the market excess\nreturns. Moreover, SMB and HML have little association between each other,\nso multicollinearity is not a problem.\n    The three excess equity returns were regressed on the three factors using\nthe lm() function in R. The code is:\n   FF_data = read.table(\"FamaFrench_mon_69_98.txt\", header = TRUE)\n   attach(FF_data)\n   library(\"Ecdat\")\n\f532            18 Factor Models and Principal Components\n                         \u221220    0 10                  \u221220   0   10                       \u221210 \u22125   0   5\n\n\n\n\n                                                                                                          10\n               ge\n\n\n\n\n                                                                                                          0\n                                                                                                          \u221220\n0 10\n\n\n\n\n                               ibm\n\u221220\n\n\n\n\n                                                                                                          20\n                                             mobil\n\n\n\n\n                                                                                                          0\n                                                                                                          \u221220\n10\n0\n\n\n\n\n                                                        Mkt.RF\n\u221220\n\n\n\n\n                                                                                                          10\n                                                                                                          5\n                                                                           SMB\n\n\n\n\n                                                                                                          0\n                                                                                                          \u221210\n5\n\n\n\n\n                                                                                             HML\n0\n\u221210 \u22125\n\n\n\n\n         \u221220   0    10                 \u221220   0   20                  \u221210    0   5   10\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.7",
      "section_title": "is a scatterplot matrix of the GE, IBM, and Mobil excess re-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.7. Scatterplot matrix of the excess returns on GE, IBM, and Mobil and the\nthree factors in the Fama\u2013French model. Mkt.RF is the return on the market portfolio\nminus the risk-free rate.\n\n\n         library(\"robust\")\n         data(CRSPmon)\n         ge = 100*CRSPmon[,1] - RF\n         ibm = 100*CRSPmon[,2] - RF\n         mobil = 100*CRSPmon[,3] - RF\n         stocks = cbind(ge, ibm, mobil)\n         fit = lm(cbind(ge, ibm, mobil) ~ Mkt.RF + SMB + HML)\n         fit\n\nand the estimated coe\ufb03cients are\n         Call:\n         lm(formula = cbind(ge, ibm, mobil) ~ Mkt.RF + SMB + HML)\n\f                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.7",
      "section_title": "Scatterplot matrix of the excess returns on GE, IBM, and Mobil and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Fitting Factor Models by Time Series Regression       533\n\n   Coefficients:\n                    ge          ibm         mobil\n   (Intercept)       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Fitting Factor Models by Time Series Regression       533",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3443      0.1460      0.1635\n   Mkt.RF            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3443",
      "section_title": "0.1460      0.1635",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.1407      0.8114      0.9867\n   SMB              -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.1407",
      "section_title": "0.8114      0.9867",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3719     -0.3125     -0.3753\n   HML               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3719",
      "section_title": "-0.3125     -0.3753",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0095     -0.2983      0.3725\n    The coe\ufb03cients of HML indicate that GE and Mobil are value stocks and\nIBM is a growth stock. Notice that GE now has a positive relationship with\nHML, not the negative relationship seen in Fig. 18.7, although its coe\ufb03cient\nis close to 0. GE seems to be somewhere in between being a growth stock and\na value stock.\n    All three equity returns have negative relationships with SMB, so, not\nsurprisingly, they behave like large stocks.\n    Recall that one important assumption of the factor model is that the\n\u0017j,t in (18.3) are uncorrelated. Violation of this assumption, that is, cross-\ncorrelations between \u0017j,t and \u0017j \u0002 ,t , j = j \u0003 , will create biases when the factor\nmodel is used to estimate correlations between the equity returns, a topic\nexplained in the next section. Lack of cross-correlation is not an assumption\nof the multivariate regression model and does not cause bias in the estimation\nof the regression coe\ufb03cients or the variances of the \u0017j,t . The biases arise only\nwhen estimating covariances between the equity returns.\n    To check for cross-correlations, we will use the residuals from the multi-\nvariate regression. Their sample correlation matrix is\n   > cor(fit$residuals)\n             ge    ibm mobil\n   ge     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0095",
      "section_title": "-0.2983      0.3725",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "cor(fit$residuals)\n             ge    ibm mobil\n   ge",
        "start": 1212,
        "end": 1272
      }
    ]
  },
  {
    "content": "1.000 0.071 -0.25\n   ibm    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.000",
      "section_title": "0.071 -0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.071 1.000 -0.10\n   mobil -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.071",
      "section_title": "1.000 -0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.254 -0.102 1.00\n    The correlation between GE and Mobil is rather far from zero and is worth\nchecking. A 95 % con\ufb01dence interval for the residual correlations between GE\nexcess returns and Mobil excess returns does not include 0, so a test would\nreject the null hypotheses that the true correlation is 0. The other correlations\nare not signi\ufb01cantly di\ufb00erent from 0. Because of the large negative GE\u2013Mobil\ncorrelation, we should be careful about using the Fama\u2013French model for es-\ntimation of the covariance matrix of the equity returns. As always, it is good\npractice to look at scatterplot matrices as well as correlations, since scat-\nterplots may be outliers or nonlinear relationships a\ufb00ecting the correlations.\nFigure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.254",
      "section_title": "-0.102 1.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8 contains a scatterplot matrix of the residuals. One sees that there\nare few outliers. although none of the outliers is really extreme, it seems worth-\nwhile to compute robust correlations estimates and to compare them with the\nordinary sample correlation matrix. Robust estimates were found using the\nfunction covRob() in R\u2019s robust package. What was found is that the robust\nestimates are all closer to zero than the nonrobust estimates, but the robust\ncorrelation estimate for GE and Mobil is still a large negative value.\n\f534         18 Factor Models and Principal Components\n\n                                  \u221220   \u221210   0   10   20\n\n\n\n\n                                                                                          10\n                                                                                          5\n                  ge\n\n\n\n\n                                                                                          0\n                                                                                          \u221210 \u22125\n                                                                                      l\n                                                                  l\n20\n10\n0\n\n\n\n\n                                         ibm                                          l\n\u221220\n\n\n\n\n                                                                                          10 20 30\n                                                                      mobil\n\n\n\n\n                                                                                          \u221210 0\n      \u221210    \u22125   0    5   10                               \u221210   0    10   20   30\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "contains a scatterplot matrix of the residuals. One sees that there",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8. Scatterplot matrix of the residuals for GE, IBM, and Mobil from the\nFama\u2013French model.\n\n\n      Call:\n      covRob(data = fit$residuals, corr = T)\n\n      Robust Estimate of Correlation:\n                ge     ibm   mobil\n      ge     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "Scatterplot matrix of the residuals for GE, IBM, and Mobil from the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.000 0.0360 -0.2479\n      ibm    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.000",
      "section_title": "0.0360 -0.2479",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.036 1.0000 -0.0687\n      mobil -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.036",
      "section_title": "1.0000 -0.0687",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.248 -0.0687 1.0000\n\n\n    This example is atypical of real applications because, for illustration pur-\nposes, the number of returns has been kept low, only three, whereas in port-\nfolio management the number of returns will be larger and might be in the\nhundreds.                                                                     \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.248",
      "section_title": "-0.0687 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4.2 Estimating Expectations and Covariances of Asset Returns\n\nSection ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "2 Estimating Expectations and Covariances of Asset Returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.7 discussed how the CAPM can simplify the estimation of expec-\ntations and covariances of asset returns. However, using the CAPM for this\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.7",
      "section_title": "discussed how the CAPM can simplify the estimation of expec-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Fitting Factor Models by Time Series Regression             535\n\npurpose can be dangerous since the estimates depend on the validity of the\nCAPM. Fortunately, it is also possible to estimate return expectations and\ncovariances using a more realistic factor model instead of the CAPM.\n   We start with two factors for simplicity. From (18.3), now with p = 2, we\nhave\n                    Rj,t = \u03b20,j + \u03b21,j F1,t + \u03b22,j F2,t + \u0017j,t .      (18.4)\nIt follows from (18.4) that\n\n                     E(Rj,t ) = \u03b20,j + \u03b21,j E(F1,t ) + \u03b22,j E(F2,t )               (18.5)\n\nand\n                    2               2\n      Var(Rj,t ) = \u03b21,j Var(F1 ) + \u03b22,j Var(F2 ) + 2\u03b21,j \u03b22,j Cov(F1 , F2 ) + \u03c3 2,j .\n\nAlso, because Rj,t and Rj \u0002 ,t are two linear combinations of the risk factors, it\nfollows from (7.8) that for any j = j \u0003 ,\n\n              Cov(Rj,t , Rj \u0002 ,t ) = \u03b21,j \u03b21,j \u0002 Var(F1 ) + \u03b22,j \u03b22,j \u0002 Var(F2 )\n                                   + (\u03b21,j \u03b22,j \u0002 + \u03b21,j \u0002 \u03b22,j )Cov(F1 , F2 ).    (18.6)\n\n   More generally, let\n                                  FT\n                                   t = (F1,t , . . . , Fp,t )                      (18.7)\nbe the vector of p factors at time t and suppose that \u03a3 F is the p\u00d7p covariance\nmatrix of F t . De\ufb01ne the vector of intercepts\n\n                                  \u03b2T\n                                   0 = (\u03b20,1 , . . . , \u03b20,n )\n\nand the matrix of loadings\n                        \u239b                                               \u239e\n                              \u03b21,1       \u00b7\u00b7\u00b7     \u03b21,j     \u00b7\u00b7\u00b7      \u03b21,n\n                            \u239c                                       .. \u239f .\n                        \u03b2 = \u239d ...        ..\n                                            .\n                                                  ..\n                                                   .\n                                                          ..\n                                                             .       . \u23a0\n                                \u03b2p,1     \u00b7\u00b7\u00b7     \u03b2p,j     \u00b7\u00b7\u00b7      \u03b2p,n\n\nAlso, de\ufb01ne\n                                    T\n                                        = (\u00171,t , . . . , \u0017n,t )                   (18.8)\nand let \u03a3 be the n \u00d7 n diagonal covariance matrix of :\n                        \u239b 2                          \u239e\n                          \u03c3 ,1 \u00b7 \u00b7 \u00b7   0   \u00b7\u00b7\u00b7   0\n                        \u239c ..    ..      .. ..     .. \u239f\n                        \u239c .         .    .     .   . \u239f\n                        \u239c                            \u239f\n                 \u03a3 =\u239c 0 \u239c       \u00b7 \u00b7 \u00b7 \u03c3 ,j \u00b7 \u00b7 \u00b7\n                                       2\n                                                 0 \u239f \u239f.\n                        \u239c .     .      .   .     .   \u239f\n                        \u239d ..      ..   ..    ..  .. \u23a0\n                                   0       \u00b7\u00b7\u00b7      0      \u00b7\u00b7\u00b7     \u03c3 2,n\n\f536     18 Factor Models and Principal Components\n\nFinally, let\n                             RTt = (R1,t . . . , Rn,t )               (18.9)\nbe the vector of all returns at time t. Model (18.3) then can be reexpressed\nin matrix notation as\n                            Rt = \u03b2 0 + \u03b2 T F t + t .                 (18.10)\nTherefore, the n \u00d7 n covariance matrix of Rt is\n                             \u03a3 R = \u03b2T \u03a3 F \u03b2 + \u03a3 .                            (18.11)\n                                                T\nIn particular, if \u03b2 j = ( \u03b21,j   \u00b7\u00b7\u00b7   \u03b2p,j )       is the jth column of \u03b2, then the\nvariance of the jth return is\n                          Var(Rj ) = \u03b2 T           2\n                                       j \u03a3 F \u03b2j + \u03c3 j ,                      (18.12)\n                                                \u0003\nand the covariance between the jth and j th returns is\n                           Cov(Rj , Rj\u0003 ) = \u03b2 T\n                                              j \u03a3 F \u03b2j\u0002 .                    (18.13)\n    To use (18.11), (18.12) or (18.13), one needs estimates of \u03b2, \u03a3 F , and \u03a3 .\nThe regression coe\ufb03cients are used to estimate \u03b2, the sample covariance of\nthe factors can be used to estimate \u03a3 F , and \u03a3  \u0002 can be the diagonal matrix\nof the mean residual sum of squared errors from the regressions; see equation\n(9.13).\n    Why estimate \u03a3 R via a factor model instead of simply using the sample\ncovariance matrix? One reason is estimation accuracy. This is another example\nof bias\u2013variance tradeo\ufb00. The sample covariance matrix is unbiased, but it\ncontains n(n + 1)/2 estimates, one for each covariance and each variance.\nEach of these parameters is estimated with error and when this many errors\naccumulate, the result can be a sizable loss of precision. In contrast, the factor\nmodel requires estimates of n\u00d7p parameters in \u03b2, p(p+1)/2 parameters in \u03a3 F ,\nand n parameters in the diagonal matrix \u03a3 , for a total of np + n + p(p + 1)/2\nparameters. Typically, n, the number of returns, is large but p, the number of\nfactors, is much smaller, so np+n+p(p+1)/2 is much smaller than n(n+1)/2.\nFor example, suppose there are 200 returns and 5 factors. Then n(n + 1)/2 =\n20,100 but np + n + p(p + 1)/2 is only 1,215. The downside of the factor\nmodel is that there will be bias in the estimate of \u03a3 R if the factor model is\nmisspeci\ufb01ed, especially if \u03a3 is not diagonal as the factor model assumes.\n    Another advantage of the factor model is expediency. Having fewer param-\neters to estimate is one convenience and another is ease of updating. Suppose\na portfolio manager has implemented a factor model for n equities and now\nneeds to add another equity. If the manager uses the sample covariance ma-\ntrix, then the n sample covariances between the new return time series and\nthe old ones must be computed. This requires that all n of the old time series\nbe available. In comparison, with a factor model, the portfolio manager needs\nonly to regress the new return time series on the factors. Only the p factor\ntime series need to be available.\n\f                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Fitting Factor Models by Time Series Regression             535",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Fitting Factor Models by Time Series Regression   537\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Fitting Factor Models by Time Series Regression   537",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.7. Estimating the covariance matrix of GE, IBM, and Mobil ex-\ncess returns\n\n    This example continues Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.7",
      "section_title": "Estimating the covariance matrix of GE, IBM, and Mobil ex-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6. Recall that the number of returns\nhas been kept arti\ufb01cially low, since with more returns it would not have been\npossible to display the results. Therefore, this example merely illustrates the\ncalculations and is not a typical application of factor modeling.\n    The estimate of \u03a3 F is the sample covariance matrix of the factors:\n\n           Mkt.RF     SMB    HML\n   Mkt.RF ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Recall that the number of returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1507 4.2326 -5.1045\n   SMB     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1507",
      "section_title": "4.2326 -5.1045",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2326 8.1811 -1.0760\n   HML    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.2326",
      "section_title": "8.1811 -1.0760",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1045 -1.0760 7.1797\n\nThe estimate of \u03b2 is the matrix of regression coe\ufb03cients (without the inter-\ncepts):\n\n          Mkt.RF      SMB       HML\n   ge    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.1045",
      "section_title": "-1.0760 7.1797",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.14071 -0.37193 0.009503\n   ibm   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.14071",
      "section_title": "-0.37193 0.009503",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.81145 -0.31250 -0.298302\n   mobil ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.81145",
      "section_title": "-0.31250 -0.298302",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98672 -0.37530 0.372520\n\nThe estimate of \u03a3 is the diagonal matrix of residual error MS values:\n\n          [,1]  [,2]  [,3]\n   [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98672",
      "section_title": "-0.37530 0.372520",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.077 0.000 0.000\n   [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.077",
      "section_title": "0.000 0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000 31.263 0.000\n   [3,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "31.263 0.000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000 0.000 27.432\n\nTherefore, the estimate of \u03b2 T \u03a3 F \u03b2 is\n\n             ge    ibm mobil\n   ge    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "0.000 27.432",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "24.960 19.303 19.544\n   ibm   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "24.960",
      "section_title": "19.303 19.544",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.303 15.488 14.467\n   mobil ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.303",
      "section_title": "15.488 14.467",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.544 14.467 16.155\n\nand the estimate of \u03b2 T \u03a3 F \u03b2 + \u03a3 is\n\n             ge    ibm mobil\n   ge    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.544",
      "section_title": "14.467 16.155",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "41.036 19.303 19.544\n   ibm   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "41.036",
      "section_title": "19.303 19.544",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.303 46.752 14.467\n   mobil ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.303",
      "section_title": "46.752 14.467",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.544 14.467 43.587\n\nFor comparison, the sample covariance matrix of the equity returns is\n\n             ge    ibm mobil\n   ge    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.544",
      "section_title": "14.467 43.587",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "40.902 20.878 14.255\n   ibm   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "40.902",
      "section_title": "20.878 14.255",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.878 46.491 11.518\n   mobil ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.878",
      "section_title": "46.491 11.518",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.255 11.518 43.357\n\f538      18 Factor Models and Principal Components\n\nThe largest di\ufb00erence between the estimate of \u03b2 T \u03a3 F \u03b2 + \u03a3 and the sample\ncovariance matrix is in the covariance between the excess returns on GE and\nMobil. The reason for this large discrepancy is that the factor model assumes\na zero residual correlation between these two variables, but, as we learned\nearlier, the data show a negative correlation of \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.255",
      "section_title": "11.518 43.357",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25.\n    The code for the calculations in this example continues the code in\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "The code for the calculations in this example continues the code in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6. The addition code is:\n      sigF = as.matrix(var(cbind(Mkt.RF, SMB, HML)))\n      bbeta = as.matrix(fit$coef)\n      bbeta = t( bbeta[-1, ])\n      n = dim(CRSPmon)[1]\n      sigeps = (n - 1) / (n - 4) * as.matrix((var(as.matrix(fit$resid))))\n      sigeps = diag(as.matrix(sigeps))\n      sigeps = diag(sigeps, nrow = 3)\n      cov_equities = bbeta %*% sigF %*% t(bbeta) + sigeps\n      options(digits = 5)\n      sigF\n      bbeta\n      sigeps\n      bbeta %*% sigF %*% t(bbeta)\n      cov_equities\n      cov(stocks)\n\n                                                                                \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "The addition code is:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5 Cross-Sectional Factor Models\n\nModels of the form (18.3) are time series factor models. They use time series\ndata, one single asset at a time, to estimate the loadings.\n    As just discussed, time series factor models do not make use of variables\nsuch as dividend yields, book-to-market value, or other variables speci\ufb01c to the\njth \ufb01rm. An alternative is a cross-sectional factor model, which is a regression\nmodel using data from many assets but from only a single holding period. For\nexample, suppose that Rj , (B/M )j , and Dj are the return, book-to-market\nvalue, and dividend yield for the jth asset for some \ufb01xed time t. Since t is \ufb01xed,\nit will not be made explicit in the notation. Then a possible cross-sectional\nfactor model is\n                      Rj = \u03b20 + \u03b21 (B/M )j + \u03b22 Dj + \u0017j .\nThe parameters \u03b21 and \u03b22 are unknown values at time t of a book-to-market\nvalue risk factor and a dividend yield risk factor. These values are estimated\nby regression.\n   There are two fundamental di\ufb00erences between time series factor models\nand cross-sectional factor models. The \ufb01rst is that with a time series factor\nmodel one estimates parameters, one asset at a time, using multiple holding\n\f                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "Cross-Sectional Factor Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5 Cross-Sectional Factor Models     539\n\nperiods, while in a cross-sectional model one estimates parameters, one single\nholding period at a time, using multiple assets. The other major di\ufb00erence\nis that in a time series factor model, the factors are directly measured and\nthe loadings are the unknown parameters to be estimated by regression. In\na cross-sectional factor model the opposite is true; the loadings are directly\nmeasured and the factor values are estimated by regression.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "Cross-Sectional Factor Models     539",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8. An industry cross-sectional factor model\n\n    This example uses the berndtInvest.csv used in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "An industry cross-sectional factor model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5. This\ndata set has monthly returns on 15 stocks over 10 years, 1978 to 1987. The\n15 stocks were classi\ufb01ed into three industries,\u201cTech,\u201d \u201cOil,\u201d and \u201cOther,\u201d as\nfollows:\n\n            tech oil other\n   CITCRP      0   0     1\n   CONED       0   0     1\n   CONTIL      0   1     0\n   DATGEN      1   0     0\n   DEC         1   0     0\n   DELTA       0   1     0\n   GENMIL      0   0     1\n   GERBER      0   0     1\n   IBM         1   0     0\n   MOBIL       0   1     0\n   PANAM       0   1     0\n   PSNH        0   0     1\n   TANDY       1   0     0\n   TEXACO      0   1     0\n   WEYER       0   0     1\n\n  We used the indicator variables of \u201ctech\u201d and \u201coil\u201d as loadings and \ufb01t the\nmodel\n                   Rj = \u03b20 + \u03b21 techj + \u03b22 oilj + \u0017j ,               (18.14)\nwhere Rj is the return on the jth stock, techj equals 1 if the jth stock is a\ntechnology stock and equals 0 otherwise, and oilj is de\ufb01ned similarly. Model\n(18.14) was \ufb01t separately for each of the 120 months. The estimates \u03b2\u00020 , \u03b2\u00021 ,\nand \u03b2\u00023 for a month were the values of the three factors for that month. The\nloadings were the known values of techj and oilj .\n    Factor 1, the values of \u03b2\u00020 , can be viewed as an overall market factor, since\nit a\ufb00ects all 15 returns. Factors 2 and 3 are the technology and oil factors.\nFor example, if the value of factor 2 is positive in any given month, then Tech\nstocks have better-than-market returns that month. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "This",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9 contains time\nseries plots of the three factor series, and Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "contains time",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.10 shows their auto- and\n\f540                  18 Factor Models and Principal Components\n\ncross-correlation functions. The largest cross-correlation is between the tech\nand oil factors at lag 0, which indicates that above- (below-) market returns\nfor technology stocks are associated with above (below) market returns for oil\nstocks.\n\n                           market                                     technology                                    oil\n\n\n\n\n                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.10",
      "section_title": "shows their auto- and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n                                                      0.2\n         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                                                      0.1\n\n\n\n\n                                                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\nfactor\n\n\n\n\n                                             factor\n\n\n\n\n                                                                                         factor\n         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "factor",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                                                      0.0\n\n\n\n\n                                                                                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n                                                      \u22120.1\n         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "\u22120.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.15\n\n\n\n\n                                                                                                  \u22120.15\n                 0    20    60       100                     0   20      60        100                    0   20    60      100\n                           month                                        month                                      month\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.15",
      "section_title": "\u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9. Time series plots of the estimated values of the three factors in the\ncross-sectional factor model.\n\n\n         The standard deviations of the three factors are\n\n         market             tech       oil\n          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "Time series plots of the estimated values of the three factors in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.049            0.069     0.053\n\n   There are other ways of de\ufb01ning the factors. For example, Zivot and Wang\n(2006) use the model\n\n                                    Rj = \u03b21 techj + \u03b22 oilj + \u03b23 otherj + \u0017j ,                                             (18.15)\n\nwith no intercept but with otherJ as a third variable. With this model, there\nis no market factor but instead factors for all three industries. The model\nwith an intercept but without other is equivalent to the model with other\nin place of the intercept, in the sense that the two models produce the same\n\ufb01tted values.                                                              \u0002\n\n    Cross-sectional factor models are sometimes called BARRA models af-\nter BARRA, Inc., a company that has been developing cross-sectional factor\nmodels and marketing the output of their models to \ufb01nancial managers.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.049",
      "section_title": "0.069     0.053",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6 Statistical Factor Models\nIn a statistical factor model, neither the factor values nor the loadings are di-\nrectly observable. All that is available is the sample Y 1 , . . . , Y n or, perhaps,\nonly the sample covariance matrix. This is the same type of data available\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Statistical Factor Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6 Statistical Factor Models           541\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Statistical Factor Models           541",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0              market                          mrkt & tech                        mrkt & oil\n\n\n\n\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "market                          mrkt & tech                        mrkt & oil",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                                         1.0\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                    0.6\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n0.2\n\n\n\n\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                         0.2\n\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                    \u22120.2\n\n\n\n\n                                                                         \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n       0     5           10    15          0     5           10    15           0   5         10      15\n                   lag                                 lag                              lag\n\n\n             tech & mrkt                             tech                           tech & oil\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0     5           10    15          0     5           10    15           0   5         10      15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                    1.0\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n0.6\n\n\n\n\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                         0.6\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                    0.2\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\u22120.2\n\n\n\n\n                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                         \u22120.2\n       \u221215   \u221210          \u22125    0          0     5           10    15           0   5         10      15\n                   lag                                 lag                              lag\n\n\n             oil & mrkt                           oil & tech                            oil\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                    1.0\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n0.6\n\n\n\n\n                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                         0.6\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                    0.2\n\n\n\n\n                                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\u22120.2\n\n\n\n\n                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                         \u22120.2\n\n\n\n\n       \u221215   \u221210          \u22125    0          \u221215   \u221210          \u22125     0          0   5         10      15\n                   lag                                 lag                              lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.10. Auto- and cross-correlation plots of the estimated three factors in the\ncross-sectional factor model. Series 1\u20133 are the market, tech, and oil factors, respec-\ntively.\n\n\nfor PCA and we will see that statistical factor analysis and PCA have some\ncommon characteristics. As with PCA, one can work with either the stan-\ndardized or unstandardized variables. R\u2019s factanal() function automatically\nstandardizes the variables.\n    We start with the multifactor model in matrix notation (18.10) and the\nreturn covariance matrix (18.11) which for convenience will be repeated as\n\n                                     Rt = \u03b2 0 + \u03b2 T F t +           t.                             (18.16)\n\f542    18 Factor Models and Principal Components\n\nand\n                             \u03a3 R = \u03b2T \u03a3 F \u03b2 + \u03a3 .                         (18.17)\n       T\nHere \u03b2 is d \u00d7 p where d is the dimension of Rt and p is the number of factors.\n    The only component of (18.17) that can be estimated directly from the\ndata is \u03a3 R . One can use this estimate to \ufb01nd estimates of \u03b2, \u03a3 F , and \u03a3 .\nHowever, it is too much to ask that all three of these matrices be identi\ufb01ed\nfrom \u03a3 R alone. Here is the problem: Let A be any p\u00d7p invertible matrix. Then\nthe returns vector Rt in (18.16) is unchanged if \u03b2 T is replaced by \u03b2 T A\u22121 and\nF t is replaced by AF t . Therefore, the returns only determine \u03b2 and F t up\nto a nonsingular linear transformation, and consequently a set of constraints\nis needed to identify the parameters. The usual constraints are the factors are\nuncorrelated and standardized, so that\n\n                                    \u03a3 F = I,                              (18.18)\n\nwhere I is the p \u00d7 p identity matrix. With these constraints, (18.17) simpli\ufb01es\nto the statistical factor model\n\n                               \u03a3 R = \u03b2T \u03b2 + \u03a3 .                           (18.19)\n\nHowever, even with this simpli\ufb01cation, \u03b2 is only determined up to a rotation,\nthat is, by multiplication by an orthogonal matrix. To appreciate why this\nis so, let P be any orthogonal matrix, so that P T = P \u22121 . Then (18.19) is\nunchanged if \u03b2 is replaced by P \u03b2 since\n\n               (P \u03b2)T (P \u03b2) = \u03b2 T P T P \u03b2 = \u03b2 T P \u22121 P \u03b2 = \u03b2 T \u03b2.\n\nTherefore, to determine \u03b2 a further set of constraints is needed. One possible\nset of constraints is that \u03b2\u03a3 \u22121 \u03b2 T is diagonal (Mardia et al., 1979, p. 258).\nOutput from R\u2019s function factanal() satis\ufb01es this constraint when the argu-\nment rotation is set to \"none\". If \u03b2 is rotated as discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.10",
      "section_title": "Auto- and cross-correlation plots of the estimated three factors in the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6.1,\nthen this constraint no longer holds.\n    If the main purpose of the statistical factor model is to estimate \u03a3 R by\n(18.19), then the choice of constraint is irrelevant since all constraints lead\nto the same product \u03b2 T \u03b2. In particular, rotation of \u03b2 does not change the\nestimate of \u03a3 R .\n    It is helpful to compare three estimates of \u03a3 R . The sample covariance\nmatrix has full rank (rank = d) provided that n > d as will be assumed here.\nInstead of the sample covariance matrix, one can perform PCA and estimate\n\u03a3 R by the sample covariance matrix of the \ufb01rst p < d principal components.\nThen\n                                 \u0002 R = O T O.\n                                 \u03a3\nwhere O T is the d \u00d7 p matrix whose columns are the \ufb01rst d principal axes\n(eigenvectors) and the rank of \u03a3\u0002 R is only p so less than full rank. In contrast,\n(18.19) provides a full-rank estimate of \u03a3 R but with a simple structure, the\nsum of a rank p matrix and a diagonal matrix.\n\f                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "1,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "d as will be assumed here.\nInstead of the sample covariance matrix, one can perform PCA and estimate\n\u03a3 R by the sample covariance matrix of the \ufb01rst p < d principal components.\nThen\n                                 \u0002 R = O T O.\n                                 \u03a3\nwhere O T is the d \u00d7 p matrix whose columns are the \ufb01rst d principal axes\n(eigenvectors) and the rank of \u03a3\u0002 R is only p so less than full rank. In contrast,\n(18.19) provides a full-rank estimate of \u03a3 R but with a simple structure, the\nsum of a rank p matrix and a diagonal matrix.",
        "start": 422,
        "end": 1012
      }
    ]
  },
  {
    "content": "18.6 Statistical Factor Models   543\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Statistical Factor Models   543",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9. Factor analysis of equity funds\n\n   This example continues the analysis of the equity funds data set that was\nused in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "Factor analysis of equity funds",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 to illustrate PCA. The code for \ufb01tting a 4-factor model\n(p = 4) using factanal() is:\n\n   equityFunds = read.csv(\"equityFunds.csv\")\n   fa_none = factanal(equityFunds[ , 2:9], 4, rotation = \"none\")\n   print(fa_none,cutoff = 0.1)\n\nHere we specify no rotations. The output is:\n   > factanal(equityFunds[,2:9],4,rotation=\"none\")\n\n   Call:\n   factanal(x = equityFunds[, 2:9], factors = 4,\n         rotation = \"none\")\n\n   Uniquenesses:\n   EASTEU LATAM     CHINA   INDIA ENERGY MINING      GOLD   WATER\n    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "to illustrate PCA. The code for \ufb01tting a 4-factor model",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "factanal(equityFunds[,2:9],4,rotation=\"none\")",
        "start": 281,
        "end": 330
      }
    ]
  },
  {
    "content": "0.735 0.368     0.683   0.015 0.005 0.129       0.005   0.778\n\n   Loadings:\n          Factor1 Factor2 Factor3 Factor4\n   EASTEU ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.735",
      "section_title": "0.368     0.683   0.015 0.005 0.129       0.005   0.778",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.387    0.169   0.293\n   LATAM   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.387",
      "section_title": "0.169   0.293",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.511   0.167   0.579\n   CHINA   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.511",
      "section_title": "0.167   0.579",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.310   0.298   0.362\n   INDIA   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.310",
      "section_title": "0.298   0.362",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.281   0.951\n   ENERGY ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.281",
      "section_title": "0.951",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.784                    0.614\n   MINING ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.784",
      "section_title": "0.614",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.786            0.425 -0.258\n   GOLD    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.786",
      "section_title": "0.425 -0.258",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.798                  -0.596\n   WATER   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.798",
      "section_title": "-0.596",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.340           0.298   0.109\n\n                    Factor1 Factor2 Factor3 Factor4\n   SS loadings         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.340",
      "section_title": "0.298   0.109",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.57    1.07    0.82    0.82\n   Proportion Var      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.57",
      "section_title": "1.07    0.82    0.82",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.32    0.13    0.10    0.10\n   Cumulative Var      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.32",
      "section_title": "0.13    0.10    0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.32    0.46    0.56    0.66\n\n   Test of the hypothesis that 4 factors are sufficient.\n   The chi square statistic is 17 on 2 degrees of freedom.\n   The p-value is 2e-04\n\n    The \u201cloadings\u201d are the estimates \u03b2\u0002 T . Since there are eight funds and four\nfactors, the loadings are in an 8 \u00d7 4 matrix fa none$loadings. The output\nabove gives the sums of squares of the eight loadings for each factor. The\nProportion Var row contains the SS loadings divided by 8, where 8 is the\nsum of the variances of the eight variables, since each variable has been stan-\ndardized to have variance equal to 1.\n\f544      18 Factor Models and Principal Components\n\n    By convention, any loading with an absolute value less than the parameter\ncutoff is not printed, and the default value of cutoff is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.32",
      "section_title": "0.46    0.56    0.66",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1.\n    Because all its loadings have the same sign, the \ufb01rst factor is an overall\nindex of the eight funds. The second factor has large loadings on the four re-\ngional funds (EASTEU, LATAM, CHINA, INDIA) and small loadings on the\nfour industry section funds (ENERGY, MINING, GOLD, WATER). The four\nregions are all emerging markets, so the second factor might be interpreted\nas an emerging markets factor. The fourth factor is a contrast of MINING\nand GOLD with ENERGY and WATER, and mimics a hedge portfolio that\nis long on ENERGY and WATER and short on GOLD and MINING. The\nthird factor is less interpretable. The uniquenesses are the diagonal elements\nof the estimate \u03a3 \u0002 .\n    The output gives a p-value for testing the null hypothesis that there are\nat most four factors. The p-value is small, indicating that the null hypoth-\nesis should be rejected. However, four is that maximum number of factors\nthat can be used by factanal() when there are only eight returns. Should\nwe be concerned that we are not using enough factors? Recall the important\ndistinction between statistical and practical signi\ufb01cance that has been empha-\nsized elsewhere in this book. One way to assess practical signi\ufb01cance is to see\nhow well the factor model can reproduce the sample correlation matrix. Since\nfactanal() standardizes the variables, the factor model estimate of the cor-\nrelation matrix is the estimate of the covariance matrix, that, using (18.19), is\n\n                                  \u0002 T\u03b2\n                                  \u03b2  \u0002 +\u03a3\n                                        \u0002 .                              (18.20)\n\nThe code to calculate this estimate is\n\n      B_none = fa_none$loadings[ , ]\n      BB_none = B_none %*% t(B_none)\n      D_none = diag(fa_none$unique)\n      Sigma_R_hat = BB_none + D_none\n\n               \u0002 T with no rotation, BB none equals \u03b2\nHere B none is \u03b2                                     \u0002 T\u03b2\u0002 and D none equals \u03a3\n                                                                             \u0002 .\n    The di\ufb00erence between this estimate and the sample correlation matrix is\na 8 \u00d7 8 matrix. We would like all of its entries to be close to 0. Unfortunately,\nthey are not as small as we would like. There are various ways to check if a\nmatrix this size is \u201csmall.\u201d The smallest entry is \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "Because all its loadings have the same sign, the \ufb01rst factor is an overall",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.063 and the largest is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.063",
      "section_title": "and the largest is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03. These are reasonably large discrepancies between correlation matrices.\nAlso, the eigenvalues of the di\ufb00erence are\n       -7.5e-02 -6.0e-03 -3.4e-15 -2.0e-15\n       -1.3e-15 3.0e-15 7.7e-03 7.3e-02\nAnother way to check for smallness of the di\ufb00erence between the two estimates\nis to look at the estimates of the variance of an equally weighted portfolio (of\nthe standardized returns), which is\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "These are reasonably large discrepancies between correlation matrices.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6 Statistical Factor Models    545\n\n                                   wT \u03a3 R w,\n\nwhere wT = (1/8, . . . , 1/8). These estimates are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "Statistical Factor Models    545",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.37 and 0.42 using the factor\nmodel and the sample correlation matrix, respectively. The absolute di\ufb00er-\nence, 0.06, is relatively large compared to either of the estimates. It is unclear\nwhether this di\ufb00erence is due to a more parsimonious and accurate \ufb01t by the\nfactor model (good) or due to bias from a lack of \ufb01t by the factor model (not\ngood).                                                                          \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.37",
      "section_title": "and 0.42 using the factor",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.6.1 Varimax Rotation of the Factors\n\nAs discussed earlier, the estimate of the covariance matrix is unchanged if the\nloadings \u03b2 are rotated by multiplication by an orthogonal matrix. Rotation\nmight increase the interpretability of the loadings. In some applications, it is\ndesirable for each loading to be either close to 0 or large, so that a variable\nwill load only on a few factors, or even on only one factor. Varimax rotation\nattempts to make each loading either small or large by maximizing the sum\nof the variances of the squared loadings. Varimax rotation is the default with\nR\u2019s factanal() function, but this can be changed as in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.6",
      "section_title": "1 Varimax Rotation of the Factors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9 where\nno rotation was used. In \ufb01nance, having variables loading on only one or a\nfew factors is not that important, and may even be undesirable, so varimax\nrotation may not advantageous.\n    We repeat again for emphasis that the estimate of \u03a3 R is not changed by\nrotation. The uniquenesses are also unchanged. Only the loadings change.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "where",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.10. Factor analysis of equity funds: Varimax rotation\n\n   The statistical factor analysis in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.10",
      "section_title": "Factor analysis of equity funds: Varimax rotation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9 is repeated here but now\nwith varimax rotation.\n   Call:\n   factanal(x = equityFunds[, 2:9], factors = 4,\n         rotation = \"varimax\")\n\n   Uniquenesses:\n   EASTEU LATAM       CHINA    INDIA ENERGY MINING         GOLD    WATER\n    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "is repeated here but now",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.735 0.368       0.683    0.015 0.005 0.129          0.005    0.778\n\n   Loadings:\n          Factor1 Factor2 Factor3 Factor4\n   EASTEU ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.735",
      "section_title": "0.368       0.683    0.015 0.005 0.129          0.005    0.778",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.436   0.175   0.148   0.148\n   LATAM ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.436",
      "section_title": "0.175   0.148   0.148",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.748    0.174           0.180\n   CHINA ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.748",
      "section_title": "0.174           0.180",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.494            0.247\n   INDIA ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.494",
      "section_title": "0.247",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.243            0.959\n   ENERGY ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.243",
      "section_title": "0.959",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.327   0.118           0.934\n\f546      18 Factor Models and Principal Components\n\n      MINING ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.327",
      "section_title": "0.118           0.934",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.655     0.637                0.168\n      GOLD   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.655",
      "section_title": "0.637                0.168",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.202     0.971\n      WATER ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.202",
      "section_title": "0.971",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.418                           0.188\n\n                       Factor1 Factor2 Factor3 Factor4\n      SS loadings         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.418",
      "section_title": "0.188",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.80    1.45    1.03    1.00\n      Proportion Var      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.80",
      "section_title": "1.45    1.03    1.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.23    0.18    0.13    0.12\n      Cumulative Var      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.23",
      "section_title": "0.18    0.13    0.12",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.23    0.41    0.54    0.66\n\n      Test of the hypothesis that 4 factors are sufficient.\n      The chi square statistic is 17 on 2 degrees of freedom.\n      The p-value is 2e-04\n    The most notable change compared to the nonrotated loadings is that now\nall loadings with an absolute value above ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.23",
      "section_title": "0.41    0.54    0.66",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 are positive. Therefore, the fac-\ntors all represent long positions, whereas before some were more like hedge\nportfolios. However, the rotated factors seem less interpretable compared\nto the unrotated factors, so a \ufb01nancial analyst might prefer the unrotated\nfactors.                                                                     \u0002\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "are positive. Therefore, the fac-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.7 Bibliographic Notes\nThe Fama\u2013French three-factor model was introduced by Fama and French\n(1993) and discussed further in Fama and French (1995, 1996). Connor (1995)\ncompares the three types of factor models and \ufb01nds that macroeconomic\nfactor models have less explanatory power than other factor models. Exam-\nple ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.7",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.5 was adopted from Zivot and Wang (2006). Sharpe, Alexander, and\nBailey (1999) has a brief description of the BARRA, Inc. factor model. The\nyields.txt data set is from the Rsafd package distributed by Professor Rene\u0301\nCarmona.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.5",
      "section_title": "was adopted from Zivot and Wang (2006). Sharpe, Alexander, and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8.1 PCA\n\nIn the \ufb01rst section of this lab, you will do a principal components analysis\nof daily yield data in the \ufb01le yields.txt. R has functions, which we will use\nlater, that automate PCA, but it is easy to do PCA \u201cfrom scratch\u201d and it\nis instructive to do this. First load the data and, to get a feel for what yield\ncurves look like, plot the yield curves on days 1, 101, 201, 301, . . ., 1101. There\nare 1352 yield curves in the data, so you will see a representative sample of\nthem. The yield curves change slowly, which is why one should look at yield\ncurves that are spaced rather far (100 days) apart.\n\f                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "1 PCA",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8 R Lab     547\n\n   yieldDat = read.table(\"yields.txt\", header = T)\n   maturity = c((0:5), 5.5, 6.5, 7.5, 8.5, 9.5)\n   pairs(yieldDat)\n   par(mfrow = c(4,3))\n   for (i in 0:11)\n   {\n   plot(maturity, yieldDat[100 * i + 1, ], type = \"b\")\n   }\n\nNext compute the eigenvalues and eigenvectors of the sample covariance ma-\ntrix, print the results, and plot the eigenvalues as a scree plot.\n\n   eig = eigen(cov(yieldDat))\n   eig$values\n   eig$vectors\n   par(mfrow = c(1, 1))\n   barplot(eig$values)\n\nThe following R code plots the \ufb01rst four eigenvectors.\n   par(mfrow=c(2, 2))\n   plot(eig$vector[ , 1], ylim = c(-0.7, 0.7), type = \"b\")\n   abline(h = 0)\n   plot(eig$vector[ , 2], ylim = c(-0.7, 0.7), type = \"b\")\n   abline(h = 0)\n   plot(eig$vector[ , 3], ylim = c(-0.7, 0.7), type = \"b\")\n   abline(h = 0)\n   plot(eig$vector[ , 4], ylim = c(-0.7, 0.7), type = \"b\")\n   abline(h = 0)\n\nProblem 1 It is generally recommended that PCA be applied to time series\nthat are stationary. Plot the \ufb01rst column of yieldDat. (You can look at other\ncolumns as well. You will see that they are fairly similar.) Does the plot appear\nstationary? Why or why not? Include your plot with your work.\n\n\nAnother way to check for stationarity is to run the augmented Dickey\u2013Fuller\ntest. You can do that with the following code:\n   library(\"tseries\")\n   adf.test(yieldDat[ , 1])\n\nProblem 2 Based on the augmented Dickey\u2013Fuller test, do you think the \ufb01rst\ncolumn of yieldDat is stationary? Why or why not?\n\n\nRun the following code to compute changes in the yield curves. Notice the use\nof [-1,] to delete the \ufb01rst row and similarly the use of [-n, ].\n\f548      18 Factor Models and Principal Components\n\n      n=dim(yieldDat)[1]\n      delta_yield = yieldDat[-1, ] - yieldDat[-n, ]\n\nPlot the \ufb01rst column of delta_yield and run the augmented Dickey\u2013Fuller\ntest to check for stationarity.\n\nProblem 3 Do you think the \ufb01rst column of delta yield is stationary? Why\nor why not?\n\n\nRun the following code to perform a PCA using the function princomp(),\nwhich is similar, although not identical, to prcomp(). By default, princomp()\ndoes a PCA on the covariance matrix, though there is an option to use the\ncorrelation matrix instead. We will use the covariance matrix. The second\nline of the code will print the names of the components in the object that is\nreturned by princomp(). As you can see, the names function can be useful\nfor learning just what is being returned. You can also get this information by\ntyping ?princomp.\n\n      pca_del = princomp(delta_yield)\n      names(pca_del)\n      summary(pca_del)\n      par(mfrow = c(1, 1)\n      plot(pca_del)\n\nProblem 4 (a) The output from names includes the following:\n         [1] \"sdev\"    \"loadings\" \"center\"      \"scores\"\n    Describe each of these components in mathematical terms. To answer this\n    part of the question, you can print and plot the components to see what\n    they contain and use R\u2019s help for further information.\n(b) What are the \ufb01rst two eigenvalues of the covariance matrix?\n(c) What is the eigenvector corresponding to the largest eigenvalue?\n(d) Suppose you wish to \u201cexplain\u201d at least 95 % of the variation in the changes\n    in the yield curves. Then how many principal components should you use?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "R Lab     547",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8.2 Fitting Factor Models by Time Series Regression\n\nIn this section, we will start with the one-factor CAPM model of Chap. 17 and\nthen extend this model to the three-factor Fama\u2013French model. We will use\nthe data set Stock_Bond_2004_to_2005.csv on the book\u2019s website, which\ncontains stock prices and other \ufb01nancial time series for the years 2004 and\n2005. Data on the Fama\u2013French factors are available at Prof. Kenneth French\u2019s\nwebsite\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "2 Fitting Factor Models by Time Series Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8 R Lab     549\n\n   http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/\n   data_library.html#Research\n\nwhere RF is the risk-free rate and Mkt.RF, SMB, and HML are the Fama\u2013French\nfactors.\n    Go to Prof. French\u2019s website and get the daily values of RF, Mkt.RF, SMB,\nand HML for the years 2004\u20132005. It is assumed here that you\u2019ve put the data\nin a text \ufb01le FamaFrenchDaily.txt. Returns on this website are expressed as\npercentages.\n    Now \ufb01t the CAPM to the four stocks using the lm command. This code\n\ufb01ts a linear regression model separately to the four responses. In each case,\nthe independent variable is Mkt.RF.\n   #   Uses daily data 2004-2005\n\n   stocks = read.csv(\"Stock_Bond_2004_to_2005.csv\",header=T)\n   attach(stocks)\n   stocks_subset = as.data.frame(cbind(GM_AC, F_AC, UTX_AC, MRK_AC))\n   stocks_diff = as.data.frame(100 * apply(log(stocks_subset),\n      2, diff) - FF_data$RF)\n   names(stocks_diff) = c(\"GM\", \"Ford\", \"UTX\", \"Merck\")\n\n   FF_data = read.table(\"FamaFrenchDaily.txt\", header = TRUE)\n   FF_data = FF_data[-1, ] # delete first row since stocks_diff\n                          # lost a row due to differencing\n\n   fit1 = lm(as.matrix(stocks_diff) ~ FF_data$Mkt.RF)\n   summary(fit1)\n\nProblem 5 The CAPM predicts that all four intercepts will be zero. For each\nstock, using \u03b1 = 0.025, can you accept the null hypothesis that its intercept is\nzero? Why or why not? Include the p-values with your work.\n\n\nProblem 6 The CAPM also predicts that the four sets of residuals will be\nuncorrelated. What is the correlation matrix of the residuals? Give a 95 % con-\n\ufb01dence interval for each of the six correlations. Can you accept the hypothesis\nthat all six correlations are zero?\n\n\nProblem 7 Regardless of your answer to Problem 6, assume for now that\nthe residuals are uncorrelated. Then use the CAPM to estimate the covariance\nmatrix of the excess returns on the four stocks. Compare this estimate with\nthe sample covariance matrix of the excess returns. Do you see any large\ndiscrepancies between the two estimates of the covariance matrix?\n\f550      18 Factor Models and Principal Components\n\n   Next, you will \ufb01t the Fama\u2013French three-factor model. Run the following\nR code, which is much like the previous code except that the regression model\nhas two additional predictor variables, SMB and HML.\n      fit2 = lm(as.matrix(stocks_diff) ~ FF_data$Mkt.RF +\n         FF_data$SMB + FF_data$HML)\n      summary(fit2)\n\nProblem 8 The CAPM predicts that for each stock, the slope (beta) for SMB\nand HML will be zero. Explain why the CAPM makes this prediction. Do you\naccept this null hypothesis? Why or why not?\n\n\nProblem 9 If the Fama\u2013French model explains all covariances between the\nreturns, then the correlation matrix of the residuals should be diagonal. What\nis the estimated correlations matrix? Would you accept the hypothesis that the\ncorrelations are all zero?\n\n\nProblem 10 Which model, CAPM or Fama\u2013French, has the smaller value\nof AIC? Which has the smaller value of BIC? What do you conclude from\nthis?\n\n\nProblem 11 What is the covariance matrix of the three Fama\u2013French fac-\ntors?\n\n\nProblem 12 In this problem, Stocks 1 and 2 are two stocks, not necessarily\nin the Stock_FX_Bond_2004_to_2005.csv data set. Suppose that Stock 1 has\nbetas of 0.5, 0.4, and \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "R Lab     549",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 with respect to the three factors in the Fama\u2013French\nmodel and a residual variance of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "with respect to the three factors in the Fama\u2013French",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "23.0. Suppose also that Stock 2 has betas of\n0.6, 0.15, and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "23.0",
      "section_title": "Suppose also that Stock 2 has betas of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.7 with respect to the three factors and a residual variance\nof ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.7",
      "section_title": "with respect to the three factors and a residual variance",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "37.0. Regardless of your answer to Problem 9, when doing this problem,\nassume that the three factors do account for all covariances.\n(a) Use the Fama\u2013French model to estimate the variance of the excess return\n    on Stock 1.\n(b) Use the Fama\u2013French model to estimate the variance of the excess return\n    on Stock 2.\n(c) Use the Fama\u2013French model to estimate the covariance between the excess\n    returns on Stock 1 and Stock 2.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "37.0",
      "section_title": "Regardless of your answer to Problem 9, when doing this problem,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.8.3 Statistical Factor Models\nThis section applies statistical factor analysis to the log returns of 10 stocks\nin the data set Stock_FX_Bond.csv. The data set contains adjusted closing\n\f                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.8",
      "section_title": "3 Statistical Factor Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9 Exercises    551\n\n(AC) prices of the stocks, as well as daily volumes and other information that\nwe will not use here.\n   The following R code will read the data, compute the log returns, and \ufb01t a\ntwo-factor model. Note that factanal works with the correlation matrix or,\nequivalently, with standardized variables.\n   dat = read.csv(\"Stock_FX_Bond.csv\")\n   stocks_ac = dat[ , c(3, 5, 7, 9, 11, 13, 15, 17)]\n   n = length(stocks_ac[ , 1])\n   stocks_returns = log(stocks_ac[-1, ] / stocks_ac[-n, ])\n   fact = factanal(stocks_returns, factors = 2, rotation = \"none\")\n   print(fact)\n\nLoadings less than the parameter cutoff are not printed. The default value\nof cutoff is 0.1, but you can change it as in \u201cprint(fact,cutoff = 0.01)\u201d\nor \u201cprint(fact, cutoff = 0)\u201d.\n\nProblem 13 What are the factor loadings? What are the variances of the\nunique risks for Ford and General Motors?\n\n\nProblem 14 Does the likelihood ratio test suggest that two factors are\nenough? If not, what is the minimum number of factors that seems su\ufb03cient?\n\n\nThe following code will extract the loadings and uniquenesses.\n   loadings = matrix(as.numeric(loadings(fact)), ncol = 2)\n   unique = as.numeric(fact$unique)\n\nProblem 15 Regardless of your answer to Problem 14, use the two-factor\nmodel to estimate the correlation of the log returns for Ford and IBM.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "Exercises    551",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.9 Exercises\n1. The \ufb01le yields2009.csv on this book\u2019s website contains daily Treasury\n   yields for 2009. Perform a principal components analysis on changes in\n   the yields. Describe your \ufb01ndings. How many principal components are\n   needed to capture 98 % of the variability?\n2. Perform a statistical factor analysis of the returns in the data set mid-\n   capD.ts on the book\u2019s website. How many factors did you select? Use\n   (18.20) to estimate the covariance matrix of the returns.\n3. Verify equation (18.6).\n4. Compute the eigenvectors in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.9",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 and o\ufb00er an interpretation of\n   the \ufb01rst two eigenvectors.\n\f552    18 Factor Models and Principal Components\n\nReferences\nConnor, G. (1995) The three types of factor models: a comparison of their\n  explanatory power. Financial Analysts Journal. 42\u201346.\nFama, E. F., and French, K. R. (1992) The cross-section of expected stock\n  returns. Journal of Finance, 47, 427\u2013465.\nFama, E. F., and French, K. R. (1993) Common risk factors in the returns on\n  stocks and bonds. Journal of Financial Economics, 33, 3\u201356.\nFama, E. F., and French, K. R. (1995) Size and book-to-market factors in\n  earnings and returns. Journal of Finance, 50, 131\u2013155.\nFama, E. F., and French, K. R. (1996) Multifactor explanations of asset pricing\n  anomalies. Journal of Finance, 51, 55\u201384.\nMardia, K. V., Kent, J. T., and Bibby, J. M. (1979) Multivariate Analysis,\n  Academic Press, London.\nSharpe, W. F., Alexander, G. J., and Bailey, J. V. (1999) Investments, 6th\n  ed., Prentice-Hall, Upper Saddle River, NJ.\nZivot, E., and Wang, J. (2006) Modeling Financial Time Series with S-PLUS,\n  2nd ed., Springer, New York.\n\f19\nRisk Management\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "and o\ufb00er an interpretation of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.1 The Need for Risk Management\n\nThe \ufb01nancial world has always been risky, and \ufb01nancial innovations such as\nthe development of derivatives markets and the packaging of mortgages have\nnow made risk management more important than ever, but also more di\ufb03cult.\n    There are many di\ufb00erent types of risk. Market risk is due to changes in\nprices. Credit risk is the danger that a counterparty does not meet contractual\nobligations, for example, that interest or principal on a bond is not paid.\nLiquidity risk is the potential extra cost of liquidating a position because\nbuyers are di\ufb03cult to locate. Operational risk is due to fraud, mismanagement,\nhuman errors, and similar problems.\n    Early attempts to measure risk such as duration analysis, discussed in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.1",
      "section_title": "The Need for Risk Management",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.8.1 and used to estimate the market risk of \ufb01xed income securities,\nwere somewhat primitive and of only limited applicability. In contrast, value-\nat-risk (VaR) and expected shortfall (ES) are widely used because they can\nbe applied to all types of risks and securities, including complex portfolios.\n    VaR uses two parameters, the time horizon and the con\ufb01dence level, which\nare denoted by T and 1 \u2212 \u03b1, respectively. Given these, the VaR is a bound\nsuch that the loss over the horizon is less than this bound with probability\nequal to the con\ufb01dence coe\ufb03cient. For example, if the horizon is one week,\nthe con\ufb01dence coe\ufb03cient is 99 % (so \u03b1 = 0.01), and the VaR is $5 million,\nthen there is only a 1 % chance of a loss exceeding $5 million over the next\nweek. We sometimes use the notation VaR(\u03b1) or Var(\u03b1, T ) to indicate the\ndependence of VaR on \u03b1 or on both \u03b1 and the horizon T . Usually, VaR(\u03b1) is\nused with T being understood.\n    If L is the loss over the holding period T , then VaR(\u03b1) is the \u03b1th upper\nquantile of L. Equivalently, if R = \u2212L is the revenue, then VaR(\u03b1) is minus\nthe \u03b1th quantile of R. For continuous loss distributions, VaR(\u03b1) solves\n\n\n\u00a9 Springer Science+Business Media New York 2015                            553\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 19\n\f554    19 Risk Management\n\n                              P {L > VaR(\u03b1)} = P {L \u2265 VaR(\u03b1)} = \u03b1,            (19.1)\n\nand for any loss distribution, continuous or not,\n\n                                 VaR(\u03b1) = inf{x : P (L > x) \u2264 \u03b1}.             (19.2)\n\n   As will be discussed later, VaR has a serious de\ufb01ciency\u2014it discourages\ndiversi\ufb01cation\u2014and for this reason it is being replaced by newer risk measures.\nOne of these newer risk measures is the expected loss given that the loss\nexceeds VaR, which is called by a variety of names: expected shortfall, the\nexpected loss given a tail event, tail loss, and shortfall. The name expected\nshortfall and the abbreviation ES will be used here.\n   For any loss distribution, continuous or not,\n                                     \u0016\u03b1\n                                         VaR(u) du\n                           ES(\u03b1) = 0                ,                    (19.3)\n                                            \u03b1\nwhich is the average of VaR(u) over all u that are less than or equal to \u03b1. If\nL has a continuous distribution,\n                       \u0015                      \u0015\n        ES(\u03b1) = E L \u0015 L > VaR(\u03b1) = E L \u0015 L \u2265 VaR(\u03b1) .                  (19.4)\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.8",
      "section_title": "1 and used to estimate the market risk of \ufb01xed income securities,",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "VaR(\u03b1)} = P {L \u2265 VaR(\u03b1)} = \u03b1,            (19.1)",
        "start": 1451,
        "end": 1502
      },
      {
        "language": "r",
        "code": "x) \u2264 \u03b1}.             (19.2)",
        "start": 1608,
        "end": 1639
      },
      {
        "language": "r",
        "code": "VaR(\u03b1) = E L \u0015 L \u2265 VaR(\u03b1) .                  (19.4)",
        "start": 2524,
        "end": 2579
      }
    ]
  },
  {
    "content": "19.1. VaR with a normally distributed loss\n\n    Suppose that the yearly return on a stock is normally distributed with\nmean ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.1",
      "section_title": "VaR with a normally distributed loss",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 and standard deviation 0.18. If one purchases $100,000 worth of\nthis stock, what is the VaR with T equal to one year?\n                      40000\n                      30000\n             VaR(\u03b1)\n                      20000\n                      10000\n\n\n\n\n                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "and standard deviation 0.18. If one purchases $100,000 worth of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00    0.05    0.10       0.15   0.20   0.25\n                                                     \u03b1\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00",
      "section_title": "0.05    0.10       0.15   0.20   0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.1. VaR(\u03b1) for 0.025 < \u03b1 < 0.25 when the loss distribution is normally\ndistributed with mean \u22124000 and standard deviation 18,000.\n\f                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.1",
      "section_title": "VaR(\u03b1) for 0.025 < \u03b1 < 0.25 when the loss distribution is normally",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 Estimating VaR and ES with One Asset          555\n\n   To answer this question, we use the fact that the loss distribution is normal\nwith mean \u22124000 and standard deviation 18,000, with all units in dollars.\nTherefore, VaR is\n                             \u22124000 + 18,000z\u03b1 ,\nwhere z\u03b1 is the \u03b1-upper quantile of the standard normal distribution. VaR(\u03b1)\nis plotted as a function of \u03b1 in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "Estimating VaR and ES with One Asset          555",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.1. VaR depends heavily on \u03b1 and in\nthis \ufb01gure ranges from 46,527 when \u03b1 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.1",
      "section_title": "VaR depends heavily on \u03b1 and in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.025 to 8,226 when \u03b1 is 0.25.    \u0002\n\n    In applications, risk measures will rarely, if ever, be known exactly as in\nthese simple examples. Instead, risk measures are estimated, and estimation\nerror is another source of uncertainty. This uncertainty can be quanti\ufb01ed using\na con\ufb01dence interval for the risk measure. We turn next to these topics.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.025",
      "section_title": "to 8,226 when \u03b1 is 0.25.    \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 Estimating VaR and ES with One Asset\n\nTo illustrate the techniques for estimating VaR and ES, we begin with the\nsimple case of a single asset. In this section, these risk measures are estimated\nusing historic data to estimate the distribution of returns. We make the as-\nsumption that returns are stationary, at least over the historic period we use.\nThis is usually a reasonable assumption. We will also assume that the returns\nare independent. Independence is a much less reasonable assumption because\nof volatility clustering, and later we will remove this assumption by using\nGARCH models.\n    Two cases are considered, \ufb01rst without and then with the assumption of\na parametric model for the return distribution.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "Estimating VaR and ES with One Asset",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.1 Nonparametric Estimation of VaR and ES\n\nWe start with nonparametric estimates of VaR and ES, meaning that the loss\ndistribution is not assumed to be in a parametric family such as the normal\nor t-distributions.\n    Suppose that we want a con\ufb01dence coe\ufb03cient of 1\u2212\u03b1 for the risk measures.\nTherefore, we estimate the \u03b1-quantile of the return distribution, which is the\n\u03b1-upper quantile of the loss distribution. In the nonparametric method, this\nquantile is estimated as the \u03b1-quantile of a sample of historic returns, which\nwe will call q\u0002(\u03b1). If S is the size of the current position, then the nonparametric\nestimate of VaR is                   np\n                                VaR (\u03b1) = \u2212S \u00d7 q\u0002(\u03b1),\nwith the minus sign converting revenue (return times initial investment) to\na loss. In this chapter, superscripts and subscripts will sometimes be placed\non VaR and ES to provide information. Here, the superscript \u201cnp\u201d means\n\u201cnonparametrically estimated.\u201d\n\f556    19 Risk Management\n\n  To estimate ES, let R1 , . . . , Rn be the historic returns and de\ufb01ne Li =\n\u2212S \u00d7 Ri . Then\n        \u0017n                           \u0017n\n         i=1 Li I{Li > VaR(\u03b1)}        i=1 Ri I{Ri < q  \u0002(\u03b1)}\n    np\n \u001f (\u03b1) = \u0017\n ES                            = \u2212S\u00d7  \u0017 n                    , (19.5)\n           n\n           i=1 I{L i > VaR(\u03b1)}          i=1 I{R i < \u0002\n                                                    q (\u03b1)}\n                                              np                       np\nwhich is the average of all Li exceeding VaR (\u03b1). Here I{Li > VaR (\u03b1)} is\n                                    np\nthe indicator that Li exceeds VaR (\u03b1), and similarly for I{Ri < q\u0002(\u03b1)}.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "1 Nonparametric Estimation of VaR and ES",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "VaR(\u03b1)}        i=1 Ri I{Ri < q  \u0002(\u03b1)}\n    np\n \u001f (\u03b1) = \u0017\n ES                            = \u2212S\u00d7  \u0017 n                    , (19.5)\n           n\n           i=1 I{L i > VaR(\u03b1)}          i=1 I{R i < \u0002\n                                                    q (\u03b1)}\n                                              np                       np\nwhich is the average of all Li exceeding VaR (\u03b1). Here I{Li > VaR (\u03b1)} is\n                                    np\nthe indicator that Li exceeds VaR (\u03b1), and similarly for I{Ri < q\u0002(\u03b1)}.",
        "start": 1146,
        "end": 1660
      }
    ]
  },
  {
    "content": "19.2. Nonparametric VaR and ES for a position in an S&P 500 index\nfund\n\n    As a simple example, suppose that you hold a $20,000 position in an S&P\n500 index fund, so your returns are those of this index, and that you want a\n24-h VaR. We estimate this VaR using the 1,000 daily returns on the S&P\n500 for the period ending in April 1991. These log returns are a subset of the\ndata set SP500 in R\u2019s Ecdat package. The full time series is plotted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "Nonparametric VaR and ES for a position in an S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.1.\nBlack Monday, with a log return of \u22120.23, occurs near the beginning of the\nshortened time series used in this example.\n    Suppose you want 95 % con\ufb01dence. The ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.1",
      "section_title": "Black Monday, with a log return of \u22120.23, occurs near the beginning of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 quantile of the returns com-\nputed by R\u2019s quantile() function is \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "quantile of the returns com-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0169. In other words, a daily return\nof \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0169",
      "section_title": "In other words, a daily return",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0169 or less occurred only 5 % of the time in the historic data, so we\nestimate that there is a 5 % chance of a return of that size occurring during\nthe next 24 h. A return of \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0169",
      "section_title": "or less occurred only 5 % of the time in the historic data, so we",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0169 on a $20,000 investment yields a revenue\nof \u2212$337.5, and therefore the estimated VaR(0.05, 24 h) is $",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0169",
      "section_title": "on a $20,000 investment yields a revenue",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "337.43.\n    ES(0.05) is obtained by averaging all returns below \u22120.0169 and multiply-\n                                               np\n                                            \u001f (0.05) = $",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "337.43",
      "section_title": "ES(0.05) is obtained by averaging all returns below \u22120.0169 and multiply-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "619.3. The code for\ning this average by \u221220,000. The result is ES\nthis example is below.\n1  data(SP500, package=\"Ecdat\")\n2  n = 2783\n 3 SPreturn = SP500$r500[(n - 999):n]\n\n 4 year = 1981 + (1:n) * (",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "619.3",
      "section_title": "The code for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1991.25 - 1981) / n\n\n 5 year = year[(n - 999):n]\n\n 6 alpha = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1991.25",
      "section_title": "- 1981) / n",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n 7 q = as.numeric(quantile(SPreturn, alpha))\n\n 8 VaR_nonp = -20000 * q\n\n 9 IEVaR = (SPreturn < q)\n\n10 sum(IEVaR)\n\n11 ES_nonp = -20000 * sum(SPreturn * IEVaR) / sum(IEVaR)\n\n12 options(digits = 5)\n\n13 VaR_nonp\n\n14 ES_nonp\n\n\n\n                                                                               \u0002\n\f                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "7 q = as.numeric(quantile(SPreturn, alpha))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 Estimating VaR and ES with One Asset        557\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "Estimating VaR and ES with One Asset        557",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.2 Parametric Estimation of VaR and ES\n\nParametric estimation of VaR and ES has a number of advantages. For ex-\nample, parametric estimation allows the use of GARCH models to adapt the\nrisk measures to the current estimate of volatility. Also, risk measures can be\neasily computed for a portfolio of stocks if we assume that their returns have a\njoint parametric distribution, such as a multivariate t-distribution. Nonpara-\nmetric estimation using sample quantiles works best when the sample size and\n\u03b1 are reasonably large. With smaller sample sizes or smaller values of \u03b1, it is\npreferable to use parametric estimation. In this section, we look at parametric\nestimation of VaR and ES when there is a single asset.\n    Let F (y|\u03b8) be a parametric family of distributions used to model the return\ndistribution and suppose that \u03b8 \u0002 is an estimate of \u03b8, such as, the MLE com-\nputed from historic returns. Then F \u22121 (\u03b1|\u03b8) \u0002 is an estimate of the \u03b1-quantile\nof the return distribution and\n                                par\n                          VaR                            \u0002\n                                      (\u03b1) = \u2212S \u00d7 F \u22121 (\u03b1|\u03b8)                 (19.6)\n\nis a parametric estimate of VaR(\u03b1). As before, S is the size of the current\nposition.\n    Let f (y|\u03b8) be the density of F (y|\u03b8). Then the estimate of expected short-\nfall is\n                                       \u0013 F \u22121 (\u03b1|\u03b8\u0302\u03b8 )\n                    \u001f\n                      par         S                          \u0002 dx.\n                    ES (\u03b1) = \u2212 \u00d7                       xf (x|\u03b8)          (19.7)\n                                  \u03b1     \u2212\u221e\n\nThe superscript \u201cpar\u201d denotes \u201cparametrically estimated.\u201d Computing this\nintegral is not always easy, but in the important cases of normal and t-\ndistributions there are convenient formulas.\n    Suppose the return has a t-distribution with mean equal to \u03bc, scale pa-\nrameter equal to \u03bb, and tail index1 \u03bd. Let f\u03bd and F\u03bd be, respectively, the\nt-density and t-distribution function with \u03bd degrees of freedom. The expected\nshortfall is\n                     \u000e        \u0007              !                \"\b\u000f\n      \u001f\n         t                      f\u03bd {F\u03bd\u22121 (\u03b1)} \u03bd + {F\u03bd\u22121 (\u03b1)}2\n      ES (\u03b1) = S \u00d7 \u2212\u03bc + \u03bb                                         .     (19.8)\n                                      \u03b1            \u03bd\u22121\nThe formula for normal loss distributions is obtained by a direct calculation\nor letting \u03bd \u2192 \u221e in (19.8). The result is\n                                 \u000e         \u0007            \b\u000f\n                                             \u03c6{\u03a6\u22121 (\u03b1)}\n               ESnorm (\u03b1) = S \u00d7 \u2212\u03bc + \u03c3                     ,           (19.9)\n                                                  \u03b1\nwhere \u03bc and \u03c3 are the mean and standard deviation of the returns and \u03c6\nand \u03a6 are the standard normal density and CDF. The superscripts \u201ct\u201d and\n1\n    The tail index parameter for the t-distribution is also commonly referred to as\n    the degrees-of-freedom parameter by its association with the theory of linear\n    regression, and some R functions use the abbreviations df or nu.\n\f558     19 Risk Management\n\n\u201cnorm\u201d denote estimates assuming a t-distributed return and normal return,\nrespectively.\n   Parametric estimation with one asset is illustrated in the next example.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "2 Parametric Estimation of VaR and ES",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.3. Parametric VaR and ES for a position in an S&P 500 index\nfund\n\n    This example uses the same data set as in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.3",
      "section_title": "Parametric VaR and ES for a position in an S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 so that paramet-\nric and nonparametric estimates can be compared. We will assume that the\nreturns are i.i.d. with a t-distribution. Under this assumption, VaR is\n                             t\n                        VaR (\u03b1) = \u2212S \u00d7 {\u0002\n                                        \u03bc + q\u03b1,t (\u0002  \u0002\n                                                  \u03bd )\u03bb},                     (19.10)\n\nwhere \u03bc   \u0002 and \u03bd\u0002 are the estimated mean, scale parameter, and tail index of a\n       \u0002, \u03bb,\nsample of returns. Also, q\u03b1,t (\u0002\u03bd ) is the \u03b1-quantile of the t-distribution with tail\nindex \u03bd\u0002, so that {\u0002\n                   \u03bc + q\u03b1,t (\u0002  \u0002 is the \u03b1th quantile of the \ufb01tted distribution.\n                             \u03bd )\u03bb}\n    The t-distribution was \ufb01t using R\u2019s fitdistr() function and the estimates\n     \u0002 = 0.000689, \u03bb\nwere \u03bc                \u0002 = 0.007164, and \u03bd\u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "so that paramet-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.984. For later reference, the\n                                           \u0011\nestimated standard deviation is \u03c3    \u0002=\u03bb  \u0002 \u03bd\u0002/(\u0002\n                                                \u03bd \u2212 2) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.984",
      "section_title": "For later reference, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01248.\n    The 0.05-quantile of the t-distribution with tail index 2.984 is \u22122.3586.\nTherefore, by (19.6),\n          t\n      VaR (0.05) = \u221220000 \u00d7 {",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01248",
      "section_title": "The 0.05-quantile of the t-distribution with tail index 2.984 is \u22122.3586.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000689 \u2212 (2.3586)(0.007164)} = $324.17.\n                                                 np\nNotice that the nonparametric estimate, VaR (0.05) = $337.55, is similar to,\nbut somewhat larger than the parametric estimate, $",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000689",
      "section_title": "\u2212 (2.3586)(0.007164)} = $324.17.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "324.17.\n    The parametric estimate of ESt (0.05) is $543.81 and is found by substi-\ntuting S = 20,000, \u03b1 = 0.05, \u03bc   \u0002 = 0.000689, \u03bb\u0002 = 0.007164, and \u03bd\u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "324.17",
      "section_title": "The parametric estimate of ESt (0.05) is $543.81 and is found by substi-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.984\n                                             t\ninto (19.8). The parametric estimate of ES (0.05) is noticeably shorter than\nthe nonparametric. The reason the two estimates di\ufb00er is that the extreme\nleft tail of the returns, roughly the smallest 10 of 1,000 returns, is heavier\nthan the tail of a t-distribution with ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.984",
      "section_title": "t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.984 degrees of freedom; see the t-plot\nin Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.984",
      "section_title": "degrees of freedom; see the t-plot",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2. The code for this example is below.\n15 data(SP500, package=\"Ecdat\")\n16 n = 2783\n17 SPreturn = SP500$r500[(n - 999):n]\n\n18 year = 1981 + (1:n) * (",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "The code for this example is below.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1991.25 - 1981) / n\n\n19 year = year[(n - 999):n]\n\n20 alpha = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1991.25",
      "section_title": "- 1981) / n",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n21 library(MASS)\n\n22 fitt = fitdistr(SPreturn, \"t\")\n\n23 param = as.numeric(fitt$estimate)\n\n24 mean = param[1]\n\n25 df = param[3]\n\n26 sd = param[2] * sqrt((df) / (df - 2))\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "21 library(MASS)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.3 Bootstrap Con\ufb01dence Intervals for VaR and ES   559\n\n                                                t\u2212probability plot,\n                                                    df=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals for VaR and ES   559",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.9837\n\n\n\n\n                             10\n                             5\n               t\u2212quantiles\n                             0\n                             \u22125\n                             \u221210\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.9837",
      "section_title": "10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20 \u22120.15 \u22120.10 \u22120.05      0.00   0.05\n                                                       data\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.20",
      "section_title": "\u22120.15 \u22120.10 \u22120.05      0.00   0.05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2. t-plot of the S&P 500 returns used in Examples 19.2 and 19.3. The\ndeviations from linearity in the tails, especially the left tail, indicate that the t-\ndistribution does not \ufb01t the data in the extreme tails. The reference line goes through\nthe \ufb01rst and third quartiles. The t-quantiles use ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "t-plot of the S&P 500 returns used in Examples 19.2 and 19.3. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.98 degrees of freedom, the MLE.\nThe deviation in the left tail of the data from the t-distribution explains why the\nparametric estimate of ES is smaller than the nonparametric estimate.\n\n\n27 lambda = param[2]\n28 qalpha = qt(alpha, df = df)\n29 VaR_par = -20000 * (mean + lambda * qalpha)\n\n30 es1 = dt(qalpha, df = df) / (alpha)\n\n31 es2 = (df + qalpha^2) / (df - 1)\n\n32 es3 = -mean + lambda * es1 * es2\n\n33 ES_par = 20000*es3\n\n34 VaR_par\n\n35 ES_par\n\n\n                                                                                        \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.98",
      "section_title": "degrees of freedom, the MLE.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.3 Bootstrap Con\ufb01dence Intervals for VaR and ES\nThe estimates of VaR and ES are precisely that, just estimates. If we had\nused a di\ufb00erent sample of historic data, then we would have gotten di\ufb00erent\nestimates of these risk measures. We just calculated VaR and ES values to\n\ufb01ve signi\ufb01cant digits, but do we really have that much precision? The reader\nhas probably guessed (correctly) that we do not, but how much precision do\nwe have? How can we learn the true precision of the estimates? Fortunately, a\ncon\ufb01dence interval for VaR or ES is rather easily obtained by bootstrapping.\nAny of the con\ufb01dence interval procedures in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.3",
      "section_title": "Bootstrap Con\ufb01dence Intervals for VaR and ES",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3 can be used. We will\n\f560    19 Risk Management\n\nsee that even with 1,000 returns to estimate VaR and ES, these risk measures\nare estimated with considerable uncertainty.\n    For now, we will assume an i.i.d. sample of historic returns and use model-\nfree resampling. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.3",
      "section_title": "can be used. We will",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4 we will allow for dependencies, for instance,\nGARCH e\ufb00ects, in the data and we will use model-based resampling.\n    Suppose we have a large number, B, of resamples of the returns data.\nThen a VaR(\u03b1) or ES(\u03b1) estimate is computed from each resample and for the\noriginal sample. The con\ufb01dence interval can be based upon either a parametric\nor nonparametric estimator of VaR(\u03b1) or ES(\u03b1). Suppose that we want the\ncon\ufb01dence coe\ufb03cient of the interval to be 1 \u2212 \u03b3. The interval\u2019s con\ufb01dence\ncoe\ufb03cient should not be confused with the con\ufb01dence coe\ufb03cient of VaR, which\nwe denote by 1 \u2212 \u03b1. The \u03b3/2-lower and -upper quantiles of the bootstrap\nestimates of VaR(\u03b1) and ES(\u03b1) are the limits of the basic percentile method\ncon\ufb01dence intervals.\n    It is worthwhile to restate the meanings of \u03b1 and \u03b3, since it is easy to\nconfuse these two con\ufb01dence coe\ufb03cients, but they need to be distinguished\nsince they have rather di\ufb00erent interpretations. VaR(\u03b1) is de\ufb01ned so that the\nprobability of a loss being greater than VaR(\u03b1) is \u03b1. On the other hand, \u03b3 is\nthe con\ufb01dence coe\ufb03cient for the con\ufb01dence interval for VaR(\u03b1) and ES(\u03b1). If\nmany con\ufb01dence intervals are constructed, then approximately \u03b3 of them do\nnot contain the true risk measure. Thus, \u03b1 is about the loss from the invest-\nment while \u03b3 is about the con\ufb01dence interval being correct. An alternative way\nto view the di\ufb00erence between \u03b1 and \u03b3 is that VaR(\u03b1) and ES(\u03b1) are measur-\ning risk due to uncertainty about future losses, assuming perfect knowledge\nof the loss distribution, while the con\ufb01dence intervals tell us the uncertainty\nof these risk measures due to imperfect knowledge of the loss distribution.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "we will allow for dependencies, for instance,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4. Bootstrap con\ufb01dence intervals for VaR and ES for a position\nin an S&P 500 index fund\n\n    In this example, we continue Examples ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "Bootstrap con\ufb01dence intervals for VaR and ES for a position",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 and 19.3 and \ufb01nd an approx-\nimate con\ufb01dence interval for VaR(\u03b1) and ES(\u03b1). We use \u03b1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "and 19.3 and \ufb01nd an approx-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 as before\nand \u03b3 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "as before",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. B = 5,000 resamples were taken.\n    The basic percentile con\ufb01dence intervals for VaR(0.05) were (297, 352)\nand (301, 346) using nonparametric and parametric estimators of VaR(0.05),\nrespectively. For ES(0.05), the corresponding basic percentile con\ufb01dence in-\ntervals were (487, 803) and (433, 605). We see that there is considerable un-\ncertainty in the risk measures, especially for ES(0.05) and especially using\nnonparametric estimation.\n    When the \ufb01rst edition was written, the bootstrap computation took ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "B = 5,000 resamples were taken.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "33.3\nminutes using an R program and a 2.13 GHz PentiumTM processor running\nunder WindowsTM . The computations took this long because the optimization\nstep to \ufb01nd the MLE for parametric estimation is moderately expensive in\ncomputational time, at least if it is repeated 5,000 times. However, the same\n\f            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "33.3",
      "section_title": "minutes using an R program and a 2.13 GHz PentiumTM processor running",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4 Estimating VaR and ES Using ARMA+GARCH Models                561\n\ncomputation took only ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "Estimating VaR and ES Using ARMA+GARCH Models                561",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.23 minutes on a 2.9 GHz MacBook Pro when the\nsecond edition was being written in 2014. Nonetheless, more computationally\nexpensive estimators could easily take one-half hour or more to bootstrap even\non a fast computer.\n    Waiting over a half an hour for the con\ufb01dence interval may not be an\nattractive proposition. However, a reasonable measure of precision can be ob-\ntained with far fewer bootstrap repetitions. One might use only 50 repetitions,\nwhich would take less than a minute. This is not enough resamples to use basic\npercentile bootstrap con\ufb01dence intervals, but instead one can use the normal\napproximation bootstrap con\ufb01dence interval, (6.4). As an example, the normal\napproximation interval for the nonparametric estimate of VaR(0.05) is (301,\n361) using only the \ufb01rst 50 bootstrap resamples. This interval gives the same\ngeneral impression of accuracy as the above basic percentile method interval,\n(297, 352), that uses all 5,000 resamples.\n    The normal approximation interval assumes that VaR(0.05) is approxi-\nmately normally distributed. This assumption is justi\ufb01ed by the central limit\ntheorem for sample quantiles (Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.23",
      "section_title": "minutes on a 2.9 GHz MacBook Pro when the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.3.1) and the fact that VaR(0.05) is a\nmultiple of a sample quantile. The normal approximation does not require\nthat the returns are normally distributed. In fact, we are modeling them as\nt-distributed when computing the parametric estimates.                       \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.3",
      "section_title": "1) and the fact that VaR(0.05) is a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4 Estimating VaR and ES Using ARMA+GARCH\nModels\nAs we have seen in Chaps. 12 and 14, daily equity returns typically have a\nsmall amount of autocorrelation and a greater amount of volatility clustering.\nWhen calculating risk measures, the autocorrelation can be ignored if it is\nsmall enough, but the volatility clustering is less ignorable. In this section, we\nuse ARMA+GARCH models so that VaR(\u03b1) and ES(\u03b1) can adjust to periods\nof high or low volatility.\n    Assume that we have n returns, R1 , . . . , Rn and we need to estimate VaR\nand ES for the next return Rn+1 . Let \u03bc   \u0002n+1|n and \u03c3  \u0002n+1|n be the estimated\nconditional mean and variance of tomorrow\u2019s return Rn+1 , conditional on the\ncurrent information set, which in this context is simply {R1 , . . . , Rn }. We\nwill also assume that Rn+1 has a conditional t-distribution with tail index \u03bd.\nAfter \ufb01tting an ARMA+GARCH model, we have estimates of \u03bd\u0002, \u03bc          \u0002n+1|n , and\n\u0002n+1|n . The estimated conditional scale parameter is\n\u03c3\n                                  \u0011\n                         \u0002n+1|n = (\u0002\n                         \u03bb            \u03bd \u2212 2)/\u0002 \u03bd \u03c3\u0002n+1|n .                 (19.11)\n                                                           \u0002 replaced by\n                                                     \u0002 and \u03bb\nVaR and ES are estimated as in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "Estimating VaR and ES Using ARMA+GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.2 but with \u03bc\n           \u0002\n\u0002n+1|n and \u03bbn+1|n .\n\u03bc\n\f562         19 Risk Management\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "2 but with \u03bc",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.5. VaR and ES for a position in an S&P 500 index fund using a\nGARCH(1,1) model\n\n   An AR(1)+GARCH(1,1) model was \ufb01t to the log returns on the S&P 500.\nThe AR(1) coe\ufb03cient was small and not signi\ufb01cantly di\ufb00erent from 0, so a\nGARCH(1,1) was used for estimation of VaR and ES. The GARCH(1,1) \ufb01t is\n36 library(rugarch)\n37 garch.t = ugarchspec(mean.model=list(armaOrder=c(0,0)),\n38                      variance.model=list(garchOrder=c(1,1)),\n39                      distribution.model=\"std\")\n40 sp.garch.t = ugarchfit(data=SPreturn, spec=garch.t)\n\n41 show(sp.garch.t)\n\n\n      Optimal Parameters\n      ------------------------------------\n              Estimate Std. Error    t value Pr(>|t|)\n      mu      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.5",
      "section_title": "VaR and ES for a position in an S&P 500 index fund using a",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000714    0.000264   2.70872 0.006754\n      omega   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000714",
      "section_title": "0.000264   2.70872 0.006754",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000003    0.000004   0.79083 0.429046\n      alpha1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000003",
      "section_title": "0.000004   0.79083 0.429046",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.032459     0.019439   1.66979 0.094961\n      beta1   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.032459",
      "section_title": "0.019439   1.66979 0.094961",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.939176    0.009296 101.02598 0.000000\n      shape   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.939176",
      "section_title": "0.009296 101.02598 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.417464    0.560553   7.88054 0.000000\n\n42   pred = ugarchforecast(sp.garch.t, data=SPreturn, n.ahead=1) ; pred\n\n             Series    Sigma\n      T+1 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.417464",
      "section_title": "0.560553   7.88054 0.000000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0007144 0.009478\n\n43 alpha = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0007144",
      "section_title": "0.009478",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n44 nu = as.numeric(coef(sp.garch.t)[5])\n45 q = qstd(alpha, mean=fitted(pred), sd=sigma(pred), nu=nu)\n\n46 VaR = -20000*q ; VaR\n\n\n\n      T+1              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "44 nu = as.numeric(coef(sp.garch.t)[5])",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "276.7298\n\n47 lambda = sigma(pred)/sqrt( (nu)/(nu-2) )\n48 qalpha = qt(alpha, df=nu)\n49 es1 = dt(qalpha, df=nu)/(alpha)\n\n50 es2 = (nu + qalpha^2) / (nu - 1)\n\n51 es3 = -mean + lambda*es1*es2\n\n52 ES_par = 20000*es3 ; ES_par\n\n\n      T+1              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "276.7298",
      "section_title": "47 lambda = sigma(pred)/sqrt( (nu)/(nu-2) )",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "413.6518\n\n   The conditional mean and standard deviation of the next return were es-\ntimated to be ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "413.6518",
      "section_title": "The conditional mean and standard deviation of the next return were es-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00071 and 0.00950. For the estimation of VaR and ES, the\nnext return was assumed to have a t-distribution with these values for the\nmean and standard deviation and tail index ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00071",
      "section_title": "and 0.00950. For the estimation of VaR and ES, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.417. The estimate of VaR was\n$",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.417",
      "section_title": "The estimate of VaR was",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "276.73 and the estimate of ES was $413.65. The VaR and ES estimates using\nthe GARCH model are considerably smaller than the parametric estimates\n\f                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "276.73",
      "section_title": "and the estimate of ES was $413.65. The VaR and ES estimates using",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.5 Estimating VaR and ES for a Portfolio of Assets      563\n\n                                                   marginal SD\n                                                   conditional SD\n\n\n\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.5",
      "section_title": "Estimating VaR and ES for a Portfolio of Assets      563",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04\n                                              *    next day's conditional SD\n\n\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "*    next day's conditional SD",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n        \u03c3^t\n              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "\u03c3^t",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.02\n              0.01\n\n\n\n\n                       1988          1989            1990            1991\n                                                                          *\n                                            year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.02",
      "section_title": "0.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.3. Estimated conditional standard deviation of the daily S&P 500 index\nreturns based on a GARCH(1,1) model. The asterisk is a forecast of the next day\u2019s\nconditional standard deviation from the end of the return series, and the height of\nthe dashed horizontal line is an estimate of the marginal (unconditional) standard\ndeviation.\n\n\nin Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.3",
      "section_title": "Estimated conditional standard deviation of the daily S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 ($323.42 and $543.81), because the conditional standard de-\nviation used here (0.00950) is smaller than the marginal standard deviation\n(0.01248) used in Example 19.2; see Fig. 19.3, where the dashed horizontal\nline\u2019s height is the marginal standard deviation and the conditional standard\ndeviation of the next day\u2019s return is indicated by a large asterisk. The marginal\nstandard deviation is in\ufb02ated by periods of higher volatility such as in October\n1987 (near Black Monday) on the left-hand side of Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "($323.42 and $543.81), because the conditional standard de-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.3.                   \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.3",
      "section_title": "\u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.5 Estimating VaR and ES for a Portfolio\nof Assets\nWhen VaR is estimated for a portfolio of assets rather than a single asset,\nparametric estimation based on the assumption of multivariate normal or t-\ndistributed returns is very convenient, because the portfolio\u2019s return will have\na univariate normal or t-distributed return. The portfolio theory and factor\nmodels developed in Chaps. 16 and 18 can be used to estimate the mean and\nvariance of the portfolio\u2019s return.\n    Estimating VaR becomes complex when the portfolio contains stocks,\nbonds, options, foreign exchange positions, and other assets. However, when a\nportfolio contains only stocks, then VaR is relatively straightforward to esti-\nmate, and we will restrict attention to this case\u2014see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.5",
      "section_title": "Estimating VaR and ES for a Portfolio",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.10 for discussion\nof the literature covering more complex cases.\n\f564    19 Risk Management\n\n    With a portfolio of stocks, means, variances, and covariances of returns\ncould be estimated directly from a sample of returns as discussed in Chap. 16\nor using a factor model as discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.10",
      "section_title": "for discussion",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4.2. They can also be estimated\nusing the multivariate time series models discussed in Chaps. 13 and 14. Once\nthese estimates are available, they can be plugged into Eqs. (16.6) and (16.7)\nto obtain estimates of the expected value and variance of the return on the\nportfolio, which are denoted by \u03bc \u0002P and \u03c3\u0002P2 . Then, analogous to (19.10), VaR\ncan be estimated, assuming normally distributed returns on the portfolio (de-\nnoted with a subscript \u201cP\u201d), by\n                       norm\n                   VaRP                   \u03bcP + \u03a6\u22121 (\u03b1)\u0002\n                              (\u03b1) = \u2212S \u00d7 {\u0002           \u03c3P },              (19.12)\n\nwhere S is the initial value of the portfolio. Moreover, using (19.9), the esti-\nmated expected shortfall is\n                                  \u000e            \u0007            \b\u000f\n              \u001f\n                 norm                            \u03c6{\u03a6\u22121 (\u03b1)}\n              ESP (\u03b1) = S \u00d7 \u2212\u0002       \u03bcP + \u03c3\u0002P                  .        (19.13)\n                                                    \u03b1\n\n    If the stock returns have a joint t-distribution, then the returns on the\nportfolio have a univariate t-distribution with the same tail index, and VaR\nand ES for the portfolio can be calculated using formulas in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "2. They can also be estimated",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.2. If the\nreturns on the portfolio have a t-distribution with mean \u03bcP , scale parameter\n\u03bbP , and tail index \u03bd, then the estimated VaR is\n                       t\n                    VaRP (\u03b1) = \u2212S \u00d7 {\u0002            \u0002P },\n                                     \u03bcP + F\u03bd\u22121 (\u03b1)\u03bb                      (19.14)\n\nand the estimated expected shortfall is\n                \u0012                            6                  7 \u0014\n                                     \u22121                \u22121\n     t\n  \u001f P (\u03b1) = S \u00d7 \u2212\u0002       \u0002P   f \u0002\n                                \u03bd {F \u0002\n                                     \u03bd  (\u03b1)}   \u0002\n                                               \u03bd + {F  \u0002\n                                                       \u03bd  (\u03b1)}2\n  ES                \u03bcP + \u03bb                                          .    (19.15)\n                                     \u03b1              \u03bd\u0002 \u2212 1\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "2. If the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6. VaR and ES for portfolios of the three stocks in the CRSPday\ndata set\n\n    This example uses the data set CRSPday used earlier in Examples ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "VaR and ES for portfolios of the three stocks in the CRSPday",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.1\nand 7.4. There are four variables\u2014returns on GE, IBM, Mobil, and the CRSP\nindex and we found in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.1",
      "section_title": "and 7.4. There are four variables\u2014returns on GE, IBM, Mobil, and the CRSP",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4 that their returns can be modeled as hav-\ning a multivariate t-distribution with tail index ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.4",
      "section_title": "that their returns can be modeled as hav-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.94. In this example, we will\nonly consider the returns on the three stocks. The t-distribution parameters\nwere reestimated without the CRSP index and \u03bd\u0002 changed slightly to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.94",
      "section_title": "In this example, we will",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.81.\n    The estimated mean was\n                                                              T\n                    \u0002 = ( ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.81",
      "section_title": "The estimated mean was",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000858\n                    \u03bc                 0.000325   0.000616 )\n\nand the estimated covariance matrix was\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000858",
      "section_title": "\u03bc                 0.000325   0.000616 )",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6 Estimation of VaR Assuming Polynomial Tails       565\n                      \u239b                                 \u239e\n                       1.27e \u2212 04 5.04e \u2212 05 3.57e \u2212 05\n                 \u0002 = \u239d 5.04e \u2212 05 1.81e \u2212 04 2.40e \u2212 05 \u23a0 .\n                 \u03a3\n                       3.57e \u2212 05 2.40e \u2212 05 1.15e \u2212 04\n\nFor an equally weighted portfolio with w = ( 1/3         1/3   1/3 )T , the mean\nreturn for the portfolio is estimated to be\n\n                                   \u0002T w = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "Estimation of VaR Assuming Polynomial Tails       565",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0006\n                              \u0002P = \u03bc\n                              \u03bc\n\nand the standard deviation of the portfolio\u2019s return is estimated as\n                               \u0011\n                        \u0002P = wT \u03a3w\n                        \u03c3           \u0002 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0006",
      "section_title": "\u0002P = \u03bc",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00846.\n\nThe return on the portfolio has a t-distribution with this mean and standard\ndeviation, and the same tail index as the multivariate t-distribution of the\nthree stock returns. The scale parameter, using \u03bd\u0002 = 5.81, is\n                        \u0011\n                   \u0002P = (\u0002\n                   \u03bb        \u03bd \u2212 2)/\u0002\n                                   \u03bd \u00d7 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00846",
      "section_title": "The return on the portfolio has a t-distribution with this mean and standard",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00846 = 0.00685.\n\nTherefore,\n                t\n             VaR (0.05) = \u2212S \u00d7 {\u0002    \u0002P q\u00020.05,t (\u0002\n                                \u03bcP + \u03bb            \u03bd )} = S \u00d7 0.0128,\n                                          t\nso, for example, with S = $20,000, VaR (0.05) = $256.\n    The estimated ES using (19.8) and S = $20,000 is\n                  \u000e          \u0007                  !                     \"\b\u000f\n \u001f\n    t\n                          \u0002    f\u03bd\u0002{\u0002        \u03bd )} \u03bd\u0002 + {\u0002\n                                   q0.05,t (\u0002                   \u03bd )}2\n                                                       q0.05,t (\u0002\nES (0.05) = S \u00d7 \u2212\u0002   \u03bcP + \u03bbP                                              = $363.\n                                      \u03b1                \u03bd\u0002 \u2212 1\n\n                                                                                \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00846",
      "section_title": "= 0.00685.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6 Estimation of VaR Assuming Polynomial Tails\n\nThere is an interesting compromise between using a totally nonparametric es-\ntimator of VaR as in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "Estimation of VaR Assuming Polynomial Tails",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.1 and a parametric estimator as in Sect. 19.2.2.\nThe nonparametric estimator is feasible for large \u03b1, but not for small \u03b1. For\nexample, if the sample had 1,000 returns, then reasonably accurate estima-\ntion of the 0.05-quantile is feasible, but not estimation of the 0.0005-quantile.\nParametric estimation can estimate VaR for any value of \u03b1, but is sensitive to\nmisspeci\ufb01cation of the tail when \u03b1 is small. Therefore, a methodology interme-\ndiary between totally nonparametric and parametric estimation is attractive.\n    The approach used in this section assumes that the return density has a\npolynomial left tail, or equivalently that the loss density has a polynomial right\ntail. Under this assumption, it is possible to use a nonparametric estimate of\n\f566    19 Risk Management\n\nVaR(\u03b10 ) for a large value of \u03b10 to obtain estimates of VaR(\u03b11 ) for small\nvalues of \u03b11 . It is assumed here that VaR(\u03b11 ) and VaR(\u03b10 ) have the same\nhorizon T .\n   Because the return density is assumed to have a polynomial left tail, the\nreturn density f satis\ufb01es\n                        f (y) \u223c Ay \u2212(a+1) , as y \u2192 \u2212\u221e,                   (19.16)\nwhere A > 0 is a constant, a > 0 is the tail index, and \u201c\u223c\u201d means that the\nratio of the left-hand to right-hand sides converges to 1. Therefore,\n                             \u0013 y\n                                            A\n                P (R \u2264 y) \u223c      f (u) du = y \u2212a , as y \u2192 \u2212\u221e,         (19.17)\n                              \u2212\u221e            a\nand if y0 > 0 and y1 > 0, then\n                                            \u0007         \b\u2212a\n                           P (R < \u2212y0 )          y0\n                                        \u2248                   .            (19.18)\n                           P (R < \u2212y1 )          y1\n    Now suppose that y0 = VaR(\u03b11 ) and y1 = VaR(\u03b10 ), where 0 < \u03b11 < \u03b10\nand, for simplicity and without loss of generality, we use S = 1 in the following\ncalculation. Then (19.18) becomes\n                                                  \u0007          \b\u2212a\n                  \u03b11     P {R < \u2212VaR(\u03b11 )}          VaR(\u03b11 )\n                     =                         \u2248                          (19.19)\n                  \u03b10     P {R < \u2212VaR(\u03b10 )}          VaR(\u03b10 )\nor                                          \u0007 \b1/a\n                               VaR(\u03b11 )       \u03b10\n                                         \u2248           ,\n                               VaR(\u03b10 )       \u03b11\nso, now dropping the subscript \u201c1\u201d of \u03b11 and writing the approximate equality\nas exact, we have\n                                                & \u03b1 '1/a\n                                                   0\n                          VaR(\u03b1) = VaR(\u03b10 )              .                (19.20)\n                                                  \u03b1\nEquation (19.20) becomes an estimate of VaR(\u03b1) when VaR(\u03b10 ) is replaced\nby a nonparametric estimate and the tail index a is replaced by one of the\nestimates discussed soon in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "1 and a parametric estimator as in Sect. 19.2.2.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 is a constant, a > 0 is the tail index, and \u201c\u223c\u201d means that the\nratio of the left-hand to right-hand sides converges to 1. Therefore,\n                             \u0013 y\n                                            A\n                P (R \u2264 y) \u223c      f (u) du = y \u2212a , as y \u2192 \u2212\u221e,         (19.17)\n                              \u2212\u221e            a\nand if y0 > 0 and y1 > 0, then\n                                            \u0007         \b\u2212a\n                           P (R < \u2212y0 )          y0\n                                        \u2248                   .            (19.18)\n                           P (R < \u2212y1 )          y1\n    Now suppose that y0 = VaR(\u03b11 ) and y1 = VaR(\u03b10 ), where 0 < \u03b11 < \u03b10\nand, for simplicity and without loss of generality, we use S = 1 in the following\ncalculation. Then (19.18) becomes\n                                                  \u0007          \b\u2212a\n                  \u03b11     P {R < \u2212VaR(\u03b11 )}          VaR(\u03b11 )\n                     =                         \u2248                          (19.19)\n                  \u03b10     P {R < \u2212VaR(\u03b10 )}          VaR(\u03b10 )\nor                                          \u0007 \b1/a\n                               VaR(\u03b11 )       \u03b10\n                                         \u2248           ,\n                               VaR(\u03b10 )       \u03b11\nso, now dropping the subscript \u201c1\u201d of \u03b11 and writing the approximate equality\nas exact, we have\n                                                & \u03b1 '1/a\n                                                   0\n                          VaR(\u03b1) = VaR(\u03b10 )              .                (19.20)\n                                                  \u03b1\nEquation (19.20) becomes an estimate of VaR(\u03b1) when VaR(\u03b10 ) is replaced\nby a nonparametric estimate and the tail index a is replaced by one of the\nestimates discussed soon in Sect.",
        "start": 1144,
        "end": 2941
      }
    ]
  },
  {
    "content": "19.6.1. Notice another advantage of (19.20),\nthat it provides an estimate of VaR(\u03b1) not just for a single value of \u03b1 but for\nall values. This is useful if one wants to compute and compare VaR(\u03b1) for a\nvariety of values of \u03b1, as is illustrated in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "1. Notice another advantage of (19.20),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.7 ahead. The value of \u03b10\nmust be large enough that VaR(\u03b10 ) can be accurately estimated, but \u03b1 can\nbe any value less than \u03b10 .\n    A model combining parametric and nonparametric components is called\nsemiparametric, so estimator (19.20) is semiparametric because the tail index\nis speci\ufb01ed by a parameter, but otherwise the distribution is unspeci\ufb01ed.\n    To \ufb01nd a formula for ES, we will assume further that for some c < 0, the\nreturns density satis\ufb01es\n                          f (y) = A|y|\u2212(a+1) ,        y \u2264 c,             (19.21)\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.7",
      "section_title": "ahead. The value of \u03b10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6 Estimation of VaR Assuming Polynomial Tails           567\n\nso that we have equality in (19.16) for y \u2264 c. Then, for any d \u2264 c,\n                              \u0013 d\n                                                    A\n                 P (R \u2264 d) =       A|y|\u2212(a+1) dy = |d|\u2212a ,                    (19.22)\n                               \u2212\u221e                   a\nand the conditional density of R given that R \u2264 d is\n                                   Ay \u2212(a+1)\n                   f (y|R \u2264 d) =             = a|d|a |y|\u2212(a+1) .              (19.23)\n                                   P (R \u2264 d)\nIt follows from (19.23) that for a > 1,\n                 & \u0015         '          \u0013 d\n                                                             a\n               E |R| \u0015 R \u2264 d = a|d|a        |y|\u2212a dy =          |d|.          (19.24)\n                                           \u2212\u221e               a\u22121\n(For a \u2264 1, this expectation is +\u221e.) If we let d = \u2212VaR(\u03b1), then we see that\n                     a              1\n          ES(\u03b1) =       VaR(\u03b1) =         VaR(\u03b1), if a > 1.                    (19.25)\n                    a\u22121          1 \u2212 a\u22121\nFormula (19.25) enables one to estimate ES(\u03b1) using an estimate of VaR(\u03b1)\nand an estimate of a.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "Estimation of VaR Assuming Polynomial Tails           567",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1,\n                 & \u0015         '          \u0013 d\n                                                             a\n               E |R| \u0015 R \u2264 d = a|d|a        |y|\u2212a dy =          |d|.          (19.24)\n                                           \u2212\u221e               a\u22121\n(For a \u2264 1, this expectation is +\u221e.) If we let d = \u2212VaR(\u03b1), then we see that\n                     a              1\n          ES(\u03b1) =       VaR(\u03b1) =         VaR(\u03b1), if a > 1.                    (19.25)\n                    a\u22121          1 \u2212 a\u22121\nFormula (19.25) enables one to estimate ES(\u03b1) using an estimate of VaR(\u03b1)\nand an estimate of a.",
        "start": 624,
        "end": 1225
      }
    ]
  },
  {
    "content": "19.6.1 Estimating the Tail Index\nIn this section, we estimate the tail index assuming a polynomial left tail. Two\nestimators will be introduced, the regression estimator and the Hill estimator.\n\nRegression Estimator of the Tail Index\nIt follows from (19.17) that\n                      log{P (R \u2264 \u2212y)} = log(L) \u2212 a log(y),                    (19.26)\nwhere L = A/a.\n    If R(1) , . . . , R(n) are the order statistics of the returns, then the number\nof observed returns less than or equal to R(k) is k, so we estimate log{P (R \u2264\nR(k) )} to be log(k/n). Then, from (19.26), we have\n                        log(k/n) \u2248 log(L) \u2212 a log(\u2212R(k) )                     (19.27)\nor, rearranging (19.27),\n                  log(\u2212R(k) ) \u2248 (1/a) log(L) \u2212 (1/a) log(k/n).                (19.28)\n    The approximation (19.28) is expected to be accurate only if \u2212R(k) is\nlarge, which means k is small, perhaps only 5 %, 10 %, or 20 % of the sample\nsize n. If we plot the points {[log(k/n), log(\u2212R(k) )] : k = 1, . . . , m} for m equal\nto a small percentage of n, say 10 %, then we should see these points fall on\nroughly a straight line. Moreover, if we \ufb01t the straight-line model (19.28) to\nthese points by least squares, then the estimated slope, call it \u03b2\u00021 , estimates\n\u22121/a. Therefore, we will call \u22121/\u03b2\u00021 the regression estimator of the tail index.\n\f568     19 Risk Management\n\nHill Estimator\n\nThe Hill estimator of the left tail index a of the return density f uses all data\nless than a constant c, where c is su\ufb03ciently small such that\n\n                                  f (y) = A|y|\u2212(a+1)                          (19.29)\n\nis assumed to be true for y < c. The choice of c is crucial and will be discussed\nbelow. Let Y(1) , . . . , Y(n) be order statistics of the returns and n(c) be the\nnumber of Y1 less than or equal to c. By (19.23), the conditional density of Yi\ngiven that Yi \u2264 c is\n                                    a|c|a |y|\u2212(a+1) .                      (19.30)\nTherefore, the likelihood for Y(1) , . . . , Y(n(c)) is\n                       \u0007          \b\u0007              \b \u0007                \b\n                          a|c|a           a|c|a            a|c|a\n               L(a) =                                \u00b7\u00b7\u00b7               ,\n                         |Y1 |a+1       |Y2 |a+1         |Yn(c) |a+1\n\nand the log-likelihood is\n                      n(c)\n       log{L(a)} =           log(a) + a log(|c|) \u2212 (a + 1) log(|Y(i) |) .     (19.31)\n                      i=1\n\nDi\ufb00erentiating the right-hand side of (19.31) with respect to a and setting the\nderivative equal to 0 gives the equation\n                                        n(c)\n                               n(c)          -       .\n                                    =     log Y(i) /c .\n                                a     i=1\n\nTherefore, the MLE of a, which is called the Hill estimator, is\n\n                                               n(c)\n                             \u0002\n                             aHill (c) = \u0017n(c)    -       ..                  (19.32)\n                                          i=1 log Y(i) /c\n\nNote that Y(i) \u2264 c < 0, so that Y(i) /c is positive.\n    How should c be chosen? Usually c is equal to one of Y1 , . . . , Yn so that\nc = Y(n(c)) , and therefore choosing c means choosing n(c). The choice involves\na bias\u2013variance tradeo\ufb00. If n(c) is too large, then f (y) = A|y|\u2212(a+1) will not\nhold for all values of y \u2264 c, causing bias. If n(c) is too small, then there will\nbe too few Yi below c and \u0002    aHill (c) will be highly variable and unstable because\nit uses too few data. However, we can hope that there is a range of values of\nn(c) where \u0002  aHill (c) is reasonably constant because it is neither too biased nor\ntoo variable.\n    A Hill plot is a plot of \u0002  aHill (c) versus n(c) and is used to \ufb01nd this range of\nvalues of n(c). In a Hill plot, one looks for a range of n(c) where the estimator\nis nearly constant and then chooses n(c) in this range.\n\f                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "1 Estimating the Tail Index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6 Estimation of VaR Assuming Polynomial Tails                                   569\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "Estimation of VaR Assuming Polynomial Tails                                   569",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.7. Estimating the left tail index of the daily S&P 500 index\nreturns\n\n    This example uses the 1,000 daily S&P 500 index returns used in Ex-\namples ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.7",
      "section_title": "Estimating the left tail index of the daily S&P 500 index",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2 and 19.3. First, the regression estimator of the tail index was\ncalculated. The values {[log(k/n), log(\u2212R(k) )] : k = 1, . . . , m} were plotted\nfor m = 50, 100, 200, and 300 to \ufb01nd the largest value of m giving a roughly\nlinear plot, of which m = 100 was selected. The plotted points and the least-\nsquares lines can be seen in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "and 19.3. First, the regression estimator of the tail index was",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4. The slope of the line with m = 100\nwas \u22120.506, so a was estimated to be 1/",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "The slope of the line with m = 100",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.506 = 1.975.\n\n\n     a                                m=50                     b                              m=100\n\n\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.506",
      "section_title": "= 1.975.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                           slope = \u22120.555                                          slope = \u22120.506\n      log(\u2212R(k))\n\n\n\n\n                                                               log(\u2212R(k))\n                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "slope = \u22120.555                                          slope = \u22120.506",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n\n\n\n\n                                              a = 1.802                                               a = 1.975\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "a = 1.802                                               a = 1.975",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n                   \u22124.0\n\n\n\n\n                                                                            \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "\u22124.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5\n\n                          \u22127    \u22126        \u22125       \u22124     \u22123                       \u22127   \u22126    \u22125        \u22124    \u22123\n                                      log(k/n)                                                log(k/n)\n\n     c                               m=200                     d                              m=300\n                                               slope = \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.5",
      "section_title": "\u22127    \u22126        \u22125       \u22124     \u22123                       \u22127   \u22126    \u22125        \u22124    \u22123",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.54                                      slope = \u22120.645\n      log(\u2212R(k))\n\n\n\n\n                                                               log(\u2212R(k))\n\n\n\n\n                                                 a = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.54",
      "section_title": "slope = \u22120.645",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.853                                           a = 1.55\n                                                                            \u22123\n                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.853",
      "section_title": "a = 1.55",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                            \u22125\n                   \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "\u22125",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.0\n\n\n\n\n                          \u22127   \u22126    \u22125    \u22124      \u22123   \u22122                         \u22127   \u22126   \u22125    \u22124    \u22123   \u22122    \u22121\n                                      log(k/n)                                                log(k/n)\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.0",
      "section_title": "\u22127   \u22126    \u22125    \u22124      \u22123   \u22122                         \u22127   \u22126   \u22125    \u22124    \u22123   \u22122    \u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.4. Plots for estimating the left tail index of the S&P 500 returns by regres-\nsion. The \u201cslope\u201d is the least-squares slope estimate and \u201ca\u201d is \u22121/slope.\n\n\n    Suppose we have invested $20,000 in an S&P 500 index fund. We will use\n\u03b10 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.4",
      "section_title": "Plots for estimating the left tail index of the S&P 500 returns by regres-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. VaR(0.1, 24 h) is estimated to be \u2212$20,000 times the 0.1-quantile\n                                                              np\nof the 1,000 returns. The sample quantile is \u22120.0117, so VaR (0.1, 24 h) =\n$234. Using (19.20) and a = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "VaR(0.1, 24 h) is estimated to be \u2212$20,000 times the 0.1-quantile",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.975 (i.e., 1/a = 0.506), we have\n                                                               \u0007             \b",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.975",
      "section_title": "(i.e., 1/a = 0.506), we have",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.506\n                                                                   0.1\n                                               VaR(\u03b1) = 234                             .                           (19.33)\n                                                                    \u03b1\n\n   The black curve in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.506",
      "section_title": "0.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.5 is a plot of VaR(\u03b1) for 0.0025 \u2264 \u03b1 \u2264 0.25\nusing (19.33) and the regression estimator of a. The red curve is the same plot\nbut with the Hill estimator of a, which is 2.2\u2014see below. The blue curve is\nVaR(\u03b1) estimated assuming t-distributed returns as discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.5",
      "section_title": "is a plot of VaR(\u03b1) for 0.0025 \u2264 \u03b1 \u2264 0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.2.2,\nand the purple curve is estimated assuming normally distributed returns. The\n\f570     19 Risk Management\n\n                                                         polynomial tail: regression\n\n\n\n\n                  1500\n                                                         polynomial tail: Hill\n                                                         t\n                                                         normal\n                  1000\n         VaR(\u03b1)\n                  500\n                  0\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.2",
      "section_title": "2,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.002   0.005   0.010   0.020       0.050     0.100    0.200\n                                                  \u03b1\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.002",
      "section_title": "0.005   0.010   0.020       0.050     0.100    0.200",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.5. Estimation of VaR(\u03b1) using formula (19.33) and the regression estimator\nof the tail index (black line), using formula (19.33) and the Hill estimator of the tail\nindex (red line), assuming t-distributed returns (blue line), and assuming normally\ndistributed returns (purple line). Note the log-scale on the x-axis.\n\n\nreturn distribution has much heavier tails than a normal distribution, and\nthe latter curve is included only to show the e\ufb00ect of model misspeci\ufb01cation.\nThe parametric estimates based on the t-distribution are similar to the esti-\nmates assuming a polynomial tail except when \u03b1 is very small. The di\ufb00erence\nbetween the two estimates for small \u03b1 (\u03b1 < 0.01) is to be expected because\nthe polynomial tail with tail index ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.5",
      "section_title": "Estimation of VaR(\u03b1) using formula (19.33) and the regression estimator",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.975 or 2.2 is heavier than the tail of the\nt-distribution with \u03bd = a = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.975",
      "section_title": "or 2.2 is heavier than the tail of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.984. The estimate based on the t-distribution is\nsomewhat biased since it assumes a symmetric density and uses data in the\nright, as well as the left, tails to estimate the left tail; the problem with this\nis that the right tail is lighter than the left tail. If \u03b1 is in the range ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.984",
      "section_title": "The estimate based on the t-distribution is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01 to\n0.2, then VaR(\u03b1) is relatively insensitive to the choice of model, except for the\npoorly \ufb01tting normal model. This is a good reason for preferring \u03b1 \u2265 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01.\n    It follows from (19.25) using the regression estimate \u0002   a = 1.975 that\n\n                           \u001f       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "It follows from (19.25) using the regression estimate \u0002   a = 1.975 that",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.975\n                           ES(\u03b1) =       VaR(\u03b1) = 2.026 VaR(\u03b1).                         (19.34)\n                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.975",
      "section_title": "ES(\u03b1) =       VaR(\u03b1) = 2.026 VaR(\u03b1).                         (19.34)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.975\n    The Hill estimator of a was also implemented. Figure 19.6 contains Hill\nplots, that is, plots of the Hill estimate \u0002\n                                           aHill (c) versus n(c). In panel (a), n(c)\nranges from 25 to 250. There seems to be a region of stability when n(c) is\nbetween 25 and 120, which is shown in panel (b). In panel (b), we see a region\nof even greater stability when n(c) is between 60 and 100. Panel (c) zooms in\non this region. We see in panel (c) that the Hill estimator is close to ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.975",
      "section_title": "The Hill estimator of a was also implemented. Figure 19.6 contains Hill",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 when\nn(c) is between 60 and 100, and we will take ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.2",
      "section_title": "when",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 as the Hill estimate. Thus,\nthe Hill estimate is similar to the regression estimate (1.975) of the tail index.\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.2",
      "section_title": "as the Hill estimate. Thus,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.8 Choosing the Horizon and Con\ufb01dence Level                                       571\n\n     a                                       b                                       c\n\n\n\n\n                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.8",
      "section_title": "Choosing the Horizon and Con\ufb01dence Level                                       571",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4\n\n\n\n\n                                                                                                      2.4\n     Hill estimator\n\n\n\n\n                                             Hill estimator\n\n\n\n\n                                                                                     Hill estimator\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "2.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                              2.2\n\n\n\n\n                                                                                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2\n                      1.6\n\n\n\n\n                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.2",
      "section_title": "1.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                                                      2.0\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2\n\n\n\n\n                            50   150   250                          40    80   120                          60   80   100\n                                 nc                                      nc                                      nc\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "50   150   250                          40    80   120                          60   80   100",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.6. Estimation of the tail index by applying a Hill plot to the daily returns\non the S&P 500 index for 1,000 consecutive trading days ending on March 4, 2003.\n(a) Full range of nc . (b) Zoom in to nc between 25 and 120. (c) Zoom in further to\nnc between 60 and 100.\n\n\n    The advantage of the regression estimate is that one can use the linearity of\nthe plots of {[log(k/n), log(\u2212R(k) )] : k = 1, . . . , m} for di\ufb00erent m to guide the\nchoice of m, which is analogous to n(c). A linear plot indicates a polynomial\ntail. In contrast, the Hill plot checks for the stability of the estimator and\ndoes not give a direct assessment whether or not the tail is polynomial.           \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.6",
      "section_title": "Estimation of the tail index by applying a Hill plot to the daily returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.7 Pareto Distributions\n\nThe Pareto distribution with location parameter c > 0 and shape parameter\na > 0 has density\n                               \u000e a \u2212(a+1)\n                                ac y      ,    y > c,\n                  f (y|a, c) =                                     (19.35)\n                                0,             otherwise.\n\nThe expectation is ac/(a \u2212 1) if a > 1, and +\u221e otherwise. The Pareto distri-\nbution has a polynomial tail and, in fact, a polynomial tail is often called a\nPareto tail.\n    Equation (19.30) states that the loss, conditional on being above |c|, has a\nPareto distribution. A property of the Pareto distribution that was exploited\nbefore [see (19.23)] is that if Y has a Pareto distribution with parameters a\nand c and if d > c, then the conditional distribution of Y , given that Y > d,\nis Pareto with parameters a and d.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.7",
      "section_title": "Pareto Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and shape parameter\na > 0 has density\n                               \u000e a \u2212(a+1)\n                                ac y      ,    y > c,\n                  f (y|a, c) =                                     (19.35)\n                                0,             otherwise.",
        "start": 77,
        "end": 349
      },
      {
        "language": "r",
        "code": "1, and +\u221e otherwise. The Pareto distri-\nbution has a polynomial tail and, in fact, a polynomial tail is often called a\nPareto tail.\n    Equation (19.30) states that the loss, conditional on being above |c|, has a\nPareto distribution. A property of the Pareto distribution that was exploited\nbefore [see (19.23)] is that if Y has a Pareto distribution with parameters a\nand c and if d > c, then the conditional distribution of Y , given that Y > d,\nis Pareto with parameters a and d.",
        "start": 384,
        "end": 870
      }
    ]
  },
  {
    "content": "19.8 Choosing the Horizon and Con\ufb01dence Level\nThe choice of horizon and con\ufb01dence coe\ufb03cient are somewhat interdependent,\nand also depend on the eventual use of the VaR estimate. For shorter horizons\nsuch as one day, a large \u03b1 (small con\ufb01dence coe\ufb03cient = 1 \u2212 \u03b1) would result\nin frequent losses exceeding VaR. For example, \u03b1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.8",
      "section_title": "Choosing the Horizon and Con\ufb01dence Level",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 would result in a loss\n\f572    19 Risk Management\n\nexceeding VaR approximately once per month since there are slightly more\nthan 20 trading days in a month. Therefore, we might wish to use smaller\nvalues of \u03b1 with a shorter horizon.\n     One should be wary, however, of using extremely small values of \u03b1, such\nas, values less than ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "would result in a loss",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01. When \u03b1 is very small, then VaR and, especially, ES\nare impossible to estimate accurately and are very sensitive to assumptions\nabout the left tail of the return distribution. As we have seen, it is useful to\ncreate bootstrap con\ufb01dence intervals to indicate the amount of precision in\nthe VaR and ES estimates. It is also important to compare estimates based\non di\ufb00erent tail assumptions as in Fig. 19.5, for example, where the three\nestimates of VaR are increasingly dissimilar as \u03b1 decreases below ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "When \u03b1 is very small, then VaR and, especially, ES",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.01.\n     There is, of course, no need to restrict attention to only one horizon or\ncon\ufb01dence coe\ufb03cient. When VaR is estimated parametrically and i.i.d. nor-\nmally distributed returns are assumed, then it is easy to reestimate VaR with\ndi\ufb00erent horizons. Suppose that \u03bc \u00021day\n                                   P         \u0002P1day are the estimated mean and\n                                         and \u03c3\nstandard deviation of the return for one day. Assuming only that returns are\ni.i.d., the mean and standard deviation for M days are\n                               \u0002M\n                               \u03bc P\n                                   days\n                                           \u00021Pday\n                                        = M\u03bc                               (19.36)\nand                                        \u221a\n                              \u0002PM days =\n                              \u03c3              \u0002P1 day .\n                                            M\u03c3                             (19.37)\nTherefore, if one assumes further that the returns are normally distributed,\nthen the VaR for M days is\n                            (           \u221a                  )\n         VaRM P\n                days\n                     = \u2212S \u00d7 M \u03bc \u00021Pday + M \u03a6\u22121 (\u03b1)\u0002 \u03c3P1 day ,        (19.38)\n\nwhere S is the size of the initial investment. The power of Eq. (19.38) is,\nfor example, that it allows one to change from a daily to a weekly horizon\nwithout reestimating the mean and standard deviation with weekly instead\nof daily returns. Instead, one simply uses (19.38) with M = 5. The danger in\nusing (19.38) is that it assumes normally distributed returns and no autocor-\nrelation or GARCH e\ufb00ects (volatility clustering) of the daily returns. If there\nis positive autocorrelation, then (19.38) underestimates the M -day VaR. If\nthere are GARCH e\ufb00ects, then (19.38) gives VaR based on the marginal dis-\ntribution, but one should be using VaR based on the conditional distribution\ngiven the current information set.\n    If the returns are not normally distributed, then there is no simple analog\nto (19.38). For example, if the daily returns are i.i.d. but t-distributed then one\ncannot simply replace the normal quantile \u03a6\u22121 (\u03b1) in (19.38) by a t-quantile.\nThe problem is that the sum of i.i.d. t-distributed random variables is not\nitself t-distributed. Therefore, if the daily returns are t-distributed then the\nsum of M daily returns is not t-distributed. However, for large values of M\nand i.i.d. returns, the sum of M independent returns will be close to normally\ndistributed by the central limit theorem, so (19.38) could be used for large M\neven if the returns are not normally distributed.\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.01",
      "section_title": "There is, of course, no need to restrict attention to only one horizon or",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.9 VaR and Diversi\ufb01cation     573\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.9",
      "section_title": "VaR and Diversi\ufb01cation     573",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.9 VaR and Diversi\ufb01cation\nA serious problem with VaR is that it may discourage diversi\ufb01cation. This\nproblem was studied by Artzner, Delbaen, Eber, and Heath (1997, 1999),\nwho ask the question, what properties can reasonably be required of a risk\nmeasure? They list four properties that any risk measure should have, and\nthey call a risk measure coherent if it has all of them.\n    One property among the four that is very desirable is subadditivity. Let\nR(P ) be a risk measure of a portfolio P , for example, VaR or ES. Then R\nis said to be subadditive, if for any two portfolios P1 and P2 , R(P1 + P2 ) \u2264\nR(P1 ) + R(P2 ). Subadditivity says that the risk for the combination of two\nportfolios is at most the sum of their individual risks, which implies that\ndiversi\ufb01cation reduces risk or at least does not increase risk. For example, if\na bank has two traders, then the risk of them combined is less than or equal\nto the sum of their individual risks if a subadditive risk measure is used.\nSubadditivity extends to more than two portfolios, so if R is subadditive,\nthen for m portfolios, P1 , . . . , Pm ,\n                 R(P1 + \u00b7 \u00b7 \u00b7 + Pm ) \u2264 R(P1 ) + \u00b7 \u00b7 \u00b7 + R(Pm ).\nSuppose a \ufb01rm has 100 traders and monitors the risk of each trader\u2019s portfolio.\nIf the \ufb01rm uses a subadditive risk measure, then it can be sure that the total\nrisk of the 100 traders is at most the sum of the 100 individual risks. Whenever\nthis sum is acceptable, there is no need to compute the risk measure for the\nentire \ufb01rm. If the risk measure used by the \ufb01rm is not subadditive, then there\nis no such guarantee.\n    Unfortunately, as the following example shows, VaR is not subadditive\nand therefore is incoherent. ES is subadditive, which is a strong reason for\npreferring ES to VaR.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.9",
      "section_title": "VaR and Diversi\ufb01cation",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.8. An example where VaR is not subadditive\n\n    This simple example has been designed to illustrate that VaR is not sub-\nadditive and can discourage diversi\ufb01cation. A company is selling par $1,000\nbonds with a maturity of one year that pay a simple interest of 5 % so that\nthe bond pays $50 at the end of one year if the company does not default. If\nthe bank defaults, then the entire $1,000 is lost. The probability of no default\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.8",
      "section_title": "An example where VaR is not subadditive",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.96. To make the loss distribution continuous, we will assume that the loss\nis N (\u221250, 1) with probability ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.96",
      "section_title": "To make the loss distribution continuous, we will assume that the loss",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.96 and N (1000, 1) with probability 0.04. The\nmain purpose of making the loss distribution continuous is to simplify calcu-\nlations. However, the loss would be continuous, for example, if the portfolio\ncontained both the bond and some stocks. Suppose that there is a second\ncompany selling bonds with exactly the same loss distribution and that the\ntwo companies are independent.\n    Consider two portfolios. Portfolio 1 buys two bonds from the \ufb01rst company\nand portfolio 2 buys one bond from each of the two companies. Both portfolios\n\f574     19 Risk Management\n\nhave the same expected loss, but the second is more diversi\ufb01ed. Let \u03a6(x|\u03bc, \u03c3 2 )\nbe the normal CDF with mean \u03bc and variance \u03c3 2 . For portfolio 1, the loss\nCDF is\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.96",
      "section_title": "and N (1000, 1) with probability 0.04. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.04 \u03a6(x|2000, 4) + 0.96 \u03a6(x| \u2212100, 4),\nwhile for portfolio 2, by independence of the two companies, the loss distri-\nbution CDF is\n\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04",
      "section_title": "\u03a6(x|2000, 4) + 0.96 \u03a6(x| \u2212100, 4),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.042 \u03a6(x|2000, 2) + 2(0.96)(0.04) \u03a6(x|950, 2) + 0.962 \u03a6(x| \u2212100, 2).\n\n    We should expect the second portfolio to seem less risky, but VaR(0.05)\nindicates the opposite. Speci\ufb01cally, VaR(0.05) is \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.042",
      "section_title": "\u03a6(x|2000, 2) + 2(0.96)(0.04) \u03a6(x|950, 2) + 0.962 \u03a6(x| \u2212100, 2).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "95.38 and 949.53 for port-\nfolios 1 and 2, respectively. Notice that a negative VaR means a negative loss\n(positive revenue). Therefore, portfolio 1 is much less risky than portfolio 2,\nas measured by VaR(0.05). For each portfolio, VaR(0.05) is shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "95.38",
      "section_title": "and 949.53 for port-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.7\nas the loss at which the CDF crosses the horizontal dashed line at 0.95.\n\n                           Portfolio 1                           Portfolio 2\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.7",
      "section_title": "as the loss at which the CDF crosses the horizontal dashed line at 0.95.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98\n\n\n\n\n                                                      0.98\n         prob\n\n\n\n\n                                               prob\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98",
      "section_title": "0.98",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.94\n\n\n\n\n                                                      0.94\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.94",
      "section_title": "0.94",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.90\n\n\n\n\n                                                      0.90\n\n\n\n\n                       0   500          1500                 0   500          1500\n                                 loss                                  loss\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.90",
      "section_title": "0.90",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.7. Example where VaR discourages diversi\ufb01cation. Plots of the CDF of\nthe loss distribution. VaR(0.05) is the loss at which the CDF crosses the horizontal\ndashed line at ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.7",
      "section_title": "Example where VaR discourages diversi\ufb01cation. Plots of the CDF of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.95.\n\n\n   Notice as well that which portfolio has the highest value of VaR(\u03b1) depends\nheavily on the values of \u03b1. When \u03b1 is below the default probability, 0.04,\nportfolio 1 is more risky than portfolio 2.                                 \u0002\n\n    Although VaR is often considered the industry standard for risk manage-\nment, Artzner, Delbaen, Eber, and Heath (1997) make an interesting observa-\ntion. They note that when setting margin requirements, an exchange should\nuse a subadditive risk measure so that the aggregate risk due to all customers\nis guaranteed to be smaller than the sum of the individual risks. Apparently,\nno organized exchanges use quantiles of loss distributions to set margin re-\nquirements. Thus, exchanges may be aware of the shortcomings of VaR, and\nVaR is not the standard for measuring risk within exchanges.\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.95",
      "section_title": "Notice as well that which portfolio has the highest value of VaR(\u03b1) depends",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.11 R Lab     575\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.11",
      "section_title": "R Lab     575",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.10 Bibliographic Notes\nRisk management is an enormous subject and we have only touched upon\na few aspects, focusing on statistical methods for estimating risk. We have\nnot considered portfolios with bonds, foreign exchange positions, interest rate\nderivatives, or credit derivatives. We also have not considered risks other than\nmarket risk or how VaR and ES can be used for risk management. To cover\nrisk management thoroughly requires at least a book-length treatment of that\nsubject. Fortunately, excellent books exist, for example, Dowd (1998), Crouhy,\nGalai, and Mark (2001), Jorion (2001), and McNeil, Frey, and Embrechts\n(2005). The last has a strong emphasis on statistical techniques, and is rec-\nommended for further reading along the lines of this chapter. Generalized\nPareto distributions were not covered here but are discussed in McNeil, Frey,\nand Embrechts.\n    Alexander (2001), Hull (2003), and Gourieroux and Jasiak (2001) have\nchapters on VaR and risk management. The semiparametric method of es-\ntimation based on the assumption of a polynomial tail and Eq. (19.20) are\nfrom Gourieroux and Jasiak (2001). Drees, de Haan, and Resnick (2000) and\nResnick (2001) are good introductions to Hill plots.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.10",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.11 R Lab\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.11",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.11.1 Univariate VaR and ES\n\nIn this section we will compare VaR and ES parametric (unconditional) esti-\nmates with those from using ARMA+GARCH (conditional) models. Consider\nthe daily returns for Coca-Cola stock from January 2007 to November 2012.\n1 CokePepsi = read.table(\"CokePepsi.csv\", header=T)\n2 price = CokePepsi[,1]\n3 returns = diff(price)/lag(price)[-1]\n\n4 ts.plot(returns)\n\n\n    First, assume that the returns are iid and follow a t-distribution. Run the\nfollowing commands to get parameter estimates in R.\n5  S = 4000\n6  alpha = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.11",
      "section_title": "1 Univariate VaR and ES",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n 7 library(MASS)\n\n 8 res = fitdistr(returns,\u2019t\u2019)\n\n 9 mu = res$estimate[\u2019m\u2019]\n\n10 lambda = res$estimate[\u2019s\u2019]\n\n11 nu = res$estimate[\u2019df\u2019]\n\n12 qt(alpha, df=nu)\n\n13 dt(qt(alpha, df=nu), df=nu)\n\n\n\nProblem 1 What quantities are being computed in the last two lines above?\n\f576    19 Risk Management\n\nProblem 2 For an investment of $4,000, what are estimates of V aRt (0.05)\nand ES t (0.05)?\n\n\n   Now, \ufb01t a ARMA(0,0)+GARCH(1,1) model to the returns and calculate\none step forecasts.\n14 library(fGarch) # for qstd() function\n15 library(rugarch)\n16 garch.t = ugarchspec(mean.model=list(armaOrder=c(0,0)),\n\n17                      variance.model=list(garchOrder=c(1,1)),\n18                      distribution.model = \"std\")\n19 KO.garch.t = ugarchfit(data=returns, spec=garch.t)\n\n20 show(KO.garch.t)\n\n21 plot(KO.garch.t, which = 2)\n\n22 pred = ugarchforecast(KO.garch.t, data=returns, n.ahead=1) ; pred\n\n23 fitted(pred) ; sigma(pred)\n\n24 nu = as.numeric(coef(KO.garch.t)[5])\n\n25 q = qstd(alpha, mean = fitted(pred), sd = sigma(pred), nu = nu) ; q\n\n26 sigma(pred)/sqrt( (nu)/(nu-2) )\n\n27 qt(alpha, df=nu)\n\n28 dt(qt(alpha, df=nu), df=nu)\n\n\n\nProblem 3 Carefully express the \ufb01tted ARMA(0,0)+GARCH(1,1) model in\nmathematical notation.\n\n\nProblem 4 What are the one-step ahead predictions of the conditional mean\nand conditional standard deviation?\n\n\nProblem 5 Again, for an investment of $4,000, what are estimates of V aRt\n(0.05) and ES t (0.05) for the next day based on the \ufb01tted ARMA+GARCH\nmodel?\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "7 library(MASS)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.11.2 VaR Using a Multivariate-t Model\nRun the following code to create a data set of returns on two stocks, DATGEN\nand DEC.\n1 library(mnormt)\n2 berndtInvest = read.csv(\"berndtInvest.csv\")\n3 Berndt = berndtInvest[,5:6]\n\n4 names(Berndt)\n\n\n\nProblem 6 Fit a multivariate-t model to the returns in Berndt; see Sect.\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.11",
      "section_title": "2 VaR Using a Multivariate-t Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.13.3 for an example of \ufb01tting such a model. What are the estimates of the\nmean vector, tail index, and scale matrix? Include your R code and output with\nyour work.\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.13",
      "section_title": "3 for an example of \ufb01tting such a model. What are the estimates of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.12 Exercises    577\n\nProblem 7\n(a) What is the distribution of the return on a $100,000 portfolio that is 30 %\n    invested in DATGEN and 70 % invested in DEC? Include your R code and\n    output with your work.\n(b) Find V aRt (0.05) and ES t (0.05) for this portfolio.\n\n\nProblem 8 Use the model-free bootstrap to \ufb01nd a basic percentile bootstrap\ncon\ufb01dence interval for VaR(0.05) for the portfolio in Problem 7. Use a 90 %\ncon\ufb01dence coe\ufb03cient for the con\ufb01dence interval. Use 250 bootstrap resamples.\nThis amount of resampling is not enough for a highly accurate con\ufb01dence\ninterval, but will give a reasonably good indication of the uncertainty in the\nestimate of VaR(0.05), which is all that is really needed.\n    Also, plot kernel density estimates of the bootstrap distribution of the tail\nindex and VaRt (0.05). Do the densities appear Gaussian or skewed? Use a\nnormality test to check if they are Gaussian.\n\n\nProblem 9 This problem uses the variable DEC. Estimate the left tail index\nusing the Hill estimator. Use a Hill plot to select nc . What is your choice of\nnc ?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.12",
      "section_title": "Exercises    577",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19.12 Exercises\n 1. This exercise uses daily BMW returns in the bmwRet data set on the book\u2019s\n    website. For this exercise, assume that the returns are i.i.d., even though\n    there may be some autocorrelation and volatility clustering is likely. Sup-\n    pose a portfolio holds $1,000 in BMW stock (and nothing else).\n    (a) Compute nonparametric estimates of VaR(0.01, 24 h) and ES(0.01,\n        24 h).\n    (b) Compute parametric estimates of VaR(0.01, 24 h) and ES(0.01, 24 h)\n        assuming that the returns are normally distributed.\n    (c) Compute parametric estimates of VaR(0.01, 24 h) and ES(0.01, 24 h)\n        assuming that the returns are t-distributed.\n    (d) Compare the estimates in (a), (b), and (c). Which do you feel are most\n        realistic?\n 2. Assume that the loss distribution has a polynomial tail and an estimate\n    of a is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19.12",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.1. If VaR(0.05) = $252, what is VaR(0.005)?\n 3. Find a source of stock price data on the Internet and obtain daily prices\n    for a stock of your choice over the last 1,000 days.\n    (a) Assuming that the loss distribution is t, \ufb01nd the parametric estimate\n        of VaR(0.025, 24 h).\n    (b) Find the nonparametric estimate of VaR(0.025, 24 h).\n    (c) Use a t-plot to decide if the normality assumption is reasonable.\n\f578      19 Risk Management\n\n   (d) Estimate the tail index assuming a polynomial tail and then use the\n        estimate of VaR(0.025, 24 h) from part (a) to estimate VaR(0.0025,\n        24 h).\n4. This exercise uses daily Microsoft price data in the msft.dat data set\n   on the book\u2019s website. Use the closing prices to compute daily returns.\n   Assume that the returns are i.i.d., even though there may be some autocor-\n   relation and volatility clustering is likely. Suppose a portfolio holds $1,000\n   in Microsoft stock (and nothing else). Use the model-free bootstrap to \ufb01nd\n   95 % con\ufb01dence intervals for parametric estimates of VaR(0.005, 24 h) and\n   ES(0.005, 24 h) assuming that the returns are t-distributed.\n5. Suppose the risk measure R is VaR(\u03b1) for some \u03b1. Let P1 and P2 be two\n   portfolios whose returns have a joint normal distribution with means \u03bc1\n   and \u03bc2 , standard deviations \u03c31 and \u03c32 , and correlation \u03c1. Suppose the\n   initial investments are S1 and S2 . Show that R(P1 + P2 ) \u2264 R(P1 ) + R(P2 )\n   under joint normality.2\n6. This problem uses daily stock price data in the \ufb01le Stock_Bond.csv on the\n   book\u2019s website. In this exercise, use only the \ufb01rst 500 prices on each stock.\n   The following R code reads the data and extracts the \ufb01rst 500 prices for\n   \ufb01ve stocks. \u201cAC\u201d in the variables\u2019 names means \u201cadjusted closing\u201d price.\n         dat = read.csv(\"Stock_Bond.csv\", header = T)\n         prices = as.matrix(dat[1:500, c(3, 5, 7, 9, 11)])\n\n      (a) What are the sample mean vector and sample covariance matrix of\n          the 499 returns on these stocks?\n      (b) How many shares of each stock should one buy to invest $50 million in\n          an equally weighted portfolio? Use the prices at the end of the series,\n          e.g., prices[,500].\n      (c) What is the one-day VaR(0.1) for this equally weighted portfolio? Use\n          a parametric VaR assuming normality.\n      (d) What is the \ufb01ve-day Var(0.1) for this portfolio? Use a parametric\n          VaR assuming normality. You can assume that the daily returns are\n          uncorrelated.\n\n\nReferences\n\nAlexander, C. (2001) Market Models: A Guide to Financial Data Analysis,\n  Wiley, Chichester.\nArtzner, P., Delbaen, F., Eber, J.-M., and Heath, D. (1997) Thinking coher-\n  ently. RISK, 10, 68\u201371.\n\n2\n    This result shows that VaR is subadditive on a set of portfolios whose returns have\n    a joint normal distribution, as might be true for portfolios containing only stocks.\n    However, portfolios containing derivatives or bonds with nonzero probabilities of\n    default generally do not have normally distributed returns.\n\f                                                            References    579\n\nArtzner, P., Delbaen, F., Eber, J.-M., and Heath, D. (1999) Coherent measures\n  of risk. Mathematical Finance, 9, 203\u2013238.\nCrouhy, M., Galai, D., and Mark, R. (2001) Risk Management, McGraw-Hill,\n  New York.\nDrees, H., de Haan, L., and Resnick, S. (2000) How to make a Hill plot, Annals\n  of Statistics, 28, 254\u2013274.\nDowd, K. (1998) Beyond Value At Risk, Wiley, Chichester.\nGourieroux, C., and Jasiak, J. (2001) Financial Econometrics, Princeton Uni-\n  versity Press, Princeton, NJ.\nHull, J. C. (2003) Options, Futures, and Other Derivatives, 5th ed., Prentice-\n  Hall, Upper Saddle River, NJ.\nJorion, P. (2001) Value At Risk, McGraw-Hill, New York.\nMcNeil, A. J., Frey, R., and Embrechts, P. (2005) Quantitative Risk Manage-\n  ment, Princeton University Press, Princeton, NJ.\nResnick, S. I. (2001) Modeling Data Networks, School of Operations Research\n  and Industrial Engineering, Cornell University, Technical Report #1345.\n\f20\nBayesian Data Analysis and MCMC\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.1",
      "section_title": "If VaR(0.05) = $252, what is VaR(0.005)?",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1 Introduction\n\nBayesian statistics is based up a philosophy di\ufb00erent from that of other\nmethods of statistical inference. In Bayesian statistics all unknowns, and in\nparticular unknown parameters, are considered to be random variables and\ntheir probability distributions specify our beliefs about their likely values. Es-\ntimation, model selection, and uncertainty analysis are implemented by using\nBayes\u2019s theorem to update our beliefs as new data are observed.\n    Non-Bayesians distinguish between two types of unknowns, parameters\nand latent variables. To a non-Bayesian, parameters are \ufb01xed quantities with-\nout probability distributions while latent variables are random unknowns with\nprobability distributions. For example, to a non-Bayesian, the mean \u03bc, the\nmoving average coe\ufb03cients \u03b81 , . . . , \u03b8q , and the white noise variance \u03c3 2 of an\nMA(q) process are \ufb01xed parameters while the unobserved white noise process\nitself consists of latent variables. In contrast, to a Bayesian, the parameters\nand the white noise process are both unknown random quantities. Since this\nchapter takes a Bayesian perspective, there is no need to distinguish between\nthe parameters and latent variables, since they can now be treated in the same\nway. Instead, we will let \u03b8 denote the vector of all unknowns and call it the\n\u201cparameter vector.\u201d In the context of time series forecasting, for example, \u03b8\ncould include both the unobserved white noise and the future values of the\nseries being forecast.\n    A hallmark of Bayesian statistics is that one must start by specifying\nprior beliefs about the values of the parameters. Many statisticians have been\nreluctant to use Bayesian analysis since the need to start with prior beliefs\nseems too subjective. Consequently, there have been heated debates between\nBayesian and non-Bayesian statisticians over the philosophical basis of statis-\ntics. However, much of mainstream statistical thought now supports the more\npragmatic notion that we should use whatever works satisfactorily.\n\n\u00a9 Springer Science+Business Media New York 2015                               581\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 20\n\f582    20 Bayesian Data Analysis and MCMC\n\n    If one has little prior knowledge about a parameter, this lack of knowledge\ncan be accommodated by using a so-called noninformative prior that provides\nvery little information about the parameter relative to the information sup-\nplied by the data. In practice, Bayesian and non-Bayesian analyses of data\nusually arrive at similar conclusions when the Bayesian analysis uses only weak\nprior information so that knowledge of the parameters comes predominately\nfrom the data.\n    Moreover, in \ufb01nance and many other areas of application, analysts often\nhave substantial prior information and are willing to use it. In business and\n\ufb01nance, there is no imperative to strive for objectivity as there is in scienti\ufb01c\nstudy. The need to specify a prior can be viewed as a strength, not a weakness,\nof the Bayesian view of statistics, since it forces the analyst to think carefully\nabout how much and what kind of prior knowledge is available.\n    There has been a tremendous increase in the use of Bayesian statistics\nover the past few decades, because the Bayesian philosophy is becoming more\nwidely accepted and because Bayesian estimators have become much easier\nto compute. In fact, Bayesian techniques often are the most satisfactory way\nto compute estimates for complex models. We have heard one researcher say\n\u201cI am not a Bayesian but I use Bayesian methods\u201d and undoubtedly others\nwould agree.\n    For an overview of this chapter, assume we are interested in a parame-\nter vector \u03b8. A Bayesian analysis starts with a prior probability distribution\nfor \u03b8 that summarizes all prior knowledge about \u03b8; \u201cprior\u201d means before\nthe data are observed. The likelihood is de\ufb01ned in the same way in a non-\nBayesian analysis, but in Bayesian statistics the likelihood has a di\ufb00er-\nent interpretation\u2014the likelihood is the conditional distribution of the data\ngiven \u03b8. The key step in Bayesian inference is the use of Bayes\u2019s theorem to\ncombine the prior knowledge about \u03b8 with the information in the data. This\nis done by computing the conditional distribution of \u03b8 given the data. This\ndistribution is called the posterior distribution. In many, if not most, practical\nproblems, it is impossible to compute the posterior analytically and numerical\nmethods are used instead. A very successful class of numerical Bayesian meth-\nods is Markov chain Monte Carlo (MCMC), which simulates a Markov chain\nin such a way that the stationary distribution of the chain is the posterior\ndistribution of the parameters. The simulated data from the chain are used\nto compute Bayes estimates and perform uncertainty analysis.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2 Bayes\u2019s Theorem\nBayes\u2019s theorem applies to both discrete events and to continuously dis-\ntributed random variables. We will start with the case of discrete events.\nThe continuous case is covered in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "Bayes\u2019s Theorem",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3.\n\f                                                     20.2 Bayes\u2019s Theorem      583\n\n    Suppose that B1 , . . . , BK is a partition of the sample space S (the set of\nall possible outcomes). By \u201cpartition\u201d is meant that Bi \u2229 Bj = \u2205 if i = j and\nB1 \u222a B2 \u222a \u00b7 \u00b7 \u00b7 \u222a BK = S. For any set A, we have that\n\n                         A = (A \u2229 B1 ) \u222a \u00b7 \u00b7 \u00b7 \u222a (A \u2229 BK ),\n\nand therefore, since B1 , . . . , BK are disjoint,\n\n                     P (A) = P (A \u2229 B1 ) + \u00b7 \u00b7 \u00b7 + P (A \u2229 BK ).             (20.1)\n\nIt follows from (20.1) and the de\ufb01nition of conditional probability that\n\n                 P (A|Bj )P (Bj )                P (A|Bj )P (Bj )\n   P (Bj |A) =                    =                                             .\n                      P (A)         P (A|B1 )P (B1 ) + \u00b7 \u00b7 \u00b7 + P (A|BK )P (BK )\n                                                                            (20.2)\n\nEquation (20.2) is called Bayes\u2019s theorem, and is also known as Bayes\u2019s rule or\nBayes\u2019s law. Bayes\u2019s theorem is a simple, almost trivial, mathematical result,\nbut its implications are profound. The importance of Bayes\u2019s theorem comes\nfrom its use for updating probabilities. Here is an example, one that is far too\nsimple to be realistic but that illustrates how Bayes\u2019s theorem can be applied.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "20.2 Bayes\u2019s Theorem      583",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1. Bayes\u2019s theorem in a discrete case\n\n    Suppose that our prior knowledge about a stock indicates that the prob-\nability \u03b8 that the price will rise on any given day is either ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "Bayes\u2019s theorem in a discrete case",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 or 0.6. Based\nupon past data, say from similar stocks, we believe that \u03b8 is equally likely to\nbe ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "or 0.6. Based",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 or 0.6. Thus, we have the prior probabilities\n\n                    P (\u03b8 = 0.4) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "or 0.6. Thus, we have the prior probabilities",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 and P (\u03b8 = 0.6) = 0.5.\n\nWe observe the stock for \ufb01ve consecutive days and its price rises on all \ufb01ve\ndays. Assume that the price changes are independent across days, so that the\nprobability that the price rises on each of \ufb01ve consecutive days is \u03b85 . Given this\ninformation, we may suspect that \u03b8 is 0.6, not ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "and P (\u03b8 = 0.6) = 0.5.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4. Therefore, the probability\nthat \u03b8 is 0.6, given \ufb01ve consecutive price increases, should be greater than the\nprior probability of 0.5, but how much greater? As notation, let A be the event\nthat the prices rises on \ufb01ve consecutive days. Then, using Bayes\u2019s theorem,\nwe have\n                                       P (A|\u03b8 = 0.6)P (\u03b8 = 0.6)\n     P (\u03b8 = 0.6|A) =\n                       P (A|\u03b8 = 0.6)P (\u03b8 = 0.6) + P (A|\u03b8 = 0.4)P (\u03b8 = 0.4)\n                               (0.6)5 (0.5)\n                     =\n                       (0.6)5 (0.5) + (0.4)5 (0.5)\n                            (0.6)5               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "Therefore, the probability",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07776\n                     =      5          5\n                                         =                     = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07776",
      "section_title": "=      5          5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8836.\n                       (0.6) + (0.4)        0.07776 + 0.01024\n\f584     20 Bayesian Data Analysis and MCMC\n\nThus, our probability that \u03b8 is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8836",
      "section_title": "(0.6) + (0.4)        0.07776 + 0.01024",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 was 0.5 before we observed \ufb01ve consecutive\nprice increases but is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "was 0.5 before we observed \ufb01ve consecutive",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8836 after observing this event. Probabilities before\nobserving data are called the prior probabilities and the probabilities con-\nditional on observed data are called the posterior probabilities, so the prior\nprobability that \u03b8 equals ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8836",
      "section_title": "after observing this event. Probabilities before",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6 is 0.5 and the posterior probability is 0.8836. \u0002\n\n\n   Bayes\u2019s theorem is extremely important because it tells us exactly how to\nupdate our beliefs in light of new information. Revising beliefs after receiving\nadditional information is something that humans do poorly without the help\nof mathematics.1 There is a human tendency to put either too little or too\nmuch emphasis on new information, but this problem can be mitigated by\nusing Bayes\u2019s theorem for guidance.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "is 0.5 and the posterior probability is 0.8836. \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 Prior and Posterior Distributions\n\nWe now assume that \u03b8 is a continuously distributed parameter vector. The\nprior distribution with density \u03c0(\u03b8) expresses our beliefs about \u03b8 prior to\nobserving data. The likelihood function is interpreted as the conditional den-\nsity of the data Y given \u03b8 and written as f (y|\u03b8). Using Eq. (A.19), the joint\ndensity of \u03b8 and Y is the product of the prior and the likelihood; that is,\n\n                              f (y, \u03b8) = \u03c0(\u03b8)f (y|\u03b8).                    (20.3)\n\nThe marginal density of Y is found by integrating \u03b8 out of the joint density\nso that                         \u0013\n                         f (y) = \u03c0(\u03b8)f (y|\u03b8)d\u03b8,                       (20.4)\n\nand the conditional density of \u03b8 given Y is\n\n                              \u03c0(\u03b8)f (Y |\u03b8)    \u03c0(\u03b8)f (Y |\u03b8)\n                  \u03c0(\u03b8|Y ) =                =\u0016                .           (20.5)\n                                 f (Y )       \u03c0(\u03b8)f (Y |\u03b8)d\u03b8\n\nEquation (20.5) is another form of Bayes\u2019s theorem. The density on the left-\nhand side of (20.5) is called the posterior density and gives the probability\ndistribution of \u03b8 after observing the data Y .\n    Notice our use of \u03c0 to denote densities of \u03b8, so that \u03c0(\u03b8) is the prior\ndensity and \u03c0(\u03b8|Y ) is the posterior density. In contrast, f is used to denote\ndensities of the data, so that f (y) is the marginal density of the data and\nf (y|\u03b8) is the conditional density given \u03b8.\n    Bayesian estimation and uncertainty analysis are based upon the poste-\nrior. The most common Bayes estimators are the mode and the mean of the\n\n1\n    See Edwards (1982) .\n\f                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "Prior and Posterior Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 Prior and Posterior Distributions    585\n\nposterior density. The mode is called the maximum a posteriori estimator, or\nMAP estimator. The mean of the posterior is\n                         \u0013                 \u0016\n                                             \u03b8 \u03c0(\u03b8)f (Y |\u03b8)d\u03b8\n                E(\u03b8|Y ) = \u03b8 \u03c0(\u03b8|Y )d\u03b8 = \u0016                             (20.6)\n                                              \u03c0(\u03b8)f (Y |\u03b8)d\u03b8\n\nand is also called the posterior expectation.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "Prior and Posterior Distributions    585",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2. Updating the prior beliefs about the probability that a stock price\nwill increase\n\n   We continue Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "Updating the prior beliefs about the probability that a stock price",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1 but change the simple, but unrealistic, prior\nthat said that \u03b8 was either ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "but change the simple, but unrealistic, prior",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4 or 0.6 to a more plausible prior where \u03b8 could\nbe any value in the interval [0, 1], but with values near 1/2 more likely.\nSpeci\ufb01cally, we use a Beta(2,2) prior so that\n\n                        \u03c0(\u03b8) = 6\u03b8(1 \u2212 \u03b8),    0 < \u03b8 < 1.\n\nLet Y be the number of times the stock price increases on \ufb01ve consecutive\ndays. Then Y is Binomial(n, \u03b8) and the density of Y is\n                         \u0007 \b\n                          5\n               f (y|\u03b8) =      \u03b8y (1 \u2212 \u03b8)5\u2212y , y = 0, 1, . . . , 5.\n                          y\n\nSince we observed that Y = 5, f (Y |\u03b8) = f (5|\u03b8) = \u03b85 and the posterior\ndensity is\n                           6 \u03b8(1 \u2212 \u03b8)\u03b85\n                \u03c0(\u03b8|5) = \u0016                 = 56 \u03b86 (1 \u2212 \u03b8),\n                           6 \u03b8(1 \u2212 \u03b8)\u03b85 d\u03b8\nwhich is a Beta(7,2) density.\n    The prior and posterior densities are shown in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "or 0.6 to a more plausible prior where \u03b8 could",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1. The posterior\ndensity is shifted towards the right compared to the prior because \ufb01ve con-\nsecutive days saw increased prices. The ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "The posterior",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 lower and upper quantiles of the\nposterior distribution are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "lower and upper quantiles of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.529 and 0.953, respectively, and are shown on the\nplot. Thus, there is 90 % posterior probability that \u03b8 is between ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.529",
      "section_title": "and 0.953, respectively, and are shown on the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.529 and\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.529",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.953. For this reason, the interval [0.529, 0.953] is called a 90 % posterior\ninterval and provides us with the set of likely values of \u03b8. Posterior inter-\nvals are Bayesian analogs of con\ufb01dence intervals and are discussed further in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.953",
      "section_title": "For this reason, the interval [0.529, 0.953] is called a 90 % posterior",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6. Posterior intervals are also called credible intervals.\n    The posterior expectation is\n               \u0013 1               \u0013 1\n                                                       56\n                   \u03b8 \u03c0(\u03b8|5)d\u03b8 =      56 \u03b87 (1 \u2212 \u03b8)d\u03b8 =     = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "Posterior intervals are also called credible intervals.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.778.     (20.7)\n                0                 0                    72\n\nThe MAP estimate is 6/7 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.778",
      "section_title": "(20.7)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.857 and its location is shown by a dotted\nvertical line in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.857",
      "section_title": "and its location is shown by a dotted",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1.\n\f586      20 Bayesian Data Analysis and MCMC\n\n                                                                 prior\n\n\n\n\n                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "586      20 Bayesian Data Analysis and MCMC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.5 1.0 1.5 2.0 2.5 3.0\n                                                                 posterior\n\n\n\n\n                density(\u03b8)\n\n\n\n\n                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5 1.0 1.5 2.0 2.5 3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0        0.2    0.4       0.6     0.8   1.0\n                                                                                   \u03b8\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2    0.4       0.6     0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1. Prior and posterior densities in Example 20.2. The dashed vertical lines\nare at the lower and upper 0.05-quantiles of the posterior, so they mark o\ufb00 a 90 %\nequal-tailed posterior interval. The dotted vertical line shows the location of the pos-\nterior mode at \u03b8 = 6/7 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "Prior and posterior densities in Example 20.2. The dashed vertical lines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.857.\n\n\n      The posterior CDF is\n        \u0013 \u03b8            \u0013 \u03b8                    \u0007 7      \b\n                                               \u03b8    \u03b88\n            \u03c0(x|5)dx =     56x (1 \u2212 x)dx = 56\n                              6\n                                                  \u2212      ,                                           0 \u2264 \u03b8 \u2264 1.\n         0              0                       7   8\n                                                                                                                      \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.857",
      "section_title": "The posterior CDF is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 Conjugate Priors\nIn Example 20.2, the prior and the posterior were both beta distributions.\nThis is an example of a family of conjugate priors. A family of distributions\nis called a conjugate prior family for a statistical model (or, equivalently, for\nthe likelihood) if the posterior is in this family whenever the prior is in the\nfamily. Conjugate families are convenient because they make calculation of\nthe posterior straightforward. All one needs to do is to update the parameters\nin the prior. To see how this is done, we will generalize Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Conjugate Priors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2.\n\n\nExample 20.3. Computing the posterior density of the probability that a stock\nprice will increase\u2014general case of a conjugate prior\n\n      Suppose now that the prior for \u03b8 is Beta(\u03b1, \u03b2) so that the prior density is\n\n                                                                 \u03c0(\u03b8) = K1 \u03b8\u03b1\u22121 (1 \u2212 \u03b8)\u03b2\u22121 ,                      (20.8)\n\nwhere K1 is a constant. As we will see, knowing the exact value of K1 is not\nimportant, but from (A.14) we know that K1 = \u0393\u0393(\u03b1)\u0393\n                                                  (\u03b1+\u03b2)\n                                                     (\u03b2) . The parameters in\n\f                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "Example 20.3. Computing the posterior density of the probability that a stock",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 Conjugate Priors      587\n\na prior density must be known, so here \u03b1 and \u03b2 are chosen by the data analyst\nin accordance with the prior knowledge about the value of \u03b8. The choice of\nthese parameters will be discussed later.\n    Suppose that the stock price is observed on n days and increases on Y\ndays (and does not increase on n \u2212 Y days). Then the likelihood is\n\n                               f (Y |\u03b8) = K2 \u03b8Y (1 \u2212 \u03b8)n\u2212Y ,                        (20.9)\n              \u0007       \b\n                  n\nwhere K2 =                is another constant. The joint density of \u03b8 and Y is\n                  Y\n\n                      \u03c0(\u03b8)f (Y |\u03b8) = K3 \u03b8\u03b1+Y \u22121 (1 \u2212 \u03b8)\u03b2+n\u2212Y \u22121 ,                  (20.10)\n\nwhere K3 = K1 K2 . Then, the posterior density is\n\n                          \u03c0(\u03b8)f (Y |\u03b8)\n        \u03c0(\u03b8|Y ) = \u0016 1                      = K4 \u03b8\u03b1+Y \u22121 (1 \u2212 \u03b8)\u03b2+n\u2212Y \u22121 .          (20.11)\n                      0\n                        \u03c0(\u03b8)f (Y |\u03b8)d\u03b8\n\nwhere\n                                                  1\n                           K4 = \u0016 1                                 .              (20.12)\n                                   0\n                                       \u03b8\u03b1+Y \u22121 (1 \u2212 \u03b8)\u03b2+n\u2212Y \u22121 d\u03b8\nThe posterior distribution is Beta(\u03b1 + Y, \u03b2 + n \u2212 Y ).\n    We did not need to keep track of the values of K1 , . . . , K4 . Since (20.11)\nis proportional to a Beta(\u03b1 + Y, \u03b2 + n \u2212 Y ) density and since all densities\nintegrate to 1, we can deduce that the constant of proportionality is 1 and the\nposterior is Beta(\u03b1 + Y, \u03b2 + n \u2212 Y ). It follows from (A.14) that\n\n                                            \u0393 (\u03b1 + \u03b2 + n)\n                              K4 =                              .\n                                       \u0393 (\u03b1 + Y )\u0393 (\u03b2 + n \u2212 Y )\n\n    It is worth noticing how easily the posterior can be found. One simply\nupdates the prior parameters \u03b1 and \u03b2 to \u03b1 + Y and \u03b2 + n \u2212 Y , respectively.\n    Using the results in Appendix A.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Conjugate Priors      587",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.7 about the mean and variance of beta\ndistributions, the mean of the posterior is\n                                                 \u03b1+Y\n                                   E(\u03b8|Y ) =                                       (20.13)\n                                                \u03b1+\u03b2+n\nand the posterior variance is\n\n                                         (\u03b1 + Y )(\u03b2 + n \u2212 Y )\n                      var(\u03b8|Y ) =\n                                     (\u03b1 + \u03b2 + n)2 (\u03b1 + \u03b2 + n + 1)\n                                     E(\u03b8|Y ){1 \u2212 E(\u03b8|Y )}\n                                   =                      .                        (20.14)\n                                       (\u03b1 + \u03b2 + n + 1)\n\n   For values of \u03b1 and \u03b2 that are small relative to Y and n, E(\u03b8|Y ) is ap-\nproximately equal to the MLE, which is Y /n. If we had little prior knowledge\n\f588    20 Bayesian Data Analysis and MCMC\n\n                                                beta densities\n\n\n\n\n                       25\n                                  \u03b1 = \u03b2 = 500\n                                  \u03b1 = \u03b2 = 20\n                                  \u03b1=\u03b2=3\n\n\n\n\n                       20\n                       15\n             density\n                       10\n                       5\n                       0\n\n\n\n\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "about the mean and variance of beta",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0        0.2      0.4       0.6    0.8   1.0\n                                                      y\n\n          Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2      0.4       0.6    0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2. Examples of beta probability densities with \u03b1 = \u03b2.\n\n\nof \u03b8, we might take both \u03b1 and \u03b2 close to 0. However, since \u03b8 is the prob-\nability of a positive daily return on a stock, we might be reasonably certain\nthat \u03b8 is close to 1/2. In that case, choosing \u03b1 = \u03b2 and both fairly large (so\nthat the prior precision is large) makes sense. One could plot several beta\ndensities with \u03b1 = \u03b2 and decide which seem reasonable choices of the prior.\nFor example, Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "Examples of beta probability densities with \u03b1 = \u03b2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2 contains plots of beta densities with \u03b1 = \u03b2 = 3, 20,\nand 500. When 500 is the common value of \u03b1 and \u03b2, then the prior is quite\nconcentrated about 1/2. This prior could be used by someone who is rather\nsure that \u03b8 is close to 1/2. Someone with less certainty might instead prefer to\nuse \u03b1 = \u03b2 = 20, which has almost all of the prior probability between ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "contains plots of beta densities with \u03b1 = \u03b2 = 3, 20,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3 and\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6. The choice \u03b1 = \u03b2 = 3 leads to a very di\ufb00use prior and would be chosen\nif one had very little prior knowledge of \u03b8 and wanted to \u201clet the data speak\nfor themselves.\u201d\n    The posterior mean in (20.13) has an interesting interpretation. Suppose\nthat we had prior information from a previous sample of size \u03b1+\u03b2 and in that\nsample the stock price increased \u03b1 times. If we combined the two samples,\nthen the total sample size would be \u03b1 + \u03b2 + n, the number of days with a price\nincrease would be \u03b1 + Y , and the MLE of \u03b8 would be (\u03b1 + Y )/(\u03b1 + \u03b2 + n), the\nposterior mean given by (20.13). We can think of the prior as having as much\ninformation as would be given by a prior sample of size \u03b1 + \u03b2 and \u03b1/(\u03b1 + \u03b2)\ncan be interpreted as the MLE of \u03b8 from that sample. Therefore, the three\npriors in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "The choice \u03b1 = \u03b2 = 3 leads to a very di\ufb00use prior and would be chosen",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2 can be viewed as having as much information as samples\nof sizes 6, 40, and 1000. For a \ufb01xed value of E(\u03b8|Y ), we see from (20.14) that\nthe posterior variance of \u03b8 becomes smaller as \u03b1, \u03b2, or n increases; this makes\nsense since n is the sample size and \u03b1+\u03b2 quanti\ufb01es the amount of information\nin the prior.\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "can be viewed as having as much information as samples",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 Conjugate Priors      589\n\n   Since it is not necessary to keep track of constants, we could have omitted\nthem from the previous calculations and, for example, written (20.8) as\n\n                             \u03c0(\u03b8) \u221d \u03b8\u03b1\u22121 (1 \u2212 \u03b8)\u03b2\u22121 .                        (20.15)\n\nIn the following examples, we will omit constants in this manner.                 \u0002\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Conjugate Priors      589",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4. Posterior distribution when estimating the mean of a normal\npopulation with known variance\n\n    Suppose Y1 , . . . , Yn are i.i.d. N (\u03bc, \u03c3 2 ) and \u03c3 2 is known. The unrealistic\nassumption that \u03c3 2 is known is made so that we can start simple and will be\nremoved later.\n    The conjugate prior for \u03bc is the family of normal distributions. To show\nthis, assume that the prior on \u03bc is N (\u03bc0 , \u03c302 ) for known values of \u03bc0 and \u03c302 .\nWe learned in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Posterior distribution when estimating the mean of a normal",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 that it is not necessary to keep track of quantities\nthat do not depend on the unknown parameters (but could depend on the data\nor known parameters), so we will keep track only of terms that depend on \u03bc.\n    Simple algebra shows that the likelihood is\n                                     5n !            \u000e                    \u000f\"\n                                            1             1\n            f (Y1 , . . . , Yn |\u03bc) =      \u221a      exp \u2212 2 (Yi \u2212 \u03bc)       2\n\n                                     i=1\n                                           2\u03c0\u03c3           2\u03c3\n                                         \u000e                          \u000f\n                                              1 -                 .\n                                   \u221d exp \u2212 2 \u22122nY \u03bc + n\u03bc        2\n                                                                      .      (20.16)\n                                            2\u03c3\nThe prior density is\n                     \u000e               \u000f   \u000e                \u000f\n              1         1                   1\n    \u03c0(\u03bc) = \u221a      exp \u2212 2 (\u03bc \u2212 \u03bc0 ) \u221d exp \u2212 2 (\u22122\u03bc\u03bc0 + \u03bc ) .\n                                   2                    2\n             2\u03c0\u03c30      2\u03c30                 2\u03c30\n                                                          (20.17)\nA precision is the reciprocal of a variance, and we let \u03c4 = 1/\u03c3 2 denote the\npopulation precision. Multiplying (20.16) and (20.17), we can see that the\nposterior density is\n                                    \u000e\u0007          \b      \u0007             \b \u000f\n                                       nY   \u03bc0            n      1\n        \u03c0(\u03bc|Y1 , . . . , Yn ) \u221d exp       +        \u03bc \u2212        +         \u03bc2\n                                       \u03c32   \u03c302          2\u03c3 2   2\u03c302\n                                    \u000e                                  \u000f\n                                                       1\n                              = exp (\u03c4Y Y + \u03c40 \u03bc0 )\u03bc \u2212 (\u03c4Y + \u03c40 )\u03bc , 2\n                                                                           (20.18)\n                                                       2\n\nwhere \u03c4Y = n\u03c4 = n/\u03c3 2 and \u03c40 = 1/\u03c302 , so that \u03c4Y is the precision of Y and\n\u03c40 is the precision of the prior distribution.\n    One can see that log{\u03c0(\u03bc|Y1 , . . . , Yn )} is a quadratic function of \u03bc, so\n\u03c0(\u03bc|Y1 , . . . , Yn ) is a normal density. Therefore, to \ufb01nd the posterior distribu-\ntion we need only compute the posterior mean and variance. The posterior\nmean is the value of \u03bc that maximizes the posterior density, that is, the pos-\nterior mode, so to calculate the posterior mean, we solve\n\f590      20 Bayesian Data Analysis and MCMC\n\n                                   \u2202\n                            0=       log{\u03c0(\u03bc|Y1 , . . . , Yn )}           (20.19)\n                                  \u2202\u03bc\nand \ufb01nd that the mean is\n                                                             nY    \u03bc0\n                                             \u03c4Y Y + \u03c4 0 \u03bc 0   \u03c3 2 + \u03c302\n                   E(\u03bc|Y1 , . . . , Yn ) =                  = n     1 .   (20.20)\n                                               \u03c4Y + \u03c40        \u03c32 + \u03c32\n                                                                    0\n\n\n    We can see from (A.10) that the precision of a normal density f (y) is \u22122\ntimes the coe\ufb03cient of y 2 in log{f (y)}. Therefore, the posterior precision is\n\u22122 times the coe\ufb03cient of \u03bc2 in (20.18). Consequently, the posterior precision\nis \u03c4Y + \u03c40 = n/\u03c3 2 + 1/\u03c302 , and the posterior variance is\n\n                                                         1\n                            Var(\u03bc|Y1 . . . , Yn ) =   n      1 .          (20.21)\n                                                      \u03c3 2 + \u03c302\n\n      In summary, the posterior distribution is\n           \u239b                    \u239e\n             nY\n                 + \u03bc0                  \u0007                      \b\n                           1    \u23a0 = N \u03c4Y Y + \u03c40 \u03bc 0 ,    1\n               2    2\n          N\u239d n\n             \u03c3     \u03c30\n                   1  , n    1                                  .         (20.22)\n             \u03c32 + \u03c32    \u03c32 + \u03c32\n                                            \u03c4Y + \u03c40   \u03c4Y + \u03c40\n                     0            0\n\n\n\nWe can see that the posterior precision (\u03c4Y + \u03c40 ) is the sum of the precision of\nY and the precision of the prior; this makes sense since the posterior combines\nthe information in the data with the information in the prior.\n   Notice that as n \u2192 \u221e, the posterior precision \u03c4Y converges to \u221e and the\nposterior distribution is approximately\n\n                                      N (Y , \u03c3 2 /n).                     (20.23)\n\nWhat this result tells us is that as the amount of data increases, the e\ufb00ect of\nthe prior becomes negligible. The posterior density also converges to (20.23)\nas \u03c30 \u2192 \u221e with n \ufb01xed, that is, as the prior becomes negligible because the\nprior precision decreases to zero.\n    A common Bayes estimator is the posterior mean given by the right-hand\nside of (20.20). Many statisticians are neither committed Bayesians nor com-\nmitted non-Bayesians and like to look at estimators from both perspectives.\nA non-Bayesian would analyze the posterior mean by examining its bias, vari-\nance, and mean-squared error. We will see that, in general, the Bayes estima-\ntor is biased but is less variable than Y , and the tradeo\ufb00 between bias and\nvariance is controlled by the choice of the prior.\n                              \u0002 denote the posterior mean. Then\n    To simplify notation, let \u03bc\n\n                                 \u0002 = \u03b4Y + (1 \u2212 \u03b4)\u03bc0 ,\n                                 \u03bc                                        (20.24)\n\n                                \u03bc|\u03bc) = \u03b4\u03bc + (1 \u2212 \u03b4)\u03bc0 , so the bias of \u03bc\nwhere \u03b4 = \u03c4Y /(\u03c4Y + \u03c40 ), and E(\u0002                                      \u0002 is\n{E(\u0002\n   \u03bc|\u03bc) \u2212 \u03bc} = (\u03b4 \u2212 1)(\u03bc \u2212 \u03bc0 ) and \u03bc\u0002 is biased unless \u03b4 = 1 or \u03bc0 = \u03bc.\n\f                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "that it is not necessary to keep track of quantities",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 Conjugate Priors    591\n\nWe will have \u03b4 = 1 only in the limit as the prior precision \u03c40 converges to 0\nand \u03bc0 = \u03bc means that the prior mean is exactly equal to the true parameter,\nbut of course this bene\ufb01cial situation cannot be arranged since \u03bc is not known.\n   The variance of \u03bc \u0002 is\n                                           \u03b42 \u03c32\n                                    \u03bc|\u03bc) =\n                                Var(\u0002            ,\n                                             n\nwhich is less than Var(Y ) = \u03c3 2 /n, except in the extreme case where \u03b4 = 1.\nWe see that smaller values of \u03b4 lead to more bias but smaller variance. The\nbest bias\u2013variance tradeo\ufb00 minimizes the mean square error of \u03bc   \u0002, which is\n                                                                  \u03b42 \u03c32\n         \u03bc) = BIAS2 (\u0002\n     MSE(\u0002                    \u03bc) = (\u03b4 \u2212 1)2 (\u03bc \u2212 \u03bc0 )2 +\n                     \u03bc) + Var(\u0002                                         .    (20.25)\n                                                                    n\nIt is best, of course, to have \u03bc0 = \u03bc, but this is not possible since \u03bc is unknown.\nWhat is known is \u03b4 = \u03c4Y /(\u03c4Y +\u03c40 ) and \u03b4 can be controlled by the choice of \u03c40 .\n     Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Conjugate Priors    591",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 shows the MSE as a function of \u03b4 \u2208 (0, 1) for three values of\n\u03bc \u2212 \u03bc0 , which is called the \u201cprior bias\u201d since it is the di\ufb00erence between the\ntrue value of the parameter and the prior mean. In this \ufb01gure \u03c3 2 /n = 1/2.\nFor each of the two larger values of the prior bias, there is a range of values of\n\u03b4 where the Bayes estimator has a smaller MSE than Y , but if \u03b4 is below this\nrange, then the Bayes estimator has a larger MSE than Y and the range of\n\u201cgood\u201d \u03b4-values decreases as the prior bias increases. If the prior bias is large\nand \u03b4 is too small, then the MSE of the Bayes estimator can be quite large\nsince it converges to the squared prior bias as \u03b4 \u2192 0; see (20.25) or Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "shows the MSE as a function of \u03b4 \u2208 (0, 1) for three values of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3.\nThis result shows the need either to have a good prior guess of \u03bc or to keep the\nprior precision small so that \u03b4 is large. However, when \u03b4 is large, then the Bayes\nestimator cannot improve much over Y and, in fact, converges to Y as \u03b4 \u2192 1.\n     In summary, it can be challenging to choose a prior that o\ufb00ers a substan-\ntial improvement over Y . One way to do this is to combine several related\n\n\n                                                          prior bias = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "This result shows the need either to have a good prior guess of \u03bc or to keep the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n                                                          prior bias = 1\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "prior bias = 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                          prior bias = 1/2\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "prior bias = 1/2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n              MSE\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "MSE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                    0.5\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                          0.0   0.2    0.4       0.6      0.8          1.0\n                                             \u03b4\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0   0.2    0.4       0.6      0.8          1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3. MSE versus \u03b4 for three values of \u201cprior bias\u201d = \u03bc\u2212\u03bc0 when \u03c3 2 /n = 1/2.\nThe horizontal line represents the MSE of the maximum likelihood estimator ( Y ).\n\f592     20 Bayesian Data Analysis and MCMC\n\nestimation problems using a hierarchical prior; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "MSE versus \u03b4 for three values of \u201cprior bias\u201d = \u03bc\u2212\u03bc0 when \u03c3 2 /n = 1/2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8. When it is not\npossible to combine related problems and there is no other way to get in-\nformation about \u03bc, then the prudent data analyst will forgo the attempt to\nimprove upon the MLE and instead will choose a small value for the prior\nprecision \u03c40 .                                                              \u0002\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "When it is not",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.5. Posterior distribution when estimating a normal precision\n\n    Now suppose that Y1 , . . . , Yn are i.i.d. with a known mean \u03bc and an un-\nknown variance \u03c3 2 and precision \u03c4 = 1/\u03c3 2 . We will show that the conjugate\npriors for \u03c4 are the gamma distributions\n                                    \u0017n           and we will \ufb01nd the posterior distri-\nbution of \u03c4 . De\ufb01ne s2 = n\u22121 i=1 (Yi \u2212 \u03bc)2 , which is the MLE of \u03c3 2 .\n    Simple algebra shows that the likelihood is\n                                                 \u0007         \b\n                                                    1\n                    f (Y1 , . . . , Yn |\u03c4 ) \u221d exp \u2212 n\u03c4 s \u03c4 n/2 .\n                                                         2\n                                                                              (20.26)\n                                                    2\nLet the prior distribution be the gamma distribution with shape parameter \u03b1\nand scale parameter b which has density\n                            \u03c4 \u03b1\u22121\n                 \u03c0(\u03c4 ) =           exp(\u2212\u03c4 /b) \u221d \u03c4 \u03b1\u22121 exp(\u2212\u03c4 /b).             (20.27)\n                           \u0393 (\u03b1)b\u03b1\nMultiplying (20.26) and (20.27), we see that the posterior density for \u03c4 is\n\n          \u03c0(\u03c4 |Y1 , . . . , Yn ) \u221d \u03c4 n/2+\u03b1\u22121 exp{\u2212(ns2 /2 + b\u22121 )\u03c4 },         (20.28)\n\nwhich shows that the posterior distribution is gamma with shape parameter\n                                  -         .\u22121\nn/2 + \u03b1 and scale parameter ns2 /2 + b\u22121        ; that is,\n                                    (        -             .\u22121 )\n      \u03c0(\u03c4 |Y1 , . . . , Yn ) = Gamma n/2 + \u03b1, ns2 /2 + b\u22121       . (20.29)\n\nWe have shown that gamma distributions are conjugate for a normal precision\nparameter.\n   The expected value of a gamma distribution is the product of the shape\nand scale parameters, so the posterior mean of \u03c4 is\n                                                      n\n                                                           +\u03b1\n                           E(\u03c4 |Y1 , . . . , Yn ) = ns22           .\n                                                     2     + b\u22121\n\nNotice that E(\u03c4 |Y1 , . . . , Yn ) converges to s\u22122 as n \u2192 \u221e, which is not surpris-\ning since the MLE of \u03c3 2 is s2 , so that the MLE of \u03c4 is s\u22122 .                   \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.5",
      "section_title": "Posterior distribution when estimating a normal precision",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.5 Central Limit Theorem for the Posterior\nFor large sample sizes, the posterior distribution obeys a central limit theorem\nthat can be roughly stated as follows:\n\f                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.5",
      "section_title": "Central Limit Theorem for the Posterior",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6 Posterior Intervals    593\n\nResult ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "Posterior Intervals    593",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6. Under suitable assumptions and for large enough sample sizes,\nthe posterior distribution of \u03b8 is approximately normal with mean equal to the\ntrue value of \u03b8 and with variance equal to the inverse of the Fisher information\nmatrix.\n\nThis result is also known as the Bernstein\u2013von Mises Theorem. See Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "Under suitable assumptions and for large enough sample sizes,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14\nfor references to a precise statement of the theorem.\n    This theorem is an important result for several reasons. First, a comparison\nwith Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "for references to a precise statement of the theorem.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1 shows that the Bayes estimator and the MLE have the same\nlarge-sample distributions. In particular, we see that for large sample sizes, the\ne\ufb00ect of the prior becomes negligible, because the asymptotic distribution does\nnot depend on the prior. Moreover, the theorem shows a connection between\ncon\ufb01dence and posterior intervals that is discussed in the next section.\n    One of the assumptions of this theorem is that the prior remains \ufb01xed\nas the sample size increases, so that eventually nearly all of the information\ncomes from the data. The more informative the prior, the larger the sample\nsize needed for the posterior distribution to approach its asymptotic limit.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.1",
      "section_title": "shows that the Bayes estimator and the MLE have the same",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6 Posterior Intervals\nBayesian posterior intervals were mentioned in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "Posterior Intervals",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2 and will now be\ndiscussed in more depth.\n    Posterior intervals have a di\ufb00erent probabilistic interpretation than con\ufb01-\ndence intervals. The theory of con\ufb01dence intervals views the parameter as \ufb01xed\nand the interval as random because it is based on a random sample. Thus,\nwhen we say \u201cthe probability that the con\ufb01dence interval will include the\ntrue parameter is . . .,\u201d it is the probability distribution of the interval, not\nthe parameter, that is being considered. Moreover, the probability expresses\nthe likelihood before the data are collected about what will happen after the\ndata are collected. For example, if we use 95 % con\ufb01dence, then the probabil-\nity is ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "and will now be",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.95 that we will obtain a sample whose interval covers the parameter.\nAfter the data have been collected and the interval is known, a non-Bayesian\nwill say that either the interval covers the parameter or it does not, so the\nprobability that the interval covers the parameter is either 1 or 0, though, of\ncourse, we do not know which value is the actual probability.\n    In the Bayesian theory of posterior intervals, the opposite is true. The\nsample is considered \ufb01xed since we use posterior probabilities, that is, proba-\nbilities conditional on the data. Therefore, the posterior interval is considered\na \ufb01xed quantity. But in Bayesian statistics, parameters are treated as random.\nTherefore, when a Bayesian says \u201cthe probability that the posterior interval\nwill include the true parameter is . . .,\u201d the probability distribution being con-\nsidered is the posterior distribution of the parameter. The random quantity is\nthe parameter, the interval is \ufb01xed, and the probability is after the data have\nbeen collected.\n\f594    20 Bayesian Data Analysis and MCMC\n\n    Despite these substantial philosophical di\ufb00erences between con\ufb01dence and\nposterior intervals, in many examples where both a con\ufb01dence interval and\na posterior interval have been constructed, one \ufb01nds that they are nearly\nequal. This is especially common when the prior is relatively noninformative\ncompared to the data, for example, in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.95",
      "section_title": "that we will obtain a sample whose interval covers the parameter.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 if \u03b1 + \u03b2 is much smaller\nthan n.\n    There are solid theoretical reasons based on central limit theorems why\ncon\ufb01dence and posterior intervals are nearly equal for large sample sizes. By\nResult ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "if \u03b1 + \u03b2 is much smaller",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6 (the central limit theorem for the posterior), a large-sample pos-\nterior interval for the ith component of \u03b8 is\n                                           \u0011\n                           E(\u03b8i |Y ) \u00b1 z\u03b1/2 var(\u03b8i |Y ).               (20.30)\n\nBy Results ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "(the central limit theorem for the posterior), a large-sample pos-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1 and 7.6 (the univariate and multivariate central limit theorems\nfor the MLE), the large-sample con\ufb01dence interval (5.20) based on the MLE\nand the large-sample posterior interval (20.30) will approach each other as\nthe sample size increases. Therefore, practically-minded non-Bayesian data\nanalysts are often happy to use a posterior interval and interpret it as a large-\nsample approximation to a con\ufb01dence interval. Except in simple problems, all\ncon\ufb01dence intervals are based on large-sample approximations. This is true\nfor con\ufb01dence intervals that use pro\ufb01le likelihood, the central limit theorem\nfor the MLE and Fisher information, or the bootstrap, in other words, for all\nof the major methods for constructing con\ufb01dence intervals.\n    There are two major types of posterior intervals, highest probability and\nequal-tails. Let \u03c8 = \u03c8(\u03b8) be a scalar function of the parameter vector \u03b8 and\nlet \u03c0(\u03c8|Y ) be the posterior density of \u03c8. A highest-probability interval is of\nthe form {\u03c8 : \u03c0(\u03c8|Y ) > k} for some constant k. As k increases from 0 to\n\u221e, the posterior probability of this interval decreases from 1 to 0, and k is\nchosen so that the probability is 1 \u2212 \u03b1. If \u03c0(\u03c8|Y ) has multiple modes, then\nthe set {\u03c8 : \u03c0(\u03c8|Y ) > k} might not be an interval and in that case it should\nbe called a posterior set or posterior region rather than a posterior interval.\nIn any case, this region has the interpretation of being the smallest set with\n1\u2212 \u03b1 posterior probability. When the highest-posterior region is an interval, it\ncan be found by computing all intervals that range from the \u03b11 -lower quantile\nof \u03c0(\u03c8|Y ) to the \u03b12 -upper quantile of \u03c0(\u03c8|Y ), where \u03b11 + \u03b12 = \u03b1, and the\nusing the shortest of these intervals.\n    The equal-tails posterior interval has lower and upper limits equal to the\nlower and upper \u03b1/2-quantiles of \u03c0(\u03c8|Y ). The two types of intervals coincide\nwhen \u03c0(\u03c8|Y ) is symmetric and unimodal, which will be at least approximately\ntrue for large samples by the central limit theorem for the posterior.\n    Posterior intervals of either type are easy to compute when using Monte\nCarlo methods; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.1",
      "section_title": "and 7.6 (the univariate and multivariate central limit theorems",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "k} for some constant k. As k increases from 0 to\n\u221e, the posterior probability of this interval decreases from 1 to 0, and k is\nchosen so that the probability is 1 \u2212 \u03b1. If \u03c0(\u03c8|Y ) has multiple modes, then\nthe set {\u03c8 : \u03c0(\u03c8|Y ) > k} might not be an interval and in that case it should\nbe called a posterior set or posterior region rather than a posterior interval.\nIn any case, this region has the interpretation of being the smallest set with\n1\u2212 \u03b1 posterior probability. When the highest-posterior region is an interval, it\ncan be found by computing all intervals that range from the \u03b11 -lower quantile\nof \u03c0(\u03c8|Y ) to the \u03b12 -upper quantile of \u03c0(\u03c8|Y ), where \u03b11 + \u03b12 = \u03b1, and the\nusing the shortest of these intervals.\n    The equal-tails posterior interval has lower and upper limits equal to the\nlower and upper \u03b1/2-quantiles of \u03c0(\u03c8|Y ). The two types of intervals coincide\nwhen \u03c0(\u03c8|Y ) is symmetric and unimodal, which will be at least approximately\ntrue for large samples by the central limit theorem for the posterior.\n    Posterior intervals of either type are easy to compute when using Monte\nCarlo methods; see Sect.",
        "start": 999,
        "end": 2123
      }
    ]
  },
  {
    "content": "20.7.3.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "3.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7. Posterior interval for a normal mean when the variance is\nknown\n\f                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Posterior interval for a normal mean when the variance is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo       595\n\n   This example continues Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo       595",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4. By (20.20) and (20.21), a (1 \u2212 \u03b1)\n100 % posterior interval for \u03bc is\n                                           /\n                       \u03c4Y Y + \u03c40 \u03bc 0             1\n                                     \u00b1 z\u03b1/2 n      1 ,            (20.31)\n                          \u03c4Y + \u03c40            \u03c3 2 + \u03c32 0\n\n\nwhere z\u03b1/2 is the \u03b1/2-upper quantile of the standard normal distribution.\n   If either n \u2192 \u221e or \u03c30 \u2192 \u221e, then the information in the prior becomes\nnegligible relative to the information in the data because \u03c4Y /\u03c40 \u2192 \u221e, and\nthe posterior interval converges to\n                                          \u03c3\n                                 Y \u00b1 z\u03b1/2 \u221a ,\n                                           n\n\nwhich is the usual non-Bayesian con\ufb01dence interval.                            \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "By (20.20) and (20.21), a (1 \u2212 \u03b1)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo\nAlthough the Bayesian calculations in the simple examples of the last few sec-\ntions were straightforward, this is generally not true for problems of practical\ninterest. Frequently, the integral in the denominator of posterior density (20.5)\nis impossible to calculate analytically. The same is true of the integral in the\nnumerator of the posterior mean given by (20.6). Because of computational\ndi\ufb03culties, until approximately 1990 Bayesian data analysis was much less\nwidely used than now. Fortunately, Monte Carlo simulation methods for ap-\nproximating posterior densities and expectations have been developed. They\nhave been a tremendous advance and not only have they made Bayesian meth-\nods practical, but also they have led to the solution of applied problems that\nheretofore could not be tackled.\n     The most widely applicable Monte Carlo method for Bayesian analy-\nsis simulates a Markov chain whose stationary distribution is the posterior.\nThe sample from this chain is used for Bayesian inference. This technique is\ncalled Markov chain Monte Carlo, or MCMC. The BUGS language implements\nMCMC. There are three widely used versions of BUGS, OpenBUGS, WinBUGS,\nand JAGS. Most BUGS programs will run on all three versions, but there are\nexceptions. JAGS is in one way the most versatile of the three versions since\nit is the only one that will run under MacOS.2\n     This section is an introduction to MCMC and BUGS. First, we discuss Gibbs\nsampling, the simplest type of MCMC. Gibbs sampling works well when it\nis applicable, but it is applicable only to limited set of problems. Next, the\nMetropolis\u2013Hastings algorithm is discussed. Metropolis\u2013Hastings is applicable\nto nearly every type of Bayesian analysis. BUGS is a sophisticated program that\nis able to select an MCMC algorithm that is suitable for a particular model.\n2\n    OpenBUGS will run on a Mac under WINE.\n\f596     20 Bayesian Data Analysis and MCMC\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.1 Gibbs Sampling\nGibbs sampling is the simplest MCMC method. Suppose that the parameter\nvector \u03b8 can be partitioned into M subvectors so that\n                                    \u239b     \u239e\n                                      \u03b81\n                                    \u239c     \u239f\n                                \u03b8 = \u239d ... \u23a0 .\n                                            \u03b8M\nLet [\u03b8 j |Y , \u03b8 k , k = j] be the conditional distribution of \u03b8 j given the data Y\nand the values of the other subvectors; [\u03b8 j |Y , \u03b8 k , k = j] is called the full\nconditional distribution of \u03b8 j . Gibbs sampling is feasible if one can sample\nfrom each of the full conditionals.\n      Gibbs sampling creates a Markov chain that repeatedly samples the sub-\nvectors \u03b8 1 , . . . , \u03b8 M in the following manner. The chain starts with an arbi-\n                                                                                      (1)\ntrary starting value \u03b8 (0) for the parameter vector \u03b8. Then the subvector \u03b8 1\nis sampled from the full conditional [\u03b8 1 |Y , \u03b8 k , k = 1] with each of the remain-\n                                                                        (0)       (1)\ning subvectors \u03b8 k , k = 1, set at its current value which is \u03b8 k . Next \u03b8 2 is\nsampled from [\u03b8 2 |Y , \u03b8 k , k = 2] with \u03b8 k , k = 2, set at its current value, which\n       (1)                    (0)\nis \u03b8 k for k = 1 and \u03b8 k for k \u2265 2. One continues it this way until each of\n\u03b8 1 , . . . , \u03b8 M has been updated and one has \u03b8 (1) = (\u03b8 1 , . . . , \u03b8 M )T .\n                                                           (1)          (1)\n\n                 (2)                     (1)                            (1)\n      Then \u03b8 is found starting at \u03b8 in the same way that \u03b8 was obtained\nstarting at \u03b8 (0) . Continuing in this way, we obtain the sequence \u03b8 (1) , . . . , \u03b8 (N )\nthat is a Markov chain with the remarkable property that its stationary distri-\nbution is the posterior distribution of \u03b8. Moreover, regardless of the starting\nvalue \u03b8 (0) , the chain will converge to the stationary distribution. After conver-\ngence to the stationary distribution, the Markov chain samples the posterior\ndistribution and the MCMC sample is used to compute posterior expectations,\nquantiles, and other characteristics of the posterior distribution.\n      Since the Gibbs sample does not start in the stationary distribution, the\n\ufb01rst N0 iterations are discarded as a burn-in period for an appropriately cho-\nsen value of N0 . We will assume that this has been done and \u03b8 (1) , . . . , \u03b8 (N ) is\nthe sample from the chain after the burn-in period. In Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "1 Gibbs Sampling",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5, methods\nfor choosing N0 are discussed.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5, methods",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8. Gibbs sampling for a normal mean and precision\n    In Example 20.7, we found the posterior for a normal mean when the pre-\ncision is known, and in Example 20.5, we found the posterior for a normal\nprecision when the mean is known. These two results specify the two full con-\nditionals and allow one to apply Gibbs sampling to the problem of estimating\na normal mean and precision when both are unknown. The idea is simple. A\nstarting value \u03c4 (0) for \u03c4 is selected. The starting value might be the MLE, for\nexample. However, there are advantages to using multiple chains with random\nstarting values that are overdispersed, meaning that their probability distri-\nbution is more scattered than that posterior distribution; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "Gibbs sampling for a normal mean and precision",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5.\n\f                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo        597\n\nThen, treating \u03c4 as known and equal to \u03c4 (0) , \u03bc(1) is drawn randomly from\nits Gaussian full conditional posterior distribution given in (20.22). Note: the\nstarting value \u03c4 (0) for the population precision \u03c4 should not be confused with\nthe precision \u03c40 in the prior for \u03bc; \u03c4 (0) is used only once, to start the Gibbs\nsampling algorithm; after burn-in, the Gibbs sample will not depend on the\nactual value of \u03c4 (0) . In contrast, \u03c40 is \ufb01xed and is part of the posterior so the\nGibbs sample should and will depend on \u03c40 .\n     After \u03bc(1) has been sampled, \u03bc is treated as known and equal to \u03bc(1) and\n  (1)\n\u03c4     is drawn from the full conditional (20.29). Gibbs sampling continues in\nthis way, alternatively between sampling \u03bc and \u03c4 from their full condition-\nals.                                                                             \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo        597",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.2 Other Markov Chain Monte Carlo Samplers\nIt is often di\ufb03cult or impossible to sample directly from the full condi-\ntionals of the posterior and then Gibbs sampling is infeasible. Fortunately,\nthere is a large variety of other sampling algorithms that can be used when\nGibbs sampling cannot be used. Programming Monte Carlo algorithms \u201cfrom\nscratch\u201d is beyond the scope of this book but is explained in the references in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "2 Other Markov Chain Monte Carlo Samplers",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14. The BUGS language discussed in Sect. 20.7.4 allows analysts to use\nMCMC without the time-consuming and error-prone process of programming\nthe details.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "The BUGS language discussed in Sect. 20.7.4 allows analysts to use",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.3 Analysis of MCMC Output\nThe analysis of MCMC output typically examines scalar-valued functions of\nthe parameter vector \u03b8. The analysis should be performed on each scalar\nquantity of interest. Let \u03c8 = \u03c8(\u03b8) be one such function. Suppose \u03b8 1 , . . . , \u03b8 N\nis an MCMC sample from the posterior distribution of \u03b8, either from a single\nMarkov chain or from combining multiple chains, and de\ufb01ne \u03c8i = \u03c8(\u03b8 i ). We\nwill assume that the burn-in period and the chain lengths are su\ufb03cient so\nthat \u03c81 , . . . , \u03c8N is a representative sample from the posterior distribution of\n\u03c8. Methods for diagnosing convergence and adequacy of the Monte Carlo\nsample size are explained in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "3 Analysis of MCMC Output",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5.\u0017\n    The MCMC sample mean \u03c8 = N \u22121 i=1 \u03c8i estimates the posterior ex-\n                                                N\n\npectation E(\u03c8|Y ), which is the most common Bayes estimator. The MCMC\n                                        (           \u0017N              )1/2\nsample standard deviation s\u03c8 = (N \u2212 1)\u22121 i=1 (\u03c8i \u2212 \u03c8)2                   estimates\nthe posterior standard deviation of \u03c8 and will be called the Bayesian stan-\ndard error . If the sample size of the data is su\ufb03ciently large, then the posterior\ndistribution will be approximately normal by Result ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5.\u0017",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6 and an approximate\n(1 \u2212 \u03b1) posterior interval for \u03c8 is\n                                  \u03c8 \u00b1 z\u03b1/2 s\u03c8 .                            (20.32)\nInterval (20.32) is an MCMC approximation to (20.30).\n\f598     20 Bayesian Data Analysis and MCMC\n\n     However, one need not use this normal approximation to \ufb01nd posterior\nintervals. If L(\u03b11 ) is the \u03b11 -lower sample quantile and U (\u03b12 ) is the \u03b12 -upper\nsample quantile of \u03c81 , . . . , \u03c8N , then [L(\u03b11 ), U (\u03b12 )] is a 1 \u2212 (\u03b11 + \u03b12 ) posterior\ninterval. For an equal-tailed posterior interval, one uses \u03b11 = \u03b12 = \u03b1/2. For a\nhighest-posterior density interval, one chooses \u03b11 and \u03b12 on a \ufb01ne grid such\nthat \u03b11 + \u03b12 = \u03b1 and U (\u03b12 ) \u2212 L(\u03b11 ) is minimized. One should check that the\nposterior density of \u03c8 is unimodal using a kernel density estimate. If there\nare several modes and su\ufb03ciently deep troughs between them, then highest-\nposterior density posterior region could be a union of intervals, not a single\ninterval. However, even in this somewhat unusual case, [L(\u03b11 ), U (\u03b12 )] might\nstill be used as the shortest 1 \u2212 \u03b1 posterior interval.\n     Kernel density estimates can be used to visualize the shapes of the poste-\nrior densities. As an example, see Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "and an approximate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 discussed in Example 20.9 ahead.\nMost automatic bandwidth selectors for kernel density estimation are based\non the assumption of an independent sample. When applied to MCMC out-\nput, they might undersmooth. If the density() function in R is used, one\nmight correct this undersmoothing by using a value of the adjust parameter\ngreater than the default value of 1. However, Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "discussed in Example 20.9 ahead.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 uses the default value\nand the amount of smoothing seems adequate; this could be due to the large\nMonte Carlo sample size, N = 10,000.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "uses the default value",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.4 JAGS\n\nJAGS is a implementation of the BUGS (Bayesian analysis Using Gibbs Sam-\npling) program that can be run from Windows, Mac OS, or Linux. JAGS can\nbe used as a standalone program or it can be called from within R using the\nrjags package.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "4 JAGS",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9. Using JAGS to \ufb01t a t-distribution to returns\n\n    In this example, a t-distribution will be \ufb01t to S&P 500 returns using JAGS\ncalled from R. The BUGS program below is in the \ufb01le univt.bug. The program\nwill run under any of OpenBUGS, WinBUGS, or JAGS. In this example, JAGS will\nbe used.\n1  model{\n2  for(i in 1:N)\n 3 {\n\n 4    r[i] ~ dt(mu, tau, k)\n 5 }\n\n 6 mu ~ dnorm(0.0, 1.0E-6)\n\n 7 tau ~ dgamma(0.1, 0.01)\n\n 8 k ~ dunif(2, 50)\n\n 9 sigma2 <- (k / (k - 2)) / tau\n\n10 sigma <- sqrt(sigma2)\n\n11 }\n\f                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "Using JAGS to \ufb01t a t-distribution to returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo      599\n\n    In BUGS, dnorm(mu,tau) is the normal distribution with mean equal to mu\nand precision equal to tau. Also, dt(mu, tau, k) is the t-distribution with\nmean equal to mu, degrees of freedom equal to k, and inverse scale parameter\nequal to the square root of tau (so tau is proportional to,\n                                                          \u0011 rather than equal\nto, the precision and the constant of proportionality is (k \u2212 2)/k). In the\nBUGS program, the \u201cfor loop\u201d (lines 3\u20135) speci\ufb01es the likelihood and lines 6\u20138\nspecify the priors for mu, tau, and k. Line 9 computes the variance from tau\nand line 10 computes the standard deviation.\n    The R program is:\n1  library(rjags)\n2  library(\"Ecdat\")\n 3 data(SP500)\n\n 4 r = SP500$r500\n\n 5 N = length(r)\n\n 6 data = list(r = r, N = N)\n\n 7 inits = function(){list(mu = rnorm(1, mean = mean(r),\n\n 8    sd = 2 * sd(r)), tau = runif(1, 0.2/var(r), 2/var(r)),\n 9    k = runif(1, 2.5, 10))}\n10 t1 = proc.time()\n\n11 univt.mcmc <- jags.model(\"univt.bug\", data = data, inits = inits,\n\n12    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n13 nthin = 20\n\n14 univt.coda = coda.samples(univt.mcmc, c(\"mu\", \"k\", \"sigma\"),\n\n15    100*nthin, thin = nthin)\n16 summary(univt.coda, digits = 2)\n\n17 t2 = proc.time()\n\n18 (t2 - t1) / 60\n\n19 pdf(\"basic_plot.pdf\", width = 4, height = 7)  ## Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo      599",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4\n20 par(mfrow = c(4, 2))\n\n21 plot(univt.coda, auto.layout = F)   ## Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "20 par(mfrow = c(4, 2))",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4\n22 graphics.off()\n\n23 gelman.diag(univt.coda)\n\n24 effectiveSize(univt.coda)\n\n25 pdf(\"gelman_plot.pdf\", width = 6, height = 6)   ## Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "22 graphics.off()",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6\n26 gelman.plot(univt.coda)\n\n27 graphics.off()\n\n28 dic.samples(univt.mcmc, 100*nthin, thin = nthin, type = \"pD:)\n\n\n\nLine 6 creates a data list that is given to JAGS and lines 7\u20139 creates a\nfunction inits() that generates starting values for each chain. The func-\ntion jags.model() at lines 11\u201312 creates an object univt.mcmc of class\njags containing a graphical model description of the model speci\ufb01ed in the\n\ufb01le univt.bug. This object is one of the arguments of coda.samples() at\nlines 14\u201315. The function coda.samples() produces an object univt.coda\nof class mcmc.list containing MCMC output. Objects of this class can be\nused as input to functions in the coda package such as gelman.diag(),\neffectiveSize(), gelman.plot(), and summary(). Line 18 prints the\n\f600      20 Bayesian Data Analysis and MCMC\n\ncomputation time in minutes. The computation time for this example was\nabout 6 minutes, but this number is, of course, hardware dependent.\n   The output from line 16 is:\n\n      > summary(univt.coda, digits = 2)\n\n      Iterations = 3020:5000\n      Thinning interval = 20\n      Number of chains = 3\n      Sample size per chain = 100\n\n      1. Empirical mean and standard deviation for each variable,\n         plus standard error of the mean:\n\n                 Mean        SD Naive SE Time-series SE\n      k     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "26 gelman.plot(univt.coda)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "summary(univt.coda, digits = 2)",
        "start": 987,
        "end": 1022
      }
    ]
  },
  {
    "content": "6.0451630 0.5443919 3.143e-02     3.137e-02\n      mu    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.0451630",
      "section_title": "0.5443919 3.143e-02     3.137e-02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0005129 0.0001850 1.068e-05     1.071e-05\n      sigma ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0005129",
      "section_title": "0.0001850 1.068e-05     1.071e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0103017 0.0002078 1.200e-05     1.146e-05\n\n      2. Quantiles for each variable:\n\n                 2.5%       25%       50%       75%     97.5%\n      k     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0103017",
      "section_title": "0.0002078 1.200e-05     1.146e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.1407126 5.6780998 6.0380664 6.4039149 7.1849194\n      mu    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.1407126",
      "section_title": "5.6780998 6.0380664 6.4039149 7.1849194",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0001289 0.0003821 0.0005046 0.0006191 0.0008763\n      sigma ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0001289",
      "section_title": "0.0003821 0.0005046 0.0006191 0.0008763",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0099304 0.0101560 0.0102910 0.0104427 0.0107435\n\n\nFigure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0099304",
      "section_title": "0.0101560 0.0102910 0.0104427 0.0107435",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 produced at line 21 contains trace plots and kernel density esti-\nmates for mu, k, and sigma arranged alphabetically. The trace plots are simply\ntime series plots of the three chains. The interpretation of trace plots is dis-\ncussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "produced at line 21 contains trace plots and kernel density esti-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5. The Gelman plot produced by line 26 is shown later as\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5. The Gelman plot produced by line 26 is shown later as",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6.\nThe diagnostics from lines 23\u201328 will discuss brie\ufb02y here and described in\nmore detail later. The Gelman diagnostics produced by line 23 are:\n      > gelman.diag(univt.coda)\n      Potential scale reduction factors:\n\n              Point est. Upper C.I.\n      k             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "The diagnostics from lines 23\u201328 will discuss brie\ufb02y here and described in",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "gelman.diag(univt.coda)\n      Potential scale reduction factors:",
        "start": 154,
        "end": 222
      }
    ]
  },
  {
    "content": "1.00       1.01\n      mu            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.00",
      "section_title": "1.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.00       1.01\n      sigma         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.00",
      "section_title": "1.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02       1.06\n\n      Multivariate psrf\n\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "1.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02\n\nThe e\ufb00ective sample sizes calculated at line 24 are:\n\f                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "The e\ufb00ective sample sizes calculated at line 24 are:",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo                  601\n\n                        Trace of k                                       Density of k\n     9\n\n\n\n\n                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo                  601",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n     8\n     7\n\n\n\n\n                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3\n     6\n\n\n\n\n                                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3",
      "section_title": "6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n     5\n\n\n\n\n              5000   5500     6000       6500   7000                5       6      7     8     9\n                            Iterations                          N = 100 Bandwidth = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1878\n\n\n                       Trace of mu                                      Density of mu\n\n\n\n\n                                                        2000\n     6e\u221204\n\n\n\n\n                                                        1000\n     \u22122e\u221204\n\n\n\n\n                                                        0\n\n              5000   5500     6000       6500   7000                0e+00        5e\u221204     1e\u221203\n                            Iterations                         N = 100 Bandwidth = 6.169e\u221205\n\n\n                     Trace of sigma                                 Density of sigma\n     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1878",
      "section_title": "Trace of mu                                      Density of mu",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0106\n\n\n\n\n                                                        1000\n     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0106",
      "section_title": "1000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0098\n\n\n\n\n                                                        0\n\n\n\n\n              5000   5500     6000       6500   7000       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0098",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0095       0.0100    0.0105     0.0110\n                            Iterations                         N = 100 Bandwidth = 6.662e\u221205\n\n      Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0095",
      "section_title": "0.0100    0.0105     0.0110",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4. Trace plots and kernel density estimates in Example 20.9.\n\n\n   > effectiveSize(univt.coda)\n          k       mu    sigma\n   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "Trace plots and kernel density estimates in Example 20.9.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "effectiveSize(univt.coda)\n          k       mu    sigma",
        "start": 69,
        "end": 130
      }
    ]
  },
  {
    "content": "366.8874 300.0000 300.0000\n\nGelman diagnostics and e\ufb00ective sample sizes are discussed soon in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "366.8874",
      "section_title": "300.0000 300.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5.\nDIC and pD , which are discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.6 and produced at line 28, are\nDIC = \u221218, 062 and pD = 2.664:\n\n   > dic.samples(univt.mcmc, 100*nthin, thin = nthin, type = \"pD\")\n     |**************************************************| 100%\n   Mean deviance: -18065\n   penalty ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "6 and produced at line 28, are",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "dic.samples(univt.mcmc, 100*nthin, thin = nthin, type = \"pD\")\n     |**************************************************| 100%\n   Mean deviance: -18065\n   penalty",
        "start": 71,
        "end": 234
      }
    ]
  },
  {
    "content": "2.664\n   Penalized deviance: -18062\n\n                                                                                                       \u0002\n\f602    20 Bayesian Data Analysis and MCMC\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.664",
      "section_title": "Penalized deviance: -18062",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5 Monitoring MCMC Convergence and Mixing\n\nThe length N0 of the burn-in period must be su\ufb03ciently large that the Markov\nchain has converged to the stationary distribution by the end of burn-in. The\nlength N of the chain after burn-in must be large enough that moments,\nquantiles, and other quantities computed from the MCMC sample are accurate\nestimates of the corresponding characteristics of posterior. Markov chains are\ndependent sequences and the chains used in MCMC typically have positive\nautocorrelation. Because of the autocorrelation, to achieve accurate estimates\nMarkov chain samples must be larger, often far larger, than would be necessary\nwith independent sampling. A chain that moves about the posterior slowly is\nsaid to mix poorly. The slower the mixing of the chain, the larger the sample\nsize needed for accurate estimation.\n    In principle, one long Markov chain is all that is needed to sample the\nposterior. However, if several chains are generated, then one can compare them\nto decide if the burn-in period N0 and chain length N are su\ufb03ciently large. If\nthe amount of between-chain variation in the chain means is large relative to\nthe within-chain variation, then the chains are mixing poorly. Consequently,\ndiagnostics for convergence and mixing can be based on between- and within-\nchain variation.\n    Between-chain variability will be arti\ufb01cially low if the chains have similar\nstarting values. For this reason, it is recommended that the starting values\nbe randomly sampled from a distribution with greater dispersion than the\nposterior. For example, one might use a Gaussian or t-distribution with mean\nequal to the MLE and covariance matrix equal to k times the inverse Fisher\ninformation for some k > 1, e.g., k = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5 Monitoring MCMC Convergence and Mixing",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1, e.g., k =",
        "start": 1736,
        "end": 1751
      }
    ]
  },
  {
    "content": "1.5 or 2.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "or 2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.10. Good mixing and poor mixing\n    Excellent and poor mixing are contrasted in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.10",
      "section_title": "Good mixing and poor mixing",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.5. The model is linear\nregression with two predictor variables and i.i.d. Gaussian noise. There are two\nsimulated data sets. In the \ufb01rst data set the predictors are highly correlated\n(sample correlation = 0.996). Trace plots for this data set are in the top row.\nIn the second data set the predictors are independent and the trace plots are\nin the middle row. Except for this di\ufb00erence in the amount of collinearity,\nthe two data sets have the same distributions. In both of these cases, there\nare three chains and for each chain there is a burn-in period of N0 = 100\niterations and then 1000 iterations that are retained. In each row, trace plots,\nare shown for the three regression coe\ufb03cients (intercept and two slopes) and\nfor the residual precision (inverse variance).\n    In each case the three chains were started at randomly chosen initial values.\nThe probability distribution was centered at the least-squares and \u201coverdis-\npersed\u201d relative to the posterior distribution. Speci\ufb01cally, the regression coe\ufb03-\ncients have a Gaussian starting value distribution centered at the least-squares\nestimate and with covariance matrix ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.5",
      "section_title": "The model is linear",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 times the covariance matrix of the\nleast-squares estimator. The noise variance had a starting distribution that\n\f                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "times the covariance matrix of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo                                 603\n\nwas uniformly distributed between ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo                                 603",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 and 4 times the least-squares estimate\n       \u00022 in (9.16)] of the noise variance. By using overdispersed starting values,\n[e.g., \u03c3\none can discover how quickly the chains move from their starting values to the\nstationary distribution. The chains for the regression coe\ufb03cients move very\nquickly in the middle row but slowly in the top row. The residual precision is\nuna\ufb00ected by collinearity and moves quickly even in the high collinearity case.\n\n       Trace of beta[1]                     Trace of beta[2]             Trace of beta[3]                      Trace of tau\n\n\n\n\n                                                                                             40\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "and 4 times the least-squares estimate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1\n1.5\n\n\n\n\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.1",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2\n\n\n\n\n                                                                                             25\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.9\n                           1.0\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.9",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                                             10\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.7\n                           0.8\n\n\n\n\n       0       400   800                    0       400   800            0       400   800                 0       400   800\n           Iterations                           Iterations                   Iterations                          Iterations\n\n\n       Trace of beta[1]                     Trace of beta[2]             Trace of beta[3]                      Trace of tau\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.7",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.98 1.02 1.06\n\n\n\n\n                                                                                             40\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.98",
      "section_title": "1.02 1.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.04\n0.8\n\n\n\n\n                                                                                             20\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.04",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.00\n0.5\n\n\n\n\n       0       400   800                    0       400   800            0       400   800                 0       400   800\n           Iterations                           Iterations                   Iterations                          Iterations\n\n\n       Trace of beta[1]                     Trace of beta[2]             Trace of beta[3]                      Trace of tau\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.00",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                                                             10 20 30 40\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "10 20 30 40",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.4\n                           1.2\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "1.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                  2.0\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\u22121.0\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "\u22121.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.6\n                           0.4\n\n\n\n\n       40000     70000                      40000     70000              40000     70000                   40000      70000\n           Iterations                           Iterations                   Iterations                          Iterations\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.6",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.5. MCMC analysis of a linear regression model with two predictor variables.\nSimulated data. Trace plots of the regression coe\ufb03cients and residual precision for\nthree chains. The burn-in period was 100 and the chain lengths are 1000. The trace\nplots contain the MCMC output after the burn-in period. The intercept is beta[1] and\nthe slopes are beta[2] and beta[3]. Top row: The two predictors are highly correlated\nand the strong collinearity is causing poor mixing of the regression coe\ufb03cients. Notice\nthat the chains have not converged to the stationary distribution by the start of\nthe sampling period and that the between-chain variation is large. Middle row:\nThe burn-in period was 100 and the chain lengths are 1000 as in the top row. The\ntwo predictors are independent and there is very good mixing because there is no\ncollinearity. Notice that the chains have converged to the stationary distribution by\nthe start of the sampling period and there is little between-chain variation. Bottom\nrow: Same data set as the top row but with a burn-in period of 5000 and chain\nlengths of 30,000. The chains have been thinned so that only every 10th iteration is\nretained.\n\f604      20 Bayesian Data Analysis and MCMC\n\n    One solution to poor mixing is to increase the burn-in period and the\nchain lengths. The bottom row uses the same data set as in the top row but\nwith a longer burn-in (5000 iterations) and longer chains (30,000 iterations).\nThe chains have been thinned so that only every 10th iteration is retained.\nThinning can speed the analysis of the MCMC output by reducing the Monte\nCarlo sample size and can improve the appearance of trace plots\u2014a trace plot\nof 3 chains of 30,000 iterations each would be almost completely \ufb01lled in. The\nchains appear to have converged to the stationary distribution by the end\nof the burn-in and to mix reasonably well over 30,000 iterations (3000 after\nthinning).\n    The BUGS code for this model is:\n1 model{\n2 for(i in 1:N){\n3 y[i] ~ dnorm(mu[i],tau)\n\n4 mu[i] <- x[i,1]*beta[1] + x[i,2]*beta[2] +       x[i,3]*beta[3]\n5 }\n\n6 for(i in 1:3)beta[i] ~ dnorm(0,.00001)\n\n7 tau ~ dgamma(0.01,0.01)\n\n8 }\n\n\n\n      The R code is:\n1  library(rjags)\n2  library(coda)\n 3 library(mvtnorm)\n\n 4 set.seed(90201)\n\n 5 N = 50\n\n 6 beta1 = 1\n\n 7 beta2 = 2\n\n 8 alpha = 1\n\n 9 x1 = rnorm(N, mean = 3, sd = 2)\n\n10 x2 = x1 + rnorm(N, mean = 3, sd = 0.2)\n\n11 x = cbind(rep(1, N), x1, x2)\n\n12 y = alpha + beta1 * x1 + beta2 * x2 + rnorm(N, mean = 0, sd = 0.2)\n\n13 data = list(y = y, x = x, N = N)\n\n14 summ = summary(lm(y ~ x1 + x2))\n\n15 betahat = as.numeric(summ$coeff)[1:3]\n\n16 covbetahat = summ$sigma^2 * solve(t(x) %*% x)\n\n17 inits = function(){list(beta = as.numeric(rmvnorm(n = 1,\n\n18    mean = betahat, sigma=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.5",
      "section_title": "MCMC analysis of a linear regression model with two predictor variables.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 * covbetahat)),\n19    tau=runif(1, 1/(4 * summ$sigma^2), 4 / summ$sigma^2))}\n20 regr <- jags.model(\"lin_reg_vect.bug\", data = data, inits = inits,\n\n21    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n22 regr.coda = coda.samples(regr, c(\"beta\", \"tau\"), 1000, thin = 1)\n\n23 regr.coda.largeN = coda.samples(regr, c(\"beta\", \"tau\"),\n\n24    50000, thin = 100)\n25 #####   no collinearity #####\n26 set.seed(90201)\n\f                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "* covbetahat)),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo      605\n\n27 x1 = rnorm(N, mean = 3, sd = 2)\n28 x2 = rnorm(N,mean = 3, sd = 2) + rnorm(N, mean = 3, sd = 0.2)\n29 x = cbind(rep(1, N), x1, x2)\n\n30 y = alpha + beta1 * x1 + beta2 * x2 + rnorm(N, mean = 0, sd = 0.2)\n\n31 data = list(y = y, x = x, N = N)\n\n32 summ = summary(lm(y ~ x1 + x2))\n\n33 betahat = as.numeric( summ$coeff )[1:3]\n\n34 covbetahat = summ$sigma^2 * solve(t(x) %*% x)\n\n35 inits=function(){list(beta = as.numeric(rmvnorm(n = 1,\n\n36    mean = betahat, sigma = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo      605",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5 * covbetahat)) ,\n37    tau=runif(1, 1 / (4 * summ$sigma^2), 4 / summ$sigma^2))}\n38 regr.noco <- jags.model(\"lin_reg_vect.bug\", data = data,\n\n39    inits = inits, n.chains = 3, n.adapt = 1000, quiet = FALSE)\n40 regr.coda.noco = coda.samples(regr.noco, c(\"beta\", \"tau\"),\n\n41    1000, thin = 1)\n42 pdf(\"linRegMCMC.pdf\", width = 7, height = 6)\n\n43 par(mfrow=c(3,4))\n\n44 traceplot(regr.coda)\n\n45 traceplot(regr.coda.noco)\n\n46 traceplot(regr.coda.largeN)\n\n47 graphics.off()\n\n\n\n                                                                               \u0002\n\n   We now introduce two widely used diagnostics, Rhat and n.eff. Suppose\none samples M chains, each of length N after burn-in. Let \u03b8 i,j be the ith iter-\nate from the jth chain and let \u03c8i,j = \u03c8(\u03b8 i,j ) for some scalar-valued function\n\u03c8. For example, to extract the kth parameter, one would use \u03c8(x) = xk , or\n\u03c8 might compute the standard deviation or the variance from the precision.\nWe also use \u03c8 to denote the estimand \u03c8(\u03b8).\n   Let\n                                             N\n                              \u03c8 \u00b7,j = N \u22121         \u03c8i,j                   (20.33)\n                                             i=1\n\nbe the mean of the jth chain and let\n                                             M\n                             \u03c8 \u00b7,\u00b7 = M \u22121          \u03c8\u00b7,j .                 (20.34)\n                                             j=1\n\n\n\u03c8 \u00b7,\u00b7 is the average of the chain means and is the Monte Carlo approximation\nto E(\u03c8|Y ). Then de\ufb01ne\n\n                            N        -M\n                                                    .2\n                        B=             \u03c8 \u00b7,j \u2212 \u03c8 \u00b7,\u00b7 .                    (20.35)\n                           M \u2212 1 j=1\n\nB/N is the sample variance of the chain means. De\ufb01ne\n\f606    20 Bayesian Data Analysis and MCMC\n                                        N\n                                              -                  .2\n                      s2j = (N \u2212 1)\u22121             \u03c8i,j \u2212 \u03c8 \u00b7,j        ,   (20.36)\n                                        i=1\n\nthe variance of the jth chain, and de\ufb01ne\n                                              M\n                               W = M \u22121             s2j .                 (20.37)\n                                              j=1\n\nW is the pooled within-chain variance. The two variances, B and W , are\ncombined into\n                           +        N \u22121      1\n                        \u001f (\u03c8|Y ) =\n                       var               W + B,                 (20.38)\n                                      N       N\nwhere, as before, Y is the data.\n   To assess convergence, one can use\n                                 /\n                                      +\n                                   \u001f (\u03c8|Y )\n                             \u0002 = var\n                             R              .                   (20.39)\n                                       W\n\nR\u0002 is called the \u201cpotential scale reduction factor\u201d in output produced by the\nfunction gelman.diag(); see the output in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "* covbetahat)) ,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9. R    \u0002 is also called the\n\u201cshrink factor\u201d or sometime the \u201cGelman shrink factor.\u201d\n    When the chains have not yet reached the stationary distribution, the\n                +\nnumerator var \u001f (\u03c8|Y ) inside the radical is an upward-biased estimate of\nvar(\u03c8|Y ) and the denominator W is a downward-biased estimator of this\nquantity. Both biases converge to 0 as the burn-in period and Monte Carlo\n                                                  \u0002 indicate nonconvergence. If\nsample size increase. Therefore, larger values of R\n \u0002\nR is approximately equal to 1, say at most 1.1, then the chains are considered\n                                                           +\nto have converged to the stationary distribution and var \u001f (\u03c8|Y ) can be used\nas an estimate of var(\u03c8|Y ). A larger value of R\u0002 is an indication that a longer\nburn-in period is needed. A small value of R \u0002 is evidence that the burn-in pe-\nriod is adequate, but we need another diagnostic, the e\ufb00ective sample size, to\nknow if the sampling period was long enough.\n    The e\ufb00ective sample size of the chain is\n                                              +\n                                         \u001f (\u03c8|Y )\n                                         var\n                            Ne\ufb00 = M N             .                       (20.40)\n                                             B\nThe interpretation of Ne\ufb00 is that the Markov chain can estimate the posterior\nexpectation of \u03c8 with approximately the same precision as would be obtained\nfrom an independent sample from the posterior of size Ne\ufb00 . (Of course, it is\nusually impossible to actually obtain an independent sample, which is why\nMCMC is used.)\n    Ne\ufb00 is derived by comparing the variance of \u03c8 \u00b7,\u00b7 from Markov chain sam-\npling with the variance of \u03c8 \u00b7,\u00b7 under hypothetical independent sampling. Since\n\f                                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "R    \u0002 is also called the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo      607\n\n\u03c8 \u00b7,\u00b7 is the average of the means of M independent chains and since B/N is\nthe sample variance of these M chain means,\n                                          B\n                                   M \u22121                                 (20.41)\n                                          N\nestimates the Monte Carlo variance of \u03c8 \u00b7,\u00b7 . Suppose instead of sampling M\nchains, each of length N , one could take an independent sample of size N \u2217\nfrom the posterior. The Monte Carlo variance of the mean of this sample\nwould be\n                                   var(\u03c8|Y )\n                                             ,\n                                      N\u2217\nwhich can be estimated by\n                                      +\n                                  \u001f (\u03c8|Y )\n                                  var\n                                               .                       (20.42)\n                                      N\u2217\nBy de\ufb01nition Ne\ufb00 is the value of N \u2217 that makes (20.41) equal to (20.42)\nand therefore N \u2217 is given by (20.40). Because B/N is the sample variance of\nM chains and because M is typically quite small, often between 2 and 5, B\nhas considerable Monte Carlo variability. Therefore, Ne\ufb00 is at best a crude\nestimate of the e\ufb00ective sample size.\n     \u0002 and Ne\ufb00 are computed by the functions gelman.diag() and effective-\n     R\nSize() in the coda package. The function gelman.plot() in the coda package\nplots the R\u0002 evaluated at various times along the simulation. The documen-\ntation for this function notes that \u201cA potential problem with gelman.diag is\nthat it may mis-diagnose convergence if the shrink factor happens to be close\nto 1 by chance. By calculating the shrink factor at several points in time,\ngelman.plot shows if the shrink factor has really converged, or whether it is\nstill \ufb02uctuating.\u201d Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo      607",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6 shows the Gelman plot from Example 20.9. The\ndashed red line is an upper ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "shows the Gelman plot from Example 20.9. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "97.5 % con\ufb01dence limit. Although R \u0002 varies during\nthe simulations, it appears to have converged to values close to 1 by the end\nof the simulations.\n     How large should Ne\ufb00 be? Of course, larger means better Monte Carlo\naccuracy, but larger values of Ne\ufb00 require more or longer chains, so we do not\nwant Ne\ufb00 to be unnecessarily large. The e\ufb00ect of Ne\ufb00 on estimation error can\nbe seen by decomposing the estimation error \u03c8 \u2212 \u03c8 \u00b7,\u00b7 into two parts, which\nwill be called E1 and E2 :\n\n      \u03c8 \u2212 \u03c8 \u00b7,\u00b7 = {\u03c8 \u2212 E(\u03c8|Y )} + E(\u03c8|Y ) \u2212 \u03c8 \u00b7,\u00b7 = E1 + E2 .           (20.43)\n\nIf E{\u03c8|Y } could be computed exactly so that it, not \u03c8 \u00b7,\u00b7 , would be the es-\ntimator of \u03c8, then E1 would be the only error. E2 is the error due to the\nMonte Carlo approximation of E{\u03c8|Y } by \u03c8 \u00b7,\u00b7 . The two errors E1 and E2 are\nuncorrelated, so\n\n                var{(\u03c8 \u2212 \u03c8 \u00b7,\u00b7 )|Y } = var(E1 |Y ) + var(E2 |Y )\n\f608      20 Bayesian Data Analysis and MCMC\n\n                                                     var(\u03c8|Y )\n                                       = var(\u03c8|Y ) +\n                                                       Ne\ufb00\n                                                   \u0007        \b\n                                                         1\n                                       = var(\u03c8|Y ) 1 +\n                                                        Ne\ufb00\n                                                                                    +\nby the de\ufb01nitions of var(\u03c8|Y ) and Ne\ufb00 and using the approximation\n                                                             \u221a             \u001f\n                                                                          var\n(\u03c8|Y ) \u2248 var(\u03c8|Y ). Using the Taylor series approximation 1 + \u03b4 \u2248 1 + \u03b4/2\nfor small values of \u03b4, we see that\n                #                                  \u0007          \b\n                                        \u0011                 1\n                  var{(\u03c8 \u2212 \u03c8 \u00b7,\u00b7 )|Y } \u2248 var(\u03c8|Y ) 1 +          .       (20.44)\n                                                        2Ne\ufb00\n             #\nRecall that var{(\u03c8 \u2212 \u03c8 \u00b7,\u00b7 )|Y } is the \u201cBayesian standard error.\u201d If Ne\ufb00 \u2265 50,\nthen we see from (20.44) that the standard error is in\ufb02ated by Monte Carlo\nerror by at most 1 %. Thus, one might use the rule-of-thumb that Ne\ufb00 should\nbe at least 50. Remember, however, that Ne\ufb00 is estimated only crudely because\nthe number of chains is small. Thus, we might want to have Ne\ufb00 at least 100\nto provide some leeway for error in the estimation of Ne\ufb00 .\n    The value of Ne\ufb00 can vary between di\ufb00erent choices of \u03c8. Recall that the\nvalues of Ne\ufb00 from Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "97.5",
      "section_title": "% con\ufb01dence limit. Although R \u0002 varies during",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 were:\n      > effectiveSize(univt.coda)\n             k       mu    sigma\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "were:",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "effectiveSize(univt.coda)\n             k       mu    sigma",
        "start": 17,
        "end": 84
      }
    ]
  },
  {
    "content": "366.8874 300.0000 300.0000\n\n    In this example, Ne\ufb00 is 300 for mu and sigma and only slightly larger for k.\n(In more complex models, much greater variation in Ne\ufb00 is common.) In this\nexample, 300 is the Monte Carlo sample size since there are three chains, each\nof length 100 after burn-in and thinning.3 Therefore, in this simple example,\nMCMC sampling is as e\ufb00ective as independent sampling; this is not a typical\ncase.\n    For convenience, R\u0002 values in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "366.8874",
      "section_title": "300.0000 300.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 are listed again:\n      > gelman.diag(univt.coda)\n      Potential scale reduction factors:\n\n              Point est. Upper C.I.\n      k             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "are listed again:",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "gelman.diag(univt.coda)\n      Potential scale reduction factors:",
        "start": 29,
        "end": 97
      }
    ]
  },
  {
    "content": "1.00       1.01\n      mu            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.00",
      "section_title": "1.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.00       1.01\n      sigma         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.00",
      "section_title": "1.01",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02       1.06\n\n      Multivariate psrf\n\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "1.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02\n3\n    The e\ufb00ective sample size can be larger than the actual sample size if there is\n    negative correlation, but negative correlation is unlikely. It is more likely that\n    some of the e\ufb00ective sample sizes exceed the actual sample sizes due to random\n    variation, i.e., estimation error.\n\f                                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo                                609\n\n   One can see that R \u0002 is at most ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo                                609",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02 for all of the parameters that were\nmonitored, which is another indication that the amount of MCMC sampling\nwas su\ufb03cient. Even the 95 % upper con\ufb01dence limits are satisfactory, at most\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "for all of the parameters that were",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.06.\n\n                                                            k                                                        mu\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.06",
      "section_title": "k                                                        mu",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.000 1.005 1.010\n\n\n\n\n                                                                  median                                                     median\n\n\n\n\n                                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.000",
      "section_title": "1.005 1.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.04\n                                                                  97.5%                                                      97.5%\n      shrink factor\n\n\n\n\n                                                                              shrink factor\n                                                                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.04",
      "section_title": "97.5%                                                      97.5%",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.02\n                                                                                              1.00\n                                          6000       6400          6800                              6000       6400          6800\n                                                 last iteration in chain                                    last iteration in chain\n\n\n                                                        sigma\n\n                                                                  median\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.02",
      "section_title": "1.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.10\n\n\n\n\n                                                                  97.5%\n      shrink factor\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.10",
      "section_title": "97.5%",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.06\n                      1.02\n\n\n\n\n                                          6000       6400          6800\n                                                 last iteration in chain\n\n                                                    Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.06",
      "section_title": "1.02",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.6. Gelman plot in Example 20.9.\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.6",
      "section_title": "Gelman plot in Example 20.9.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.6 DIC and pD for Model Comparisons\n\nIn this section, we introduce two widely used statistics, DIC and pD . DIC is\nused to compare several models for the same data set and is a Bayesian analog\nof AIC. pD is a Bayesian analog to the number of parameters in the model.\n    Recall from Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "6 DIC and pD for Model Comparisons",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.12 that the deviance, denoted now by D(Y , \u03b8), is minus\ntwice the log-likelihood, and AIC de\ufb01ned by (5.29) is\n\n                                                                AIC = D(Y , \u03b8 ML ) + 2p,                                              (20.45)\n\n      \u0002ML is the MLE and p is the dimension of \u03b8. A Bayesian analog of the\nwhere \u03b8\nMLE is the posterior mean, the usual Bayes estimator, which can be estimated\nby MCMC.\n\f610     20 Bayesian Data Analysis and MCMC\n\n    We need a Bayesian analog of p, the number of parameters. It may seem\nstrange at \ufb01rst that we do not simply use p itself as in a non-Bayesian analysis.\nAfter all, the number of parameters has not changed just because we now have\na prior and are using Bayesian estimation. However, the prior information\nused in a Bayesian analysis somewhat constrains the estimated parameters,\nwhich makes the e\ufb00ective number of parameters less than p. To appreciate\nwhy this is true, consider an example where there are d returns on equities\nthat are believed to be similar. Assume the returns have a multivariate normal\ndistribution. Let\u2019s focus on the d expected returns, call them \u03bc1 , . . . , \u03bcd . To a\nnon-Bayesian, there are two ways to model \u03bc1 , . . . , \u03bcd . The \ufb01rst is to assume\nthat they are all equal, say to \u03bc, and then there is only one parameter to model\nthe means. The other possibility is to assume that the expected returns are\nnot equal so that there are d parameters.\n    A Bayesian can achieve a compromise between these two extreme by spec-\nifying a prior such that \u03bc1 , . . . , \u03bcd are similar but not identical. For example,\nwe could assume that they are i.i.d. N (\u03bc, \u03c3\u03bc2 ), and \u03c3\u03bc2 would specify the degree\nof similarity. It is important to appreciate that \u03c3\u03bc2 can be estimated from the\ndata, that is, there is no need to specify in advance the degree of similarity\nbetween the means. The result of using such prior information is that the\ne\ufb00ective number of parameters to specify \u03bc1 , . . . , \u03bcd is greater than 1 but less\nthan d.\n    The e\ufb00ective number of parameters is de\ufb01ned as\n                                  \u0002 avg \u2212 D(Y , \u03b8),\n                             pD = D                                          (20.46)\n\nwhere\n                                            M       N\n                            \u03b8 = (N M )\u22121                \u03b8 i,j\n                                            j=1 i=1\n\nis the average of the MCMC sample of \u03b8 i,j and estimates the posterior expec-\ntation of \u03b8, and\n                                          M     N\n                       \u0002 avg = (N M )\u22121\n                       D                            D(Y , \u03b8 i,j )\n                                          j=1 i=1\n\nis an MCMC estimate of\n\n                             Davg = E{D(Y , \u03b8)|Y }.                          (20.47)\n\nIn analogy with (20.45), DIC is de\ufb01ned as\n\n                             DIC = D(Y , \u03b8) + 2pD .\n\nBy (20.46), we have as well that\n\n                                      \u0002 avg + pD .\n                                DIC = D\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.12",
      "section_title": "that the deviance, denoted now by D(Y , \u03b8), is minus",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7 Markov Chain Monte Carlo        611\n\nThe function dic.samples() in the rjags package reports D   \u0002 avg as the \u201cmean\ndeviance,\u201d pD as the \u201cpenalty,\u201d and DIC as the \u201cpenalized deviance.\u201d See the\noutput in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Markov Chain Monte Carlo        611",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9.\n    As the following example illustrates, pD is primarily a measure of the\nposterior variability of \u03b8, which increases as p increases or the amount of\nprior information about \u03b8 decreases relative to the information in the sample.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "As the following example illustrates, pD is primarily a measure of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.11. pD when estimating a normal mean with known precision\n\n   Suppose that Y = (Y1 , . . . , Yn ) are i.i.d. N (\u03bc, 1), so \u03b8 = \u03bc in this example.\nThen the log-likelihood is\n                               n\n                        1                  n\n         log{L(\u03bc)} = \u2212        (Yi \u2212 \u03bc)2 \u2212 log(2\u03c0)\n                        2 i=1              2\n                          \u0012 n                         \u0014\n                        1                               n\n                     =\u2212         (Yi \u2212 Y ) + n(Y \u2212 \u03bc) \u2212 log(2\u03c0),\n                                         2          2\n                        2 i=1                           2\n\nand so\n                         n\n           D(Y , \u03bc) =         (Yi \u2212 Y )2 + n(Y \u2212 \u03bc)2 + n log(2\u03c0).            (20.48)\n                        i=1\n\nWhen pD is computed, quantities not depending on \u03bc cancel with the sub-\ntraction in (20.46). Therefore, for the purpose of computing pD , we can use\n\n                                D(Y , \u03bc) = n(Y \u2212 \u03bc)2 .                       (20.49)\n\nThen\n                        D{Y , E(\u03bc|Y )} = {Y \u2212 E(\u03bc|Y )}2 ,                    (20.50)\nand\n\n             Davg = n E{(Y \u2212 \u03bc)2 |Y }\n                     &                   \u0018                 \u0019'\n                  = n {Y \u2212 E(\u03bc|Y )}2 + E {E(\u03bc|Y ) \u2212 \u03bc}2 |Y\n                     \u001a                           \u001b\n                  = n {Y \u2212 E(\u03bc|Y )}2 + Var(\u03bc|Y )\n                   = D{Y , E(\u03bc|Y )} + nVar(\u03bc|Y ),                            (20.51)\n\nbecause {Y \u2212E(\u03bc|Y )} and {E(\u03bc|Y )\u2212\u03bc} are conditionally uncorrelated given\nY . Therefore,\n                  \u0002 avg \u2212 D{Y , E(\u03bc|Y )}\n             pD = D\n                                                                 n\n                 \u2248 Davg \u2212 D{Y , E(\u03bc|Y )} = nVar(\u03bc|Y ) =               ,      (20.52)\n                                                               n + \u03c40\n\f612      20 Bayesian Data Analysis and MCMC\n\nwhere the last equality uses (20.21) and \u03c40 is the prior precision for \u03bc. The\napproximation (\u201c\u2248\u201d) in (20.52) becomes equality as the Monte Carlo sample\nsize N increases to \u221e.\n    As \u03c40 \u2192 0, the amount of prior information becomes negligible and the\nright-hand side of (20.52) converges to p = 1. Conversely, as \u03c40 \u2192 \u221e, the\namount of prior information increases without bound and the right-hand side\nof (20.52) converges to 0. This is an example of a general phenomenon\u2014more\nprior information means less e\ufb00ective parameters.                          \u0002\n\n   Generally, pD \u2248 p when p is small and there is little prior information. In\nother cases, such as when d means are modeled as coming from a common\nnormal distribution, pD could be considerably less than p\u2014see Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.11",
      "section_title": "pD when estimating a normal mean with known precision",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12.\n   When comparing models using DIC, smaller is better, though, like AIC and\nBIC, DIC should never be used blindly. Often subject-matter considerations\nor model simplicity will lead an analyst to select a model other than the one\nminimizing DIC.\n   The function dic.sample() returns both DIC and pD , as can be seen in\nthe output from Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "When comparing models using DIC, smaller is better, though, like AIC and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 which was:\n      > dic.samples(univt.mcmc, 100*nthin, thin = nthin, type = \"pD\")\n        |**************************************************| 100%\n      Mean deviance: -18065\n      penalty ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "which was:",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "dic.samples(univt.mcmc, 100*nthin, thin = nthin, type = \"pD\")\n        |**************************************************| 100%\n      Mean deviance: -18065\n      penalty",
        "start": 22,
        "end": 194
      }
    ]
  },
  {
    "content": "2.664\n      Penalized deviance: -18062\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.664",
      "section_title": "Penalized deviance: -18062",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8 Hierarchical Priors\n\nA common situation is having a number of parameters that are believed to\nhave similar, but not identical, values. For example, the expected returns on\nseveral equities might be thought similar. In such cases, it can be useful to\npool information about the parameters to improve the speci\ufb01cation of the\nprior, because the use of good prior information will improve the accuracy\nof the estimation. A e\ufb00ective method for pooling information is a Bayesian\nanalysis with a so-called \u201chierarchical prior\u201d that allows one to shrink the\nestimates toward each other or toward some other target. An example of the\nlatter would be shrinking the sample covariance matrix of returns toward an\nestimate from the CAPM or another factor model. This type of shrinkage\nwould achieve a tradeo\ufb00 between the high variability of the sample covariance\nmatrix and the potential bias of the covariance matrix estimator from a factor\nmodel when the factor model does not \ufb01t perfectly.\n    As before, let the likelihood be f (y|\u03b8). The likelihood is the \ufb01rst layer (or\nstage) in the hierarchy. So far in this chapter, the prior density of \u03b8, which is\nthe second layer, has been \u03c0(\u03b8|\u03b3), where the parameter vector \u03b3 in the prior\nhas a known value, say \u03b3 0 . For example, in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "Hierarchical Priors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.3 the prior had a beta\ndistribution with both parameters \ufb01xed.\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.3",
      "section_title": "the prior had a beta",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8 Hierarchical Priors   613\n\n    In a hierarchical or multistage prior, \u03b3 is unknown and has its own prior\n\u03c0(\u03b3|\u03b4) (the third layer). Typically, \u03b4 has a known value, though one can add\nfurther layers to the hierarchy by making \u03b4 unknown with its own prior, and\nso forth.\n    It is probably easiest to understand hierarchical priors using examples.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "Hierarchical Priors   613",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12. Estimating expected returns on midcap stocks\n\n    This example uses the midcapD.ts dataset. This data set contains 500\ndaily returns on 20 midcap stocks and on the market and was used in\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "Estimating expected returns on midcap stocks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.2.\n    The data set will be divided into the \u201ctraining data,\u201d which contains the\n\ufb01rst 100 days of returns and the \u201ctest\u201d data containing the last 400 days of\nreturns. Only the training data will be used for estimation. The test data will\nbe used to compare the estimates from the training data. The test data sample\nsize was chosen intentionally to be relatively large so that we can consider the\nmean returns from the test data to be the \u201ctrue\u201d expected returns on the 20\nstocks, though, of course, this is only an approximation. The \u201ctrue\u201d expected\nreturns will be estimated using the training data.\n    We will compare three possible estimators of the true expected returns.\n(a) sample means (the 20 mean returns on the midcap stocks for the \ufb01rst 100\n    days);\n(b) pooled estimation (total shrinkage where every expected return has the\n    same estimate);\n(c) Bayes estimation with a hierarchical prior (shrinkage).\n    Method (a) is the \u201cusual\u201d non-Bayesian estimator where each expected\nreturn is estimated by the sample mean of that stock. In method (b), every\nexpected return has the same estimate, which is the \u201cmean of means,\u201d that is,\nthe average of the 20 means from (a). Bayes shrinkage, which will be explained\nin this example, shrinks the 20 individual means toward the mean of means\nusing a hierarchical prior. Bayesian shrinkage is a compromise between (a)\nand (b). Shrinkage was also used in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.2",
      "section_title": "The data set will be divided into the \u201ctraining data,\u201d which contains the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.10 though in that example\nthe amount of shrinkage was chosen arbitrarily because Bayesian methods had\nnot yet been introduced.\n    Let Ri,t be the tth daily return on i stock expressed as a percentage. For\nBayesian shrinkage, the \ufb01rst layer will be the simple model\n\n                                Ri,t = \u03bci + \u0017i,t ,\n\nwhere \u0017i,t are i.i.d. N (0, \u03c3 2 ). This model has several unrealistic aspects: (a)\nthe assumption that the standard deviation of \u0017i,t does not depend on i; (b)\nthe assumption that \u0017i,t and \u0017i\u0002 ,t are independent (we know that there will\nbe cross-sectional correlations); (c) the assumption that there are no GARCH\ne\ufb00ects; (d) the assumption that the \u0017i,t are normally distributed rather than\n\f614    20 Bayesian Data Analysis and MCMC\n\nhaving heavy tails. Nonetheless, for the purpose of estimating expected re-\nturns, this model should be adequate. Remember, \u201call models are wrong but\nsome models are useful,\u201d and, of course, what is \u201cuseful\u201d depends on the\nobjectives of the analysis.\n   The hierarchy prior has second layer\n\n                             \u03bci \u223c i.i.d. N (\u03b1, \u03c3\u03bc2 ).\n\nThe assumption here is that the expected returns for the 20 midcap stocks\nhave been sampled from a large population of expected returns, perhaps of\nall midcap stocks or even a larger population. The mean of that population\nis \u03b1 and the standard deviation is \u03c3\u03bc .\n    If we used a non-hierarchical prior, then we would need to specify values\nof \u03b1 and \u03c3\u03bc . This is exactly what was done in Example 20.4, except in that\nexample \u03c3 2 also was known. We probably have a rough idea of the values of\n\u03b1 and \u03c3\u03bc , but it is unlikely that we have precise information about them, and\nwe saw in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.10",
      "section_title": "though in that example",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 that a rather accurate speci\ufb01cation of the prior is\nneeded for the Bayes estimator to improve upon the sample means. In fact,\nthe Bayes estimator can easily be inferior to the sample means if the prior is\npoorly chosen.\n    The third layer will be a prior on \u03b1 and \u03c3\u03bc and will let us use the data to\nestimate these parameters. It is important to appreciate why we can estimate\n\u03b1 and \u03c3\u03bc in this example, but they could not be estimated in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "that a rather accurate speci\ufb01cation of the prior is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4.\nThe reason is that we now have 20 expected returns (the \u03bci ) that are dis-\ntributed with the same mean \u03b1 and standard deviation \u03c3\u03bc . In contrast, in\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "The reason is that we now have 20 expected returns (the \u03bci ) that are dis-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.4 there is only a single \u03bc and so it not possible to estimate the\nmean and variance of the population from which this \u03bc was sampled.\n    Because there is now a substantial amount of information in the data about\n\u03b1, \u03c3 2 , and \u03c3\u03bc2 , we could use fairly noninformative priors for them to \u201clet the\ndata speak for themselves.\u201d\n    The BUGS program for this example is:\n1  model{\n2  for (i in 1:n)\n 3 {\n\n 4   for (j in 1:m)\n 5   {\n 6     x1[i,j] ~ dnorm(mu[j], tau_eps)\n 7   }\n 8 }\n\n 9 for (j in 1:m)\n\n10 {\n\n11   mu[j] ~ dnorm(alpha, tau_mu)\n12 }\n\n13 alpha ~ dnorm(0.0, 1.0E-3)\n\n14 tau_eps ~ dgamma(0.1, 0.01)\n\n15 tau_mu ~ dgamma(0.1, 0.01)\n\f                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.4",
      "section_title": "there is only a single \u03bc and so it not possible to estimate the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8 Hierarchical Priors   615\n\n16 sigma_eps <- 1 / sqrt(tau_eps)\n17 sigma_mu <- 1 / sqrt(tau_mu)\n18 }\n\n\nThe R code for this example is below:\n1  library(rjags)\n2  dat = read.csv(\"midcapD.ts.csv\")\n 3 market = 100 * as.matrix(dat[, 22])\n\n 4 x = 100 * as.matrix(dat[, -c(1, 22)])\n\n 5 m = 20\n\n 6 k = 100\n\n 7 x1 = x[1:k, ]\n\n 8 x2 = x[(k+1):500, ]\n\n 9 mu1 = apply(x1, 2, mean)\n\n10 mu2 = apply(x2, 2, mean)\n\n11 means = apply(x1, 2, mean)\n\n12 sd2 = apply(x1, 2, sd)\n\n13 tau_mu = 1 / mean(sd2^2)\n\n14 tau_eps = 1 / sd(means)^2\n\n15 n = k\n\n16 data = list(x1 = x1, n = n, m = m)\n\n17 inits.midCap = function(){list(alpha = 0.001, mu = means,\n\n18    tau_eps = tau_eps, tau_mu = tau_mu)}\n19 midCap <- jags.model(\"midCap.bug\", data = data, inits = inits.midCap,\n\n20    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n21 nthin = 20\n\n22 midCap.coda = coda.samples(midCap, c(\"mu\", \"tau_mu\", \"tau_eps\",\n\n23    \"alpha\", \"sigma_mu\", \"sigma_eps\"), 500 * nthin, thin = nthin)\n24 summ.midCap = summary(midCap.coda)\n\n25 summ.midCap\n\n26 post.means = summ.midCap[[1]][2:21, 1]\n\n27 pdf(\"midcap.pdf\", width = 6, height = 3.75)\n\n28 par(mfrow = c(1, 2))\n\n29 plot(c(rep(1, m), rep(2, m)), c(mu1, mu2),\n\n30      xlab = \"estimate                         target\",ylab = \"mean\",\n31      main = \"sample means\",\n32      ylim = c(-0.3, 0.7), axes = FALSE)\n33 axis(2)\n\n34 axis(1, labels = FALSE, tick = TRUE, lwd.tick = 0)\n\n35 for (i in 1:m){lines(1:2, c(mu1[i], mu2[i]), col = i)}\n\n36 plot(c(rep(1, m), rep(2, m)), c(post.means, mu2),\n\n37      xlab = \"estimate                         target\", ylab = \"mean\",\n38      main = \"Bayes\",\n39      ylim=c(-0.3, 0.7), axes = FALSE)\n40 axis(2)\n\n41 axis(1, labels = FALSE, tick = TRUE, lwd.tick = 0)\n\n42 for (i in 1:m){lines(1:2, c(post.means[i], mu2[i]) ,col=i)}\n\n43 graphics.off()\n\n44 options(digits = 2)\n\f616      20 Bayesian Data Analysis and MCMC\n\n45 sum((mu1 - mu2)^2 )\n46 sum((post.means - mu2)^2)\n47 sum((mean(mu1) - mu2)^2)\n\n\nThe output is below.\n      > summ.midCap\n\n      Iterations = 20:10000\n      Thinning interval = 20\n      Number of chains = 3\n      Sample size per chain = 500\n\n      1. Empirical mean and standard deviation for each variable,\n         plus standard error of the mean:\n\n                   Mean         SD Naive SE Time-series SE\n      alpha     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "Hierarchical Priors   615",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "summ.midCap",
        "start": 1959,
        "end": 1974
      }
    ]
  },
  {
    "content": "0.08730   0.102433 2.645e-03     3.101e-03\n      mu[1]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.08730",
      "section_title": "0.102433 2.645e-03     3.101e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.11121   0.171456 4.427e-03     4.546e-03\n      mu[2]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.11121",
      "section_title": "0.171456 4.427e-03     4.546e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12128   0.169230 4.369e-03     4.892e-03\n      mu[3]     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12128",
      "section_title": "0.169230 4.369e-03     4.892e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07871   0.170849 4.411e-03     4.702e-03\n\n       (edited to save space)\n\n      mu[19]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07871",
      "section_title": "0.170849 4.411e-03     4.702e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05082 0.175466 4.531e-03         4.422e-03\n      mu[20]    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05082",
      "section_title": "0.175466 4.531e-03         4.422e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03997 0.184614 4.767e-03         4.873e-03\n      sigma_eps ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03997",
      "section_title": "0.184614 4.767e-03         4.873e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.30691 0.067810 1.751e-03         1.629e-03\n      sigma_mu  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.30691",
      "section_title": "0.067810 1.751e-03         1.629e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.14970 0.067054 1.731e-03         1.671e-03\n      tau_eps   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.14970",
      "section_title": "0.067054 1.731e-03         1.671e-03",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05395 0.001699 4.386e-05         4.074e-05\n      tau_mu   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05395",
      "section_title": "0.001699 4.386e-05         4.074e-05",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "75.70128 68.715669 1.774e+00        1.738e+00\n\n      2. Quantiles for each variable:\n\n                   2.5%       25%       50%       75%      97.5%\n      alpha    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "75.70128",
      "section_title": "68.715669 1.774e+00        1.738e+00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.11465 0.017800    0.08600   0.15560    0.29111\n      mu[1]    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.11465",
      "section_title": "0.017800    0.08600   0.15560    0.29111",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.21768 -0.001040   0.10910   0.21864    0.43408\n      mu[2]    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.21768",
      "section_title": "-0.001040   0.10910   0.21864    0.43408",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.21107 0.018025    0.11704   0.22382    0.47727\n      mu[3]    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.21107",
      "section_title": "0.018025    0.11704   0.22382    0.47727",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.27262 -0.032547   0.08392   0.19634    0.40900\n\n       (edited to save space)\n\n      mu[19]   -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.27262",
      "section_title": "-0.032547   0.08392   0.19634    0.40900",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.30441 -0.059642 0.05612 0.16521     0.38525\n      mu[20]   -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.30441",
      "section_title": "-0.059642 0.05612 0.16521     0.38525",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.34436 -0.069959 0.04462 0.16479     0.37048\n      sigma_eps ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.34436",
      "section_title": "-0.069959 0.04462 0.16479     0.37048",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.17808 4.261352 4.30714 4.35226      4.43733\n      sigma_mu  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.17808",
      "section_title": "4.261352 4.30714 4.35226      4.43733",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.06155 0.100600 0.13853 0.18225      0.31293\n      tau_eps   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.06155",
      "section_title": "0.100600 0.13853 0.18225      0.31293",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05079 0.052792 0.05390 0.05507      0.05729\n      tau_mu   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05079",
      "section_title": "0.052792 0.05390 0.05507      0.05729",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.21311 30.107752 52.10738 98.81013 263.94725\n\n   The posterior means of \u03c3\u03bc and \u03c3 are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.21311",
      "section_title": "30.107752 52.10738 98.81013 263.94725",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.150 % and 4.31 %, respectively\n(the returns are as percentages). If we look at precisions instead of standard\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.150",
      "section_title": "% and 4.31 %, respectively",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8 Hierarchical Priors     617\n\ndeviations, then we \ufb01nd that the posterior means of \u03c4\u03bc and \u03c4 are ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "Hierarchical Priors     617",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "75.7 and\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "75.7",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0540. Using the notation of (20.24), in the present example \u03c4Y is 100\u03c4 = 5.4\nand \u03c40 = \u03c4\u03bc = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0540",
      "section_title": "Using the notation of (20.24), in the present example \u03c4Y is 100\u03c4 = 5.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "75.7. Therefore, \u03b4 in (20.24) is 5.4/(5.4 + 75.7) = 0.067. Recall\nthat \u03b4 close to 0 (far from 1) results in substantial shrinkage, so \u03b4 equal to\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "75.7",
      "section_title": "Therefore, \u03b4 in (20.24) is 5.4/(5.4 + 75.7) = 0.067. Recall",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.064 causes a great amount of shrinkage of the sample means toward the\nmean of means, as can be seen in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.064",
      "section_title": "causes a great amount of shrinkage of the sample means toward the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.\n   To compare the estimators, we use the sum of squared errors (SSE)\nde\ufb01ned as\n                                        20\n                               SSE =          \u03bc i \u2212 \u03bc i )2 ,\n                                             (\u0002\n                                       i=1\n\nwhere \u03bci is the ith \u201ctrue\u201d mean from the test data and \u03bc  \u0002i is an estimate from\nthe training data. The values of the SSE are found in Table ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "To compare the estimators, we use the sum of squared errors (SSE)",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1. The SSE\nfor the sample means is about 11 (1.9/0.18) times larger than for the Bayes\nestimate. Clearly, shrinkage is very successful in this example.\n    Interestingly, complete shrinkage to the pooled mean is even better than\nBayesian shrinkage. Bayesian shrinkage attempts to estimate the optimal\namount of shrinkage, but, of course, it cannot do this perfectly. Although\ncomplete shrinkage is better than Bayesian shrinkage in this example, com-\nplete shrinkage is, in general, dangerous since it will have a large SSE in\nexamples where the true means di\ufb00er more than in this case. If one has a\nstrong prior belief that the true means are very similar, one should use this\n\n\n                    sample means                                    Bayes\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "The SSE",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                      0.6\n      mean\n\n\n\n\n                                               mean\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                      0.2\n             \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                      \u22120.2\n\n\n\n\n               estimate            target               estimate                 target\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7. Estimation of the average returns for 20 midcap stocks. \u201cTarget\u201d is the\nquantity being estimated, speci\ufb01cally the average return over 400 days of test data.\n\u201cEstimate\u201d is an estimate based on the 100 previous days of training data. On the\nleft, the estimates are the 20 individual sample means. On the right, the estimates are\nthe sample means shrunk toward their mean. In each panel, the estimate and target\nfor each stock are connected by a line. On the left, the sample means of the training\ndata are so variable that the stocks with smaller (larger) means in the training data\noften have larger (smaller) means in the test data. The Bayes estimates on the right\nare much closer to the targets.\n\f618    20 Bayesian Data Analysis and MCMC\n\nbelief when specifying a prior for \u03c3\u03bc . Instead of using a noninformative prior\nas in this example, one would use a prior more concentrated near 0.          \u0002\n\n\nTable ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "Estimation of the average returns for 20 midcap stocks. \u201cTarget\u201d is the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.1. Sum of squared errors (SSE) for three estimators of the expected\nreturns of 20 midcap stocks.\n                             Estimate        SSE\n                             (a) sample means ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.1",
      "section_title": "Sum of squared errors (SSE) for three estimators of the expected",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.9\n                             (b) pooled mean 0.12\n                             (c) Bayes        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.9",
      "section_title": "(b) pooled mean 0.12",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.18\n\n\n20.9 Bayesian Estimation of a Covariance Matrix\n\nIn this section, we assume that Y 1 , . . . , Y n is an i.i.d. sample from a d-\ndimensional N (\u03bc, \u03a3) distribution or a d-dimensional t\u03bd (\u03bc, \u039b) distribution.\nWe will focus on estimation of the covariance matrix \u03a3 of a multivariate nor-\nmal distribution or the scale matrix \u039b of a multivariate t-distribution. The pre-\ncision matrix is de\ufb01ned as \u03a3 \u22121 or \u039b\u22121 for the Gaussian and t-distributions,\nrespectively. This de\ufb01nition is analogous to the univariate case where the pre-\ncision is de\ufb01ned as the reciprocal of the variance or squared scale parameter.\n    We will start with Gaussian distributions.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.18",
      "section_title": "20.9 Bayesian Estimation of a Covariance Matrix",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9.1 Estimating a Multivariate Gaussian Covariance Matrix\n\nIn the multivariate Gaussian case, the conjugate prior for the precision ma-\ntrix \u03a3 \u22121 is the Wishart distribution. The Wishart distribution, denoted by\nWishart(\u03bd, A), has a univariate parameter \u03bd called the degrees of freedom and\na matrix parameter A that can be any nonsingular covariance matrix. There\nis a simple de\ufb01nition of the Wishart(\u03bd, A) distribution when \u03bd is an integer.\nLet Z i , . . . , Z n be i.i.d. N (\u03bc, A). In this case, the distribution of\n                             n\n                                  (Z i \u2212 \u03bc)(Z i \u2212 \u03bc)T\n                            i=1\n\nis Wishart(n, A). Also, the distribution of\n                             n\n                                  (Z i \u2212 Z)(Z i \u2212 Z)T                    (20.53)\n                            i=1\n\nis Wishart(n \u2212 1, A). Because the sum in (20.53) is n \u2212 1 times the sample\ncovariance matrix, the Wishart distribution is important for inference about\nthe covariance matrix of a Gaussian distribution.\n\f                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "1 Estimating a Multivariate Gaussian Covariance Matrix",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 Bayesian Estimation of a Covariance Matrix                     619\n\n     The density of a Wishart(\u03bd, A) distribution for any positive value of \u03bd is\n                                                  \u000e              \u000f\n                           \u2212\u03bd/2                     1     \u22121\n      f (W ) = C(\u03bd, d) |A|      |W | (\u03bd\u2212d\u22121)/2\n                                               exp \u2212 tr(A W )           (20.54)\n                                                    2\nwith normalizing constant\n                            \u0012                                   \u0007           \b\u0014\u22121\n                                                      5\n                                                      d\n                                                                    \u03bd+1\u2212i\n                                \u03bdd/2       d(d\u22121)/4\n                C(\u03bd, d) =       2      \u03c0                    \u0393                      .\n                                                      i=1\n                                                                      2\n\nThe argument W is a nonsingular covariance matrix. The expected value is\nE(W ) = \u03bdA. In the univariate case (d = 1), the Wishart distribution is a\ngamma distribution.\n   If W is Wishart(\u03bd, A) distributed, then the distribution of W \u22121 is called\nthe inverse Wishart distribution with parameters \u03bd and A\u22121 and denoted\nInv-Wishart(\u03bd, A\u22121 ).\n   Let Y = (Y 1 , . . . , Y n ) denote the data. To derive the full conditional for\nthe precision matrix \u03a3 \u22121 , assume that \u03bc is known. We know from (7.15) that\nthe likelihood is\n                  5n !                      \u000e                              \u000f\"\n            \u22121                   1               1         T \u22121\n    f (Y |\u03a3 ) =                          exp \u2212 (Y i \u2212 \u03bc) \u03a3 (Y i \u2212 \u03bc) .\n                  i=1\n                          (2\u03c0)d/2 |\u03a3|1/2         2\n\nAfter some simpli\ufb01cation,\n                                            n\n                                             \u0012                          \u0014\n                                         1\n         f (Y |\u03a3 \u22121 ) \u221d |\u03a3 \u22121 |n/2 exp \u2212       (Y i \u2212 \u03bc)T \u03a3 \u22121 (Y i \u2212 \u03bc) .\n                                         2 i=1\n\nDe\ufb01ne\n                                       n\n                            S=             (Y i \u2212 \u03bc)(Y i \u2212 \u03bc)T .\n                                    i=1\n\nNext\n n\n                                           \u0012 n                                 \u0014\n      (Y i \u2212 \u03bc)T \u03a3 \u22121 (Y i \u2212 \u03bc) = tr               (Y i \u2212 \u03bc)T \u03a3 \u22121 (Y i \u2212 \u03bc)           = tr(\u03a3 \u22121 S).\ni=1                                          i=1\n                                                                         (20.55)\nThe \ufb01rst equality in (20.55) is the trivial result that a scalar is also a 1 \u00d7 1\nmatrix and equal to its trace. The second equality uses the result that\ntr(AB) = tr(BA) for any matrices B and A such that the products BA\nand AB are de\ufb01ned. If follows that\n                                             \u000e              \u000f\n                       \u22121        \u22121 n/2         1      \u22121\n                f (Y |\u03a3 ) \u221d |\u03a3 |        exp \u2212 tr(\u03a3 S) .                  (20.56)\n                                                2\n\nSuppose that the prior on the precision matrix \u03a3 \u22121 is Wishart(\u03bd0 , \u03a3 \u22121\n                                                                      0 ).\nThen the prior density is\n\f620      20 Bayesian Data Analysis and MCMC\n                                            \u000e               \u000f\n                                              1\n           \u03c0(\u03a3 \u22121 ) \u221d |\u03a3 \u22121 |(\u03bd0 \u2212d\u22121)/2 exp \u2212 tr(\u03a3 \u22121 \u03a3 0 ) .          (20.57)\n                                              2\nSince the posterior density is proportional to the product of the prior den-\nsity and the likelihood, it follows from (20.56) and (20.57) that the posterior\ndensity is\n                                          !                       \"\n                                             1\n    \u03c0(\u03a3 \u22121 |Y ) \u221d |\u03a3 \u22121 |(n+\u03bd0 \u2212d\u22121)/2 exp \u2212 tr \u03a3 \u22121 (S + \u03a3 0 ) .      (20.58)\n                                             2\n\nTherefore, the posterior distribution of \u03a3 \u22121 is Wishart n + \u03bd0 , (S + \u03a3 0 )\u22121 .\nThe posterior expectation is\n\n                    E(\u03a3 \u22121 |Y ) = (n + \u03bd0 ) (S + \u03a3 0 )\u22121 .              (20.59)\n\nIf \u03bd0 and \u03a3 0 are both small, then\n\n                              E(\u03a3 \u22121 |Y ) \u2248 nS \u22121                       (20.60)\n\nThe MLE of \u03a3 is n\u22121 S, so the MLE of \u03a3 \u22121 is nS \u22121 . Therefore, for small\nvalues of \u03bd0 and \u03a3 0 , the Bayesian estimator of \u03a3 \u22121 is close to the MLE.\n    The full conditional for \u03a3 \u22121 can be combined with a model for \u03bc to\nestimate both parameters. For application to asset returns, a hierarchical prior\nfor \u03bc such as in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "Bayesian Estimation of a Covariance Matrix                     619",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12 might be used. In either case, an MCMC\nanalysis would be straightforward.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "might be used. In either case, an MCMC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9.2 Estimating a Multivariate-t Scale Matrix\n\nThe Wishart distribution is not a conjugate prior for the scale matrix of a\nmultivariate t-distribution, but it can be used as the prior nonetheless, since\nMCMC does not require the use of conjugate priors.\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "2 Estimating a Multivariate-t Scale Matrix",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.13. Estimating the correlation matrix of the CRSPday data\n\n    In Example 7.4, the correlation matrix of the CRSPday returns data was\nestimated by maximum likelihood. In this example, the MLE will be compared\nto a Bayes estimate and the two estimates will be found to be very similar.\nThe BUGS program used in this example is\n      model{\n      for(i in 1:N)\n      {\n        y[i,1:m] ~ dmt(mu[], tau[,], df_likelihood)\n      }\n      mu[1:m] ~ dmt(mu0[], Prec_mu[,], df_prior)\n      tau[1:m,1:m] ~ dwish(Prec_tau[,], df_wishart)\n      lambda[1:m,1:m] <- inverse(tau[,])\n      }\n\f                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.13",
      "section_title": "Estimating the correlation matrix of the CRSPday data",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 Bayesian Estimation of a Covariance Matrix    621\n\nIn the BUGS program, mu is the mean vector, tau is the precision matrix,\nlambda is the scale matrix of the returns. Also, dmt is the multivariate-t\ndistribution, and dwish is the Wishart distribution.\n    At the time of this writing (Dec 2014), JAGS does not have a sampler that\nwill sample the posterior from this model. Therefore, WinBUGS will be used\nand will be called using the bugs() function in the R2WinBUGS package. The\nR code is below.\n1  library(R2WinBUGS)\n2  library(MASS) # need to mvrnorm\n 3 library(MCMCpack) # need for rwish\n\n 4 library(mnormt)\n\n 5 data(CRSPday, package = \"Ecdat\")\n\n 6 y = CRSPday[,4:7]\n\n 7 N = dim(y)[1]\n\n 8 m = dim(y)[2]\n\n 9 mu0 = rep(0,m)\n\n10 Prec_mu = diag(rep(1, m)) / 10000\n\n11 Prec_tau =   diag(rep(1, m)) / 10000\n12 df_wishart = 6\n\n13 df_likelihood = 6\n\n14 df_prior = 6\n\n15 data = list(y = y, N = N, Prec_mu = Prec_mu,\n\n16    Prec_tau = Prec_tau,\n17    mu0 = mu0, m = m, df_likelihood = df_likelihood,\n18    df_prior = df_prior, df_wishart = df_wishart)\n19 inits_t_CRSP = function(){list(mu = mvrnorm(1, mu0,\n\n20    diag(rep(1, m) / 100)),\n21    tau = rwish(6, diag(rep(1, m)) / 100))}\n22 library(R2WinBUGS)\n\n23 multi_t.sim = bugs(data, inits_t_CRSP ,\n\n24    model.file = \"mult_t_CRSP.bug\",\n25    parameters = c(\"mu\", \"tau\"), n.chains = 3,\n26    n.iter = 2200, n.burnin = 200, n.thin = 2,\n27    program = \"WinBUGS\", bugs.seed = 13, codaPkg = FALSE)\n28 print(multi_t.sim, digits = 2)\n\n29 tauhat = multi_t.sim$mean$tau\n\n30 lambdahat = solve(tauhat)\n\n31 sdinv = diag(1/sqrt(diag(lambdahat)))\n\n32 cor = sdinv %*% lambdahat %*% sdinv\n\n33 print(cor,digits=4)\n\n\n    The data list that is an input to the BUGS program contain y which is\nthe matrix of returns, df_likelihood which is the degrees of freedom of the\nt-distribution in the likelihood, mu0 which is the prior mean for mu, df_prior\nwhich is the degrees of freedom in the t prior on mu, and df_wishart which\nis the degrees of freedom of the Wishart prior on tau.\n    Ideally, df_likelihood should be an unknown parameter, but WinBUGS\ndoes not allow this parameter to be estimated. Instead, we \ufb01x it at the MLE\n\f622       20 Bayesian Data Analysis and MCMC\n\n(rounded to 6) computed in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "Bayesian Estimation of a Covariance Matrix    621",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.4. The need to \ufb01x this parameter at\nthe MLE is due to limitations of WinBUGS and could, with considerably more\ne\ufb00ort, be circumvented by programming the MCMC in R or another language\nrather than using WinBUGS.\n    Note that codaPkg = FALSE was speci\ufb01ed in the call to bugs(); this was\nnot necessary since it is the default. When codaPkg = FALSE then bugs() re-\nturns an object of class bugs which cannot be used directly by functions in the\ncoda package since these functions take objects of class mcmc.list. However,\nthe function as.mcmc.list() will convert a bugs object to an mcmc.list\nobject.\n    There were three chains, each of length 2000 after a burn-in of 200 and\nthinned to every second iteration. Thus, the total sample size was 3000 after\nthinning. The convergence to the stationary distribution and mixing were\nboth quite rapid. Ne\ufb00 was at least 1500 and R\u0002 essentially 1 for all parameters,\nwhich indicate adequate burn-in and chain lengths.\n      > print(multi_t.sim, digits = 2)\n      Inference for Bugs model at \"mult_t_CRSP.bug\", fit using WinBUGS,\n       3 chains, each with 2200 iterations (first 200 discarded), n.thin = 2\n       n.sims = 3000 iterations saved\n                 mean   sd   2.5%    25%    50%    75% 97.5% Rhat n.eff\n      mu[1]         0    0      0      0      0      0      0    1 3000\n      mu[2]         0    0      0      0      0      0      0    1 3000\n      mu[3]         0    0      0      0      0      0      0    1 3000\n      mu[4]         0    0      0      0      0      0      0    1 1800\n      tau[1,1] 14706 473 13780 14390 14690 15020 15630           1 3000\n\n      (edited to save space)\n\n      tau[4,4] 65197 2102 61180 63738 65190 66600 69360           1     3000\n      deviance -69858  61 -69980 -69900 -69860 -69820 -69730      1     3000\n\n      For each parameter, n.eff is a crude measure of effective sample size,\n      and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\n      DIC info (using the rule, pD = Dbar-Dhat)\n      pD = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.4",
      "section_title": "The need to \ufb01x this parameter at",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "print(multi_t.sim, digits = 2)\n      Inference for Bugs model at \"mult_t_CRSP.bug\", fit using WinBUGS,\n       3 chains, each with 2200 iterations (first 200 discarded), n.thin = 2\n       n.sims = 3000 iterations saved\n                 mean   sd   2.5%    25%    50%    75% 97.5% Rhat n.eff\n      mu[1]         0    0      0      0      0      0      0    1 3000\n      mu[2]         0    0      0      0      0      0      0    1 3000\n      mu[3]         0    0      0      0      0      0      0    1 3000\n      mu[4]         0    0      0      0      0      0      0    1 1800\n      tau[1,1] 14706 473 13780 14390 14690 15020 15630           1 3000",
        "start": 964,
        "end": 1617
      }
    ]
  },
  {
    "content": "13.8 and DIC = -69843.9\n      DIC is an estimate of expected predictive error (lower deviance is better).\n\nSince \u03bc is close to zero, multi t.sim needed to be printed again, this time\nwith more digits than 2:\n                  mean      sd     2.5%       25%       50%       75%      97.5% Rhat n.eff\n      mu[1]    9.4e-04 2.4e-04 4.6e-04    7.7e-04   9.4e-04   1.1e-03    1.4e-03    1 3000\n      mu[2]    4.4e-04 2.9e-04 -1.3e-04   2.4e-04   4.6e-04   6.4e-04    1.0e-03    1 3000\n      mu[3]    6.9e-04 2.3e-04 2.3e-04    5.3e-04   6.9e-04   8.4e-04    1.1e-03    1 3000\n      mu[4]    7.7e-04 1.3e-04 5.1e-04    6.8e-04   7.7e-04   8.6e-04    1.0e-03    1 1800\n\nThe Bayes estimate of the precision matrix was converted to a correlation\nmatrix at lines 29\u201333 of the R program. The estimated correlation matrix is\nbelow.\n             [,1]   [,2]   [,3]   [,4]\n      [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.8",
      "section_title": "and DIC = -69843.9",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0000 0.3191 0.2841 0.6756\n      [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0000",
      "section_title": "0.3191 0.2841 0.6756",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3191 1.0000 0.1586 0.4696\n      [3,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3191",
      "section_title": "1.0000 0.1586 0.4696",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2841 0.1586 1.0000 0.4300\n      [4,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2841",
      "section_title": "0.1586 1.0000 0.4300",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6756 0.4696 0.4300 1.0000\n\f                                              ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6756",
      "section_title": "0.4696 0.4300 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.10 Stochastic Volatility Models      623\n\nIn Example 7.4, the MLE of the correlation matrix was found to be\n   $cor\n          [,1]   [,2]   [,3]   [,4]\n   [1,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.10",
      "section_title": "Stochastic Volatility Models      623",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0000 0.3192 0.2845 0.6765\n   [2,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0000",
      "section_title": "0.3192 0.2845 0.6765",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.3192 1.0000 0.1584 0.4698\n   [3,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.3192",
      "section_title": "1.0000 0.1584 0.4698",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2845 0.1584 1.0000 0.4301\n   [4,] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2845",
      "section_title": "0.1584 1.0000 0.4301",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6765 0.4698 0.4301 1.0000\n\nNotice the similarity between the Bayes estimate and the MLE.                           \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6765",
      "section_title": "0.4698 0.4301 1.0000",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9.3 Non-Wishart Priors for the Covariate Matrix\n\nWe saw in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "3 Non-Wishart Priors for the Covariate Matrix",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.13 that a Wishart prior with noninformative choices\nof the prior parameters more or less replicates maximum likelihood estima-\ntion. Often, however, one wishes to shrink the covariance matrix toward some\ntarget, perhaps a estimate from a factor model. See Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.13",
      "section_title": "that a Wishart prior with noninformative choices",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16.\n\n\n20.10 Stochastic Volatility Models\nStochastic volatility models are an alternative to GARCH models for modeling\nconditional heteroscedasticity. In the ARIMA/GARCH models of Chap. 14,\nthere was a single white noise process that drove both the conditional mean\nand the conditional variance. In contrast, stochastic volatility models use one\nwhite noise process to drive the conditional expectation and another to drive\nthe conditional variance. Therefore, stochastic volatility models are more chal-\nlenging to \ufb01t because the unobserved volatility process is driven by its own\nwhite noise process, which, of course, is also unobserved; see (20.63) below.\nBayesian analysis is particularly good at dealing with unobserved variables\nand seems the best way to meet the challenge of \ufb01tting stochastic volatility\nmodels.\n   We will illustrate stochastic volatility models with the model\n                                          k\n                             Yt = \u03bc +          \u03b2j Xj,t + at ,                      (20.61)\n                                         j=1\n\nwhere Yt is an observed process, e.g., the returns on an asset, and Xj,t , j =\n1, . . . , k, is the jth covariate at time t and could be a lagged value of Yt .\nAlso, at is a weak white noise process with conditional heteroscedasticity.\nSpeci\ufb01cally,                              \u0011\n                                     a t = ht \u0017 t                       (20.62)\nwhere log(ht ) follows the ARMA(p,q) process\n                              p                       q\n           log(ht ) = \u03b20 +         \u03c6j log(ht\u2212j ) +         \u03b8j vt\u2212j + vt ,          (20.63)\n                             j=1                     j=1\n\f624    20 Bayesian Data Analysis and MCMC\n\n\u0017t and vt are mutually \u221a\n                       independent iid white noises, and Var(\u0017t ) = 1. No-\ntice from (20.62) that ht is the conditional standard deviation of Yt . As\nmentioned above, none of the variables in (20.63) are observable.\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "20.10 Stochastic Volatility Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14. Fitting an ARMA(1,1) stochastic volatility model to the S&P\n500 stock returns\n\n    As an illustration, model (20.61)\u2013(20.63) will be \ufb01t to daily S&P 500 log\nreturns from January 2011 through October 2014. Model (20.61) will be used\nwith no covariates so that Yt = \u03bc + at . An ARMA(1,1) stochastic volatility\nmodel will be used so that log(ht ) = \u03b20 + \u03c6 log(ht\u22121 ) + \u03b8vt\u22121 + vt . In (20.62)\n\u0017t has a t-distribution.\n    A BUGS program to \ufb01t the stochastic volatility model is below.\n1  model\n2  {\n 3   for (i in 1:N)\n 4   {\n 5       y[i] ~ dt(mu, tau[i], nu)\n 6     }\n 7     logh[1] ~ dnorm(0, 1.0E-6)\n 8     for(i in 2:N)\n 9     {\n10       logh[i] ~ dnorm(beta0 + phi * logh[i-1] + theta * v[i-1], tau_v)\n11       v[i] <- logh[i] - beta0 + phi * logh[i-1] + theta * v[i-1]\n12     }\n13     for (i in 1:N)\n14     {\n15       tau[i] <- exp(-logh[i])\n16       h[i] <- 1/tau[i]\n17     }\n18     mu ~ dnorm(0.0, 1)\n19     beta0 ~ dnorm(0, 0.0001)\n20     phi ~ dnorm(0.4, 0.0001)\n21     theta ~ dnorm(0, 0.0001)\n22     tau_v ~ dgamma(0.01, 0.01)\n23     v[1] ~ dnorm(0, 0.001)\n24     nu ~ dunif(1,30)\n25     sigma_v <- 1 / sqrt(tau_v)\n26 }\n\n\nLine 5 speci\ufb01es the likelihood conditional on ht . Line 7 gives a prior for the\nh1 . Lines 8\u201312 specify model (20.63) starting at t = 2 with p = q = 1; line 23\ngives v1 a noninformative prior to start this recursion. Lines 18\u201325 specify\ndi\ufb00use priors on \u03bc, \u03b20 , \u03c6, \u03b8, and \u03c3v .\n     S&P 500 prices from Jan 3, 2011 to Oct 31, 2014 are in the \ufb01le S&\nP500 new.csv. There are 964 log returns starting on Jan 4, 2011. The fol-\nlowing R code computes the log returns and \ufb01t the stochastic volatility model\nto the log returns. The MCMC took about 10 minutes.\n\f                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "Fitting an ARMA(1,1) stochastic volatility model to the S&P",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.10 Stochastic Volatility Models   625\n\n1  library(rjags)\n2  dat = read.csv(\"S&P500_new.csv\")\n 3 prices = dat$Adj.Close\n\n 4 y = diff(log(prices))\n\n 5 #####  get initial estimates #####\n 6 N = length(y)\n\n 7 logy2 = log(y^2)\n\n 8 fitar = lm(logy2[2:N] ~ logy2[1:(N - 1)])\n\n 9 beta0Init = as.numeric(fitar$coef[1])\n\n10 phiInit = as.numeric(fitar$coef[2])\n\n11 sfitar = summary(fitar)\n\n12 tauInit = 1/sfitar$sigma^2\n\n13 #####  Set up for MCMC #####\n14 N = length(y)\n\n15 data = list(y = y, N = N)\n\n16 inits_stochVol_ARMA11 = function(){list(mu = rnorm(1, mean = mean(y),\n\n17    sd = sd(y) / sqrt(N)), logh = log(y^2),\n18    beta0 = runif(1, beta0Init * 1.5, beta0Init/1.5),\n19    phi = runif(1,phiInit / 1.5, phiInit * 1.5),\n20    tau_v = runif(1, tauInit / 1.5, tauInit * 1.5),\n21    theta = runif(1, -0.5, 0.5))}\n22 stochVol_ARMA11 <- jags.model(\"stochVol_ARMA11.bug\", data = data,\n\n23    inits = inits_stochVol_ARMA11,\n24    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n25 nthin = 20\n\n26 stochVol_ARMA.coda = coda.samples(stochVol_ARMA11, c(\"mu\", \"beta0\",\n\n27    \"phi\", \"theta\", \"tau_v\", \"nu\", \"tau\"), 100 * nthin, thin = nthin)\n28 summ_stochVol_ARMA11 = summary(stochVol_ARMA.coda)\n\n29 head(summ stochVol_ARMA11[[1]], 8)\n\n30 tail(summ stochVol_ARMA11[[1]], 8)\n\n31 dic.stochVol_ARMA11 = dic.samples(stochVol_ARMA11, 100 * nthin,\n\n32    thin = nthin, type = \"pD\")\n33 dic.stochVol_ARMA11\n\n\n\nLines 8\u201312 compute rough initial values for \u03b20 , \u03c6, and \u03c3v by using Yt2 a proxy\nfor ht and regressing log(Yt2 ) on log(Yt\u22121\n                                          2\n                                            ).\n    Since nearly 1000 variables are monitored, we do not want to look at the\noutput for all of them. Instead, the MCMC output is summarized for \u03b20 , \u03bc,\n\u03c6, \u03b8, \u03c4v , and the \ufb01rst and last few hi .\n\n    > head(summ_stochVol_ARMA11[[1]], 8)\n                    Mean           SD     Naive SE Time-series SE\n    beta0 -1.327118e+01 6.986557e+00 4.033691e-01    2.956974e+00\n    mu      8.977085e-04 2.290611e-04 1.322485e-05   1.197846e-05\n    nu      2.116160e+01 5.785812e+00 3.340440e-01   3.884508e-01\n    phi     3.651321e-05 2.254858e-02 1.301843e-03   8.364831e-03\n    tau[1] 2.379568e+05 4.180067e+05 2.413363e+04    2.413332e+04\n    tau[2] 6.815322e+04 5.764209e+04 3.327968e+03    3.933972e+03\n    tau[3] 6.472217e+04 5.089951e+04 2.938685e+03    3.247533e+03\n\f626      20 Bayesian Data Analysis and MCMC\n\n      tau[4] 6.030660e+04 4.190914e+04 2.419625e+03    2.815956e+03\n      > tail(summ_stochVol_ARMA11[[1]], 8)\n                       Mean           SD     Naive SE Time-series SE\n      tau[959] 1.328635e+04 7.236487e+03 4.177988e+02   4.432135e+02\n      tau[960] 1.468882e+04 8.124815e+03 4.690864e+02   5.062925e+02\n      tau[961] 1.318162e+04 7.091475e+03 4.094265e+02   3.959164e+02\n      tau[962] 1.514622e+04 8.918848e+03 5.149299e+02   5.163099e+02\n      tau[963] 1.574039e+04 1.065051e+04 6.149072e+02   6.165450e+02\n      tau[964] 1.403511e+04 8.598674e+03 4.964447e+02   5.021540e+02\n      tau_v    5.161879e+00 1.409690e+00 8.138847e-02   1.765265e-01\n      theta    4.819691e-01 1.219201e-02 7.039058e-04   3.262414e-03\n\n      > dic.stochVol_ARMA11\n      Mean deviance: -6653\n      penalty ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.10",
      "section_title": "Stochastic Volatility Models   625",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "head(summ_stochVol_ARMA11[[1]], 8)\n                    Mean           SD     Naive SE Time-series SE\n    beta0 -1.327118e+01 6.986557e+00 4.033691e-01    2.956974e+00\n    mu      8.977085e-04 2.290611e-04 1.322485e-05   1.197846e-05\n    nu      2.116160e+01 5.785812e+00 3.340440e-01   3.884508e-01\n    phi     3.651321e-05 2.254858e-02 1.301843e-03   8.364831e-03\n    tau[1] 2.379568e+05 4.180067e+05 2.413363e+04    2.413332e+04\n    tau[2] 6.815322e+04 5.764209e+04 3.327968e+03    3.933972e+03\n    tau[3] 6.472217e+04 5.089951e+04 2.938685e+03    3.247533e+03\n\f626      20 Bayesian Data Analysis and MCMC",
        "start": 1801,
        "end": 2412
      },
      {
        "language": "r",
        "code": "tail(summ_stochVol_ARMA11[[1]], 8)\n                       Mean           SD     Naive SE Time-series SE\n      tau[959] 1.328635e+04 7.236487e+03 4.177988e+02   4.432135e+02\n      tau[960] 1.468882e+04 8.124815e+03 4.690864e+02   5.062925e+02\n      tau[961] 1.318162e+04 7.091475e+03 4.094265e+02   3.959164e+02\n      tau[962] 1.514622e+04 8.918848e+03 5.149299e+02   5.163099e+02\n      tau[963] 1.574039e+04 1.065051e+04 6.149072e+02   6.165450e+02\n      tau[964] 1.403511e+04 8.598674e+03 4.964447e+02   5.021540e+02\n      tau_v    5.161879e+00 1.409690e+00 8.138847e-02   1.765265e-01\n      theta    4.819691e-01 1.219201e-02 7.039058e-04   3.262414e-03",
        "start": 2486,
        "end": 3145
      },
      {
        "language": "r",
        "code": "dic.stochVol_ARMA11\n      Mean deviance: -6653\n      penalty",
        "start": 3151,
        "end": 3214
      }
    ]
  },
  {
    "content": "119.6\n      Penalized deviance: -6533\n\n DIC and pD are called the \u201cpenalized deviance\u201d and the \u201cpenalty\u201d in\nthe output of dic.samples() and in this example are \u22126536 and 127.3,\nrespectively.                                                      \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "119.6",
      "section_title": "Penalized deviance: -6533",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.11 Fitting GARCH Models with MCMC\nLike stochastic volatility models, GARCH models are easy to \ufb01t by MCMC.\nOne reason for \ufb01tting a GARCH model with BUGS is that this model can then\nbe compared using DIC with a stochastic volatility model that is also \ufb01t using\nBUGS.\n   The following BUGS program \ufb01ts a GARCH(1,1) model with t-distributed\nnoise. This program runs under JAGS but crashes with no useful error messages\nunder OpenBUGS and WinBUGS. The model is\n\n                         yt = \u03bc + a t\n                              \u0011\n                         a t = ht \u0017 t\n                         ht = \u03b10 + \u03b11 a2t\u22121 + \u03b21 ht\u22121\n                         \u0017t \u223c t\u03bd (0, 1).\n\n1  model{\n2  for (t in 1:N)\n 3 {\n\n 4   y[t] ~ dt(mu, tau[t], nu)\n 5   a[t] <- y[t] - mu\n 6   tau[t] <- 1/h[t]\n 7 }\n\n 8 for (t in 2:N)\n\n 9 {\n\n10   h[t] <- alpha0 + alpha1 * pow(a[t-1], 2) + beta1 * h[t-1]\n\f                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.11",
      "section_title": "Fitting GARCH Models with MCMC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.11 Fitting GARCH Models with MCMC     627\n\n11 }\n12 mu ~ dnorm(0, 0.001)\n13 h[1] ~ dunif(0, 0.0012)\n\n14 alpha0 ~ dunif(0, 0.2)\n\n15 alpha1 ~ dunif(0.00001, 0.8)\n\n16 beta0 ~ dunif(0.00001, 0.8)\n\n17 nu ~ dunif(1,30)\n\n18 }\n\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.11",
      "section_title": "Fitting GARCH Models with MCMC     627",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15. Fitting a GARCH(1,1) model to the S&P 500 stock returns\n\nThe following R code \ufb01ts a GARCH(1,1) model to the data in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "Fitting a GARCH(1,1) model to the S&P 500 stock returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14.\n1  library(rjags)\n2  dat = read.csv(\"S&P500_new.csv\")\n 3 prices = dat$Adj.Close\n\n 4 y = diff(log(prices))\n\n 5 N = length(y)\n\n 6 data = list(y = y, N = N)\n\n 7 inits_garch11 = function(){list(alpha0 = runif(1, 0.001, 0.25),\n\n 8     beta1 = runif(1, 0.001, 0.25), mu = runif(1, 0.001, 0.25),\n 9     alpha1 = runif(1, 0.001, 0.25), nu = runif(1, 2, 10))}\n10 garch11 <- jags.model(\"garch11.bug\", data=data,\n\n11     inits = inits_garch11,\n12     n.chains = 3, n.adapt = 1000, quiet = FALSE)\n13 nthin = 20\n\n14 garch11.coda = coda.samples(garch11,c(\"mu\", \"beta1\", \"alpha0\",\n\n15    \"alpha1\", \"nu\", \"tau\"), 100*nthin, thin = nthin)\n16 dic.garch11 = dic.samples(garch11, 100*nthin, thin = nthin)\n\n17 dic.garch11\n\n18 diffdic(dic.garch11, dic.stochVol_ARMA11)\n\n19 summ_garch11 = summary(garch11.coda)\n\n20 head(summ_garch11[[1]])\n\n21 tail(summ_garch11[[1]])\n\n\nThe output is below.\n     > dic.garch11\n     Mean deviance: -6508\n     penalty ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "1  library(rjags)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "dic.garch11\n     Mean deviance: -6508\n     penalty",
        "start": 879,
        "end": 932
      }
    ]
  },
  {
    "content": "5.737\n     Penalized deviance: -6502\n     > diffdic(dic.garch11, dic.stochVol_ARMA11)\n     Difference: ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.737",
      "section_title": "Penalized deviance: -6502",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "diffdic(dic.garch11, dic.stochVol_ARMA11)\n     Difference:",
        "start": 42,
        "end": 103
      }
    ]
  },
  {
    "content": "30.90573\n     Sample standard error: 15.12843\n     > head(summ_garch11[[1]])\n                    Mean           SD     Naive SE Time-series SE\n     alpha0 3.875413e-06 9.405668e-07 5.430365e-08   6.060383e-08\n     alpha1 1.287614e-01 2.170856e-02 1.253344e-03   1.465383e-03\n     beta1 7.677071e-01 2.621658e-02 1.513615e-03    1.870831e-03\n     mu     8.663882e-04 2.290809e-04 1.322599e-05   1.247927e-05\n\f628      20 Bayesian Data Analysis and MCMC\n\n                                                                      Stoch. vol\n\n\n\n                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "30.90573",
      "section_title": "Sample standard error: 15.12843",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "head(summ_garch11[[1]])\n                    Mean           SD     Naive SE Time-series SE\n     alpha0 3.875413e-06 9.405668e-07 5.430365e-08   6.060383e-08\n     alpha1 1.287614e-01 2.170856e-02 1.253344e-03   1.465383e-03\n     beta1 7.677071e-01 2.621658e-02 1.513615e-03    1.870831e-03\n     mu     8.663882e-04 2.290809e-04 1.322599e-05   1.247927e-05\n\f628      20 Bayesian Data Analysis and MCMC",
        "start": 51,
        "end": 453
      }
    ]
  },
  {
    "content": "0.030\n                                                                      GARCH\n\n          conditional std. dev.\n                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.030",
      "section_title": "GARCH",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\n                                  0.010\n                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.020",
      "section_title": "0.010",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n\n\n\n\n                                          2011   2012   2013   2014\n                                                        year\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "2011   2012   2013   2014",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.8. The conditional standard deviations of the log returns estimated by the\nstochastic volatility model in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.8",
      "section_title": "The conditional standard deviations of the log returns estimated by the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14 and the GARCH(1,1) model.\n\n\n      nu     7.570399e+00 1.990408e+00 1.149163e-01   1.077795e-01\n      tau[1] 7.432585e+04 1.242142e+05 7.171511e+03   7.376238e+03\n      > tail(summ_garch11[[1]])\n                   Mean       SD Naive SE Time-series SE\n      tau[959] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "and the GARCH(1,1) model.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "tail(summ_garch11[[1]])\n                   Mean       SD Naive SE Time-series SE\n      tau[959]",
        "start": 174,
        "end": 272
      }
    ]
  },
  {
    "content": "10765.65 1050.613 60.65714        56.26545\n      tau[960] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10765.65",
      "section_title": "1050.613 60.65714        56.26545",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12495.45 1207.416 69.71019        64.84224\n      tau[961] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12495.45",
      "section_title": "1207.416 69.71019        64.84224",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "15145.85 1500.491 86.63091        81.57595\n      tau[962] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "15145.85",
      "section_title": "1500.491 86.63091        81.57595",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14266.32 1339.490 77.33549        72.44932\n      tau[963] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14266.32",
      "section_title": "1339.490 77.33549        72.44932",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17144.03 1653.541 95.46724        90.31518\n      tau[964] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17144.03",
      "section_title": "1653.541 95.46724        90.31518",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "19112.62 1869.545 107.93821      107.54300\n\n   Line 18 of the R code uses the function diffdic() in the rjags package to\ncompare the stochastic volatility and GARCH models by DIC. In the output\nfrom diffdic(), we see that DIC for the GARCH model is larger than for\nthe stochastic volatility model; the di\ufb00erence between the two DIC values is\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "19112.62",
      "section_title": "1869.545 107.93821      107.54300",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "30.9 but with a standard error of 15.1. Figure 20.8 compares the estimates of\nthe conditional standard deviations of the log returns by the two models. Fig-\nure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "30.9",
      "section_title": "but with a standard error of 15.1. Figure 20.8 compares the estimates of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9 compares the ACF\u2019s of the squared standardized residuals4 from the\ntwo models and also assuming that the log returns are i.i.d. Despite the large\ndi\ufb00erence in DIC, the two models are about equally successful in modeling\nthe conditional heteroscedasticity.\n                                                                            \u0002\n\n\n\n\n4\n    A residual is standardized by dividing it by its conditional standard deviation.\n\f                                                                                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "compares the ACF\u2019s of the squared standardized residuals4 from the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12 Fitting a Factor Model                            629\n\na           Constant conditional variance      b                Stochastic Volatility          c                    GARCH\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "Fitting a Factor Model                            629",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                     1.0\n\n\n\n\n                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                                     0.8\n\n\n\n\n                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n      0.8\n\n\n\n\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n\n\n\n\n                                                                                                     0.6\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\nACF\n\n\n\n\n                                               ACF\n\n\n\n\n                                                                                               ACF\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                                     0.4\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                     0.2\n\n\n\n\n                                                                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n      0.2\n\n\n\n\n                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                                     0.0\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n             0   5   10   15    20   25   30                0    5   10   15    20   25   30               0   5   10   15    20   25    30\n\n                          Lag                                             Lag                                           Lag\n\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0   5   10   15    20   25   30                0    5   10   15    20   25   30               0   5   10   15    20   25    30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.9. ACF\u2019s of the squared standardized residuals. (a) Assuming a constant\nconditional standard deviation. The model is Rt = \u03bc + t where t is independent\nwhite noise. (b) Assuming an ARMA(1,1) stochastic volatility model. (c) Assuming\na GARCH(1,1) model.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.9",
      "section_title": "ACF\u2019s of the squared standardized residuals. (a) Assuming a constant",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12 Fitting a Factor Model\nFactor models can be \ufb01t easily using JAGS. An advantage of a Bayesian analysis\nof a factor model is that a hierarchial model can be used to shrink the betas\ntowards each other.\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "Fitting a Factor Model",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16. Fitting a one factor model to stock returns\n\nIn this example, the factor will be the returns on the S&P 500 so this is a\nBayesian version of the CAPM. The model that will be used is\n\n                                                     Rj,t = \u03b2j RM,t + \u0017j,t .\n\nwhere, as in Chap. 17, Rj,t is the return on the j stock at time t, j = 1, . . . , 10,\nand RM,t is the return on the S&P 500 at time t. It is assumed that for\nj = 1, . . . , 10, {\u0017j,t , t = 1, . . .} are mutually independent i.i.d. white noise\nprocesses with var(\u0017j,t ) = \u03c3 2,j . For simplicity we have assumed that all the\nalphas are zero.\n   We will put a hierarchical on \u03b21 , . . . , \u03b210 , speci\ufb01cally\n\n                                                           \u03b2j \u223c N (\u03bc\u03b2 , \u03c3\u03b22 ),\n\nwith non-informative priors on \u03bc\u03b2 and \u03c3\u03b22 .\n    The BUGS program is below. At line 11, \u03bc\u03b2 is called meanbeta and \u03c3\u03b2\u22122 is\ncalled taubeta. Also, tauepsilon on line 6 is \u03c3 \u22122 .\n1 model{\n2 for (t in 1:N)\n3   {\n4   for (j in 1:m)\n\f630       20 Bayesian Data Analysis and MCMC\n\n5     {\n6             R[t,j] ~ dnorm(beta[j]*mkt[t], tauepsilon[j])\n7         }\n8     }\n9  for (j in 1:m)\n10 {\n\n11   beta[j] ~ dnorm(meanbeta, taubeta)\n12   tauy[j] ~ dgamma(0.1, 0.001)\n13 }\n\n14 meanbeta ~ dnorm(1, 0.000001)\n\n15 taubeta ~ dunif(1, 100)\n\n16 }\n\n\nThis example is a continuation of Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "Fitting a one factor model to stock returns",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.11 in that it uses the stock\nprice data in the \ufb01le Stock Bond.csv. Since there are nearly 5000 days of\nreturns, one can create 20 blocks of returns, each block with 250 days except\nthat the last block would be somewhat short of 250. Each block can be used\nfor training (parameter estimation) and the next block for testing.\n    The R program below illustrates this strategy using only the \ufb01rst two\nblocks, the \ufb01rst as training data and the second as test data. During training,\nthe optimal allocation vector w is estimated. Here \u201coptimal\u201d means maximiz-\ning the expected utility of the returns using the utility function\n\n                         U (R; \u03bb) = 1 \u2212 exp{\u2212\u03bb(1 + R)}.                 (20.64)\n\nThe value of \u03bb is set equal to 3 at line 38 of the R code. A value of \u03bb in\nthe range 2 to 8 is reasonable since these are daily returns and typically in\nthe range \u00b1",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.11",
      "section_title": "in that it uses the stock",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05. Also, in (20.64) we are implicitly assuming that the initial\nwealth is equal to 1, since the initial wealth is not explicitly included there.\n   The \ufb01rst part of the R program \ufb01ts the factor model:\n1  library(rjags)\n2  dat = read.csv(\"Stock_Bond.csv\")\n 3 y = dat[, c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 22)]\n\n 4 n = dim(y)[1]\n\n 5 m = dim(y)[2] - 1\n\n 6 r = y[-1, ] / y[-n, ] - 1\n\n 7 k1 = 250\n\n 8 k2 = k1 + 250\n\n 9 rtrain = r[1:k1, 1:m]\n\n10 mkt_train = r[1:k1, 11]\n\n11 rtest = r[(k1+1):k2, 1:m]\n\n12 data = list(R = rtrain, N = k1, mkt = mkt_train, m = m)\n\n13 inits.Capm = function(){list(beta = rep(1,m))}\n\n14 Capm.jags <- jags.model(\"BayesCapm.bug.R\", data = data,\n\n15    inits = inits.Capm, n.chains = 1, n.adapt = 1000, quiet = FALSE)\n16 nthin = 10\n\n17 N = 500\n\n18 Capm.coda = coda.samples(Capm.jags,\n\f                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "Also, in (20.64) we are implicitly assuming that the initial",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.12 Fitting a Factor Model     631\n\n19    c(\"beta\", \"tauepsilon\", \"taubeta\"),\n20    N * nthin, thin = nthin)\n21 MCMC_out = Capm.coda[[1]]\n\n22 summ = as.matrix(summary(Capm.coda)[[1]][,1])\n\n23 beta = summ[1:10]\n\n24 taubeta = summ[11]\n\n25 tauy = summ[12:21]\n\n26 sigmaepsilon = tauepsilon^(-.5)\n\n\nThe next section of R code de\ufb01nes the utility function and \ufb01nds the optimal\nallocation vector w by using quadratic programming as in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.12",
      "section_title": "Fitting a Factor Model     631",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.11. The\nvector w is found twice, once using the sample mean vector and covariance\nmatrix of the training sample returns to estimate \u03bc and \u03a3 in Eq. (16.20)\n(model-free) and once using the factor model to estimate \u03bc and \u03a3 (CAPM).\n27 ExUtil = function(w)\n28 {\n29   -1 + exp(-lambda * (1 + t(w) %*% mu) +\n30   lambda^2 * t(w) %*% Omega %*% w / 2 )\n31 }\n\n32\n\n33 mu_model_free = colMeans(rtrain)\n34 Omega_model_free = cov(rtrain)\n35 mu_Capm = beta * mean(mkt)\n\n36 Omega_Capm = beta %o% beta * var(mkt_train) + diag(sigmaepsilon^2)\n\n37\n\n38 lambda = 3\n39 library(quadprog)\n40 mu = mu_model_free\n\n41 Omega = Omega_model_free\n\n42 opt1 = solve.QP(Dmat = as.matrix(lambda^2 * Omega),\n\n43    dvec = lambda * mu, Amat = as.matrix(rep(1,10)),\n44    bvec = 1, meq = 1)\n45 w_model_free = opt1$solution\n\n46\n\n47 mu = mu_Capm\n48 Omega = Omega_Capm\n49 opt2 = solve.QP(Dmat = as.matrix(lambda^2 * Omega),\n\n50    dvec = lambda * mu, Amat = as.matrix(rep(1,10)),\n51    bvec = 1, meq = 1)\n52 w_Capm = opt2$solution\n\n\nNext, the utility of the portfolio\u2019s returns is averaged over the test data using\nthe model-free and CAPM based estimates of the optimal portfolio. Also, the\nmean and standard deviations of the portfolio\u2019s returns on the test data are\ncomputed.\n53   return_model_free = as.matrix(rtest) %*% w_model_free\n54   ExUt_model_free = mean(1 - exp(-lambda * return_model_free))\n55\n\f632      20 Bayesian Data Analysis and MCMC\n\n56   return_Capm = as.matrix(rtest) %*% w_Capm\n57   ExUt_Capm = mean(1 - exp(-lambda * return_Capm))\n58\n\n59 print(c(ExUt_model_free, ExUt_Capm), digits = 2)\n60 print(c(mean(return_model_free), mean(return_Capm)), digits = 2)\n61 print(c(sd(return_model_free), sd(return_Capm)), digits = 2)\n\n\nThe output is below. We see that the portfolio selected using the CAPM\nestimates outperformed the portfolio based on the sample mean vector and\ncovariance matrix. Interestingly, the CAPM selected portfolio not only has a\nhigher average utility, but it also has both a higher mean and a lower stan-\ndard deviation of the returns compared to the model-free estimates. Usually\na higher expected return comes with higher risk (larger standard deviation),\nbut a better estimate can achieve a higher return without higher risk.\n62 > print(c(ExUt_model_free, ExUt_Capm), digits = 2)\n63 [1] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.11",
      "section_title": "The",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "print(c(ExUt_model_free, ExUt_Capm), digits = 2)\n63 [1] -",
        "start": 2230,
        "end": 2289
      }
    ]
  },
  {
    "content": "0.0179 0.0023\n64 > print(c(mean(return_model_free), mean(return_Capm)), digits = 2)\n\n65 [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0179",
      "section_title": "0.0023",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "print(c(mean(return_model_free), mean(return_Capm)), digits = 2)",
        "start": 17,
        "end": 85
      }
    ]
  },
  {
    "content": "0.00060 0.00099\n\n66 > print(c(sd(return_model_free), sd(return_Capm)), digits = 2)\n\n67 [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00060",
      "section_title": "0.00099",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "print(c(sd(return_model_free), sd(return_Capm)), digits = 2)",
        "start": 20,
        "end": 84
      }
    ]
  },
  {
    "content": "0.067 0.012\n\n\n    These results are based on only one block of training data and one block\nof test data, and by themselves they are not a convincing demonstration of\nthe superiority of CAPM estimates. The analysis could be continued using all\ntwenty blocks of data; see Exercise 4.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.067",
      "section_title": "0.012",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.10 plots the allocation vectors, w, using model-free and CAPM-\nbased estimators. The model-free w oscillates widely and has substantial\nshort-selling. In contrast, the CAPM-based w is much closer to assigning\nequal weights to the 10 stocks and has minimal short selling.\n    Another issue is how hierarchical Bayes estimation of the CAPM compares\nwith ordinary least-squares estimation. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.10",
      "section_title": "plots the allocation vectors, w, using model-free and CAPM-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.11 plots the least-squares\nestimates of \u03b2 versus the Bayes estimates. With the exception of the largest\nbeta, the Bayes estimates are only slightly shrunk together and are similar\nto the least-squares estimates. This suggest that the Bayes estimates will,\nat best, be only a moderate improvement over least-squares in this example.\nMost of the improvement is due to using the CAPM to estimate the expected\nreturns.                                                                   \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.11",
      "section_title": "plots the least-squares",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.13 Sampling a Stationary Process\n                                                                       +\nThis section provides the theory behind the statistics B, W , and var\u001f (\u03c8|Y )\nused in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.13",
      "section_title": "Sampling a Stationary Process",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5 to monitor MCMC convergence and mixing.\n   Suppose that Y1 , Y2 , . . . , Yn is a sample from a stationary\n                                                          \u0017n       process with\nmean \u03bc and autocovariance function \u03b3(h). Let Y = n\u22121 i=1 Yi be the sample\nmean. Then\n\f                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5 to monitor MCMC convergence and mixing.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.13 Sampling a Stationary Process          633\n\n                                                    lambda = 3\n\n\n\n                       2\n                       1\n\n\n\n                                         *            *\n                             *     *                              *     *\n        w\n\n\n\n\n                                                                                      *\n                       0\n\n\n\n\n                                              *                             *             *\n                       \u22121\n\n\n\n\n                                 model\u2212free\n                       \u22122\n\n\n\n\n                             *   CAPM\n\n                                   2          4               6             8             10\n                                                          Index\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.13",
      "section_title": "Sampling a Stationary Process          633",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.10. Allocation (or weight) vectors using the sample mean vector and co-\nvariance matrix (model-free) and CAPM-based estimates.\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.10",
      "section_title": "Allocation (or weight) vectors using the sample mean vector and co-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.4\n        beta \u2212 Bayes\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.4",
      "section_title": "beta \u2212 Bayes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.2\n                       1.0\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.2",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                  0.8         1.0                 1.2           1.4\n                                              beta \u2212 least squares\n\n Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.8         1.0                 1.2           1.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.11. Plot of the Bayes estimator of \u03b2 versus the least-squares estimator.\n\f634    20 Bayesian Data Analysis and MCMC\n                                       n        n\n                                  \u22122\n                 var(Y ) = n                            Cov(Yi , Yj )\n                                       i=1 j=1\n                                        n   n\n                            = n\u22122                       \u03b3(i \u2212 j)\n                                       i=1 j=1\n                                       \u0012                      n\u22121\n                                                                                  \u0014\n                                  \u22122\n                            =n             n\u03b3(0) + 2                \u03b3(h)(n \u2212 h)\n                                                              h=1\n                           \u03b3(0)\n                            =   Rn ,                                  (20.65)\n                            n\n              (      \u0017n\u22121       -     .)\nwhere Rn = 1 + 2 h=1 \u03c1(h) 1 \u2212 nh . If Y1 , Y2 , . . . , Yn is an uncorrelated\nprocess (white noise), then Rn = 1 and (20.65) agrees with (7.13).\n   Most stationary processes generated by MCMC have \u03c1(h) \u2265 0 for all h\nso that Rn is in\ufb02ated by the autocorrelation. The in\ufb02ation can be severe.\nConsider the case of a stationary AR(1) process, Yn = \u03c6Yn1 + \u0017i . AR(1)\nprocesses often are reasonably good approximations to MCMC processes. For\nan AR(1) process we can approximate Rn :\n           \u0012       \u221e\n                          \u0014 \u0012 \u221e             \u0014 \u0007               \b\n                                                    2             1+\u03c6\n    Rn \u2248 1 + 2        \u03c1(h) = 2        \u03c6 \u22121 =\n                                       h\n                                                          \u22121 =          ,\n                                                  1\u2212\u03c6             1\u2212\u03c6\n                  h=1             h=0\n                                                                      (20.66)\nwhere we have used summation formula for geometric series (3.4) with T = \u221e.\nNotice that the right-hand side of (20.66) increases without bound as \u03c6 \u2192 1.\n   From the identity\n                   n                        n\n                        (Yi \u2212 \u03bc)2 =              (Yi \u2212 Y )2 + n(Y \u2212 \u03bc)2 ,\n                  i=1                      i=1\n\nwe obtain                   \u0012 n                         \u0014\n                        E          (Yi \u2212 Y )        2\n                                                            = \u03b3(0)(n \u2212 Rn )           (20.67)\n                             i=1\n\nsince \u03b3(0) = E (Yi \u2212 \u03bc)2 and \u03b3(0)Rn = E n(Y \u2212 \u03bc)2 by de\ufb01nitions.\nTherefore, an unbiased estimate of the process variance \u03b3(0) is\n                                 \u0017n\n                                       (Yi \u2212 Y )2\n                          \u0002(0) = i=1\n                          \u03b3                       .             (20.68)\n                                      n \u2212 Rn\nWhen the process is uncorrelated so that Rn = 1, the right-hand side of (20.68)\nis the sample variance (A.7). For positively autocorrelated processes, Rn > 1\nand the sample variance (which uses 1 in place of Rn ) is biased downward.\n    To obtain an unbiased estimate of \u03b3(0), one can use\n                         \u0017n                  \u0003\n                            i=1 (Yi \u2212 Y ) + \u03b3(0)Rn\n                                         2\n                                                   ,                    (20.69)\n                                      n\n\f                                               ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.11",
      "section_title": "Plot of the Bayes estimator of \u03b2 versus the least-squares estimator.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1\nand the sample variance (which uses 1 in place of Rn ) is biased downward.\n    To obtain an unbiased estimate of \u03b3(0), one can use\n                         \u0017n                  \u0003\n                            i=1 (Yi \u2212 Y ) + \u03b3(0)Rn\n                                         2\n                                                   ,                    (20.69)\n                                      n",
        "start": 3078,
        "end": 3522
      }
    ]
  },
  {
    "content": "20.14 Bibliographic Notes    635\n\n        \u0003n is an unbiased estimator of \u03b3(0)Rn . There are several methods\nwhere \u03b3(0)R\nfor estimating \u03b3(0)Rn . The simplest uses several independent realizations of\nthe process. Let Y 1 , . . . , Y M be\n                                   \u0017m the means of M independent realizations of\nthe process and let Y = M \u22121 j=1 Y j . Then\n                                         \u0017M\n                                             (Y \u2212 Y )2\n                                \u0003n = j=1 j\n                               \u03b3(0)R                                     (20.70)\n                                             M \u22121\n                                                    +\n                                                   \u001f (\u03c8|Y ) used in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "Bibliographic Notes    635",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5\nis an unbiased estimator of \u03b3(0)Rn . The statistic var\nfor MCMC monitoring is a special case of (20.68) and (20.70).\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.14 Bibliographic Notes\nThere are many excellent books on Bayesian statistics. Gelman, Carlin, Stern,\nDunson, Vehtari, and Rubin (2013) and Carlin and Louis (2008) are introduc-\ntions to Bayesian statistics written at about the same mathematical level as\nthis book. Box and Tiao (1973) is a classic work on Bayesian statistics with\na wealth of examples and still worth reading despite its age. Berger (1985)\nis a standard reference on Bayesian analysis and decision theory. Bernardo\nand Smith (1994) and Robert (2007) are more recent books on Bayesian the-\nory. Rachev, Hsu, Bagasheva, and Fabozzi (2008) covers many applications of\nBayesian statistics to \ufb01nance.\n    Albert (2007) is an excellent introduction to Bayesian computations in R.\nChib and Greenberg (1995) explain how the Metropolis\u2013Hastings algorithm\nworks and why its stationary distribution is the posterior. Congdon (2001,\n2003) covers the more recent developments in Bayesian computing with an\nemphasis on OpenBUGS software. There are other Bayesian Monte Carlo sam-\nplers besides MCMC, for example, importance sampling. Robert and Casella\n(2005) discuss these as well as MCMC. Gelman et al., (2013) have examples\nof Bayesian computations in R and OpenBUGS in an appendix. Lunn, Thomas,\nBest, and Spiegelhalter (2000) describe the design of OpenBUGS. Lunn, Jack-\nson, Best, and Spiegelhalter (2013) is a comprehensive introduction to BUGS.\n    The diagnostics R\u0002 and Ne\ufb00 are due to Gelman and Rubin (1992) though\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.14",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.5 uses the somewhat di\ufb00erent notation of Gelman et al., (2013).\nSpiegelhalter, Best, Carlin, and van der Linde (2002) proposed DIC and pD .\nThe Gelman plot was introduced by Brooks and Gelman (1998). Kass et al.\n(1998) discuss their practical experiences with MCMC.\n    Bayesian modeling of yield curves models is discussed by Chib and Erga-\nshev (2009). Bayesian time series are discussed by Albert and Chib (1993),\nChib and Greenberg (1994), and Kim, Shephard, and Chib (1998); the \ufb01rst\ntwo papers cover ARMA process and the last covers ARCH and stochastic\nvolatility models. There is a vast literature on the important and di\ufb03cult prob-\nlem of Bayesian estimation of covariance matrices with nonconjugate priors.\nDaniels and Kass (1999) review some of the literature in addition to providing\ntheir own suggestions.\n\f636      20 Bayesian Data Analysis and MCMC\n\n    We have not discussed empirical Bayes inference, but Carlin and Louis\n(2000) can be consulted for an introduction to that literature. Empirical Bayes\ninference uses a hierarchical prior but estimates the parameters in the lower\nlevel in a non-Bayesian manner and then, treating those parameter as known\nand \ufb01xed, performs a Bayesian analysis. The result is shrinkage estimation\nmuch like that achieved by a Bayesian analysis. The advantage of an empir-\nical Bayes analysis is that it can be somewhat simpler than a fully Bayesian\nanalysis. The disadvantage is that it underestimates uncertainty because es-\ntimated parameters in the prior are treated as if they were known. There are\nshrinkage estimators that are not exactly Bayesian or even empirical Bayes\nprocedures. Ledoit and Wolf (2003) propose a shrinkage estimator for the co-\nvariance matrix of stock returns. Their shrinkage target is an estimate from\na factor model, for example, the CAPM. Shrinkage estimation goes back at\nleast to Stein (1956) and is often called Stein estimation.\n    The central limit theorem for the posterior is discussed by Gelman et al.\n(2013), Lehmann (1983), and van der Vaart (1998), in increasing order of\ntechnical level.\n    See Greyserman, Jones, and Strawderman (2006) for more information on\nportfolio selection by Bayesian methods.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "5 uses the somewhat di\ufb00erent notation of Gelman et al., (2013).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15.1 Fitting a t-Distribution by MCMC\n\nIn this section of the lab, you will \ufb01t the t-distribution to monthly returns on\nIBM using JAGS to estimate the posterior distribution by MCMC sampling.\n    Run the following R code to load the rjags package, input the data, and\nprepare the data for use by JAGS.\n      library(rjags)\n      data(CRSPmon, package = \"Ecdat\")\n      ibm = CRSPmon[ , 2]\n      r = ibm\n      N = length(r)\n      ibm_data = list(r = r, N = N)\n\nNext, put the following BUGS code in a text \ufb01le. I will assume that you name\nthis \ufb01le univt.bug, though you can use another name provided you make\nappropriate changes in the R code that follows. BUGS code is somewhat similar\nto, but not the same as, R code. For example, in R \u201cdt\u201d is the t-density, but\nin BUGS it is the t-distribution.\n      model{\n      for (t in 1:N)\n      {\n\f                                                             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "1 Fitting a t-Distribution by MCMC",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15 R Lab     637\n\n      r[t] ~ dt(mu, tau, k)\n   }\n   mu ~ dnorm(0.0, 1.0E-6)\n   tau ~ dgamma(0.1, 0.01)\n   nu ~ dunif(2, 50)\n   sigma2 <- (k / (k - 2)) / tau\n   sigma <- sqrt(sigma2)\n   }\n    BUGS programs are di\ufb03cult to debug, so be careful to enter the code exactly\nas it appears here. It has been tested and runs as written, but any error\nwill cause problems. Our experience is that JAGS is better at providing error\nmessages and easier to debug than WinBUGS and OpenBUGS.\n    The BUGS code above provides a description of the statistical model and\nspeci\ufb01es the prior distributions. The model states that the data are i.i.d. from\na t-distribution. The ~ symbol assigns a distribution to a random variable\nso y[i] ~ dt(mu, tau, k) gives the likelihood of the data. Here mu, tau,\nand k are the mean, precision, and degrees of freedom, respectively, of the\nt-distribution. For a t-distribution, the precision is \u03c4 = 1/\u03bb2 where \u03bb is the\nscale parameter. Also, mu ~ dnorm(0.0, 1.0E-6) speci\ufb01es the prior for the\nmean mu to be normal with mean 0 and precision 1.0E-6. The precision of a\nnormal distribution is the reciprocal of its variance, so here the prior variance\nof \u03bc is 1.0E6.\n    The symbol <- is used to assign a value (rather than a distribution) to\na variable. Thus, sigma <- 1/sqrt(tau) makes sigma the scale parameter\nof the t-distribution of the data. In R, \u201c=\u201d can be used in place of \u201c<-\u201d for\nassigning a value to a variable, but this is not true in BUGS. The parameter\nsigma is not needed, but, by de\ufb01ning this variable in the BUGS program, we\ngenerate a sample from its posterior distribution.\n    Next, run the following R code that de\ufb01nes a function inits(). This func-\ntion is used to generate random starting values for the chains.\n   inits = function(){list(mu = rnorm(1, 0, 0.3),\n      tau = runif(1, 1, 10), k = runif(1, 1, 30))}\nThe next code uses the jags.model() and coda.samples() functions in the\nrjags package. Notice that the arguments specify the data, the function to\ncreate initial values of the chains, the \ufb01le containing the BUGS program, the\nparameters to be monitored and returned, the number of chains, the number\nof iterations per chain, the number of iterations to discard as burn-in, the\namount of thinning.\n   univ_t <- jags.model(\"univt.bug\", data = ibm_data,\n     inits = inits, n.chains = 3, n.adapt = 1000, quiet = FALSE)\n   nthin = 2\n   univ_t.coda = coda.samples(univ_t, c(\"mu\", \"tau\", \"k\",\n      \"sigma\"), n.iter = 500 * nthin, thin = nthin)\n\f638      20 Bayesian Data Analysis and MCMC\n\nNext, print and plot the results.\n\n      summary(univ_t.coda)\n      effectiveSize(univ_t.coda)\n      gelman.diag(univ_t.coda)\n\nProblem 1\n(a) Which parameter mixes best according to Ne\ufb00 in the output?\n(b) Which parameter mixes worst according to Ne\ufb00 in the output?\n(c) Give a 95 % posterior interval for the degrees-of-freedom parameter.\n\n\nNext, plot the results to check for convergence to the stationary distribution\n(posterior distribution) using Gelman plots and trace plots.\n\n      gelman.plot(univ_t.coda)\n      par(mfrow = c(2, 2))\n      traceplot(univ_t.coda)\n\nPlotting the ACFs gives much insight into how well the chains are mixing. The\nless autocorrelation, the better. The function autocorr.plot() plots ACFs\nseparately for each chain.\n\n      par(mfrow = c(2, 2))\n      autocorr.plot(univ_t.coda, auto.layout = FALSE)\n\nProblem 2\n(a) Which parameter mixes best and which mixes worse according to the ACF\n    plots? Explain your answers.\n(b) Find the posterior skewness and kurtosis of the degrees of freedom param-\n    eter.\n\n\nThe function densityplot() gives kernel density estimate from each chain.\n\n      library(lattice)\n      densityplot(univ_t.coda)\n\n\nProblem 3 Which posterior densities are most skewed?\n\n\nThe kurtosis of a t-distribution is 3(\u03bd \u2212 2)/(\u03bd \u2212 4) if \u03bd > 4 and is +\u221e if \u03bd \u2264 4.\nVariables in R can have in\ufb01nite values: Inf is +\u221e and -Inf is \u2212\u221e, so R can\nhandle in\ufb01nite values of kurtosis if they occur.\n\f                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "R Lab     637",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "4 and is +\u221e if \u03bd \u2264 4.\nVariables in R can have in\ufb01nite values: Inf is +\u221e and -Inf is \u2212\u221e, so R can\nhandle in\ufb01nite values of kurtosis if they occur.",
        "start": 3821,
        "end": 4029
      }
    ]
  },
  {
    "content": "20.15 R Lab     639\n\nProblem 4 Write R code to compute 1500 MCMC values of the kurtosis.\n(1500 = 3*2000/2; there are 3 chains of length 2000 after burn-in and they\nare thinned to every 2nd iteration.)\n(a) Find the 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "R Lab     639",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99 quantiles of the pos-\n    terior distribution of the kurtosis of IBM returns.\n(b) Estimate the posterior probability that the kurtosis of the distribution of\n    IBM returns is \ufb01nite.\n(c) Compute the 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, and ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.99",
      "section_title": "quantiles of the pos-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.99 quantiles of the\n    bootstrap distribution of the sample kurtosis of IBM. Take 1000 resamples\n    using both a model-free and a model-based bootstrap. Compare the two sets\n    of bootstrap quantiles with the posterior quantiles in (a).\n(d) Compare 90 % bootstrap basic percentile con\ufb01dence intervals for the kur-\n    tosis with the 90 % posterior interval. Which interval is shortest? Why\n    might it be shortest?\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.99",
      "section_title": "quantiles of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15.2 AR Models\n\nIn this section of the lab, you will \ufb01t an AR(1) model to the changes in the\nlog of the GDP. First, run the following code to process the data. Notice that\nthe log-GDP time series is di\ufb00erenced before \ufb01tting.\n 1 library(rjags)\n 2 data(Tbrate, package = \"Ecdat\")\n 3 #  r = the 91-day treasury bill rate\n 4 #  y = the log of real GDP\n 5 #  pi = the inflation rate\n 6 del_dat = diff(Tbrate)\n\n 7 y = del_dat[,2]\n\n 8 N = length(y)\n\n 9 GDP_data=list(y = y, N = N)\n\n\n\n     Next create a \ufb01le called ar1.bug containing the following WinBUGS code.\n 1 model{\n 2 for(i in 2:N){\n 3    y[i] ~ dnorm(mu + phi * (y[i-1] - mu), tau)\n 4 }\n\n 5 mu ~ dnorm(0, 0.00001)\n\n 6 phi ~ dnorm(0, 0.00001)\n\n 7 tau ~ dgamma(0.1, 0.0001)\n\n 8 sigma <- 1/sqrt(tau)\n\n 9 }\n\n\n\nFinally, run the following code to \ufb01t an AR(1) model using JAGS and also\nusing R\u2019s arima() function to compute the MLE, which will be compared\nwith the Bayes estimator.\n\f640    20 Bayesian Data Analysis and MCMC\n\n 1 inits = function(){list(mu = rnorm(1, 0, 2 * sd(y) / sqrt(N)),\n 2    phi = rnorm(1, 0, 0.3), tau = runif(1, 1, 10))}\n 3 ar1 <- jags.model(\"ar1.bug\", data = GDP_data, inits = inits,\n\n 4    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n 5 nthin = 20\n\n 6 ar1.coda = coda.samples(ar1, c(\"mu\", \"phi\", \"sigma\"),\n\n 7    n.iter = 500 * nthin, thin = nthin)\n 8 summary(ar1.coda, digits = 3)\n\n 9 arima(y, order = c(1, 0, 0))\n\n\n\nProblem 5 Construct time series and ACF plots of the parameters phi and\nsigma.\n(a) Do you believe that the MCMC sample size is adequate? Why or why not?\n    Is the burn-in iterations adequate? Why or why not? If you feel that either\n    the number of iterations or the length of the burn-in period is inadequate,\n    then rerun with a larger burn-in period and/or MCMC sample size.\n(b) Compute the MLEs for this model using arima(). How closely do the\n    Bayes estimates and MLEs agree? Could you explain any possible dis-\n    agreement?\n(c) The model in the BUGS program does not assume that the time series is in\n    its stationary distribution. In fact, the model does not even assume that\n    there is a stationary distribution. Explain why.\n(d) Modify the BUGS program to utilize the marginal distribution of y1 , assum-\n    ing that the process starts in its stationary distribution.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "2 AR Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15.3 MA Models\n\nNext you will \ufb01t an MA(1) to simulated data. The function arima.sim() is\nused to create the data.\n 1 library(rjags)\n 2 set.seed(5640)\n 3 N = 600\n\n 4 y = arima.sim(n = N, list(ma = -0.5), sd = 0.4)\n\n 5 y = as.numeric(y) + 3\n\n 6 q = 5\n\n 7 ma.sim_data = list(y = y, N = N, q = q)\n\n 8 inits.ma = function(){list(mu = rnorm(1, mean(y), 2*sd(y)/sqrt(N)),\n\n 9    theta = rnorm(1, -0.05, 0.1), tau = runif(1, 5, 8))}\n\nPut the following BUGS program in the \ufb01le ma1.bug. This program not only\n\ufb01ts the MA(1) model but also predicts q steps ahead; q is an input parameter\nchosen by the user and, from the viewpoint of BUGS, q is part of the data and\nis set equal to 5 in the code above. The predicted values will be included in\nthe output and called ypred.\n\f                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "3 MA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15 R Lab    641\n\n 1 model{\n 2 for (i in 2:N)\n 3 {\n\n 4    w[i] <- y[i] - mu - theta * w[i-1]\n 5 }\n\n 6 w[1] ~ dnorm(0, 0.01)\n\n 7 for (i in 2:N)\n\n 8 {\n\n 9    y[i] ~ dnorm(mu + theta * w[i-1], tau)\n10 }\n\n11 mu ~ dnorm(0, 0.0001)\n\n12 theta ~ dnorm(0, 0.0001)\n\n13 tau ~ dgamma(0.01, 0.0001)\n\n14 sigma <- 1/sqrt(tau)\n\n15 for (i in 1:q)\n\n16 {\n\n17    ypred[i] ~ dnorm(theta * w[N + i - 1], tau)\n18    w[i + N] <- ypred[i] - theta * w[N + i - 1]\n19 }\n\n20 }\n\n\n\nNow run this R code.\n 1 ma1 <- jags.model(\"ma1.bug\", data = ma.sim_data, inits = inits.ma,\n 2    n.chains = 3, n.adapt = 1000, quiet = FALSE)\n 3 nthin = 5\n\n 4 ma1.coda = coda.samples(ma1, c(\"mu\", \"theta\", \"sigma\", \"ypred\"),\n\n 5    n.iter = 500 * nthin, thin = nthin)\n 6 summary(ma1.coda)\n\n\n\nProblem 6\n(a) Do you believe that the MCMC sample size is adequate? Why or why not?\n    If you feel it is inadequate, than rerun JAGS with a larger MCMC sample\n    size. Is the length of the burn-in periods adequate?\n(b) Construct time series and ACF plots of the parameters theta, sigma,\n    ypred[1], and ypred[2]. What do the plots tell us about MCMC mixing\n    and convergence?\n(c) Find a 90 % posterior interval for the next observation after the observed\n    data.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "R Lab    641",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.15.4 ARMA Models\n\nCreate a simulated sample from an ARMA(1,1) process with the following R\ncode.\n     set.seed(5640)\n     N = 600\n\f642      20 Bayesian Data Analysis and MCMC\n\n      y = arima.sim(n = N, list(ar = 0.9, ma = -0.5), sd = 0.4)\n      y = as.numeric(y)\n\nProblem 7 Create BUGS and R code to \ufb01t the ARMA(1,1) model to the sim-\nulated data. Monitor the result to make certain that the MCMC sample size\nis large enough.\n(a) Discuss how well the chains mix and whether the Monte Carlo sample size\n    is adequate.\n(b) Find 99 % posterior intervals for the AR and MA parameters.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.15",
      "section_title": "4 ARMA Models",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16 Exercises\n1. Show in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.2 that the MAP estimator is 6/7.\n2. Verify (20.26).\n3. In the derivation of (20.51), it was stated that \u201c{Y \u2212 E(\u03bc|Y )} and\n   {E(\u03bc|Y ) \u2212 \u03bc} are conditionally uncorrelated given Y .\u201d Verify this state-\n   ment.\n4. Continue the analysis in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.2",
      "section_title": "that the MAP estimator is 6/7.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16. Divide the data into 20 blocks\n   of 250 days each, except that the last block will have only 212 days. Use\n   each of the \ufb01rst 19 blocks as training data with the subsequent block as\n   test data. How does portfolio selection based on the CAPM compare with\n   model-free estimation when averaged over the 19 pairs of training and test\n   data sets?\n5. One of the strength of \ufb01tting models by MCMC using BUGS is that a very\n   wide range of models can be \ufb01t. As an example, in this exercise a regression\n   model with MA(1) errors will be used. For data, use the \ufb01rst 1500 returns5\n   on GM and on the S&P 500 index in the data set Stock Bond.csv. Fit\n   the model\n                            Rt = \u03b2RM,t + \u0017t + \u03b8\u0017t\u22121 .\n   where Rt is the tth return on GM, RM,t is the tth return on the S&P 500,\n   and \u00171 , . . . , \u00171500 are i.i.d. N (0, \u03c3 2 ). Use non-informative priors on \u03b2, \u03b8, and\n   \u03c32 .\n6. Expand the model in Exercise 5 so that \u00171 , . . . is a GARCH(1,1) process.\n   Revise the BUGS and R code of Exercise 5 to \ufb01t this expanded model.\n7. So far we have treated the sample mean vector and covariance matrix as\n   \ufb01xed when considering the risk of a portfolio. Stated di\ufb00erently, estimation\n   risk has been ignored. A methodology for taking risk due to estimation er-\n   ror into account was proposed by Greyserman, Jones, and Strawderman\n   (2006). Assume that the vector of returns R is N (\u03bc, \u03a3) distributed. Let\n   (\u03bc(k) , \u03a3 (k) ), k = 1, . . . , K, be an MCMC sample from the posterior distri-\n   bution of (\u03bc, \u03a3). For each k, let R(k) be N (\u03bc(k) , \u03a3 (k) ) distributed. Then\n 5\n     JAGS had trouble when the full data set was used, probably because there are\n     nearly 5000 latent variables. This problem is likely hardware dependent.\n\f                                                                References      643\n\n   R(1) , . . . , R(K) is a sample from the posterior predictive distribution of R\n   and take uncertainty about \u03bc and \u03a3 into account and\n                                  K\n                           K \u22121         U {X0 (1 + wT R(k) )}                (20.71)\n                                  k=1\n\n   estimates the expected utility if the allocation vector is w. Here, as in\n   Sect. 16.8, X0 is the initial wealth and U is the utility function. Con-\n   tinue the analysis in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "Divide the data into 20 blocks",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16 using the CAPM model and maxi-\n   mize (20.71). Maximizing (20.71) is a nonlinear optimization problem, so\n   a good starting value is essential. As a starting value, use the w found in\n   Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "using the CAPM model and maxi-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.16 that ignores estimation error.\n\n\nReferences\nAlbert, J. (2007) Bayesian Computation with R, Springer, New York.\nAlbert, J. H. and Chib, S. (1993) Bayes inference via Gibbs sampling of au-\n  toregressive time series subject to Markov mean and variance shifts, Journal\n  of Business & Economic Statistics, 11, 1\u201315.\nBerger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis 2nd\n  ed., Springer-Verlag, Berlin.\nBernardo, J. M., and Smith, A. F. M. (1994) Bayesian Theory, Wiley, Chich-\n  ester.\nBox, G. E. P., and Tiao, G. C. (1973) Bayesian Inference in Statistical Anal-\n  ysis, Addison-Wesley, Reading, MA.\nBrooks, S. P. and Gelman, A. (1998) General Methods for Monitoring Con-\n  vergence of Iterative Simulations. Journal of Computational and Graphical\n  Statistics, 7, 434\u2013455.\nCarlin, B. P., and Louis, T. A. (2000) Empirical Bayes: Past, present and\n  future. Journal of the American Statistical Association, 95, 1286\u20131289.\nCarlin, B. , and Louis, T. A. (2008) Bayesian Methods for Data Analysis, 3rd\n  ed., Chapman & Hall, New York.\nChib, S., and Ergashev, B. (2009) Analysis of multifactor a\ufb03ne yield curve\n  models. Journal of the American Statistical Association, 104, 1324\u20131337.\nChib, S., and Greenberg, E. (1994) Bayes inference in regression models with\n  ARMA(p, q) errors. Journal of Econometrics, 64, 183\u2013206.\nChib, S., and Greenberg, E. (1995) Understanding the Metropolis\u2013Hastings\n  algorithm. American Statistician, 49, 327\u2013335.\nCongdon, P. (2001) Bayesian Statistical Modelling, Wiley, Chichester.\nCongdon, P. (2003) Applied Bayesian Modelling, Wiley, Chichester.\nDaniels, M. J., and Kass, R. E. (1999) Nonconjugate Bayesian estimation\n  of covariance matrices and its use in hierarchical models. Journal of the\n  American Statistical Association, 94, 1254\u20131263.\n\f644    20 Bayesian Data Analysis and MCMC\n\nEdwards, W. (1982) Conservatism in human information processing. In Judge-\n  ment Under Uncertainty: Heuristics and Biases, D. Kahneman, P. Slovic,\n  and A. Tversky, ed., Cambridge University Press, New York.\nGelman, A., and Rubin, D. B. (1992) Inference from iterative simulation using\n  multiple sequence (with discussion). Statistical Science, 7, 457\u2013511.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin,\n  D. B. (2013) Bayesian Data Analysis, 3rd ed., Chapman & Hall, London.\nGreyserman, A., Jones, D. H., and Strawderman, W. E. (2006) Portfolio se-\n  lection using hierarchical Bayesian analysis and MCMC methods, Journal\n  of Banking and Finance, 30, 669\u2013678.\nKass, R. E., Carlin, B. P., Gelman, A., and Neal, R. (1998) Markov chain\n  Monte Carlo in practice: A roundtable discussion. American Statistician,\n  52, 93\u2013100.\nKim, S., Shephard, N., and Chib, S. (1998) Stochastic volatility: likelihood\n  inference and comparison with ARCH models.Review of Economic Studies,\n  65, 361\u2013393.\nLedoit, O., and Wolf, M. (2003) Improved estimation of the covariance ma-\n  trix of stock returns with an application to portfolio selection. Journal of\n  Empirical Finance, 10, 603\u2013621.\nLehmann, E. L. (1983) Theory of Point Estimation, Wiley, New York.\nLunn, D., Jackson, C., Best, N., Thomas, A., and Spiegelhalter, D. (2013)\n  The BUGS Book, Chapman & Hall.\nLunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) OpenBUGS\u2014\n  A Bayesian modelling framework: Concepts, structure, and extensibility.\n  Statistics and Computing, 10, 325\u2013337.\nRachev, S. T., Hsu, J. S. J., Bagasheva, B. S., and Fabozzi, F. J. (2008)\n  Bayesian Methods in Finance, Wiley, Hoboken, NJ.\nRobert, C. P. (2007) The Bayesian Choice: From Decision-Theoretic Founda-\n  tions to Computational Implementation, 2nd ed., Springer, New York.\nRobert, C. P., and Casella, G. (2005) Monte Carlo Statistical Methods, 2nd\n  ed., Springer, New York.\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., and van der Linde, A. (2002)\n  Bayesian measures of model complexity and \ufb01t. Journal of the Royal Sta-\n  tistical Society, Series B, Methodological, 64, 583\u2013616.\nStein, C. (1956) Inadmissibility of the usual estimator for the mean of a mul-\n  tivariate normal distribution. In Proceedings of the Third Berkeley Sympo-\n  sium on Mathematical and Statistical Probability, J. Neyman, ed., Univer-\n  sity of California, Berkeley, pp. 197\u2013206, Volume 1.\nvan der Vaart, A. W. (1998) Asymptotic Statistics, Cambridge University\n  Press, Cambridge.\n\f21\nNonparametric Regression and Splines\n\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.16",
      "section_title": "that ignores estimation error.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1 Introduction\nAs discussed in Chap. 9, regression analysis estimates the conditional expec-\ntation of a response given predictor variables. The conditional expectation is\ncalled the regression function and is the best predictor of the response based\nupon the predictor variables, because it minimizes the expected squared pre-\ndiction error.\n    There are three types of regression, linear, nonlinear parametric, and non-\nparametric. Linear regression assumes that the regression function is a lin-\near function of the parameters and estimates the intercept and slopes (re-\ngression coe\ufb03cients). Nonlinear parametric regression, which was discussed\nin Sect. 11.2, does not assume linearity but does assume that the regression\nfunction is of a known parametric form, for example, the Nelson-Siegel model.\nIn this chapter, we study nonparametric regression, where the form of the re-\ngression function is also nonlinear but, unlike nonlinear parametric regression,\nnot speci\ufb01ed by a model but rather determined from the data. Nonparametric\nregression is used when we know, or suspect, that the regression function is\ncurved, but we do not have a model for the curve.\n    There are many techniques for nonparametric regression, but local poly-\nnomial regression and splines are the most widely used, and only these will be\ndiscussed here. Local polynomial regression and splines generally work well\nand, since they usually give similar estimates, it is di\ufb03cult to recommend\none over the over. Local polynomial estimation might be somewhat simpler\nto understand. Splines are used in many areas of mathematics, such as, for\ninterpolation, and so it is worthwhile to be familiar with them. Also, splines\nare useful as components in complex models. The R lab at the end of this\nchapter gives an example.\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                             645\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5 21\n\f646                       21 Nonparametric Regression and Splines\n\n       a                                                         b\n\n\n\n\n                                                                 change in return\n       return\n\n\n\n\n                                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n                           0.8\n\n\n\n\n                                                                                    \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                           0.2\n\n\n\n\n                                  1960 1970 1980 1990 2000                                 1960 1970 1980 1990 2000\n                                             year                                                     year\n\n\n       c                                                         d\n\n\n\n\n                                                                                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.12\n                                                                 squared residual\n       change in return\n                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.12",
      "section_title": "squared residual",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                                                    0.06\n                           \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                    0.2    0.6       1.0                            0.00     0.2    0.6       1.0\n                                            return                                                   return\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2    0.6       1.0                            0.00     0.2    0.6       1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1. Risk-free monthly returns. The returns are 1/12th the yearly rate.\n(a) Time series plot of the returns. (b) Time series plot of the changes in the returns.\n(c) Plot of changes in returns against lagged returns with a local linear estimate of\nthe drift. (d) Plot of squared residuals against lagged returns with a local linear es-\ntimate of the squared di\ufb00usion coe\ufb03cient.\n\n\n   Models for the evolution of short-term interest rates are important in \ufb01-\nnance, for example, because they are needed for the pricing of interest rate\nderivatives. Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "Risk-free monthly returns. The returns are 1/12th the yearly rate.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1 contains plots of the monthly risk-free returns1 in the\nCapm data set in R\u2019s Ecdat package. This data set has been used for various\npurposes in several previous chapters. Here we will use it to illustrate non-\nparametric regression. Panels (a) and (b) are time series plots of the returns\nand the changes in the returns.\n   A common model for changes in short-term interest rates is\n\n                                                 \u0394rt = \u03bc(rt\u22121 ) + \u03c3(rt\u22121 )\u0017t ,                                        (21.1)\n\nwhere rt is the rate at time t, \u0394rt = rt \u2212 rt\u22121 , \u03bc(\u00b7) is the drift function, \u03c3(\u00b7)\nis the volatility function, also called the di\ufb00usion function, and \u0017t is N (0, 1)\nnoise. Many di\ufb00erent parametric models have been proposed for \u03bc(\u00b7) and\n\u03c3(\u00b7), for example, by Merton (1973), Vasicek (1977), Cox, Ingersoll, and Ross\n(1985), Yau and Kohn (2003), and Chan et al. (1992). The simplest model,\ndue to Merton (1973), is that \u03bc(\u00b7) and \u03c3(\u00b7) are constant. Chan et al. (1992)\nassume that \u03bc(r) = \u03b2(r \u2212 \u03b1) and \u03c3(r) = \u03b8r\u03b3 , where \u03b1 > 0, \u03b2 < 0, \u03b8 > 0, and\n\u03b3 are unknown parameters\u2014this process reverts to a mean equal to \u03b1. Chan\net al.\u2019s model was used as an example of nonlinear regression in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "contains plots of the monthly risk-free returns1 in the",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, \u03b2 < 0, \u03b8 > 0, and\n\u03b3 are unknown parameters\u2014this process reverts to a mean equal to \u03b1. Chan\net al.\u2019s model was used as an example of nonlinear regression in Sect.",
        "start": 1032,
        "end": 1199
      }
    ]
  },
  {
    "content": "11.12.1.\n1\n    The risk-free rate is called the risk-free return in the Capm package.\n\f                                                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "1.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1 Introduction     647\n\nThe approach of Yau and Kohn (2003) that is used here is to model both \u03bc(\u00b7)\nand \u03c3(\u00b7) nonparametrically. Doing this allows one to check which parametric\nmodels, if any, \ufb01t the data and to have a nonparametric alternative if none of\nthe parametric models \ufb01ts well.\n     The solid red curves in Fig. 21.1c and d are estimates of \u03bc(\u00b7) and \u03c3 2 (\u00b7)\nby a nonparametric regression method local linear regression, a special case\nof local polynomial regression. By (21.1), E(\u0394rt ) = \u03bc(rt\u22121 ) and Var(\u0394rt ) =\n                \u0002(\u00b7) is obtained by regressing \u0394rt on rt\u22121 and \u03c3\n\u03c3 2 (rt\u22121 ), so \u03bc                                              \u00022 (\u00b7) by regressing\n                    2\n{\u0394rt \u2212 \u03bc  \u0002(rt\u22121 )} on rt\u22121 . The latter is an example of estimating a conditional\nvariance; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "Introduction     647",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.2.\n     The code to produce Fig. 21.1 is below. The local linear regression esti-\nmates were produced by the function locpoly() in the KernSmooth package.\nThis function computes the \ufb01tted function on a grid of 401 equally spaced\npoints; this is su\ufb03cient for plotting the \ufb01tted function as in lines 21 and 24\nbut not for computing the residuals. Instead, to compute squared residuals at\nline 11, the spline() function is used at line 10 to interpolate the \ufb01t from\nthe 401-point grid to the values of the explanatory variable.\n1  library(Ecdat)\n2  library(KernSmooth)\n 3 data(Capm)\n\n 4 attach(Capm)\n\n 5 n = length(rf)\n\n 6 year = seq(1960.125, 2003, length = n)\n\n 7 diffrf = diff(Capm$rf)\n\n 8 rf_lag = rf[1:(n-1)]\n\n 9 ll_mu <- locpoly(rf_lag, diffrf, bandwidth = dpill(rf_lag, diffrf))\n\n10 muhat = spline(ll_mu$x, ll_mu$y, xout = rf_lag)$y\n\n11 epsilon_sqr = (diffrf-muhat)^2\n\n12 ll_sig <- locpoly(rf_lag, epsilon_sqr,\n\n13    bandwidth = dpill(rf_lag, epsilon_sqr) )\n14 pdf(\"riskfree01.pdf\", width = 6, height = 5)\n\n15 par(mfrow=c(2, 2))\n\n16 plot(year, rf, ylab = \"return\", main = \"(a)\", type = \"l\" )\n\n17 plot(year[2:n], diffrf, ylab = \"change in  return\", main = \"(b)\",\n18    type = \"l\", xlab = \"year\")\n19 plot(rf_lag, diffrf, ylab = \"change in return\", xlab = \"return\",\n\n20    main=\"(c)\",type=\"p\",cex=.7)\n21 lines(ll_mu$x, ll_mu$y, lwd = 4, col = \"red\")\n\n22 plot(rf_lag, (diffrf - muhat)^2, xlab = \"return\",\n\n23    ylab = \"squared residual\", main = \"(d)\", cex = 0.7)\n24 lines(ll_sig$x, ll_sig$y, lwd = 4, col = \"red\")\n\n25 graphics.off()\n\f648      21 Nonparametric Regression and Splines\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.2",
      "section_title": "The code to produce Fig. 21.1 is below. The local linear regression esti-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2 Local Polynomial Regression\nLocal polynomial regression is based on the principle that a smooth function\ncan be approximated locally by a low-degree polynomial. Suppose we have a\nsample (Xi , Yi ), i = 1, . . . , n, and E(Y |X = x) = \u03bc(x) for a smooth function\n\u03bc. The function \u03bc will be estimated on a grid of x-values, x1 , . . . , xM . These\ncould, but need not, be the same values X1 , . . . , Xn , as where we observe Y .\n    The estimation is done one point at a time on the grid x1 , . . . , xM . To\nestimate \u03bc at x\u0002 , one \ufb01ts a pth-degree polynomial using only (Xi , Yi ) with Xi\nnear x\u0002 . This is done using weights determined by a kernel function K. K is a\nprobability density function symmetric about 0 and such that K(x) decreases\nas |x| increases, for instance, a normal density with mean 0. We have seen\nkernels used for density estimation in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "Local Polynomial Regression",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2.\n    The regression function at x\u0002 is estimated by kernel-weighted least squares,\nwhich minimizes\n  n    \u001a                                                 \u001b2\n        Yi \u2212 \u03b20 + \u03b21 (Xi \u2212 x\u0002 ) + \u00b7 \u00b7 \u00b7 + \u03b2p (Xi \u2212 x\u0002 )p    K{(Xi \u2212 x\u0002 )/h} (21.2)\n i=1\n\n\nand then \u03bc\u0002(x\u0002 ) = \u03b2\u00020 since the regression model \u03b20 +\u03b21 (x\u2212x\u0002 )+\u00b7 \u00b7 \u00b7+\u03b2p (x\u2212x\u0002 )p\nequals \u03b20 at x = x\u0002 . The weights K{(Xi \u2212 x\u0002 )/h} decrease as |Xi \u2212 x\u0002 |\nincreases, so only the data near x\u0002 are used. The parameter h is called the\nbandwidth and determines how much data are used for estimation; the larger\nthe value of h, the more data used.\n            6\n            5\n\n\n\n\n                                    * * **** ** * * *\n                                     *      ** * * ***** **\n                            * * **** **        **          *** *\n                      ** *** **\n            4\n\n\n\n\n                  *\n                 * ** ** *                                    ** *\n                   * *                                            ********* ** ** *\n            3\n        y\n\n\n\n\n                                                                           *** * *\n            2\n            1\n            0\n\n\n\n\n                ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.2",
      "section_title": "The regression function at x\u0002 is estimated by kernel-weighted least squares,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0         0.2          0.4          0.6          0.8          1.0\n                                                 x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2          0.4          0.6          0.8          1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2. Local linear \ufb01t (solid curve) to 75 data points (asterisks) with bandwidth\nchosen by the direct plug-in method. The regression function \u03bc is estimated at each\nof the 75 points and the estimates are connected to create the solid curve. Estimation\nat x25 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "Local linear \ufb01t (solid curve) to 75 data points (asterisks) with bandwidth",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.32 and x55 = 0.72 is illustrated by the kernels (dashed curves), the linear\n\ufb01ts (solid lines), and the \ufb01tted points (large + ).\n\f                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.32",
      "section_title": "and x55 = 0.72 is illustrated by the kernels (dashed curves), the linear",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2 Local Polynomial Regression       649\n\n    Local linear estimation, where p = 1, is illustrated in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "Local Polynomial Regression       649",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2. The kernel\nfunctions are shown as blue dashed curves at two points, x25 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "The kernel",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.32 and\nx75 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.32",
      "section_title": "and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.72. Above each kernel, the local linear \ufb01t is shown as a red line and\nthe large \u201c+\u201d is placed at {x, \u03bc\u0002(x)}. The black curve \u03bc\u0002 is obtained by \ufb01nding\nlocal \ufb01ts on a grid of 75 x\u0002 -values and plotting {x\u0002 , \u03bc\n                                                        \u0002(x\u0002 )} for all x\u0002 on this\ngrid. For example, the curve in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.72",
      "section_title": "Above each kernel, the local linear \ufb01t is shown as a red line and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2 used the R function locpoly() in\nR\u2019s KernSmooth package and has a grid of 401 equally spaced x-values (the\ndefault). Often the grid is simply the observed X-values, X1 , . . . , Xn .\n    The bandwidth h is called a \u201csmoothing parameter\u201d because it determines\nthe smoothness of \u03bc  \u0002. A larger value of h gives a smoother curve. The choice\nof h is important. If h is too large, then the polynomial approximation may\nbe poor and the estimate of \u03bc(x) will be badly biased. Conversely, if h is too\nsmall, then too few data are used and the estimate of \u03bc will be too variable.\nA good choice of the bandwidth minimizes the mean squared error of the\nestimator, which is the variance plus the squared bias. Both the squared bias\nand variance of the estimator are unknown and must be estimated, or at\nleast their sum must be estimated. Automatic bandwidth selection, which\neither directly or indirectly estimates and attempts to minimize the mean-\nsquared error, has been an area of intense research and a number of data-based\nbandwidth selectors are available. The curve in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "used the R function locpoly() in",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2 used the bandwidth\nchosen by the popular direct plug-in (dpi) bandwidth selector of Ruppert,\nSheather and Wand (1995). The dpi selector estimates the mean integrated\nsquared error (MISE) of \u03bc  \u0002, which is\n                         6\u0013                             7\n                              max(Xi )\n                                                      2\n                       E                 {\u03bc(x) \u2212 \u03bc\n                                                 \u0002(x)} dx ,                    (21.3)\n                             min(Xi )\n\n\nand \ufb01nds the bandwidth that minimizes the estimated MISE.\n    Nonparametric regression estimators are also called smoothers because\nthey smooth out the noise in the data. Using a bandwidth that is too small\ncauses over\ufb01tting, which is undersmoothing. Conversely, a bandwidth that is\ntoo large will result in under\ufb01tting, which is oversmoothing\u2014see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "used the bandwidth",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.2 for\nfurther discussion of under- and oversmoothing in the context of kernel density\nestimation.\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.2",
      "section_title": "for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3 illustrates the e\ufb00ect of varying the bandwidth. The solid black\ncurve uses the dpi bandwidth, the dashed red curve uses three times the\ndpi bandwidth, and the dotted-and-dashed blue curve uses one-third the dpi\nbandwidth. The dashed red curve is too smooth to follow the data closely,\nthat is, it under\ufb01ts, while the dotted-and-dashed blue curve is wiggly because\nit is tracking random noise in the data, that is, it over\ufb01ts. In this example,\nthe data were simulated, so the true regression function, \u03bc(x) = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "illustrates the e\ufb00ect of varying the bandwidth. The solid black",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.6 + 0.1x +\n        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.6",
      "section_title": "+ 0.1x +",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5\n\u0017n x ), is known and\nsin(5                           it is possible to calculate the average squared error,\n   i=1 {\u0002\n        \u03bc (X i ) \u2212 \u03bc(X i )}2\n                             , for  each bandwidth. The average squared errors are\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u0017n x ), is known and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.34 and 2.27 times larger using 3*dpi and dpi/3, respectively, compared to\nusing dpi.\n\f650     21 Nonparametric Regression and Splines\n\n    Besides the dpi bandwidth selector, the bandwidth can also be chosen by\nminimizing either the AIC or GCV (generalized cross-validation) criterion.\nThe de\ufb01nition of AIC for a parametric model uses the number of parameters\nin the model, but local polynomial estimation is not parametric, so one cannot\ncount parameters. Nonetheless, it is possible to de\ufb01ne the \u201ce\ufb00ective number\nof parameters\u201d and this is done in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.34",
      "section_title": "and 2.27 times larger using 3*dpi and dpi/3, respectively, compared to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3.1. GCV is de\ufb01ned in Sect. 21.3.2.\n\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "1. GCV is de\ufb01ned in Sect. 21.3.2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1. Local polynomial estimation of forward rates.\n\n                                                                                                                                 o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "Local polynomial estimation of forward rates.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.0\n\n\n\n\n                                                                                                o o\n                                                                                                              o\n                                                                                                                   o                                       o\n                                                                                                                  o o        o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.0",
      "section_title": "o o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.5\n\n\n\n\n                                                                                                 o                                               o o        o\n                                                                                                                                             o        ooo o\n                                                                                                      o                 oo                        o o\n                                                               o\n                                                o                      o           o                                                                 o                  o\n                                                                                       oo                                                                           o\n                                                          o                                 o             o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.5",
      "section_title": "o                                               o o        o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "4.0\n\n\n\n\n                                                                                                                                         o\n                                                         o o                   o                                                                                o\n                        o                   o                                                                                        o\n                                                                           o                                                                                                  o\n                                                                                                                                                                            oo o\n         y\n\n\n\n\n                    o           o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.0",
      "section_title": "o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.5\n\n\n\n\n                                                    oo\n                                    o                                                                                                                                                                                     o\n                                                                   o\n                                                                                                                                                                                                       o\n                                                                                                                                                                                           o\n                                        o\n                            o                                                                                                                                                                      o                                  o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.5",
      "section_title": "oo",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "3.0\n\n\n\n\n                                                                                                                                                                                       o\n                                                                                                                                                                                                           o o\n                                                                                                                                                                                   o                        o                 o           o\n                                                                                                                                                                                               o                                  o               o\n                                            dpi                                                                                                                                                                       o\n\n                                            3*dpi                                                                                                                                                                oo                           o\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "o",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.5\n\n\n\n\n                                            dpi/3\n\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.5",
      "section_title": "dpi/3",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0                                         0.2                                                0.4                                    0.6                           0.8                                                1.0\n                                                                                                                                         x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2                                                0.4                                    0.6                           0.8                                                1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3. Local linear estimators with three bandwidths: dpi (direct plug-in), which\ngives an appropriate amount of smoothing; three times the dpi, which oversmooths\n(under\ufb01ts); and one-third the dpi, which undersmooths (over\ufb01ts). Simulated data.\n\n\n    The R code in this section computes two estimates of the forward rate\nfunction, a parametric estimate based on the Nelson-Siegel model and a non-\nparametric estimates using local polynomial smoothing. The program is split\ninto several chucks of code with discussion between the chucks.\n    Line 1 reads in prices of STRIPS on Dec 31, 1995. This data set was used\nin Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "Local linear estimators with three bandwidths: dpi (direct plug-in), which",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. There are 29 1/4 years of quarterly maturities T, for a total of\n117 (= ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "There are 29 1/4 years of quarterly maturities T, for a total of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "29.25 * 4) prices.\n    The (price of a zero-coupon\n                      )            bond as a percentage of par is price =\n            \u0016T                                \u0016T\n100 exp \u2212 0 f (s)ds so that \u2212log(price) = 0 f (s)ds \u2212 log(100). Thus, the\nintegrated forward rate, called Int_F, is de\ufb01ned on line 7 to be \u2212 log(price) +\nlog(100). Lines 4\u20137 use the order() function to order the maturities T from\nsmallest to largest and to order the prices accordingly. Ordering the data by\nT is needed when the plotted points are connected by lines. If they were not\nordered by T, then the plot could look like a spider web.\n\f                                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "29.25",
      "section_title": "* 4) prices.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2 Local Polynomial Regression    651\n\n1 dat = read.table(\"strips_dec95.txt\", header = T)\n2 T = dat$T\n3 n = length(T)\n\n4 ord = order(T)\n\n5 T = T[ord]\n\n6 price = dat$price[ord]\n\n7 Int_F = - log(price) + log(100)\n\n\n    In lines 8\u20139, the function locfit() in the locfit package estimates the\n\ufb01rst derivative of Int_F by local cubic \ufb01tting to produce an estimate of the\nforward rate (since Int_F estimates the integrated forward rate). Note that\nderiv = 1 speci\ufb01es estimation of the \ufb01rst derivative and deg = 3 speci\ufb01es us-\ning cubic polynomial \ufb01tting. The function locfit() is similar to locpoly()\nused in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "Local Polynomial Regression    651",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.1 to create the curves in Fig. 21.1. We could have used\nlocpoly() again here but wanted to illustrate both local polynomial regres-\nsion functions.\n8    library(locfit)\n9    fit_loc_Int_F = locfit(Int_F ~ T, deriv = 1, deg = 3)\n\n    The function Nelson-Siegel() in lines 10\u201318 returns a list containing\nthe forward rate, called f, and the integrated forward rate, called int_f.\nLines 19\u201324 estimate the parameters of the Nelson-Siegel model by \ufb01tting\nthe Nelson-Siegel integrated forward rate to Int_F. On line 21, Yhat is the\nintegrated forward rate because \u201c[[2]]\u201d is the second element of the list that\nis output by NelsonSiegel().\n10 NelsonSiegel = function(theta){\n11    #### f = forward rate and int_f = intergrated forward rate ####\n12    f = theta[1] + (theta[2] + theta[3] * T) * exp(-theta[4] * T)\n13    int_f = theta[1] * T - theta[2] / theta[4]\n14    * (exp(-theta[4] * T) - 1) -\n15    theta[3] * (T * exp(-theta[4] * T) / theta[4] +\n16    (exp(-theta[4] * T) - 1) / theta[4]^2)\n17     list(\"f\" = f, \"inf_t\" = int_f)\n18 }\n\n19 fit_NS = optim(c(0.05, 0.001, 0.001, 0.08),\n\n20 fn = function(theta){\n\n21    Yhat = NelsonSiegel(theta)[[2]]\n22    sum((Int_F - Yhat)^2)},\n23    control = list(maxit=30000, reltol = 1e-10))\n24 NS_yhat = NelsonSiegel(fit_NS$par)[[1]]\n\n\n    Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.1",
      "section_title": "to create the curves in Fig. 21.1. We could have used",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4 is produced by lines 25\u201336. Notice that the Nelson-Siegel \ufb01t\n(in blue) tends to overestimate the forward rate when 5 < T < 15 and T > 25\nand to underestimate when T < 3 and 15 < T < 25. In comparison, the local\npolynomial estimates show no bias.\n25   pdf(\"strips02.pdf\",width=6,height=5)\n26   par(mfrow=c(1,1))\n\f652                  21 Nonparametric Regression and Splines\n\n27 plot(fit_loc_Int_F, ylim = c(.025,.075), ylab = \"Forward rate\",\n28    col = \"red\", lwd = 3)\n29 lines(fit_loc_Int_F, col = \"red\", lwd=3) # to widen to linewidth = 3\n\n30 lines(T, NS_yhat, col = \"blue\", lwd = 3, lty = 4)\n\n31 points(T[2:n], diff(Int_F) / diff(T))\n\n32 legend(\"bottomleft\", c(\"local cubic\",\n\n33    \"Nelson-Siegel\", \"Empirical\"),\n34    lty=c(1, 4,NA),pch=c(NA, NA, \"o\"),\n35    col=c(\"red\", \"blue\", \"black\"), lwd = c(3, 3, NA))\n36 graphics.off()\n                      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "is produced by lines 25\u201336. Notice that the Nelson-Siegel \ufb01t",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "25\nand to underestimate when T < 3 and 15 < T < 25. In comparison, the local\npolynomial estimates show no bias.\n25   pdf(\"strips02.pdf\",width=6,height=5)\n26   par(mfrow=c(1,1))\n\f652                  21 Nonparametric Regression and Splines",
        "start": 137,
        "end": 379
      }
    ]
  },
  {
    "content": "0.03 0.04 0.05 0.06 0.07\n      Forward rate\n\n\n\n\n                                                   local cubic\n                                                   Nelson\u2212Siegel\n                                                 o Empirical\n\n                                                 0          5      10   15   20   25   30\n                                                                        T\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "0.04 0.05 0.06 0.07",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4. Local cubic (nonparametric), Nelson-Siegel (parametric), and empirical\nestimates of the forward rate curve. Notice that the nonparametric estimates are\nmuch closer than the parametric estimates to the empirical estimates.\n\n\n                                                                                            \u0002\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "Local cubic (nonparametric), Nelson-Siegel (parametric), and empirical",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2.1 Lowess and Loess\n\nLoess and its earlier version lowess are local polynomial smoothers with spa-\ntially varying bandwidths controlled by a parameter called span. Span is the\nfraction of the data used for estimation at each point. The bandwidth, call it\nh(x, span), for estimation at a point x is adjusted so that whenever span \u2264 1\nthen K{(Xi \u2212 x)/h(x, span)} is nonzero for span \u00d7 100 % of the Xi .\n    If span = 1, then all of the data are used for estimation at each point, but\nthe data farthest from Xi get small weights. Because of these small weights,\nfor small data sets, a lowess (or loess) smooth with a span of 1 might not be\nsmooth enough. To solve this problem, span is de\ufb01ned for values greater than\n1 by\n                          h(x, span) = span \u00d7 h(x, 1).\n\f                                                                 ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "1 Lowess and Loess",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3 Linear Smoothers     653\n\nAs span increases beyond 1, the weights K{(Xi \u2212x)/h(x, span)} become more\nand more equal. As span \u2192 \u221e, the weights converge to a constant, K(0), and\nthe lowess (or loess) \ufb01t converges to a polynomial regression \ufb01t.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "Linear Smoothers     653",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3 Linear Smoothers\nLocal polynomial regression as well as penalized spline regression\u2014to be cov-\nered soon\u2014are examples of linear smoothers. A linear smoother has an n \u00d7 n\nsmoother matrix H, which does not depend on Y , such that\n\n                                             Y\u0002 = HY ,                                   (21.4)\n\nwhere Y = (Y1 , . . . , Yn )T is the vector of responses and Y\u0002 = (Y\u00021 , . . . , Y\u0002n )T is\nthe vector of \ufb01tted values. Equation (21.4) can be written as\n                                     n\n                            Y\u0002i =            Hij Yj ,   i = 1, . . . , n.                (21.5)\n                                    j=1\n\n    The smoother matrix will depend on a smoothing parameter, which for lo-\ncal polynomial regression is the bandwidth. We will let \u03bb denote the smoothing\nparameter and denote the smoother matrix by H(\u03bb). The smoother matrix\nis an analog of the hat matrix of linear regression and is, itself, often called a\nhat matrix.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "Linear Smoothers",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3.1 The Smoother Matrix and the E\ufb00ective Degrees\nof Freedom\n\nIn a parametric model, the number of parameters quanti\ufb01es the ability of\nthe model to \ufb01t the data. In nonparametric estimation, the potential to \ufb01t\n(and over\ufb01t) can be quanti\ufb01ed by the e\ufb00ective number of parameters or the\ne\ufb00ective degrees of freedom of the \ufb01t. Conceptually, the e\ufb00ective number of\nparameters is similar to the Bayesian pD in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "1 The Smoother Matrix and the E\ufb00ective Degrees",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.6.\n    By (21.5), the hat diagonal H(\u03bb)ii gives the leverage or self-in\ufb02uence of the\nYi since it is the weight given to Yi when calculating Y\u0002i . A large value of H(\u03bb)ii\nmeans a high potential for over\ufb01tting. The e\ufb00ective number of parameters is\nthe sum of the leverages:\n                                         n\n                            p e\ufb00 =            H(\u03bb)ii = tr{H(\u03bb)}.                         (21.6)\n                                     i=1\n\nIf p e\ufb00 is too small (too large), then the data are under\ufb01t (over\ufb01t).\n    The residual mean sum of squares is\n                  n\n                       (Yi \u2212 Y\u0002i )2 = \u0012Y \u2212 Y\u0002 \u00122 = \u0012{I \u2212 H(\u03bb)}Y \u00122 ,                     (21.7)\n                 i=1\n\f654    21 Nonparametric Regression and Splines\n\nwhere I is the n \u00d7 n identity matrix. The noise variance is estimated by\n                                   \u0012{I \u2212 H(\u03bb)}Y \u00122\n                         \u0002(\u03bb)2 =\n                         \u03c3                         ,                     (21.8)\n                                       n \u2212 p e\ufb00\nwhich is a direct analog of (9.16).\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "6.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3.2 AIC, CV, and GCV\nFor linear regression models, AIC is\n                                     \u03c3 2 ) + 2(1 + p),\n                         AIC = n log(\u0002\nwhere 1 + p is the number of parameters (intercept plus p slopes). For a linear\nsmoother, AIC uses p e\ufb00 in place of p + 1, so that\n                                      \u03c3 2 (\u03bb)} + 2 p e\ufb00 .\n                       AIC(\u03bb) = n log{\u0002\nWe can then select \u03bb by minimizing AIC.\n  The cross-validation or CV statistic is\n                                      n\n                         CV(\u03bb) =            {Yi \u2212 Y\u0002\u2212i (\u03bb)}2\n                                      i=1\n\nwhere, to prevent over\ufb01tting, Y\u0002\u22121 (\u03bb) is the ith \ufb01tted value computed with the\nith observation deleted. One, of course, should choose a \u03bb with a small value\nof CV(\u03bb).\n    The generalized cross-validation statistic (GCV) is\n                                          \u0012Y \u2212 Y\u0002 (\u03bb)\u00122\n                          GCV(\u03bb) =                         2   .         (21.9)\n                                             (n \u2212 p e\ufb00 )\nGCV(\u03bb) can be computed more quickly than CV(\u03bb) and GCV(\u03bb) is a good\napproximation to CV(\u03bb)/n2 so minimizing GCV is another way to choose \u03bb.\n    AIC and GCV can both be computed very quickly and usually give essen-\ntially the same amount of smoothing. In fact, it has been shown theoretically\nthat both criteria should give similar estimates. Therefore, it does not matter\nmuch which is used, but GCV is more commonly used than AIC in nonpara-\nmetric regression.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "2 AIC, CV, and GCV",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4 Polynomial Splines\nThe use of polynomial splines in nonparametric regression, as well as many\nother areas of mathematics, is based on the same principle as local polynomial\nregression\u2014a smooth function can be accurately approximated locally by a\nlow-degree polynomial. A pth-degree polynomial spline is constructed by piec-\ning together pth-degree polynomials, so that they join together at speci\ufb01ed\nlocations called knots. The polynomials are spliced together, so that the spline\nhas p \u2212 1 continuous derivatives. The pth derivative of the spline is constant\nbetween knots and can jump at the knots.\n\f                                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "Polynomial Splines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4 Polynomial Splines       655\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "Polynomial Splines       655",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4.1 Linear Splines with One Knot\n\nWe start simple, a linear spline with one knot. Figure 21.5a illustrates such a\nspline. This spline is de\ufb01ned as\n                                \u000e\n                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "1 Linear Splines with One Knot",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 + 0.2x,   x < 2,\n                        f (x) =\n                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "+ 0.2x,   x < 2,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 + 0.7x, x \u2265 2.\n\nBecause ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "+ 0.7x, x \u2265 2.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 + 0.2x = 0.9 = \u22120.5 + 0.7x when x = 2, the two linear compo-\nnents are equal at the point x = 2, so that they join together there.\n   The point x = 2 where the spline switches from one linear function to\nthe other is called a knot. A linear spline with a knot at the point t can be\nconstructed as follows. The spline is de\ufb01ned to be s(x) = a + bx for x < t\nand s(x) = c + dx for x > t. The parameters a, b, c, and d can be chosen\narbitrarily except that they must satisfy the equality constraint\n\n\n     a          Linear spline              b         Linear plus function\n                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "+ 0.2x = 0.9 = \u22120.5 + 0.7x when x = 2, the two linear compo-",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "t. The parameters a, b, c, and d can be chosen\narbitrarily except that they must satisfy the equality constraint",
        "start": 385,
        "end": 501
      }
    ]
  },
  {
    "content": "3.0\n     2.0\n\n\n\n\n                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "3.0",
      "section_title": "2.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n     1.5\n\n\n\n\n                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "1.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n     1.0\n     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                           0.0\n\n\n\n\n           0    1      2     3       4           0       1     2     3         4\n                       x                                       x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.5. (a) Example of a linear spline with a knot at 2. (b) The linear plus\nfunction (x \u2212 1)+ with a knot at 1 (black) and its \ufb01rst derivative (red).\n\n\n\n                                 a + bt = c + dt,                              (21.10)\n\nwhich assures us that the two lines join together at x = t. Solving for c\nin (21.10), we get c = a + (b \u2212 d)t. Substituting this expression for c into the\nde\ufb01nition of s(x) and doing some rearranging, we have\n                         \u000e\n                                     a + bx,           x < t,\n                  s(x) =                                                 (21.11)\n                            a + bx + (d \u2212 b)(x \u2212 t), x \u2265 t.\n\n   Recall the de\ufb01nition that for any number y,\n                                   \u000e\n                                     0, y < 0,\n                           (y)+ =\n                                     y, y \u2265 0.\n\f656    21 Nonparametric Regression and Splines\n\nBy this de\ufb01nition,                      \u000e\n                                              0,     x < t,\n                           (x \u2212 t)+ =\n                                            x \u2212 t,   x \u2265 t.\nWe call (x \u2212 t)+ a linear plus function with a knot at t. It is also called a\ntruncated line, though we will stick with \u201cplus function.\u201d The spline s(x)\nin (21.11) can be written using this plus function:\n\n                         s(x) = a + bx + (d \u2212 b)(x \u2212 t)+ .\n\nThe plus function simpli\ufb01es the problem of keeping the spline continuous at t.\nFigure 21.5b illustrates a linear plus function with a knot at 1 and its \ufb01rst\nderivative. Notice that\n                                        \u000e\n                           d              0, x < t,\n                             (x \u2212 t)+ =\n                          dx              1, x > t.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "(a) Example of a linear spline with a knot at 2. (b) The linear plus",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "t.",
        "start": 1769,
        "end": 1775
      }
    ]
  },
  {
    "content": "21.4.2 Linear Splines with Many Knots\n\nPlus functions are very convenient when de\ufb01ning linear splines with more\nthan one knot, because plus functions automatically join the component linear\nfunctions together so that the spline is continuous. For example, suppose we\nwant a linear spline to have K knots, t1 < \u00b7 \u00b7 \u00b7 < tK , for the spline to equal\ns(x) = \u03b20 + \u03b21 x for x < t1 , and for the \ufb01rst derivative of the spline to jump by\nthe amount bk at knot tk , for k = 1, . . . , K. Then the spline can be constructed\nfrom linear plus functions, one for each knot:\n\n      s(x) = \u03b20 + \u03b21 x + b1 (x \u2212 t1 )+ + b2 (x \u2212 t2 )+ + \u00b7 \u00b7 \u00b7 + bK (x \u2212 tK )+ .\n\nBecause the plus functions are continuous, the spline is the sum of continuous\nfunctions and is therefore continuous itself.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "2 Linear Splines with Many Knots",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4.3 Quadratic Splines\n\nA linear spline is continuous but has \u201ckinks\u201d at its knots, where its \ufb01rst\nderivative jumps. If we want a function without these kinks, we cannot use\na linear spline. A quadratic spline is a function obtained by piecing together\nquadratic polynomials. More precisely, s(x) is a quadratic spline with knots\nt1 < \u00b7 \u00b7 \u00b7 < tK if s(x) equals one quadratic polynomial to the left of t1 and\nequals a second quadratic polynomial between t1 and t2 , and so on. The\nquadratic polynomials are pieced together, so that the spline is continuous\nand, to guarantee no kinks, its \ufb01rst derivative is also continuous. Figure 21.6a\nshows a quadratic spline with a knot at 1. Notice that the function does not\nhave a kink at the knot but changes from convex to concave there.\n    As with linear splines, continuity can be enforced by using plus functions.\nDe\ufb01ne the quadratic plus function\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "3 Quadratic Splines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4 Polynomial Splines    657\n\n      a                                           b\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "Polynomial Splines    657",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n\n\n\n\n                                                  4\n                                                            plus fn.\n                                                            derivative\n\n\n\n\n                                                  3\n                                                            2nd derivative\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                  2\n      ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                                                  1\n      \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n\n\n\n\n                                                  0\n             ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0 0.5 1.0 1.5 2.0 2.5 3.0              0.0 0.5 1.0 1.5 2.0 2.5 3.0\n                          x                                        x\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.5 1.0 1.5 2.0 2.5 3.0              0.0 0.5 1.0 1.5 2.0 2.5 3.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.6. (a) Quadratic spline with a knot at 1. The dotted red vertical line marks\nthe knot\u2019s location. (b) The quadratic plus function (x \u2212 1)2+ with a knot at 1 (black)\nand its \ufb01rst (red) and second (blue) derivatives.\n\n                                         \u000e\n                                             0,           x < t,\n                           (x \u2212 t)2+ =\n                                             (x \u2212 t)2 ,   x \u2265 t.\n\nNotice that (x \u2212 t)2+ equals {(x \u2212 t)+ }2 , not {(x \u2212 t)2 }+ = (x \u2212 t)2 .\n   Figure 21.6b shows a quadratic plus function and its \ufb01rst and second\nderivatives. One can see that for x > t\n                                d\n                                  (x \u2212 t)2+ = 2(x \u2212 t)+\n                               dx\nand\n                               d2\n                                  (x \u2212 t)2+ = 2(x \u2212 t)0+ ,\n                              dx2\nwhere (x \u2212 t)0+ = {(x \u2212 t)+ }0 , so that (x \u2212 t)0+ is the 0th-degree plus function\n                                         \u000e\n                                            0, x < t,\n                            (x \u2212 t)0+ =\n                                            1, x \u2265 t.\n\nTherefore, the second derivative of (x \u2212 t)2+ jumps from 0 to 2 at the knot t.\n   A quadratic spline with knots t1 < \u00b7 \u00b7 \u00b7 < tK can be written as\n\n  s(x) = \u03b20 + \u03b21 x + \u03b22 x2 + b1 (x \u2212 t1 )2+ + b2 (x \u2212 t2 )2+ + \u00b7 \u00b7 \u00b7 + bK (x \u2212 tK )2+ .\n\nThe second derivative of s jumps by the amount 2bk at knot tk for k =\n1, . . . , K.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.6",
      "section_title": "(a) Quadratic spline with a knot at 1. The dotted red vertical line marks",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "t\n                                d\n                                  (x \u2212 t)2+ = 2(x \u2212 t)+\n                               dx\nand\n                               d2\n                                  (x \u2212 t)2+ = 2(x \u2212 t)0+ ,\n                              dx2\nwhere (x \u2212 t)0+ = {(x \u2212 t)+ }0 , so that (x \u2212 t)0+ is the 0th-degree plus function\n                                         \u000e\n                                            0, x < t,\n                            (x \u2212 t)0+ =\n                                            1, x \u2265 t.",
        "start": 614,
        "end": 1148
      }
    ]
  },
  {
    "content": "21.4.4 pth Degree Splines\n\nThe way to de\ufb01ne a general pth-degree spline with knots t1 < \u00b7 \u00b7 \u00b7 < tK should\nnow be obvious:\n\f658     21 Nonparametric Regression and Splines\n\n  s(x) = \u03b20 + \u03b21 x + \u00b7 \u00b7 \u00b7 + \u03b2p xp + b1 (x \u2212 t1 )p+ + \u00b7 \u00b7 \u00b7 + bK (x \u2212 tK )p+ ,     (21.12)\n\nwhere, as we have seen for the speci\ufb01c case of p = 2, (x\u2212t)p+ equals {(x\u2212t)+ }p .\nThe \ufb01rst p \u2212 1 derivatives of s are continuous while the pth derivative takes a\njump equal to p! bk at the kth knot.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "4 pth Degree Splines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.4.5 Other Spline Bases\n\nGiven a degree p and knots \u03ba1 , . . . , \u03baK , the polynomials 1, x, . . . , xp and plus\nfunctions (x \u2212 \u03ba1 )p+ , . . . , (x \u2212 \u03baK )p+ form a spline basis. What this means is\nthat any pth degree spline with knots \u03ba1 , . . . , \u03baK is a linear combination of\nthese basis functions. The basis of polynomials and plus functions is simple to\nunderstand, but is known to be numerically unstable if the number of knots is\nlarge. For this reason, other bases are often used for numerical computations.\nThe B-spline basis is particular popular. It is assumed here that the reader\nwill not be programming spline estimators from scratch but rather will be\nusing spline software. Therefore, B-splines and other bases will not be covered\nhere, but see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.4",
      "section_title": "5 Other Spline Bases",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.6 for further reading.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.6",
      "section_title": "for further reading.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.5 Penalized Splines\n\nBecause a pth degree spline with K knots has 1 + p + K parameters, an\nordinary least-squares \ufb01t will usually over\ufb01t the data unless both p and K are\nkept small, for instance, 1 + p + K \u2264 6. (There is nothing especial about the\nnumber 6 and it is just being used as a rule of thumb. Any number between\n5 and 10 would be equally good.) An example is the quadratic spline with\none knot (so 1 + p + K = 4) used as a forward-rate curve in Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "Penalized Splines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3.\nHowever, a spline with p and K both small is essentially a parametric model.\nTo have the \ufb02exibility of a nonparametric model, that is, a wide range of\npotential values of p e\ufb00 , we need to have K large and \ufb01nd another way to\navoid over\ufb01tting. Penalized least-squares estimation does this.\n    Let \u03bc(x; \u03b2) = B(x)T \u03b2 be a spline, where \u03b2 is a vector of coe\ufb03cients\nand B(x) = (B1 (x), . . . , B1+p+K (x))T is a spline basis. For example, B(x) =\n(1, x, . . . , xp , (x \u2212 \u03ba1 )p+ , . . . , (x \u2212 \u03baK )p+ ) if we use model (21.12). A penalized\nleast-squares estimator minimizes over \u03b2 the penalized sum of squares\n                            n\n                                {Yi \u2212 \u03bc(Xi ; \u03b2)} + \u03bb \u03b2 T D\u03b2,\n                                                 2\n                                                                                   (21.13)\n                          i=1\n\nwhere D is a positive semide\ufb01nite matrix and \u03bb > 0 is a penalty parameter.\n   A common choice of D has the i, jth element equal to\n                                  \u0013 b\n                                         (2)     (2)\n                                        Bi (x)Bj (x)dx                             (21.14)\n                                   a\n\f                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "However, a spline with p and K both small is essentially a parametric model.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 is a penalty parameter.\n   A common choice of D has the i, jth element equal to\n                                  \u0013 b\n                                         (2)     (2)\n                                        Bi (x)Bj (x)dx                             (21.14)\n                                   a",
        "start": 922,
        "end": 1277
      }
    ]
  },
  {
    "content": "21.5 Penalized Splines    659\n                                                                    (2)\nfor some a < b, such as, a = min(Xi ) and b = max(Xi ). Here Bi (x) is the\nsecond derivative of Bi (x). With this D,\n                                   \u0013 b(            )2\n                          T\n                     \u03bb \u03b2 D\u03b2 = \u03bb         \u03bc(2) (x; \u03b2) dx,           (21.15)\n                                     a\n       (2)\nSince \u03bc (x) is the amount of curvature of \u03bc at x, this choice of D penalizes\nwiggly functions and, if \u03bb is chosen appropriately, prevents over\ufb01tting. If \u03bb =\n0, then there is no penalization and the e\ufb00ective number of parameters is\n1 + p + K. With this D, in the limit as \u03bb \u2192 \u221e, any curvature at all receives\nan in\ufb01nite penalty, so the estimator converges to a linear polynomial \ufb01t and\nthe e\ufb00ective number of parameters converges to 2. Any value of p e\ufb00 between\n2 and 1 + p + K is achievable by the some value of \u03bb between the extremes of\n0 and \u221e.\n    Let X be the n \u00d7 (1 + p + K) matrix with i, jth element Bj (Xi ) and let\nY = (Y1 , . . . , Yn )T . The penalized least-squares estimate is\n                                   &             '\u22121\n                            \u0002\n                            \u03b2(\u03bb) = X T X + \u03bbD         X TY ,            (21.16)\n\nwhich is obtained by setting the gradient of (21.13) equal to zero and solving.\nThe \ufb01tted values are\n                        (                        )\n                \u0002\n     Y\u0002 (\u03bb) = X \u03b2(\u03bb)  = X(X T X + \u03bbD)\u22121 X T Y = H(\u03bb) Y ,                (21.17)\n            (                   )\nwhere H(\u03bb) = X(X T X + \u03bbD)\u22121 X T is the smoother matrix.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "Penalized Splines    659",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.5.1 Cubic Smoothing Splines\nA very widely used nonparametric regression estimator is the cubic smooth-\ning spline. This estimator uses a knot at each unique value of {X1 , . . . , Xn }\nand the second-derivative penalty in (21.15). Using this many knots is not\nreally necessary, and a variation on the cubic smoothing spline also uses\npenalty (21.15) but fewer knots. The knots could be equally-spaced or at\nselected quantiles of {X1 , . . . , Xn }.\n    The function smooth.spline() in R \ufb01ts a cubic smoothing spline if the\nargument all.knots is TRUE or if n < 50. If all.knots = FALSE and n > 49,\nthen it uses less than the full set of knots.\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "1 Cubic Smoothing Splines",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "49,\nthen it uses less than the full set of knots.",
        "start": 594,
        "end": 647
      }
    ]
  },
  {
    "content": "21.5.2 Selecting the Amount of Penalization\nThe penalty parameter \u03bb determines the amount of smoothing and can be\nchosen by AIC or GCV. Another popular method for choosing \u03bb is REML\n(restricted maximum likelihood). REML is based on a so-called mixed model,\nwhere some of the spline coe\ufb03cients are random variables. A description of\nmixed models and REML is beyond the scope of this book, but the interested\nreader may consult the references in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "2 Selecting the Amount of Penalization",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.6.\n\f660    21 Nonparametric Regression and Splines\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.6",
      "section_title": "660    21 Nonparametric Regression and Splines",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.2. Estimating the drift and volatility for the evolution of the\nrisk-free returns\n\n   In this example, we return to estimating the drift and squared volatility\nfunctions for the evolution of the risk-free returns. Three estimators will be\nused: local linear, local quadratic, and a penalized spline. The R code is:\n1  library(Ecdat)\n2  library(KernSmooth)\n 3 library(locfit)\n\n 4 library(mgcv)\n\n 5 data(Capm)\n\n 6 attach(Capm)\n\n 7 n = length(rf)\n\n 8 year = seq(1960.125,2003,length=n)\n\n 9 diffrf=diff(Capm$rf)\n\n10 rf_lag = rf[1:(n-1)]\n\n11 log_rf_lag = log(rf_lag)\n\n12 ll_mu <- locpoly(rf_lag, diffrf, bandwidth = dpill(rf_lag,diffrf))\n\n13 muhat = spline(ll_mu$x, ll_mu$y, xout = rf_lag)$y\n\n14 epsilon_sqr = (diffrf - muhat)^2\n\n15 ll_sig <- locpoly(rf_lag, epsilon_sqr,\n\n16    bandwidth = dpill(rf_lag, epsilon_sqr) )\n17 gam_mu = gam(diffrf ~ s(rf_lag, bs = \"cr\"), method = \"REML\")\n\n18 epsilon_sqr = (diffrf-gam_mu$fit)^2\n\n19 gam_sig = gam(epsilon_sqr ~ s(rf_lag, bs = \"cr\"), method = \"REML\")\n\n20 locfit_mu = locfit(diffrf ~ rf_lag)\n\n21 epsilon_sqr = (diffrf - fitted(locfit_mu))^2\n\n22 locfit_sig = locfit(epsilon_sqr ~ rf_lag)\n\n23 std_res = (diffrf - fitted(locfit_mu)) / sqrt(fitted(locfit_sig))\n\n24 min(rf_lag[(gam_mu$fit < 0)])\n\n25 orrf = order(rf_lag)\n\n26 pdf(\"riskfree02.pdf\", width = 8, height = 4)\n\n27 par(mfrow=c(1, 2))\n\n28 plot(rf_lag[orrf], gam_mu$fit[orrf], type = \"l\", lwd = 3, lty = 1,\n\n29    xlab = \"lagged rate\", ylab = \"change in rate\", main = \"(a)\")\n30 lines(ll_mu$x,ll_mu$y, lwd = 3, lty = 2, col = \"red\")\n\n31 lines(locfit_mu, lwd = 3, lty = 3, col = \"blue\")\n\n32 legend(0.1, -0.05, c(\"spline\", \"local linear\", \"local quadratic\"),\n\n33    lty = c(1, 2, 3), cex = 0.85, lwd = 3,\n34    col = c(\"black\", \"red\", \"blue\"))\n35 rug(rf_lag)\n\n36 abline(h = 0, lwd = 2)\n\n37 plot(rf_lag[orrf], gam_sig$fit[orrf], type=\"l\", lwd = 3, lty = 1,\n\n38    ylim = c(0, 0.03), xlab = \"lagged rate\",\n39    ylab = \"squared residual\", main = \"(b)\")\n40 lines(ll_sig$x,ll_sig$y, lwd = 3, lty = 2, col = \"red\")\n\n41 lines(locfit_sig, lwd = 3, lty = 3, col = \"blue\")\n\n42 abline(h = 0, lwd = 2)\n\f                                                                                           ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.2",
      "section_title": "Estimating the drift and volatility for the evolution of the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.5 Penalized Splines                 661\n\n43 legend(\"topleft\", c(\"spline\", \"local linear\", \"local quadratic\"),\n44    lty = c(1, 2, 3), cex = 0.85, lwd = 3,\n45    col = c(\"black\", \"red\", \"blue\"))\n46 rug(rf_lag)\n\n47 graphics.off()\n\n\n\n    The \ufb01rst estimator, local linear, is computed at line 12 using the function\nlocpoly() in R\u2019s KernSmooth package. The dpi plug-in bandwidth selector is\ncomputed using the function dpill() in this package.2\n    In the R code, the changes in the risk-free returns (diffrf) are regressed on\nthe lagged returns (rf_lag) to estimate the drift. The local linear estimator\nis computed on an equally-spaced grid, and to compute residuals the function\nspline() is used at line 13 to interpolate the \ufb01t to the observed values of\nrf_lag. Finally, the squared residuals (epsilon_sqr) computed at line 14\nare regressed at lines 15\u201316 on the lagged returns to estimate the squared\nvolatility function. The estimated drift function is in the object ll_mu and\nthe estimated squared volatility function is in ll_sig.\n    The penalized spline estimator is computed at line 17 by the gam() func-\ntion in the mgcv package. The speci\ufb01cation bs = \"cr\" requests a cubic spline\n\ufb01t with penalty (21.15). The REML method is used to select the amount of\nsmoothing.\n    The local quadratic estimator is computed at line 20 with the function\nlocfit() in R\u2019s locfit package. Spline interpolation is not necessary here,\nsince with locfit() the \ufb01tted values can be computed with the fitted\nfunction.\n\n\na                                                               b\n                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "Penalized Splines                 661",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.030\n                  0.00\n\n\n\n\n                                                                                                 spline\n                                                                                                 local linear\n                                                                                                 local quadratic\n                                                                squared residual\n                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.030",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.020\nchange in rate\n                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.020",
      "section_title": "change in rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n\n\n\n\n                                spline\n                                local linear\n                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "spline",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.010\n                  \u22120.10\n\n\n\n\n                                local quadratic\n                                                                                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.010",
      "section_title": "\u22120.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000\n                  \u22120.15\n\n\n\n\n                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000",
      "section_title": "\u22120.15",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2   0.4   0.6    0.8    1.0   1.2                              0.2   0.4    0.6    0.8    1.0   1.2\n                                      lagged rate                                                       lagged rate\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4   0.6    0.8    1.0   1.2                              0.2   0.4    0.6    0.8    1.0   1.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7. Risk-free monthly returns. (a) Estimates of the drift function. (b) Esti-\nmates of the squared volatility function.\n\n\n\n\n 2\n                 \u201cdpill\u201d means \u201cdirect plug-in, local linear.\u201d\n\f662         21 Nonparametric Regression and Splines\n\n     All three estimated drift functions are shown in Fig. 21.7a and the squared\nvolatility function estimates are in Fig. 21.7b.\n     The drift functions have a general decreasing trend and are negative to the\nright of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "Risk-free monthly returns. (a) Estimates of the drift function. (b) Esti-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.51 (approximately), except that the estimates have humps around\n0.9\u2013",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.51",
      "section_title": "(approximately), except that the estimates have humps around",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0 and the spline and local linear estimates are slightly positive at this\nhump. It is likely that the hump is due to random variation, which increases\nas one moves from left to right (see Fig. 21.1). If we use the local quadratic\n\ufb01t, then the estimated drift is positive to the left of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "and the spline and local linear estimates are slightly positive at this",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.51 and negative to the\nright of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.51",
      "section_title": "and negative to the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.51. The drift will cause reversion to a mean of 0.51, which is an\nannual rate of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.51",
      "section_title": "The drift will cause reversion to a mean of 0.51, which is an",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.12 % = (12)(0.51) %. The Chan et al. (1992) drift function,\n\u03bc(r) = \u03b2(r \u2212 \u03b1), is also mean-reverting, but linear. In contrast, the local\nquadratic estimated drift function in Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.12",
      "section_title": "% = (12)(0.51) %. The Chan et al. (1992) drift function,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7 is nonlinear and shows much\nfaster reversion to the mean when the rate is high.\n     The squared volatility estimates show that volatility increases with the\nrate, at least to a point. For very high rates, the estimated volatility function\nbecomes decreasing. There is not enough data with extremely high rates to tell\nif this phenomenon is \u201creal\u201d or due to random estimation error. The extremely\nhigh rates occurred only for the brief period in the early 1980s; see Fig. 21.1a.\n     The standardized residuals {\u0394rt \u2212 \u03bc  \u0002(rt\u22121 )}/\u0002\n                                                    \u03c3 (rt\u22121 ) show negative serial\ncorrelation and GARCH-type volatility clustering; see Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "is nonlinear and shows much",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.8. Neither of\nthese is surprising. Negative lag-1 autocorrelation is common in a di\ufb00erenced\nseries and volatility clustering is certainly to be expected in any \ufb01nancial\ntime series. This case study could be continued by \ufb01tting an ARMA/GARCH\nmodel to the standardized residuals.                                            \u0002\n\n\na     standardized residuals\n                               b                           standardized residuals\n                                                                                    c squared standardized residuals\n                                     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.8",
      "section_title": "Neither of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 0.4 0.6 0.8 1.0\n\n\n\n\n                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4 0.6 0.8 1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n4\n\n\n\n\n                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n2\n\n\n\n\n                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                               ACF\n\n\n\n\n                                                                                    ACF\n0\n\n\n\n\n                                                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "ACF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                                          0.2\n\u22122\n\n\n\n\n                                     \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                          0.0\n\u22124\n\n\n\n\n     1960     1980     2000                                0   5   10 15 20 25                  0   5   10 15 20 25\n               year                                                 Lag                                  Lag\n\nFig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.8. Risk-free monthly returns. Residual analysis. (a) Time series plot of\nstandardized residuals. (b) ACF of standardized residuals. (c) ACF of squared stan-\ndardized residuals.\n\f                                                          ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.8",
      "section_title": "Risk-free monthly returns. Residual analysis. (a) Time series plot of",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.5 Penalized Splines   663\n\nExample ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.5",
      "section_title": "Penalized Splines   663",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.3. Spline estimation of a forward rate\n\n    This example used the STRIPS data that were already analyzed in Exam-\nple ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.3",
      "section_title": "Spline estimation of a forward rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. In that example, an unpenalized spline was \ufb01t to the bond prices by\nnonlinear regression, and smoothness was controlled by using only knot.\n    The function gam() in the mgcv is a powerful tool that can \ufb01t a wide\nvariety of spline models with penalties. In this example, gam() is used to \ufb01t a\ncubic spline to the empirical forward rates that are de\ufb01ned at the beginning\nof Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "In that example, an unpenalized spline was \ufb01t to the bond prices by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.3. The code is below.\n1  dat = read.table(\"strips_dec95.txt\", header = T)\n2  T = dat$T\n 3 n = length(T)\n\n 4 ord = order(T)\n\n 5 T = T[ord]\n\n 6 price = dat$price[ord]\n\n 7 Int_F = - log(price) + log(100)\n\n 8 emp_forward = diff(Int_F)/diff(T)\n\n 9 library(mgcv)\n\n10 X = T[-1]\n\n11 fit_gam = gam(emp_forward ~ s(X, bs = \"cr\"))\n\n12 pred_gam = predict(fit_gam, as.data.frame(X)  )\n13 pdf(\"forward_spline.pdf\", width = 6, height = 5)\n\n14 plot(X, emp_forward, xlab = \"maturity\", ylab = \"forward rate\")\n\n15 lines(X, pred_gam, col = \"red\", lwd = 2)\n\n16 graphics.off()\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.3",
      "section_title": "The code is below.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.07\n                        0.06\n         forward rate\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.07",
      "section_title": "0.06",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n                        0.04\n                        ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.04",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.03\n\n\n\n\n                               0   5    10       15       20       25       30\n                                              maturity\n\n                Fig. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.03",
      "section_title": "0   5    10       15       20       25       30",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.9. Empirical forward rates (circle) with a spline \ufb01t.\n\f664      21 Nonparametric Regression and Splines\n\n     Figure ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.9",
      "section_title": "Empirical forward rates (circle) with a spline \ufb01t.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.9 shows the empirical forward rates and the cubic spline \ufb01t to\nthem. Notice that the spline goes through the empirical forward rates with\nlittle sign of bias and yet is smooth.                                     \u0002\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.9",
      "section_title": "shows the empirical forward rates and the cubic spline \ufb01t to",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.6 Bibliographic Notes\n\nRuppert, Wand, and Carroll (2003) and Wood (2006) o\ufb00er comprehensive in-\ntroductions to nonparametric and semiparametric modeling and their appli-\ncations. Wand and Jones (1995) and Fan and Gijbels (1996) are good sources\nof information about local polynomial regression. REML is discussed in detail\nby Ruppert, Wand, and Carroll (2003) and Wood (2006) . Wasserman (2006)\nis an interesting modern synthesis of nonparametric estimation. Wood (2006)\nis a good introduction to his mgcv package.\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.6",
      "section_title": "Bibliographic Notes",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7 R Lab\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "R Lab",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7.1 Additive Model for Wages, Education, and Experience\n\nThis section uses the Current Population Survey data in the CPS1988 data\nset introduced in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "1 Additive Model for Wages, Education, and Experience",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.4.1. We will \ufb01t spline e\ufb00ects for both predictors,\neducation and experience. This is easily done with the gam() function in\nthe mgcv package. The model being \ufb01t is\n\n  log(wage) = \u03b20 + s1 (education) + s2 (experience) + \u03b21 ethnicity + \u0017i ,\n\nwhere \u03b20 is the intercept, s1 and s2 are splines, ethnicity is 0 for Caucasians\nand 1 for African Americans, and \u0017i is white noise. To \ufb01t this model, print its\nsummary, and plot the estimates of s1 and s2 , run:\n      library(AER)\n      library(mgcv)\n      data(CPS1988)\n      attach(CPS1988)\n      fitGam = gam(log(wage)~s(education)+s(experience)+ethnicity)\n      summary(fitGam)\n      par(mfrow=c(1,2))\n      plot(fitGam)\n\nProblem 1 What are the estimates of \u03b20 and \u03b21?\n\n\nProblem 2 Describe the shapes of s1 and s2 .\n\f                                                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.4",
      "section_title": "1. We will \ufb01t spline e\ufb00ects for both predictors,",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7 R Lab      665\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "R Lab      665",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.7.2 An Extended CKLS Model for the Short Rate\nIn this section, we use splines to extend the CKLS model in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.7",
      "section_title": "2 An Extended CKLS Model for the Short Rate",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.12 by\nletting the drift parameters a and \u03b8 vary with time so that\n                           \u03bc(t, r) = a(t) {\u03b8(t) \u2212 r}.                    (21.18)\nOne could also let the volatility parameters \u03c3 and \u03b3 vary as well with t, but,\nfor simplicity, we will not do that here. We will \ufb01t this model with a(t) being\nlinear in time and \u03b8(t) being a piecewise linear spline. [Letting both a(t) and\n\u03b8(t) be splines can lead to unstable estimates, so we will restrict a(t) to be\nlinear.] First, read in the data, and then create the knots and the truncated\nline basis functions.\n   # CKLS, extended\n   library(Ecdat)\n   data(Irates)\n   r1 = Irates[,1]\n   n = length(r1)\n   lag_r1 = lag(r1)[-n]\n   delta_r1 = diff(r1)\n   n = length(lag_r1)\n   knots = seq(from=1950,to=1985,length=10)\n   t = seq(from=1946,to =1991+2/12,length=n)\n   X1 = outer(t,knots,FUN=\"-\")\n   X2 = X1 * (X1>0)\n   X3 = cbind(rep(1,n), (t - 1946),X2)\n   m2 = dim(X3)[2]\n   m = m2 - 1\n\nProblem 3 How many knots are being used here? What does the outer()\nfunction do here? What is done by the statement X2 = X1 * (X1>0)?\nDescribe what is in the variable X3.\n\n\nNow \ufb01t the CKLS model with time-varying drift.\n   nlmod_CKLS_ext = nls(delta_r1 ~ X3[,1:2]%*%a *\n      (X3%*%theta-lag_r1),\n      start=list(theta = c(10,rep(0,m)),\n      a=c(.01,0)),control=list(maxiter=200))\n   AIC(nlmod_CKLS_ext)\n   param4 = summary(nlmod_CKLS_ext)$parameters[,1]\n   par(mfrow=c(1,3))\n   plot(t,X3%*%param4[1:m2],ylim=c(0,16),ylab=\"rate\",\n      main=\"(a)\",col=\"red\",type=\"l\",lwd=2)\n   lines(t,lag_r1)\n   legend(\"topleft\",c(\"theta(t)\",\"lagged rate\"),lwd=c(2,1),\n      col=c(\"red\",\"black\"))\n\f666      21 Nonparametric Regression and Splines\n\n      plot(t,X3[,1:2]%*%param4[(m2+1):(m2+2)],ylab=\"a(t)\",\n         col=\"red\",type=\"l\",lwd=2,main=\"(b)\")\n\n      res_sq = residuals(nlmod_CKLS_ext)^2\n      nlmod_CKLS_ext_res <- nls(res_sq ~ A*lag_r1^B,\n         start=list(A=.2,B=1/2) )\n\n      plot(lag_r1,sqrt(res_sq),pch=5,ylim=c(0,6),ylab=\"\",main=\"(c)\")\n      lines(lag_r1,sqrt(fitted(nlmod_CKLS_ext_res)),\n         lw=3,col=\"red\",type=\"l\")\n      legend(\"topleft\",c(\"abs res\",\"volatility fn\"),lty=c(NA,1),\n         pch=c(5,NA),col=c(\"black\",\"red\"),lwd=1:2)\n\nProblem 4 Explain why X3[,1:2]%*%a is a linear function but X3%*%theta is a\nspline.\n\n\nProblem 5 What is the interpretation of a time-varying \u03b8? Note that in panel (a),\n\u03b8 seems to track the interest rate. Does this make sense? Why or why not?\n\n\nProblem 6 Would you accept or reject the null hypothesis that a(t) is constant,\nthat is, that the slope of the linear function a(t) is zero? Justify your answer.\n\n\n\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.12",
      "section_title": "by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "21.8 Exercises\n 1. A linear spline s(t) has knots at 1, 2, and 3. Also, s(0) = 1, s(1) = 1.3, s(2) = 5.5,\n    s(4) = 6, and s(5) = 6.\n    (a) What is s(0.5)?\n    (b) What is s(3)?\n                  \u00144\n    (c) What is 2 s(t) dt?\n 2. Suppose that (21.1) holds with \u03bc(r) = 0.1(",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "21.8",
      "section_title": "Exercises",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.035 \u2212 r) and \u03c3(r) = 2.3r.\n    (a) What is the expected value of rt given that rt\u22121 = 0.04?\n    (b) What is the variance of rt given that rt\u22121 = 0.02?\n 3. Let the spline s(x) be de\ufb01ned as\n\n                           s(x) = (x)+ \u2212 3(x \u2212 1)+ + (x \u2212 2)+ .\n\n    (a) Is s(x) either a probability density function (pdf) or a cumulative distribu-\n        tion function (cdf)? Explain your answer.\n    (b) If X is a random variable and s is its pdf or cdf [whichever is the correct\n        answer in (a)], then what is the 90th percentile of X?\n 4. Let s be the spline\n\n                     s(x) = 1 + 0.65x + x2 + (x \u2212 1)2+ + 0.6(x \u2212 2)2+ .\n\n      (a) What are s(1.5) and s\u0002 (1.5)?\n      (b) What is s\u0002\u0002 (2.2)?\n\f                                                                 References     667\n\nReferences\nChan, K. C., Karolyi, G. A., Longsta\ufb00, F. A., and Sanders, A. B. (1992) An empir-\n  ical comparison of alternative models of the short-term interest rate. Journal of\n  Finance, 47, 1209\u20131227.\nCox, J. C., Ingersoll, J. E., and Ross, S. A. (1985) A theory of the term structure\n  of interest rates. Econometrica, 53, 385\u2013407.\nFan, J., and Gijbels, I. (1996) Local Polynomial Modelling and Its Applications,\n  Chapman & Hall, London.\nMerton, R. C. (1973) Theory of rational option pricing. Bell Journal of Economics\n  and Management Science, 4, 141\u2013183.\nRuppert, D., Sheather, S., and Wand, M. P. (1995) An e\ufb00ective bandwidth selec-\n  tor for local least squares kernel regression, Journal of the American Statistical\n  Association, 90, 1257\u20131270.\nRuppert, D., Wand, M. P., and Carroll, R. J. (2003) Semiparametric Regression,\n  Cambridge University Press, Cambridge.\nVasicek, O. A. (1977) An equilibrium characterization of the term structure. Journal\n  of Financial Economics, 5, 177\u2013188.\nWand, M. P., and Jones, M. C. (1995) Kernel Smoothing, Chapman & Hall, London.\nWasserman, L. (2006) All of Nonparametric Statistics, Springer, New York.\nWood, S. (2006) Generalized Additive Models: An Introduction with R, Chapman &\n  Hall, Boca Raton, FL.\nYau, P., and Kohn, R. (2003) Estimation and variable selection in nonparametric\n  heteroskedastic regression. Statistics and Computing, 13, 191\u2013208.\n\fA\nFacts from Probability, Statistics,\nand Algebra\n\n\n\n\nA.1 Introduction\n\nIt is assumed that the reader is already familiar with the basics of probability,\nstatistics, matrix algebra, and other mathematical topics needed in this book,\nand so the goal of this appendix is merely to provide a quick review and cover\nsome more advanced topics that may not be familiar.\n\n\nA.2 Probability Distributions\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.035",
      "section_title": "\u2212 r) and \u03c3(r) = 2.3r.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.1 Cumulative Distribution Functions\n\nThe cumulative distribution function (CDF) of Y is de\ufb01ned as\n\n                              FY (y) = P {Y \u2264 y}.\n\nIf Y has a PDF fY , then\n                                       \u0013 y\n                            FY (y) =         fY (u) du.\n                                        \u2212\u221e\n\nMany CDFs and PDFs can be calculated by computer software packages, for\ninstance, pnorm(), pt(), and pbinom() in R calculate, respectively, the CDF\nof a normal, t, and binomial random variable. Similiarly, dnorm(), dt(), and\ndbinom() calculate the PDFs of these distributions.\n\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                              669\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5\n\f670      A Facts from Probability, Statistics, and Algebra\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.1",
      "section_title": "Cumulative Distribution Functions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.2 Quantiles and Percentiles\n\nIf the CDF F (y) of a random variable Y is continuous and strictly increasing,\nthen it has an inverse function F \u22121 . For each q between 0 and 1, F \u22121 (q) is\ncalled the q-quantile or 100qth percentile.\n    The median is the 50 % percentile or 0.5-quantile. The 25 % and 75 %\npercentiles (0.25- and 0.75-quantiles) are called the \ufb01rst and third quartiles\nand the median is the second quartile. The three quartiles divide the range of\na continuous random variable into four groups of equal probability. Similarly,\nthe 20 %, 40 %, 60 %, and 80 % percentiles are called quintiles and the 10 %,\n20 %, . . . , 90 % percentiles are called deciles.\n    For any CDF F , invertible or not, the pseudo-inverse is de\ufb01ned as\n\n                            F \u2212 (x) = inf{y : F (y) \u2265 x}.\n\nHere \u201cinf\u201d is the in\ufb01num or greatest lower bound of a set; see Appendix A.5.\nFor any q between 0 and 1, the qth quantile will de\ufb01ned as F \u2212 (q). If F is\ninvertible, then F \u22121 = F \u2212 , so this de\ufb01nition of quantile agree with the one\nfor invertible CDFs. F \u2212 is often called the quantile function.\n    Sometimes a (1 \u2212 \u03b1)-quantile is called an \u03b1-upper quantile, to emphasize\nthe amount of probability above the quantile. In analogy, a quantile might\nalso be referred to as lower quantile.\n    Quantiles are said to \u201crespect transformations\u201d in the following sense. If\nY is a random variable whose q-quantile equals yq , if g is a strictly increasing\nfunction, and if X = g(Y ), then g(yq ) is the q-quantile of X; see (A.5).\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.2",
      "section_title": "Quantiles and Percentiles",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.3 Symmetry and Modes\n\nA probability density function (PDF) f is said to be symmetric about \u03bc if\nf (\u03bc \u2212 y) = f (\u03bc + y) for all y. A mode of a PDF is a local maximum, that is a\nvalue y such that for some \u0017 > 0, f (y) > f (x) if y \u2212\u0017 < x < y or y < x < y +\u0017.\nA PDF with one mode is called unimodal , with two modes bimodal , and with\ntwo or more modes multimodal .\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.3",
      "section_title": "Symmetry and Modes",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, f (y) > f (x) if y \u2212\u0017 < x < y or y < x < y +\u0017.\nA PDF with one mode is called unimodal , with two modes bimodal , and with\ntwo or more modes multimodal .",
        "start": 206,
        "end": 365
      }
    ]
  },
  {
    "content": "2.4 Support of a Distribution\n\nThe support of a discrete distribution is the set of all y that have a positive\nprobability. More generally, a point y is in the support of a distribution if, for\nevery \u0017 > 0, the interval (y \u2212\u0017, y +\u0017) has positive probability. For example, the\nsupport of a normal distribution is (\u2212\u221e, \u221e), the support of a gamma or log-\nnormal distribution is [0, \u221e), and the support of a binomial(n, p) distribution\nis {0, 1, 2, . . . , n} provided p = 0, ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.4",
      "section_title": "Support of a Distribution",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, the interval (y \u2212\u0017, y +\u0017) has positive probability. For example, the\nsupport of a normal distribution is (\u2212\u221e, \u221e), the support of a gamma or log-\nnormal distribution is [0, \u221e), and the support of a binomial(n, p) distribution\nis {0, 1, 2, . . . , n} provided p = 0,",
        "start": 202,
        "end": 472
      }
    ]
  },
  {
    "content": "1.1\n1\n    It is assumed that most readers are already familiar with the normal, gamma, log-\n    normal, and binomial distributions. However, these distributions will be discussed\n    in some detail later.\n\f                      A.3 When Do Expected Values and Variances Exist?         671\n\nA.3 When Do Expected Values and Variances Exist?\nThe expected value of a random variable could be in\ufb01nite or not exist at\nall. Also, a random variable need not have a well-de\ufb01ned and \ufb01nite vari-\nance. To appreciate these facts, let Y be a random variable with density fY .\nThe expectation of Y is         \u0013   \u221e\n                                        yfY (y)dy\n                                   \u2212\u221e\n\nprovided that this integral is de\ufb01ned. If\n               \u0013 0                        \u0013 \u221e\n                    yfY (y)dy = \u2212\u221e and        yfY (y)dy = \u221e,                 (A.1)\n                 \u2212\u221e                           0\n\nthen the expectation is, formally, \u2212\u221e + \u221e, which is not de\ufb01ned, so the ex-\npectation does not exist. If integrals in (A.1) are both \ufb01nite, then E(Y ) exists\nand equals the sum of these two integrals. The expectation can exist but be\nin\ufb01nite, because if\n              \u0013 0                            \u0013 \u221e\n                    yfY (y)dy = \u2212\u221e and            yfY (y)dy < \u221e,\n                 \u2212\u221e                            0\n\nthen E(Y ) = \u2212\u221e, and if\n              \u0013 0                    \u0013 \u221e\n                  yfY (y)dy > \u2212\u221e and     yfY (y)dy = \u221e,\n                 \u2212\u221e                            0\n\nthen E(Y ) = \u221e.\n    If E(Y ) is not de\ufb01ned or is in\ufb01nite, then the variance that involves E(Y )\ncannot be de\ufb01ned either. If E(Y ) is de\ufb01ned and \ufb01nite, then the variance is\nalso de\ufb01ned. The variance is \ufb01nite if E(Y 2 ) < \u221e; otherwise the variance is\nin\ufb01nite.\n    The nonexistence of \ufb01nite expected values and variances is of impor-\ntance for modeling \ufb01nancial markets data, because, for example, the popular\nGARCH models discussed in Chap. 14 need not have \ufb01nite expected values\nand variances. Also, t-distributions that, as demonstrated in Chap. 5, can\nprovide good \ufb01ts to equity returns may have nonexistent means or variances.\n    One could argue that any variable Y derived from \ufb01nancial markets will be\nbounded, that is, that there is a constant M < \u221e such that P (|Y | \u2264 M ) = 1.\nIn this case, the integrals in (A.1) are both \ufb01nite, in fact at most M , and E(Y )\nexists and is \ufb01nite. Also, E(Y 2 ) \u2264 M 2 , so the variance of Y is \ufb01nite. So should\nwe worry at all about the mathematically niceties of whether expected values\nand variances exist and are \ufb01nite? The answer is that we should. A random\nvariable might be bounded in absolute value by a very large constant M and\nyet, if M is large enough, behave much like a random variable that does not\nhave an expected value or has an expected value that is in\ufb01nite or has a \ufb01nite\nexpected value but an in\ufb01nite variance. This can be seen in the simulations\n\f672      A Facts from Probability, Statistics, and Algebra\n\nof GARCH processes. Results from computer simulations are bounded by the\nmaximum size of a number in the computer. Yet these simulations behave as\nif the variance were in\ufb01nite.\n\n\nA.4 Monotonic Functions\n\nThe function g is increasing if g(x1 ) \u2264 g(x2 ) whenever x1 < x2 and strictly\nincreasing if g(x1 ) < g(x2 ) whenever x1 < x2 . Decreasing and strictly decreas-\ning are de\ufb01ned similarly, and g is (strictly) monotonic if it is either (strictly)\nincreasing or (strictly) decreasing.\n\n\nA.5 The Minimum, Maximum, In\ufb01num,\nand Supremum of a Set\nThe minimum and maximum of a set are its smallest and largest values, if\nthese exists. For example, if A = {x : 0 \u2264 x \u2264 1}, then the minimum\nand maximum of A are 0 and 1. However, not all sets have a minimum or a\nmaximum, for example, B = {x : 0 < x < 1} has neither a minimum nor a\nmaximum. Every set as an in\ufb01num (or inf) and a supremum (or sup). The inf\nof a set C is the largest number that is less than or equal to all elements of C.\nSimilarly, the sup of C is the smallest number that is greater than or equal\nto every element of C. The set B just de\ufb01ned has an inf of 0 and a sup of 1.\nThe following notation is standard: min(C) and max(C) are the minimum\nand maximum of C, if these exist, and inf(C) and sup(C) are the in\ufb01num and\nsupremum.\n\n\nA.6 Functions of Random Variables\n\nSuppose that X is a random variable with PDF fX (x) and Y = g(X) for g\na strictly increasing function. Since g is strictly increasing, it has an inverse,\nwhich we denote by h. Then Y is also a random variable and its CDF is\n\n      FY (y) = P (Y \u2264 y) = P {g(X) \u2264 y} = P {X \u2264 h(y)} = FX {h(y)}.         (A.2)\n\nDi\ufb00erentiating (A.2), we \ufb01nd the PDF of Y :\n\n                             fY (y) = fX {h(y)}h\u0003 (y).                      (A.3)\n\nApplying a similar argument to the case, where g is strictly decreasing, one\ncan show that whenever g is strictly monotonic, then\n\n                            fY (y) = fX {h(y)}|h\u0003 (y)|.                     (A.4)\n\f                                                      A.7 Random Samples     673\n\nAlso from (A.2), when g is strictly increasing, then\n                            FY\u22121 (p) = g{FX\n                                          \u22121\n                                             (p)},                         (A.5)\nso that the pth quantile of Y is found by applying g to the pth quantile of\nX. When g is strictly decreasing, then it maps the pth quantile of X to the\n(1 \u2212 p)th quantile of Y .\n\nResult A.1 Suppose that Y = a + bX for some constants a and b = 0. Let\ng(x) = a + bx, so that the inverse of g is h(y) = (y \u2212 a)/b and h\u0003 (y) = 1/b.\nThen\n                    FY (y) = FX {b\u22121 (y \u2212 a)}, b > 0,\n                           = 1 \u2212 FX {b\u22121 (y \u2212 a)}, b < 0,\n                    fY (y) = |b|\u22121 fX {b\u22121 (y \u2212 a)},\nand\n                     FY\u22121 (p) = a + bFX\n                                      \u22121\n                                         (p), b > 0\n                                      \u22121\n                              = a + bFX (1 \u2212 p), b < 0.\n\n\nA.7 Random Samples\nWe say that {Y1 , . . . , Yn } is a random sample from a probability distribution\nif they each have that probability distribution and are independent. In this\ncase, we also say that they are independent and identically distributed or sim-\nply i.i.d. The probability distribution is often called the population and its\nexpected value, variance, CDF, and quantiles are called the population mean,\npopulation variance, population CDF, and population quantiles. It is worth\nmentioning that the population is, in e\ufb00ect, in\ufb01nite. There is a statistical\ntheory of sampling, usually without replacement, from \ufb01nite populations, but\nsampling of this type will not concern us here. Even in cases where the popu-\nlation is \ufb01nite, such as, when sampling house prices, the population is usually\nlarge enough, so that it can be treated as in\ufb01nite.\n    If Y1 , . . . , Yn is a sample from an unknown probability distribution, then\nthe population mean can be estimated by the sample mean\n                                          n\n                               Y = n\u22121         Yi ,                        (A.6)\n                                         i=1\n\nand the population variance can be estimated by the sample variance\n                                \u0017n\n                                      (Yi \u2212 Y )2\n                          s2Y = i=1              .                  (A.7)\n                                     n\u22121\nThe reason for the denominator of n\u22121 rather than n is discussed in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.1",
      "section_title": "1",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "\u2212\u221e and     yfY (y)dy = \u221e,\n                 \u2212\u221e                            0",
        "start": 1420,
        "end": 1498
      },
      {
        "language": "r",
        "code": "0,\n                           = 1 \u2212 FX {b\u22121 (y \u2212 a)}, b < 0,\n                    fY (y) = |b|\u22121 fX {b\u22121 (y \u2212 a)},\nand\n                     FY\u22121 (p) = a + bFX\n                                      \u22121\n                                         (p), b > 0\n                                      \u22121\n                              = a + bFX (1 \u2212 p), b < 0.",
        "start": 5632,
        "end": 5983
      }
    ]
  },
  {
    "content": "5.9.\nThe sample standard deviation is sY , the square root of s2Y .\n\f674      A Facts from Probability, Statistics, and Algebra\n\nA.8 The Binomial Distribution\nSuppose that we conduct n experiments for some \ufb01xed (nonrandom) integer\nn. On each experiment there are two possible outcomes called \u201csuccess\u201d and\n\u201cfailure\u201d; the probability of a success is p, and the probability of a failure is\nq = 1 \u2212 p. It is assumed that p and q are the same for all n experiments.\nLet Y be the total number of successes, so that Y will equal 0, 1, 2, . . ., or n.\nIf the experiments are independent, then\n                            \u0007 \b\n                               n\n               P (Y = k) =        pk q n\u2212k for k = 0, 1, 2, . . . , n,\n                               k\n\nwhere                          \u0007 \b\n                                n        n!\n                                   =            .\n                                k    k!(n \u2212 k)!\n   The distribution of Y is called the binomial distribution and denoted\nBinomial(n, p). The expected value of Y is np and its variance is npq. The\nBinomial(1, p) distribution is also called the Bernoulli distribution and its\ndensity is\n                     P (Y = y) = py (1 \u2212 p)1\u2212y , y = 0, 1.              (A.8)\nNotice that py is equal to either p (when y = 1) or 1 (when y = 0), and\nsimilarly for (1 \u2212 p)1\u2212y .\n   The functions pbinom(), dbinom(), qbinom(), and rbinom() compute bi-\nnomial CDFs, pdfs, quantiles, and random numbers, respectively. For\nexample,\n      > pbinom(3,6,0.5)\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9",
      "section_title": "The sample standard deviation is sY , the square root of s2Y .",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "pbinom(3,6,0.5)\n      [1]",
        "start": 1502,
        "end": 1530
      }
    ]
  },
  {
    "content": "0.65625\n\nshows that the probability of 3 or less heads in 6 tosses of a fair coin is 0.65625.\n\n\nA.9 Some Common Continuous Distributions\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.65625",
      "section_title": "shows that the probability of 3 or less heads in 6 tosses of a fair coin is 0.65625.",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.1 Uniform Distributions\n\nThe uniform distribution on the interval (a, b) is denoted by Uniform(a, b) and\nhas PDF equal to 1/(b \u2212 a) on (a, b) and equal to 0 outside this interval. It is\neasy to check that if Y is Uniform(a, b), then its expectation is\n                                     \u0013 b\n                                 1                  a+b\n                       E(Y ) =           Y dY =          ,\n                                b\u2212a a                2\nwhich is the midpoint of the interval. Also,\n                           \u0013 b\n                2      1                   Y 3 |ba   b2 + ab + a2\n            E(Y ) =            Y 2 dY =            =              .\n                     b\u2212a a               3(b \u2212 a)          3\n\f                            A.9 Some Common Continuous Distributions                 675\n\nTherefore,\n                                                    \u0007         \b2\n                                   b2 + ab + a2         a+b            (b \u2212 a)2\n      \u03c3Y2 = E(Y 2 ) \u2212 {E(Y )}2 =                \u2212                  =            .\n                                         3               2                12\n\nReparameterization means replacing the parameters of a distribution by an\nequivalent set. The uniform distribution\n                                 \u221a          can be reparameterized by using\n\u03bc = (a + b)/2 and \u03c3 = (b \u2212 a)/ 12 as the parameters. Then \u03bc is a location\nparameter and \u03c3 is the scale parameter. Which parameterization of a distri-\nbution is used depends upon which aspects of the distribution one wishes to\nemphasize. The parameterization (a, b) of the uniform speci\ufb01es its endpoints\nwhile the parameterization (\u03bc, \u03c3) gives the mean and standard deviation. One\nis free to move back and forth between two or more parameterizations, using\nwhichever is most useful in a given context. The uniform distribution does not\nhave a shape parameter since the shape of its density is always rectangular.\n    The functions punif(), dunif(), qunif(), and runif() compute uniform\nCDFs, pdfs, quantiles, and random numbers, respectively. For example,\n   > runif(3,0,5)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.1",
      "section_title": "Uniform Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "runif(3,0,5)\n   [1]",
        "start": 2092,
        "end": 2114
      }
    ]
  },
  {
    "content": "1.799252 4.003232 3.978002\n\nare three random numbers uniformly distributed between 0 and 5.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.799252",
      "section_title": "4.003232 3.978002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2 Transformation by the CDF and Inverse CDF\n\nIf Y has a continuous CDF F , then F (Y ) has a Uniform(0,1) distribution.\nF (Y ) is often called the probability transformation of Y . This fact is easy to\nsee if F is strictly increasing, since then F \u22121 exists, so that\n\n             P {F (Y ) \u2264 y} = P {Y \u2264 F \u22121 (y)} = F {F \u22121 (y)} = y.                  (A.9)\n\nThe result holds even if F is not strictly increasing, but the proof is slightly\nmore complicated. It is only necessary that F be continuous.\n   If U is Uniform(0,1) and F is a CDF, then Y = F \u2212 (U ) has F as its CDF.\nHere F \u2212 is the pseudo-inverse of F . This can be proved easily when F is\ncontinuous and strictly increasing, since then F \u22121 = F \u2212 and\n\n             P (Y \u2264 y) = P {F \u22121 (U ) \u2264 y} = P {Y \u2264 F (y)} = F (y).\n\nIn fact, the result holds for any CDF F , but it is more di\ufb03cult to prove in\nthe general case. F \u2212 (U ) is often called the quantile transformation since F \u2212\nis the quantile function.\n\f676      A Facts from Probability, Statistics, and Algebra\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "Transformation by the CDF and Inverse CDF",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.3 Normal Distributions\n\nThe standard normal distribution has the familiar bell-shaped density\n                            1   -       .\n                    \u03c6(y) = \u221a exp \u2212y 2 /2 ,           \u2212\u221e < y < \u221e.\n                            2\u03c0\nThe standard normal has mean 0 and variance 1. If Z is standard normal, then\nthe distribution of \u03bc + \u03c3Z is called the normal distribution with mean \u03bc and\nvariance \u03c3 2 and denoted by N (\u03bc, \u03c3 2 ). By Result A.1, the N (\u03bc, \u03c3 2 ) density is\n                       \u0007     \b               \u000e            \u000f\n                   1     y\u2212\u03bc          1          (y \u2212 \u03bc)2\n                     \u03c6          =\u221a        exp \u2212             .              (A.10)\n                  \u03c3       \u03c3          2\u03c0\u03c3            2\u03c3 2\nThe parameter \u03bc is a location parameter and \u03c3 is a scale parameter. The\nnormal distribution does not have a shape parameter since its density is always\nthe same bell-shaped curve.2 The standard normal CDF is\n                                     \u0013 y\n                             \u03a6(y) =      \u03c6(u)du.\n                                            \u2212\u221e\n\n\u03a6 can be evaluated using software such as R\u2019s pnorm function. If Y is N (\u03bc, \u03c3 2 ),\nthen since Y = \u03bc + \u03c3Z, where Z is standard normal, by Result A.1,\n\n                                FY (y) = \u03a6{(y \u2212 \u03bc)/\u03c3}.                            (A.11)\n\nNormal distribution are also called Gaussian distributions after the great Ger-\nman mathematician Carl Friedrich Gauss.\n\nNormal Quantiles\n\nThe q-quantile of the N (0, 1) distribution is \u03a6\u22121 (q) and, more generally, the\nq-quantile of an N (\u03bc, \u03c3 2 ) distribution is \u03bc + \u03c3\u03a6\u22121 (q). The \u03b1-upper quantile\nof \u03a6, that is, \u03a6\u22121 (1 \u2212 \u03b1), is denoted by z\u03b1 . As shown later, z\u03b1 is widely used\nfor con\ufb01dence intervals.\n    For example, z",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.3",
      "section_title": "Normal Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1 and z0.01 are 1.282 and 2.326, respectively, as can be\nseen in the following R output:\n      > round(qnorm(c(0.1, 0.01), lower.tail = FALSE), 3)\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "and z0.01 are 1.282 and 2.326, respectively, as can be",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "round(qnorm(c(0.1, 0.01), lower.tail = FALSE), 3)\n      [1]",
        "start": 97,
        "end": 159
      }
    ]
  },
  {
    "content": "1.282 2.326\n\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.282",
      "section_title": "2.326",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.4 The Lognormal Distribution\n\nIf Z is distributed N (\u03bc, \u03c3 2 ), then Y = exp(Z) is said to have a Lognormal(\u03bc,\n\u03c3 2 ) distribution. In other words, Y is lognormal if its logarithm is normally\n\n2\n    In contrast, a t-density is also a bell curve, but the exact shape of the bell depends\n    on a shape parameter, the degrees of freedom which is a tail index.\n\f                               A.9 Some Common Continuous Distributions         677\n\n                                   lognormal densities\n\n                                                           \u03bc = 1.0, \u03c3 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.4",
      "section_title": "The Lognormal Distribution",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                                                           \u03bc = 1.0, \u03c3 = 0.5\n\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "\u03bc = 1.0, \u03c3 = 0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.30\n                                                           \u03bc = 1.5, \u03c3 = 0.5\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.30",
      "section_title": "\u03bc = 1.5, \u03c3 = 0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.20\n          density\n                    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.20",
      "section_title": "density",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.10\n                    0.00\n\n\n\n\n                           0        5                10                  15\n                                             y\n\nFig. A.1. Examples of lognormal probability densities. Here \u03bc and \u03c3 are the log-\nmean and log-standard deviation, that is, the mean and standard deviation of the\nlogarithm of the lognormal random variable.\n\n\ndistributed. We will call \u03bc the log-mean and \u03c3 the log-standard deviation.\nAlso, \u03c3 2 will be called the log-variance.\n    The median of Y is exp(\u03bc) and the expected value of Y is exp(\u03bc + \u03c3 2 /2).3\nThe expectation is larger than the median because the lognormal distribution\nis right skewed, and the skewness is more extreme with larger values of \u03c3.\nSkewness is discussed further in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.10",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.4. The probability density functions\nof several lognormal distributions are shown in Fig. A.1.\n    The log-mean \u03bc is a scale parameter and the log-standard deviation \u03c3\nis a shape parameter. The lognormal distribution does not have a location\nparameter since its support is \ufb01xed to start at 0.\n    Use the functions plnorm(), dlnorm(), qlnorm(), and rlnorm() for the\nlognormal distribution. For example,\n     > options(digits = 3)\n     > dlnorm(0.5, meanlog = 1, sdlog = 2)\n     [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.4",
      "section_title": "The probability density functions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "options(digits = 3)\n     > dlnorm(0.5, meanlog = 1, sdlog = 2)\n     [1]",
        "start": 410,
        "end": 484
      }
    ]
  },
  {
    "content": "0.279\n\ncomputes the lognormal density at 0.5 when the log-mean is 1 and the log-\nstandard deviation is 2.\n\n\n\n\n3\n    It is important to remember that if Y is lognormal(\u03bc, \u03c3), then \u03bc is the expected\n    value of log(Y ), not of Y .\n\f678    A Facts from Probability, Statistics, and Algebra\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.279",
      "section_title": "computes the lognormal density at 0.5 when the log-mean is 1 and the log-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.5 Exponential and Double-Exponential Distributions\n\nThe exponential distribution with scale parameter \u03b8 > 0, which we denote by\nExponential(\u03b8), has CDF\n                          F (y) = 1 \u2212 e\u2212y/\u03b8 ,       y > 0.\nThe Exponential(\u03b8) distribution has PDF\n                                          e\u2212y/\u03b8\n                                f (y) =         ,                        (A.12)\n                                            \u03b8\nexpected value \u03b8, and standard deviation \u03b8. The inverse CDF is\n                    F \u22121 (y) = \u2212\u03b8 log(1 \u2212 y),        0 < y < 1.\nUse the functions pexp(), dexp(), qexp(), and rexp() for the exponential\ndistribution.\n    The double-exponential or Laplace distribution with mean \u03bc and scale pa-\nrameter \u03b8 has PDF\n                                     e\u2212|y\u2212\u03bc|/\u03b8\n                             f (y) =           .                     (A.13)\n                                         2\u03b8\nIf Y has a double-exponential distribution with mean \u03bc, then |Y \u2212 \u03bc| has\nan exponential distribution.\n                  \u221a          A double-exponential distribution has a stan-\ndard deviation of 2\u03b8. The mean \u03bc is a location parameter and \u03b8 is a scale\nparameter.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.5",
      "section_title": "Exponential and Double-Exponential Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0, which we denote by\nExponential(\u03b8), has CDF\n                          F (y) = 1 \u2212 e\u2212y/\u03b8 ,       y > 0.\nThe Exponential(\u03b8) distribution has PDF\n                                          e\u2212y/\u03b8\n                                f (y) =         ,                        (A.12)\n                                            \u03b8\nexpected value \u03b8, and standard deviation \u03b8. The inverse CDF is\n                    F \u22121 (y) = \u2212\u03b8 log(1 \u2212 y),        0 < y < 1.\nUse the functions pexp(), dexp(), qexp(), and rexp() for the exponential\ndistribution.\n    The double-exponential or Laplace distribution with mean \u03bc and scale pa-\nrameter \u03b8 has PDF\n                                     e\u2212|y\u2212\u03bc|/\u03b8\n                             f (y) =           .                     (A.13)\n                                         2\u03b8\nIf Y has a double-exponential distribution with mean \u03bc, then |Y \u2212 \u03bc| has\nan exponential distribution.\n                  \u221a          A double-exponential distribution has a stan-\ndard deviation of 2\u03b8. The mean \u03bc is a location parameter and \u03b8 is a scale\nparameter.",
        "start": 106,
        "end": 1166
      }
    ]
  },
  {
    "content": "9.6 Gamma and Inverse-Gamma Distributions\n\nThe gamma distribution with scale parameter b > 0 and shape parameter\n\u03b1 > 0 has density\n                              y \u03b1\u22121\n                                       exp(\u2212y/b),\n                             \u0393 (\u03b1)b\u03b1\nwhere \u0393 is the gamma function de\ufb01ned in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.6",
      "section_title": "Gamma and Inverse-Gamma Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and shape parameter\n\u03b1 > 0 has density\n                              y \u03b1\u22121\n                                       exp(\u2212y/b),\n                             \u0393 (\u03b1)b\u03b1\nwhere \u0393 is the gamma function de\ufb01ned in Sect.",
        "start": 89,
        "end": 300
      }
    ]
  },
  {
    "content": "5.5.2. The mean, variance, and\nskewness coe\ufb03cient of this distribution are b\u03b1, b2 \u03b1, and 2\u03b1\u22121/2 , respectively.\nFigure A.2 shows gamma densities with shape parameters equal to 0.75, 3/2,\nand 7/2 and each with a mean equal to 1.\n   The gamma distribution is often parameterized using \u03b2 = 1/b, so that the\ndensity is\n                             \u03b2 \u03b1 y \u03b1\u22121\n                                       exp(\u2212\u03b2y).\n                               \u0393 (\u03b1)\nWith this form of the parameterization, \u03b2 is an inverse-scale parameter and\nthe mean and variance are \u03b1/\u03b2 and \u03b1/\u03b2 2 . Also, \u03b2 is often called the rate\nparameter, e.g., in R.\n   Use the functions pgamma(), dgamma(), qgamma(), and rgamma() for the\ngamma distribution. For example, the median of the gamma distribution with\n\u03b1 = 2 and \u03b2 = 3 can be computed in two equivalent ways:\n\f                                     A.9 Some Common Continuous Distributions      679\n\n                                          gamma densities\n\n\n\n\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "2. The mean, variance, and",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.0\n                                                   \u03b1=0.75\n                                                   \u03b1=3/2\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.0",
      "section_title": "\u03b1=0.75",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.5                             \u03b1=7/2\n         density\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.5",
      "section_title": "\u03b1=7/2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.0\n                   0.5\n                   ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.0",
      "section_title": "0.5",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                         0            1           2              3       4\n                                                  y\n\nFig. A.2. Examples of gamma probability densities with di\ufb00ering shape parameters.\nIn each case, the scale parameter has been chosen so that the expectation is 1.\n\n\n   > qgamma(0.5, shape = 2, rate = 3)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0            1           2              3       4",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qgamma(0.5, shape = 2, rate = 3)\n   [1]",
        "start": 303,
        "end": 345
      }
    ]
  },
  {
    "content": "0.559\n   > qgamma(0.5, shape = 2, scale = 1/3)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.559",
      "section_title": "> qgamma(0.5, shape = 2, scale = 1/3)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qgamma(0.5, shape = 2, scale = 1/3)\n   [1]",
        "start": 9,
        "end": 54
      }
    ]
  },
  {
    "content": "0.559\n\n    If X has a gamma distribution with inverse-scale parameter \u03b2 and shape\nparameter \u03b1, then we say that 1/X has an inverse-gamma distribution with\nscale parameter \u03b2 and shape parameter \u03b1. The mean of this distribution is\n\u03b2/(\u03b1 \u2212 1) provided \u03b1 > 1 and the variance is \u03b2 2 /{(\u03b1 \u2212 1)2 (\u03b1 \u2212 2)} provided\nthat \u03b1 > 2.\n\n\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.559",
      "section_title": "If X has a gamma distribution with inverse-scale parameter \u03b2 and shape",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "1 and the variance is \u03b2 2 /{(\u03b1 \u2212 1)2 (\u03b1 \u2212 2)} provided\nthat \u03b1 > 2.",
        "start": 250,
        "end": 320
      }
    ]
  },
  {
    "content": "9.7 Beta Distributions\nThe beta distribution with shape parameters \u03b1 > 0 and \u03b2 > 0 has density\n                             \u0393 (\u03b1 + \u03b2) \u03b1\u22121\n                                        y  (1 \u2212 y)\u03b2\u22121 ,     0 < y < 1.          (A.14)\n                             \u0393 (\u03b1)\u0393 (\u03b2)\nThe mean and variance are \u03b1/(\u03b1 + \u03b2) and (\u03b1\u03b2)/{(\u03b1 + \u03b2)2 (\u03b1 + \u03b2 + 1)}, and\nif \u03b1 > 1 and \u03b2 > 1, then the mode is (\u03b1 \u2212 1)/(\u03b1 + \u03b2 \u2212 2).\n    Figure A.3 shows beta densities for several choices of shape parameters. A\nbeta density is right-skewed, symmetric about 1/2, or left-skewed depending\non whether \u03b1 < \u03b2, \u03b1 = \u03b2, or \u03b1 > \u03b2.\n    Use the functions pbeta(), dbeta(), qbeta(), and rbeta() for the beta\ndistribution. For example, the code below created Fig. A.3.\n\f680      A Facts from Probability, Statistics, and Algebra\n\n                                          beta densities\n\n\n\n\n                     5\n                                                    \u03b1 = 3, \u03b2 = 9\n                                                    \u03b1 = 5, \u03b2 = 5\n                     4\n                     3                              \u03b1 = 4, \u03b2 = 1/2\n           density\n                     2\n                     1\n                     0\n\n\n\n\n                         ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.7",
      "section_title": "Beta Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0 and \u03b2 > 0 has density\n                             \u0393 (\u03b1 + \u03b2) \u03b1\u22121\n                                        y  (1 \u2212 y)\u03b2\u22121 ,     0 < y < 1.          (A.14)\n                             \u0393 (\u03b1)\u0393 (\u03b2)\nThe mean and variance are \u03b1/(\u03b1 + \u03b2) and (\u03b1\u03b2)/{(\u03b1 + \u03b2)2 (\u03b1 + \u03b2 + 1)}, and\nif \u03b1 > 1 and \u03b2 > 1, then the mode is (\u03b1 \u2212 1)/(\u03b1 + \u03b2 \u2212 2).\n    Figure A.3 shows beta densities for several choices of shape parameters. A\nbeta density is right-skewed, symmetric about 1/2, or left-skewed depending\non whether \u03b1 < \u03b2, \u03b1 = \u03b2, or \u03b1 > \u03b2.\n    Use the functions pbeta(), dbeta(), qbeta(), and rbeta() for the beta\ndistribution. For example, the code below created Fig. A.3.\n\f680      A Facts from Probability, Statistics, and Algebra",
        "start": 69,
        "end": 781
      }
    ]
  },
  {
    "content": "0.0   0.2        0.4          0.6           0.8   1.0\n                                                y\n\nFig. A.3. Examples of beta probability densities with di\ufb00ering shape parameters.\n\n\n      pdf(\"beta_densities.pdf\", width = 6, height = 5) ## Figure A.3\n      par(lwd = 2)\n      x = seq(0, 1, 0.01)\n      plot(x, dbeta(x, 3, 9), type = \"l\", lty = 1, xlab = \"y\",\n           ylab = \"density\", main = \"beta densities\", ylim = c(0, 5))\n      lines(x, dbeta(x, 5, 5), type = \"l\", lty = 2, col = \"red\")\n      lines(x, dbeta(x, 4, 1/2), type = \"l\", lty = 5, col = \"blue\")\n      legend(0.4, 5, c(\n        expression(paste(alpha,\" = 3, \",beta,\" = 9\")) ,\n        expression(paste(alpha,\" = 5, \",beta,\" = 5\")),\n        expression(paste(alpha,\" = 4, \",beta,\" = 1/2\"))),\n             lty = c(1, 2,5 ), col = c(\"black\", \"red\", \"blue\"), lwd = 2)\n      graphics.off()\n\n\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.2        0.4          0.6           0.8   1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.8 Pareto Distributions\nA random variable X has a Pareto distribution, named after the Swiss eco-\nnomics professor Vilfredo Pareto (1848\u20131923), if its CDF for some a > 0\n                                    & c 'a\n                        F (x) = 1 \u2212        , x > c,                  (A.15)\n                                      x\nwhere c > 0 is the minimum possible value of X.\n   The PDF of the distribution in (A.15) is\n                                    aca\n                                f (x) = , x > c,                          (A.16)\n                                   xa+1\nso a Pareto distribution has polynomial tails and a is the tail index. It is also\ncalled the Pareto constant.\n\f                                       A.10 Sampling a Normal Distribution           681\n\nA.10 Sampling a Normal Distribution\nA common situation is that we have a random sample from a normal distri-\nbution and we wish to have con\ufb01dence intervals for the mean and variance or\ntest hypotheses about these parameters. Then, the following distributions are\nvery important, since they are the basis for many commonly used con\ufb01dence\nintervals and tests.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.8",
      "section_title": "Pareto Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "0\n                                    & c 'a\n                        F (x) = 1 \u2212        , x > c,                  (A.15)\n                                      x\nwhere c > 0 is the minimum possible value of X.\n   The PDF of the distribution in (A.15) is\n                                    aca\n                                f (x) = , x > c,                          (A.16)\n                                   xa+1\nso a Pareto distribution has polynomial tails and a is the tail index. It is also\ncalled the Pareto constant.\n\f                                       A.10 Sampling a Normal Distribution           681",
        "start": 167,
        "end": 784
      }
    ]
  },
  {
    "content": "10.1 Chi-Squared Distributions\nSuppose that Z1 , . . . , Zn are i.i.d. N (0, 1). Then, the distribution of Z12 + \u00b7 \u00b7 \u00b7 +\nZn2 is called the chi-squared distribution with n degrees of freedom. This distri-\nbution has an expected value of n and a variance of 2n. The \u03b1-upper quantile\nof this distribution is denoted by \u03c72\u03b1,n and is used in tests and con\ufb01dence in-\ntervals about variances; see Appendix A.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "Chi-Squared Distributions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.1 for the latter. Also, as discussed\nin Sect. 5.11, \u03c72\u03b1,n is used in likelihood ratio testing. As an example, \u03c720.05,10\nis ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.1",
      "section_title": "for the latter. Also, as discussed",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.31 and can be computed in two ways:\n   > qchisq(0.05, 10, lower.tail = FALSE)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.31",
      "section_title": "and can be computed in two ways:",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qchisq(0.05, 10, lower.tail = FALSE)\n   [1]",
        "start": 42,
        "end": 88
      }
    ]
  },
  {
    "content": "18.31\n   > qchisq(0.95, 10)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.31",
      "section_title": "> qchisq(0.95, 10)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qchisq(0.95, 10)\n   [1]",
        "start": 9,
        "end": 35
      }
    ]
  },
  {
    "content": "18.31\n    So far, the degrees-of-freedom parameter has been an integer-valued, but\nthis can be generalized. The chi-squared distribution with \u03bd degrees of free-\ndom is equal to the gamma distribution with scale parameter equal to 2 and\nshape parameter equal to \u03bd/2. Thus, since the shape parameter of a gamma\ndistribution can be any positive value, the chi-squared distribution can be\nde\ufb01ned for any positive value of \u03bd as the gamma distribution with scale and\nshape parameters equal to 2 and \u03bd/2, respectively.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.31",
      "section_title": "So far, the degrees-of-freedom parameter has been an integer-valued, but",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "10.2 F -Distributions\nIf U and W are independent and chi-squared-distributed with n1 and n2\ndegrees of freedom, respectively, then the distribution of\n                                         U/n1\n                                         W/n2\nis called the F -distribution with n1 and n2 degrees of freedom. The \u03b1-upper\nquantile of this distribution is denoted by F\u03b1,n1 ,n2 . F\u03b1,n1 ,n2 is used as a critical\nvalue for F -tests in regression. For example, F0.95,3,7 is 4.347:\n   > qf(0.95, 3, 7)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "10.2",
      "section_title": "F -Distributions",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qf(0.95, 3, 7)\n   [1]",
        "start": 478,
        "end": 502
      }
    ]
  },
  {
    "content": "4.347\n   > qf(0.05, 3, 7, lower.tail = FALSE)\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.347",
      "section_title": "> qf(0.05, 3, 7, lower.tail = FALSE)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "qf(0.05, 3, 7, lower.tail = FALSE)\n   [1]",
        "start": 9,
        "end": 53
      }
    ]
  },
  {
    "content": "4.347\n    The degrees-of-freedom parameters of the chi-square, t-, and F -distribu-\ntions are shape parameters.\n\f682     A Facts from Probability, Statistics, and Algebra\n\nA.11 Law of Large Numbers and the Central Limit\nTheorem for the Sample Mean\nSuppose that Y n is the mean of an i.i.d. sample Y1 , . . . , Yn . We assume that\ntheir common expected value E(Y1 ) exists and is \ufb01nite and call it \u03bc. The law\nof large numbers states that\n\n                            P (Y n \u2192 \u03bc as n \u2192 \u221e) = 1.\n\nThus, the sample mean will be close to the population mean for large enough\nsample sizes. However, even more is true. The famous central limit theorem\n(CLT) states that if the common variance \u03c3 2 of Y1 , . . . , Yn is \ufb01nite, then\nthe probability distribution of Y n gets closer to a normal distribution as n\nconverges to \u221e. More precisely, the CLT states that\n            \u221a\n         P { n(Y n \u2212 \u03bc) \u2264 y} \u2192 \u03a6(y/\u03c3) as n \u2192 \u221e for all y.               (A.17)\n\nStated di\ufb00erently, for large n, Y is approximately N (\u03bc, \u03c3 2 /n).\n    Students often misremember or misunderstand the CLT. A common mis-\nconception is that a large population is approximately normally distributed.\nThe CLT says nothing about the distribution of a population; it is only a state-\nment about the distribution of a sample mean. Also, the CLT does not assume\nthat the population is large; it is the size of the sample that is converging to in-\n\ufb01nity. Assuming that the sampling is with replacement, the population could\nbe quite small, in fact, with only two elements.\n    When the variance of Y1 , . . . , Yn is in\ufb01nite, then the limit distribution of\nY n may still exist but will be a nonnormal stable distribution.\n    Although the CLT was \ufb01rst discovered for the sample mean, other estima-\ntors are now known to also have approximate normal distributions for large\nsample sizes. In particular, there are central limit theorems for the maximum\nlikelihood estimators of Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "4.347",
      "section_title": "The degrees-of-freedom parameters of the chi-square, t-, and F -distribu-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9 and the least-squares estimators discussed\nin Chap. 9. This is very important, since most estimators we use will be\nmaximum likelihood estimators or least-squares estimators. So, if we have a\nreasonably large sample, we can assume that these estimators have an ap-\nproximately normal distribution and the normal distribution can be used for\ntesting and constructing con\ufb01dence intervals.\n\n\nA.12 Bivariate Distributions\nLet fY1 ,Y2 (y1 , y2 ) be the joint density of a pair of random variables (Y1 , Y2 ).\nThen, the marginal density of Y1 is obtained by \u201cintegrating out\u201d Y2 :\n                                       \u0013\n                            fY1 (y1 ) = fY1 ,Y2 (y1 , y2 ) dy2 ,\n                            \u0016\nand similarly fY2 (y2 ) =       fY1 ,Y2 (y1 , y) dy1 .\n\f                                                    A.13 Correlation and Covariance            683\n\n   The conditional density of Y2 given Y1 is\n                                                     fY1 ,Y2 (y1 , y2 )\n                              fY2 |Y1 (y2 |y1 ) =                       .                   (A.18)\n                                                        fY1 (y1 )\n\nEquation (A.18) can be rearranged to give the joint density of Y1 and Y2 as\nthe product of a marginal density and a conditional density:\n\n      fY1 ,Y2 (y1 , y2 ) = fY1 (y1 )fY2 |Y1 (y2 |y1 ) = fY2 (y2 )fY1 |Y2 (y1 |y2 ).         (A.19)\n\nThe conditional expectation of Y2 given Y1 is just the expectation calculated\nusing fY2 |Y1 (y2 |y1 ):\n                                          \u0013\n                         E(Y2 |Y1 = y1 ) = y2 fY2 |Y1 (y2 |y1 )dy2 ,\n\nwhich is, of course, a function of y1 . The conditional variance of Y2 given Y1 is\n                             \u0013\n        Var(Y2 |Y1 = y1 ) = {y2 \u2212 E(Y2 |Y1 = y1 )}2 fY2 |Y1 (y2 |y1 ) dy2 .\n\n   A formula that is important elsewhere in this book is\n\n              fY1 ,...,Yn (y1 , . . . , yn ) = fY1 (y1 )fY2 |Y1 (y2 |y1 ) \u00b7 \u00b7 \u00b7\n                                              fYn |Y1 ,...,Yn\u22121 (yn |y1 , . . . , yn\u22121 ),   (A.20)\n\nwhich follows from repeated use of (A.19).\n   The marginal mean and variance are related to the conditional mean and\nvariance by\n                           E(Y ) = E{E(Y |X)}                      (A.21)\nand\n                      Var(Y ) = E{Var(Y |X)} + Var{E(Y |X)}.                                (A.22)\nResult (A.21) has various names, especially the law of iterated expectations\nand the tower rule.\n   Another useful formula is that if Z is a function of X, then\n\n                                   E(ZY |X) = ZE(Y |X).                                     (A.23)\n\nThe idea here is that, given X, Z is constant and can be factored outside the\nconditional expectation.\n\n\nA.13 Correlation and Covariance\nExpectations and variances summarize the individual behavior of random\nvariables. If we have two random variables, X and Y , then it is convenient to\nhave some way to summarize their joint behavior\u2014correlation and covariance\ndo this.\n\f684      A Facts from Probability, Statistics, and Algebra\n\n      The covariance between two random variables X and Y is\n                                    \u001a                        \u001b\n               Cov(X, Y ) = \u03c3XY = E {X \u2212 E(X)}{Y \u2212 E(Y )} .\n\nThe two notations Cov(X, Y ) and \u03c3XY will be used interchangeably. If (X, Y )\nis continuously distributed, then using (A.36), we have\n                      \u0013\n              \u03c3XY = {x \u2212 E(X)}{y \u2212 E(Y )}fXY (x, y) dx dy.\n\nThe following are useful formulas:\n                   \u03c3XY = E(XY ) \u2212 E(X)E(Y ),                               (A.24)\n                   \u03c3XY = E[{X \u2212 E(X)}Y ],                                  (A.25)\n                   \u03c3XY = E[{Y \u2212 E(Y )}X],                                  (A.26)\n                   \u03c3XY = E(XY ) if E(X) = 0 or E(Y ) = 0.                  (A.27)\n    The covariance between two variables measures the linear association be-\ntween them, but it is also a\ufb00ected by their variability; all else equal, random\nvariables with larger standard deviations have a larger covariance. Correla-\ntion is covariance after this size e\ufb00ect has been removed, so that correlation\nis a pure measure of how closely two random variables are related, or more\nprecisely, linearly related. The Pearson correlation coe\ufb03cient between X and\nY is\n                        Corr(X, Y ) = \u03c1XY = \u03c3XY /\u03c3X \u03c3Y .                  (A.28)\nThe Pearson correlation coe\ufb03cient is sometimes called simply the correla-\ntion coe\ufb03cient, though there are other types of correlation coe\ufb03cients; see\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9",
      "section_title": "and the least-squares estimators discussed",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "8.5.\n    Given a bivariate sample {(Xi , Yi )}ni=1 , the sample covariance, denoted by\nsXY or \u03c3\u0002XY , is\n                                            n\n                       \u0002XY = (n \u2212 1)\u22121\n                 sXY = \u03c3                         (Xi \u2212 X)(Yi \u2212 Y ),        (A.29)\n                                           i=1\n\nwhere X and Y are the sample means. Often the factor (n \u2212 1)\u22121 is replaced\nby n\u22121 , but this change has little e\ufb00ect relative to the random variation in\n\u0002XY . The sample correlation is\n\u03c3\n                                            sXY\n                           \u03c1\u0002XY = rXY =           ,                   (A.30)\n                                           sX sY\nwhere sX and sY are the sample standard deviations.\n    To provide the reader with a sense of what particular values of a correla-\ntion coe\ufb03cient imply about the relationship between two random variables,\nFig. A.4 shows scatterplots and the sample correlation coe\ufb03cients for nine\nbivariate random samples. A scatterplot is just a plot of a bivariate sample,\n{(Xi , Yi )}ni=1 . Each plot also contain the linear least-squares \ufb01t (Chap. 9) to\nillustrate the linear relationship between y and x. Notice that\n\f                                                                                       A.13 Correlation and Covariance                                           685\n\n\u2022      an absolute correlation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "8.5",
      "section_title": "Given a bivariate sample {(Xi , Yi )}ni=1 , the sample covariance, denoted by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.25 or less is weak\u2014see panels (a) and (b);\n\u2022      an absolute correlation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.25",
      "section_title": "or less is weak\u2014see panels (a) and (b);",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5 is only moderately strong\u2014see (c);\n\u2022      an absolute correlation of ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "is only moderately strong\u2014see (c);",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9 is strong\u2014see (d);\n\u2022      an absolute correlation of 1 implies an exact linear relationship\u2014see (e)\n       and (h);\n\u2022      a strong nonlinear relationship may or may not imply a high correlation\u2014\n       see (f) and (g);\n\u2022      positive correlations imply an increasing relationship (as X increases, Y\n       increases on average)\u2014see (b)\u2013(e) and (g);\n\u2022      negative correlations imply a decreasing relationship (as X increases, Y\n       decreases on average)\u2014see (h) and (i).\n\n\nIf the correlation between two random variables is equal to 0, then we say\nthat they are uncorrelated.\n\n\na                                 r=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "is strong\u2014see (d);",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.008\n                                                              b                        r=0.25\n                                                                                                                   c                       r=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.008",
      "section_title": "b                        r=0.25",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.5\n\n\n\n\n                                                                                                                       0.8\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.5",
      "section_title": "0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                                                                       0.6\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n    0.6\ny\n\n\n\n\n                                                              y\n\n\n\n\n                                                                                                                   y\n\n                                                                                                                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "0.6",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n                                                                  0.4\n    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.4",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                                                                                                                       0.2\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n\n\n\n\n                      0.0   0.2   0.4       0.6   0.8   1.0              0.0    0.2    0.4       0.6   0.8                    0.0   0.2   0.4       0.6    0.8\n                                        x                                                    x                                                  x\n\n\nd                                  r=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.0   0.2   0.4       0.6   0.8   1.0              0.0    0.2    0.4       0.6   0.8                    0.0   0.2   0.4       0.6    0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.9\n                                                              e                             r=1\n                                                                                                                   f                      r=\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.9",
      "section_title": "e                             r=1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05\n    0.2 0.4 0.6 0.8\n\n\n\n\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "0.2 0.4 0.6 0.8",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.8\n\n\n\n\n                                                                                                                       0.10\ny\n\n\n\n\n                                                              y\n\n\n\n\n                                                                                                                   y\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.8",
      "section_title": "0.10",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.4\n\n\n\n\n                                                                                                                       0.00\n                                                                  ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.4",
      "section_title": "0.00",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.0\n\n\n\n\n                      0.0   0.2   0.4       0.6   0.8                    0.0    0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4       0.6   0.8    1.0\n                                        x                                                    x                                                  x\n\n\ng                                 r=",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0",
      "section_title": "0.0   0.2   0.4       0.6   0.8                    0.0    0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4       0.6   0.8    1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.84\n                                                              h                         r=\u22121\n                                                                                                                   i                      r=\u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.84",
      "section_title": "h                         r=\u22121",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.88\n                                                                                                                       0.2\n                                                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.88",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2\n    0.2\n    ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1\n\n\n\n\n                                                                                                                       \u22120.2\ny\n\n\n\n\n                                                              y\n\n\n\n\n                                                                                                                   y\n                                                                  \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "\u22120.2",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n    \u22120.1\n\n\n\n\n                                                                                                                       \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22120.1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.6\n                                                                  \u22121.0\n\n\n\n\n                            ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6",
      "section_title": "\u22121.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2   0.4       0.6   0.8   1.0                    0.2    0.4        0.6   0.8   1.0              0.0   0.2   0.4       0.6   0.8    1.0\n                                        x                                                    x                                                  x\n\n\nFig. A.4. Sample correlation coe\ufb03cients for nine random samples. Each plot also\ncontains the linear regression line of y on x.\n\f686      A Facts from Probability, Statistics, and Algebra\n\n      If X and Y are independent, then for all functions g and h,\n\n                       E{g(X)h(Y )} = E{g(X)}E{h(Y )}.                    (A.31)\n\nThis fact can be used to prove that if X and Y are independent, then \u03c3XY = 0,\nso the variables are uncorrelated. The opposite is not true. For example, if\nX is uniformly distributed on [\u22121, 1] and Y = X 2 , then a simple calculation\nshows that \u03c3XY = 0, but the two random variables are not independent. The\nkey point here is that Y is related to X, in fact, completely determined by\nX, but the relationship is highly nonlinear and correlation measures linear\nassociation.\n   Another example of random variables that are uncorrelated but depen-\ndent is the bivariate t-distribution. For this distribution, the two variates are\ndependent even when their correlation is 0; see Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "0.4       0.6   0.8   1.0                    0.2    0.4        0.6   0.8   1.0              0.0   0.2   0.4       0.6   0.8    1.0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.6.\n   If E(Y |X) = 0, then Y and X are uncorrelated, since\n\n                            E(Y ) = E{E(Y |X)} = 0                        (A.32)\n\nby the law of iterated expectations, and then\n\n       Cov(Y, X) = E(Y X) = E{E(Y X|X)} = E{XE(Y |X)} = 0                 (A.33)\n\nby (A.27), a second application of the law of iterated expectations, (A.23)\nwith Z = X, and (A.32).\n   Result (A.22) has an important interpretation. If X is known and one\nneeds to predict Y , then E(Y |X) is the best predictor in that it minimizes\nthe expected squared prediction error. If the best predictor is used, then the\nprediction error is Y \u2212 E(Y |X) and E{Y \u2212 E(Y |X)}2 is the expected squared\nprediction error. From the law of iterated expectations, that latter is\n                          & \u0018                   \u0019'\n  E{Y \u2212 E(Y |X)}2 = E E {Y \u2212 E(Y |X)}2 |X = E{Var(Y |X)}, (A.34)\n\nthe \ufb01rst summand on the right-hand side of (A.22). Also, Var{E(Y |X)}, the\nsecond summand there, is the variability of the best predictor and a measure\nof how well E(Y |X) can track Y \u2014the more E(Y |X) can vary, the better\nit can track Y . Therefore, the sum of the tracking ability and the expected\nsquared prediction error is the constant Var(Y )\u2014increasing the tracking abil-\nity decreases the expected squared prediction error.\n    Some insight can be gained by looking at the worst and best cases. The\nworst case is when X is independent of Y . Then, E(Y |X) = E(Y ), the track-\ning ability is Var{E(Y |X)} = 0, and the expected squared prediction takes\non its maximum value, Var(Y ). The best case is when Y is a function of X,\nsay y = g(X) for some g. Then, E(Y |X) = g(X) = Y , the prediction error is\n0, and the tracking ability is Var(Y ), its maximum possible value.\n\f                                                                   A.14 Multivariate Distributions                  687\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.6",
      "section_title": "If E(Y |X) = 0, then Y and X are uncorrelated, since",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.1 Normal Distributions: Conditional Expectations\nand Variance\n\nThe calculation of conditional expectations and variances can be di\ufb03cult for\nsome probability distributions, but it is quite easy for a pair (Y1 , Y2 ) that has\na bivariate normal distribution.\n   For a bivariate normal pair, the conditional expectation of Y2 given Y1\nequals the best linear predictor4 of Y2 given Y1 :\n                                                                      \u03c3Y1 ,Y2\n                      E(Y2 |Y1 = y1 ) = E(Y2 ) +                              {y1 \u2212 E(Y1 )}.\n                                                                       \u03c3Y2 1\nTherefore, for normal random variables, best linear prediction is the same as\nbest prediction. Also, the conditional variance of Y2 given Y1 is the expected\nsquared prediction error:\n                                  Var(Y2 |Y1 = y1 ) = \u03c3Y2 2 (1 \u2212 \u03c12Y1 ,Y2 ).                                   (A.35)\nIn general, Var(Y2 |Y1 = y1 ) is a function of y1 but we see in (A.35) that for the\nspecial case of a bivariate normal distribution, Var(Y2 |Y1 = y1 ) is constant,\nthat is, independent of y1 .\n\n\nA.14 Multivariate Distributions\nMultivariate distributions generalized the bivariate distributions of Appendix\nA.12. A random vector is a vector whose elements are random variable. A ran-\ndom vector of continuously distributed random variables, Y = (Y1 , . . . , Yd ),\nhas a multivariate probability density function fY1 ,...,Yd (y1 , . . . , yd ) if\n                                      \u0013 \u0013\n         P {(Y1 , . . . , Yd ) \u2208 A} =     fY1 ,...,Yd (y1 , . . . , yd ) dy1 \u00b7 \u00b7 \u00b7 dyd\n                                                              A\n\nfor all sets A \u2282 \u0002 .       p\n\n    The PDF of Yj is obtained by integrating the other variates out of fY1 ,...,Yd :\n      fYj (yj )\n       \u0013        \u0013          \u0013                \u0013\n     =     \u00b7\u00b7\u00b7                        \u00b7\u00b7\u00b7            fY1 ,...,Yd (y1 , . . . , yd ) dy1 \u00b7 \u00b7 \u00b7 dyj\u22121 dyj+1 \u00b7 \u00b7 \u00b7 dyd .\n         y1         yj\u22121       yj+1             yd\n\nSimilarly, the PDF of any subset of (Y1 , . . . , Yd ) is obtained by integrating the\nother variables out of fY1 ,...,Yd (y1 , . . . , yd ).\n   The expectation of a function g of Y1 , . . . , Yd is given by the formula\n                           \u0013     \u0013\n  E{g(Y1 , . . . , Yd )} =   \u00b7\u00b7\u00b7     g(y1 , . . . , yd )fY1 ,...,Yd (y1 , . . . , yd ) dy1 \u00b7 \u00b7 \u00b7 dyd .\n                                      y1             yd\n                                                                                                               (A.36)\n 4\n     See Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.1",
      "section_title": "Normal Distributions: Conditional Expectations",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.\n\f688       A Facts from Probability, Statistics, and Algebra\n\n   If Y1 , . . . , Yd are discrete, then their joint probability distribution speci\ufb01es\nP {Y1 = x1 , . . . , Yd = yd } for all values of y1 , . . . , yd . If Y1 , . . . , Yd are discrete\nand independent, then\n\n           P {Y1 = y1 , . . . , Yd = yd } = P {Y1 = y1 } \u00b7 \u00b7 \u00b7 P {Yd = yd }.                        (A.37)\n\nThe joint CDF of Y1 , . . . , Yd , whether they are continuous or discrete, is\n\n                     FY1 ,...,Yd (x1 , . . . , yd ) = P (Y1 \u2264 y1 , . . . , Yd \u2264 yd ).\n\nSuppose there is a sample of size n of d-dimensional random vectors, {Y i =\n(Yi,1 , . . . , Yi,d ) : i = 1, . . . , n}. Then the empirical CDF is\n                                        \u0017n\n                                             I{Yi,j \u2264 yj , for j = 1, . . . , d}\n              Fn (y1 , . . . , yd ) = i=1                                        . (A.38)\n                                                        n\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "688       A Facts from Probability, Statistics, and Algebra",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "14.1 Conditional Densities\n\nThe conditional density of Y1 , . . . , Yq given Yq+1 . . . , Yd , where 1 \u2264 q < d, is\n\n                                                                      fY1 ,...,Yd (y1 , . . . , yd )\n  fY1 ,...,Yq | Yq+1 ...,Yd (y1 , . . . , yq | yq+1 . . . , yd ) =                                   . (A.39)\n                                                                     fYq+1 ...,Yd (yq+1 . . . , yd )\n\nSince Y1 , . . . , Yd can be arranged in any order that is convenient, (A.39) pro-\nvides a formula for the conditional density of any subset of the variables, given\nthe other variables. Also, (A.39) can be rearranged to give the multiplicative\nformula\n                                  fY1 ,...,Yd (y1 , . . . , yd )\n   = fY1 ,...,Yq | Yq+1 ...,Yd (y1 , . . . , yq | yq+1 . . . , yd )fYq+1 ...,Yd (yq+1 . . . , yd ). (A.40)\nRepeated use of (A.40) gives a formula that will be useful later for calculating\nlikelihoods for dependent data\n\n                                          fY1 ,...,Yd (y1 , . . . , yd )\n\n  = fY1 (y1 ) fY2 |Y1 (y2 |y1 ) fY3 |Y1 ,Y2 (y3 |y1 , y2 ) \u00b7 \u00b7 \u00b7 fYd |Y1 ,...,Yd\u22121 (yd |y1 , . . . , yd\u22121 ).\n                                                                                                      (A.41)\nIf Y1 , . . . , Yd are independent, then\n\n                          fY1 ,...,Yd (y1 , . . . , yd ) = fY1 (y1 ) \u00b7 \u00b7 \u00b7 fYd (yd ).               (A.42)\n\n\nA.15 Stochastic Processes\nA discrete-time stochastic process is a sequence of random variables {Y1 , Y2 ,\nY3 , . . .}. The distribution of Yn is called its marginal distribution. The process\nis said to be Markov, or Markovian, if the conditional distribution of Yn+1\n\f                                                             A.16 Estimation       689\n\ngiven {Y1 , Y2 , . . . , Yn } equals the conditional distribution of Yn+1 given Yn , so\nYn+1 depends only on the previous value of the process. The AR(1) process\nin Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "14.1",
      "section_title": "Conditional Densities",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "12.4 is a simple example of a Markov process. A process generated by\ncomputer simulation will be Markov if only Yn and random numbers indepen-\ndent of {Y1 , Y2 , . . . , Yn\u22121 } are used to generate Yn+1 . An important example\nis Markov chain Monte Carlo, the topic of Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "12.4",
      "section_title": "is a simple example of a Markov process. A process generated by",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "20.7.\n    A distribution \u03c0 is a stationary distribution for a Markov process if, for\nall n, Yn+1 has distribution \u03c0 whenever Yn has distribution \u03c0.\n    Stochastic processes can also have a continuous-time parameter. Examples\nare Brownian motion and geometric Brownian motion, which are used, inter\nalia, to model the log-prices and prices of equities, respectively, in continu-\nous time.\n\n\nA.16 Estimation\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "20.7",
      "section_title": "A distribution \u03c0 is a stationary distribution for a Markov process if, for",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.1 Introduction\n\nOne of the major areas of statistical inference is estimation of unknown\nparameters, such as a population mean, from data. An estimator is de\ufb01ned as\nany function of the observed data. The key question is which of many possible\nestimators should be used. If \u03b8 is an unknown parameter and \u03b8\u0002 is an estimator,\nthen E(\u03b8)\u0002 \u2212 \u03b8 is called the bias and E{\u03b8\u0002\u2212 \u03b8}2 is called the mean-squared error\n(MSE). One seeks estimators that are e\ufb03cient, that is, having the smallest\npossible value of the MSE (or of some other measure of inaccuracy). It can be\nshown from simple algebra that the MSE is the squared bias plus the variance,\nthat is,\n                                        \u0002 \u2212 \u03b8}2 + Var(\u03b8),\n                       E{\u03b8\u0002 \u2212 \u03b8}2 = {E(\u03b8)                \u0002               (A.43)\nso an e\ufb03cient estimator will have both a small bias and a small variance. An\nestimator with a zero bias is called unbiased. However, it is not necessary to\nuse an unbiased estimator\u2014we only want the bias to be small, not necessarily\nexactly zero. One should be willing to accept a small bias if this leads to a\nsigni\ufb01cant reduction in variance.\n    The most popular methods of estimation are least squares (Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.1",
      "section_title": "Introduction",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "9.2.1),\nmaximum likelihood (Sects. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "9.2",
      "section_title": "1),",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.9 and 5.14), and Bayes estimation (Chap. 20).\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.9",
      "section_title": "and 5.14), and Bayes estimation (Chap. 20).",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "16.2 Standard Errors\n\nWhen a estimator is calculated from a random sample, it is a random variable,\nbut this fact is often not appreciated by beginning students. When \ufb01rst ex-\nposed to statistical estimation, students tend not to think of estimators such\nas a sample mean as random. If we have only a single sample, then the sam-\nple mean does not appear random. However, if we realize that the observed\n\f690     A Facts from Probability, Statistics, and Algebra\n\nsample is only one of many possible samples that could have been drawn, and\nthat each sample has a di\ufb00erent sample mean, then we see that the mean is\nin fact random.\n     Since an estimator is a random variable, it has an expectation and a stan-\ndard deviation. We have already seen that the di\ufb00erence between its expec-\ntation and the parameter is called the bias. The standard deviation of an\nestimator is called its standard error. If there are unknown parameters in the\nformula for this standard deviation, then they can be replaced by estimates. If\n\u03b8\u0002 is an estimator of \u03b8, then s\u03b8\u0002 will denote its standard error with any unknown\nparameters replaced by estimates.\n\n\nExample A.1. The standard error of the mean\n\n    Suppose that Y1 , . . . , Yn are i.i.d. with mean \u03bc and variance\n                                                                 \u221a        \u03c3 2 . Then,\n                                                                                 \u221a it\nfollows from (7.13) that the\u221a    standard   deviation of Y  is \u03c3/   n. Thus,   \u03c3/  n, or\nwhen \u03c3 is unknown\u221a   s Y  /    n,\n                                \u221a is called the standard   error of the sample    mean.\nThat is, sY is \u03c3/ n or sY / n depending on whether or not \u03c3 is known. \u0002\n\n\nA.17 Con\ufb01dence Intervals\nInstead of estimating an unknown parameter by a single number, it is often\nbetter to provide a range of numbers that gives a sense of the uncertainty of\nthe estimate. Such ranges are called interval estimates. One type of interval\nestimate, the Bayesian credible interval, is introduced in Chap. 20. Another\ntype of interval estimate is the con\ufb01dence interval. A con\ufb01dence interval is\nde\ufb01ned by the requirement that the probability that the interval will include\nthe true parameter is a speci\ufb01ed value called the con\ufb01dence coe\ufb03cient,, so,\nfor example, if a large number of independent 90 % intervals are constructed,\nthen approximately 90 % of them will contain the parameter.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "16.2",
      "section_title": "Standard Errors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.1 Con\ufb01dence Interval for the Mean\n\nIf Y is the mean of a sample from a normal population, then\n\n                                   Y \u00b1 t\u03b1/2,n\u22121 sY                              (A.44)\n\nis a con\ufb01dence interval with (1 \u2212 \u03b1) con\ufb01dence. This con\ufb01dence interval is\nderived in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.1",
      "section_title": "Con\ufb01dence Interval for the Mean",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "6.3.2. If \u03b1 = 0.05 (0.95 or 95 % con\ufb01dence) and if n is reason-\nably large, then t\u03b1/2,n\u22121 is approximately 2, so Y \u00b1 2\u221a sY is often used as an\napproximate 95 % con\ufb01dence\u221ainterval. Since sY = sY / n, the con\ufb01dence can\nalso be written as Y \u00b1 2 sY / n. When n is reasonably large, say 20 or more,\nthen Y will be approximately normally distributed by the central limit theo-\nrem, and the assumption that the population itself is normal can be dropped.\n\f                                             A.17 Con\ufb01dence Intervals     691\n\nExample A.2. Con\ufb01dence interval for a normal mean\n\n    Suppose we have a sample of size 25 from a normal distribution, s2Y = 2.7,\nY = 16.1, and we want a 99 % con\ufb01dence interval for \u03bc. We need t0.005,24 .\nThis quantile can be found, for example, using the R function qt and\nt0.005,24 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "6.3",
      "section_title": "2. If \u03b1 = 0.05 (0.95 or 95 % con\ufb01dence) and if n is reason-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.797. Then, the 99 % con\ufb01dence interval for \u03bc is\n                            \u221a\n                     (2.797) ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.797",
      "section_title": "Then, the 99 % con\ufb01dence interval for \u03bc is",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "2.7\n              16.1 \u00b1     \u221a        = 16.1 \u00b1 0.919 = [15.18, 17.02].\n                           25\nSince n = 25 is reasonably large, this interval should have approximately 99 %\ncon\ufb01dence even if the population is not normally distributed. The exception\nwould be if the population was extremely heavily skewed or had very heavy\ntails; in such cases a sample size larger than 25 might be necessary for this\ncon\ufb01dence interval to have near 99 % coverage.\n    Just how large a sample is needed for Y to be nearly normally distributed\ndepends on the population. If the population is symmetric and the tails are\nnot extremely heavy, then approximate normality is often achieved with n\naround 10. For skewed populations, 30 observations may be needed, and even\nmore in extreme cases. If the data appear to come from a highly skewed or\nheavy-tailed population, it might be better to assume a parametric model and\ncompute the MLE as discussed in Chap. 5 and perhaps to use the bootstrap\n(Chap. 6) for \ufb01nding the con\ufb01dence interval.\n    The function t.test() computes a con\ufb01dence interval for a normal mean.\nThe output below gives a 99 % con\ufb01dence interval for daily log-returns on Ford\nusing t.test() and then using (A.44). The interval is (\u22120.000417, 0.003407)\n   > ford = read.csv(\"RecentFord.csv\")\n   > returns = diff(log(ford[ , 7]))\n   > options(digits = 3)\n   > t.test(returns, conf.level = 0.99)\n\n   One Sample t-test\n\n   data: returns\n   t = 2.02, df = 1256, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "2.7",
      "section_title": "16.1 \u00b1     \u221a        = 16.1 \u00b1 0.919 = [15.18, 17.02].",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "ford = read.csv(\"RecentFord.csv\")\n   > returns = diff(log(ford[ , 7]))\n   > options(digits = 3)\n   > t.test(returns, conf.level = 0.99)",
        "start": 1259,
        "end": 1398
      }
    ]
  },
  {
    "content": "0.04388\n   alternative hypothesis: true mean is not equal to 0\n   99 percent confidence interval:\n    -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.04388",
      "section_title": "alternative hypothesis: true mean is not equal to 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000417 0.003407\n   sample estimates:\n   mean of x\n     ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000417",
      "section_title": "0.003407",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.00149\n\n   > n = length(returns)\n   > mean(returns) + c(-1, 1) *\n      qt(0.995, n - 1) * sd(returns) / sqrt(n)\n   [1] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.00149",
      "section_title": "> n = length(returns)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "n = length(returns)\n   > mean(returns) + c(-1, 1) *\n      qt(0.995, n - 1) * sd(returns) / sqrt(n)\n   [1] -",
        "start": 12,
        "end": 121
      }
    ]
  },
  {
    "content": "0.000417 0.003407                                                   \u0002\n\f692      A Facts from Probability, Statistics, and Algebra\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000417",
      "section_title": "0.003407                                                   \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.2 Con\ufb01dence Intervals for the Variance\nand Standard Deviation\nA (1 \u2212 \u03b1) con\ufb01dence interval for the variance of a normal distribution is\ngiven by              6                        7\n                        (n \u2212 1)s2Y (n \u2212 1)s2Y\n                                  ,              ,                (A.45)\n                         \u03c72\u03b1/2,n\u22121 \u03c721\u2212\u03b1/2,n\u22121\nwhere n is the sample size, s2Y is the sample variance given by equation (A.7),\nand, as de\ufb01ned in Appendix A.10.1, \u03c72\u03b3,n\u22121 is the (1 \u2212 \u03b3)-quantile of the chi-\nsquare distribution with n \u2212 1 degrees of freedom.\n\n\nExample A.3. Con\ufb01dence interval for a normal standard deviation\n\n    Suppose we have a sample of size 25 from a normal distribution, s2Y = 2.7,\nand we want a 90 % con\ufb01dence interval for \u03c3 2 . The quantiles we need for\nconstructing the interval are \u03c720.95,24 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.2",
      "section_title": "Con\ufb01dence Intervals for the Variance",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "13.848 and \u03c720.05,24 = 36.415. These\nvalues can be found using software such as qchisq() in R. The 90 % con\ufb01dence\ninterval for \u03c3 2 is\n                    !                      \"\n                      (2.7)(24) (2.7)(24)\n                               ,             = [1.78, 4.68].\n                       ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "13.848",
      "section_title": "and \u03c720.05,24 = 36.415. These",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "36.415      13.848\nTaking square roots of both endpoints, we get ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "36.415",
      "section_title": "13.848",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "1.33 < \u03c3 < 2.16 as a 90 %\ncon\ufb01dence interval for the standard deviation.\n    As another example, con\ufb01dence intervals for the variance and standard\ndeviation of daily Ford returns are calculate below. The con\ufb01dence interval\nfor the standard deviation is (0.0253, 0.0273).\n      > ford = read.csv(\"RecentFord.csv\")\n      > returns = diff(log(ford[,7]))\n      > n = length(returns)\n      > options(digits = 3)\n      > ci = (n - 1) * var(returns) / qchisq(c(0.025, 0.975), n - 1,\n         lower.tail = FALSE)\n      > ci\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "1.33",
      "section_title": "< \u03c3 < 2.16 as a 90 %",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "ford = read.csv(\"RecentFord.csv\")\n      > returns = diff(log(ford[,7]))\n      > n = length(returns)\n      > options(digits = 3)\n      > ci = (n - 1) * var(returns) / qchisq(c(0.025, 0.975), n - 1,\n         lower.tail = FALSE)\n      > ci\n      [1]",
        "start": 277,
        "end": 526
      }
    ]
  },
  {
    "content": "0.000639 0.000748\n      > sqrt(ci)\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000639",
      "section_title": "0.000748",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "sqrt(ci)\n      [1]",
        "start": 24,
        "end": 45
      }
    ]
  },
  {
    "content": "0.0253 0.0273\n                                                                             \u0002\n    Unfortunately, the assumption that the population is normally distributed\ncannot be dispensed with, even if the sample size is large. If a normal prob-\nability plot or test of normality (see Sect. 4.4) suggests that the population\nmight be nonnormally distributed, then one might instead construct a con\ufb01-\ndence interval for \u03c3 using the bootstrap; see Chap. 6. Another possibility is\nto assume a nonnormal parametric model such as the t-model if the data are\nsymmetric and heavy-tailed; see Example ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.0253",
      "section_title": "0.0273",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.3.\n\f                                                  A.18 Hypothesis Testing      693\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.3",
      "section_title": "A.18 Hypothesis Testing      693",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "17.3 Con\ufb01dence Intervals Based on Standard Errors\n\nMany estimators are approximately unbiased and approximately normally\ndistributed. Then, an approximate 95 % con\ufb01dence interval is the estimator\nplus or minus twice its standard error; that is,\n\n                                     \u03b8\u0002 \u00b1 2 s\u03b8\u0002\n\nis an approximate 95 % con\ufb01dence interval for \u03b8.\n\n\nA.18 Hypothesis Testing\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "17.3",
      "section_title": "Con\ufb01dence Intervals Based on Standard Errors",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.1 Hypotheses, Types of Errors, and Rejection Regions\n\nStatistical hypothesis testing uses data to decide whether a certain state-\nment called the null hypothesis is true. The negation of the null hypothesis\nis called the alternative hypothesis. For example, suppose that Y1 , . . . , Yn are\ni.i.d. N (\u03bc, 1) and \u03bc is unknown. The null hypothesis could be that \u03bc is 1.\nThen, we write H0 : \u03bc = 1 and H1 : \u03bc = 1 to denote the null and alternative\nhypotheses.\n     There are two types of errors that we hope to avoid. If the null hypothesis\nis true but we reject it, then we are making a type I error. Conversely, if the\nnull hypothesis is false and we accept it, then we are making a type II error.\n     The rejection region is the set of possible samples that lead us to reject\nH0 . For example, suppose that \u03bc0 is a hypothesized value of \u03bc and the null\nhypothesis is H0 : \u03bc = \u03bc0 and the alternative is H1 : \u03bc = \u03bc0 . One rejects H0 if\n|Y \u2212 \u03bc0 | exceeds an appropriately chosen cuto\ufb00 value c called a critical value.\nThe rejection region is chosen to keep the probability of a type I error below\na prespeci\ufb01ed small value called the level and often denoted by \u03b1. Typical\nvalues of \u03b1 used in practice are 0.01, 0.05, or ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.1",
      "section_title": "Hypotheses, Types of Errors, and Rejection Regions",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.1. As \u03b1 is made smaller, the\nrejection region must be made smaller. In the example, since we reject the\nnull hypothesis when |Y \u2212 \u03bc0 | exceeds c, the critical value c gets larger as\nthe \u03b1 gets smaller.\u221aThe value of c is easy to determine. Assuming that \u03c3 is\nknown, c is z\u03b1/2 \u03c3/ n, where, as de\ufb01ned in Appendix A.9.3, z\u03b1/2 is the \u03b1/2-\nupper quantile of the standard normal distribution. If \u03c3 is unknown, then\n\u03c3 is replaced by sX and z\u03b1/2 is replaced by t\u03b1/2,n\u22121 , where, as de\ufb01ned in\nSect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.1",
      "section_title": "As \u03b1 is made smaller, the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "5.5.2, t\u03b1/2,n\u22121 is the \u03b1/2-upper quantile of the t-distribution with n \u2212 1\ndegrees of freedom. The test using the t-quantile is called the one-sample\nt-test.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "5.5",
      "section_title": "2, t\u03b1/2,n\u22121 is the \u03b1/2-upper quantile of the t-distribution with n \u2212 1",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.2 p-Values\n\nRather than specifying \u03b1 and deciding whether to accept or reject the null\nhypothesis at that \u03b1, we might ask \u201cfor what values of \u03b1 do we reject the null\nhypothesis?\u201d The p-value for a sample is de\ufb01ned as the smallest value of \u03b1 for\n\f694       A Facts from Probability, Statistics, and Algebra\n\nwhich the null hypothesis is rejected. Stated di\ufb00erently, to perform the test\nusing a given sample, we \ufb01rst \ufb01nd the p-value of that sample, and then H0 is\nrejected if we decide to use \u03b1 larger than the p-value and H0 is accepted if we\nuse \u03b1 smaller than the p-value. Thus,\n\u2022     a small p-value is evidence against the null hypothesis\nwhile\n\u2022     a large p-value shows that the data are consistent with the null hypothesis.\n\n\nExample A.4. Interpreting p-values\n\n   If the p-value of a sample is 0.033, then we reject H0 if we use \u03b1 equal to\n",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.2",
      "section_title": "p-Values",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 or 0.1, but we accept H0 if we use \u03b1 = 0.01.                           \u0002\n\n\n   The p-value not only tells us whether the null hypothesis should be ac-\ncepted or rejected, but it also tells us whether or not the decision to accept\nor reject H0 is a close call. For example, if we are using \u03b1 = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "or 0.1, but we accept H0 if we use \u03b1 = 0.01.                           \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.05 and the\np-value were 0.047, then we would reject H0 but we would know the decision\nwas close. If instead the p-value were 0.001, then we would know the decision\nwas not so close.\n   When performing hypothesis tests, statistical software routinely calculates\np-values. Doing this is much more convenient than asking the user to specify\n\u03b1, and then reporting whether the null hypothesis is accepted or rejected for\nthat \u03b1.\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.05",
      "section_title": "and the",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.3 Two-Sample t-Tests\n\nTwo-sample t-tests are used to test hypotheses about the di\ufb00erence between\ntwo population means. The independent-samples t-test is used when we sam-\nple independently from the two populations. Let \u03bci , Y i , si , and ni be the\npopulation mean, sample mean, sample standard deviation, and sample size\nfor the ith sample, i = 1, 2, respectively. Let \u03940 be a hypothesized value of\n\u03bc1 \u2212\u03bc2 . We assume that the two populations have the same standard deviation\nand estimate this parameter by the pooled standard deviation, which is\n                                \u000e                               \u000f1/2\n                                    (n1 \u2212 1)s21 + (n2 \u2212 1)s22\n                      spool =                                          .   (A.46)\n                                          n1 + n2 \u2212 2\n\n      The independent-samples t-statistic is\n\n                                         Y 1 \u2212 Y 2 \u2212 \u03940\n                                    t=        #          .\n                                         spool n11 + n12\n\f                                                  A.18 Hypothesis Testing       695\n\nIf the hypotheses are H0 : \u03bc1 \u2212 \u03bc2 = \u03940 and H1 : \u03bc1 \u2212 \u03bc2 = \u03940 , then H0 is\nrejected if |t| > t\u03b1/2|n1 +n2 \u22122 . If the hypotheses are H0 : \u03bc1 \u2212 \u03bc2 \u2264 \u03940 and\nH1 : \u03bc1 \u2212 \u03bc2 > \u03940 , then H0 is rejected if t > t\u03b1 |n1 +n2 \u22122 and if they are H0 :\n\u03bc1 \u2212 \u03bc2 \u2265 \u03940 and H1 : \u03bc1 \u2212 \u03bc2 < \u03940 , then H0 is rejected if t < \u2212t\u03b1|n1 +n2 \u22122 .\n    Sometimes the samples are paired rather than independent. For example,\nsuppose we wish to compare returns on small-cap versus large-cap5 stocks\nand for each of n years we have the returns on a portfolio of small-cap stocks\nand on a portfolio of large-cap stocks. For any year, the returns on the two\nportfolios will be correlated, so an independent-samples test is not valid. Let\ndi = Xi,1 \u2212 Xi,2 be the di\ufb00erence between the observations from populations\n1 and 2 for the ith pair, and let d and sd be the sample mean and standard\ndeviation of d1 , . . . , dn . The paired-sample t-statistics is\n                                       d \u2212 \u03940\n                                  t=       \u221a .                              (A.47)\n                                       sd / n\nThe rejection regions are the same as for the independent-samples t-tests\nexcept that the degrees-of-freedom parameter for the t-quantiles is n\u22121 rather\nthan n1 + n2 \u2212 2.\n    The power of a test is the probability of correctly rejecting H0 when H1\nis true. Paired samples are often used to obtain more power. In the example\nof comparing small- and large-cap stocks, the returns on both portfolios will\nhave high year-to-year variation, but the di will be free of this variation, so\nthat sd should be relatively small compared to s1 and s2 . A small variation in\nthe data means that \u03bc1 \u2212 \u03bc2 can be more accurately estimated and deviations\nof this parameter from \u03940 are more likely to be detected.\n    Since d = Y 1 \u2212 Y 2 , the numerators in (A.46) and (A.47) are equal. What\ndi\ufb00ers are the denominators. The denominator in (A.47) will be smaller than\nin (A.46) when the correlation between observations (Yi,2 , Yi,2 ) in a pair is\npositive. It is the smallness of the denominator in (A.47) that gives the paired\nt-test increased power.\n    Suppose someone had a paired sample but incorrectly used the independent-\nsamples t-test. If the correlation between Yi,1 and Yi,2 is zero, then the paired\nsamples behave the same as independent samples and the e\ufb00ect of using the\nincorrect test would be small. Suppose that this correlation is positive. The\nresult of using the incorrect test would be that if H0 is false, then the true\np-value would be overestimated and one would be less likely to reject H0 than\nif the paired-sample test had been used. However, if the p-value is small, then\none can be con\ufb01dent in rejecting H0 because the p-value for the paired-sample\ntest would be even smaller.6 Unfortunately, statistical methods are often used\n5\n  The market capitalization of a stock is the product of the share price and the\n  number of shares outstanding. If stocks are ranked based on market capitalization,\n  then all stocks below some speci\ufb01ed quantile would be small-cap stocks and all\n  above another speci\ufb01ed quantile would be large-cap.\n6\n  An exception would be the rare situation, where Yi,1 and Yi,2 are negatively\n  correlated.\n\f696      A Facts from Probability, Statistics, and Algebra\n\nby researchers without a solid understanding of the underlying theory, and\nthis can lead to misapplications. The hypothetical use just described of an\nincorrect test is often a reality, and it is sometimes necessary to evaluate\nwhether the results that are reported can be trusted.\n    Con\ufb01dence intervals can also be constructed for the di\ufb00erence between the\ntwo means and are\n                                                     \u0010\n                                                       1    1\n                    Y 1 \u2212 Y 2 \u00b1 t\u03b1/2|n1 +n2 \u22122 spool      +            (A.48)\n                                                       n1   n2\nfor unpaired samples and\n                                                   \u221a\n                            d \u00b1 t\u03b1/2|n1 +n2 \u22122 sd / n.                 (A.49)\nfor paired samples.\n\n\nExample A.5. A Paired Two-sample t-test and Con\ufb01dence Interval\n\n   In the next example, a 95 % con\ufb01dence interval is created for the di\ufb00erence\nbetween the mean daily log-returns on Merck and P\ufb01zer. Since the prices were\ntaken over the same time intervals, the daily log-returns are highly correlated\n(\u0002\n \u03c1 = 0.547), so a paired test and interval were used. The con\ufb01dence interval\nwas also calculated using (A.49).\n      > prices = read.csv(\"Stock_Bond.csv\")\n      > prices_Merck = prices[ , 11]\n      > return_Merck = diff(log(prices_Merck))\n      > prices_Pfizer = prices[ , 13]\n      > return_Pfizer = diff(log(prices_Pfizer))\n      > cor(return_Merck,return_Pfizer)\n      [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.3",
      "section_title": "Two-Sample t-Tests",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "t\u03b1/2|n1 +n2 \u22122 . If the hypotheses are H0 : \u03bc1 \u2212 \u03bc2 \u2264 \u03940 and\nH1 : \u03bc1 \u2212 \u03bc2 > \u03940 , then H0 is rejected if t > t\u03b1 |n1 +n2 \u22122 and if they are H0 :\n\u03bc1 \u2212 \u03bc2 \u2265 \u03940 and H1 : \u03bc1 \u2212 \u03bc2 < \u03940 , then H0 is rejected if t < \u2212t\u03b1|n1 +n2 \u22122 .\n    Sometimes the samples are paired rather than independent. For example,\nsuppose we wish to compare returns on small-cap versus large-cap5 stocks\nand for each of n years we have the returns on a portfolio of small-cap stocks\nand on a portfolio of large-cap stocks. For any year, the returns on the two\nportfolios will be correlated, so an independent-samples test is not valid. Let\ndi = Xi,1 \u2212 Xi,2 be the di\ufb00erence between the observations from populations\n1 and 2 for the ith pair, and let d and sd be the sample mean and standard\ndeviation of d1 , . . . , dn . The paired-sample t-statistics is\n                                       d \u2212 \u03940\n                                  t=       \u221a .                              (A.47)\n                                       sd / n\nThe rejection regions are the same as for the independent-samples t-tests\nexcept that the degrees-of-freedom parameter for the t-quantiles is n\u22121 rather\nthan n1 + n2 \u2212 2.\n    The power of a test is the probability of correctly rejecting H0 when H1\nis true. Paired samples are often used to obtain more power. In the example\nof comparing small- and large-cap stocks, the returns on both portfolios will\nhave high year-to-year variation, but the di will be free of this variation, so\nthat sd should be relatively small compared to s1 and s2 . A small variation in\nthe data means that \u03bc1 \u2212 \u03bc2 can be more accurately estimated and deviations\nof this parameter from \u03940 are more likely to be detected.\n    Since d = Y 1 \u2212 Y 2 , the numerators in (A.46) and (A.47) are equal. What\ndi\ufb00ers are the denominators. The denominator in (A.47) will be smaller than\nin (A.46) when the correlation between observations (Yi,2 , Yi,2 ) in a pair is\npositive. It is the smallness of the denominator in (A.47) that gives the paired\nt-test increased power.\n    Suppose someone had a paired sample but incorrectly used the independent-\nsamples t-test. If the correlation between Yi,1 and Yi,2 is zero, then the paired\nsamples behave the same as independent samples and the e\ufb00ect of using the\nincorrect test would be small. Suppose that this correlation is positive. The\nresult of using the incorrect test would be that if H0 is false, then the true\np-value would be overestimated and one would be less likely to reject H0 than\nif the paired-sample test had been used. However, if the p-value is small, then\none can be con\ufb01dent in rejecting H0 because the p-value for the paired-sample\ntest would be even smaller.6 Unfortunately, statistical methods are often used\n5\n  The market capitalization of a stock is the product of the share price and the\n  number of shares outstanding. If stocks are ranked based on market capitalization,\n  then all stocks below some speci\ufb01ed quantile would be small-cap stocks and all\n  above another speci\ufb01ed quantile would be large-cap.\n6\n  An exception would be the rare situation, where Yi,1 and Yi,2 are negatively\n  correlated.\n\f696      A Facts from Probability, Statistics, and Algebra",
        "start": 1214,
        "end": 4414
      },
      {
        "language": "r",
        "code": "prices = read.csv(\"Stock_Bond.csv\")\n      > prices_Merck = prices[ , 11]\n      > return_Merck = diff(log(prices_Merck))\n      > prices_Pfizer = prices[ , 13]\n      > return_Pfizer = diff(log(prices_Pfizer))\n      > cor(return_Merck,return_Pfizer)\n      [1]",
        "start": 5648,
        "end": 5907
      }
    ]
  },
  {
    "content": "0.547\n      > t.test(return_Merck, return_Pfizer, paired = TRUE)\n\n      Paired t-test\n\n      data: return_Merck and return_Pfizer\n      t = -0.406, df = 4961, p-value = ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.547",
      "section_title": "> t.test(return_Merck, return_Pfizer, paired = TRUE)",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "t.test(return_Merck, return_Pfizer, paired = TRUE)",
        "start": 12,
        "end": 66
      }
    ]
  },
  {
    "content": "0.6849\n      alternative hypothesis: true difference in means is not equal to 0\n      95 percent confidence interval:\n       -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.6849",
      "section_title": "alternative hypothesis: true difference in means is not equal to 0",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.000584 0.000383\n      sample estimates:\n      mean of the differences\n                       -1e-04\n\n      > differences = return_Merck - return_Pfizer\n      > n = length(differences)\n      > mean(differences) + c(-1,1) * qt(0.025, n - 1,\n         lower.tail = FALSE) * sd(differences) / sqrt(n)\n      [1] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000584",
      "section_title": "0.000383",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "differences = return_Merck - return_Pfizer\n      > n = length(differences)\n      > mean(differences) + c(-1,1) * qt(0.025, n - 1,\n         lower.tail = FALSE) * sd(differences) / sqrt(n)\n      [1] -",
        "start": 109,
        "end": 309
      }
    ]
  },
  {
    "content": "0.000584 0.000383                                                 \u0002\n\f                                                        A.19 Prediction     697\n\nA.",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.000584",
      "section_title": "0.000383                                                 \u0002",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "18.4 Statistical Versus Practical Signi\ufb01cance\n\nWhen we reject a null hypothesis, we often say there is a statistically signif-\nicant e\ufb00ect. In this context, the word \u201csigni\ufb01cant\u201d is easily misconstrued. It\ndoes not mean that there is an e\ufb00ect of practical importance. For example,\nsuppose we were testing the null hypothesis that the means of two populations\nare equal versus the alternative that they are unequal. Statistical signi\ufb01cance\nsimply means that the two sample means are su\ufb03ciently di\ufb00erent that this\ndi\ufb00erence cannot reasonably be attributed to mere chance. Statistical signif-\nicance does not mean that the population means are so dissimilar that their\ndi\ufb00erence is of any practical importance. When large samples are used, small\nand unimportant e\ufb00ects are likely to be statistically signi\ufb01cant.\n     When determining practical signi\ufb01cance, con\ufb01dence intervals are more use-\nful than tests. In the case of the comparison between two population means,\nit is important to construct a con\ufb01dence interval and to conclude that there\nis an e\ufb00ect of practical signi\ufb01cance only if all di\ufb00erences in that interval are\nlarge enough to be of practical importance. How large is \u201clarge enough\u201d is not\na statistical question but rather must be answered by a subject-matter expert.\nFor an example, suppose a di\ufb00erence between the two population means that\nexceeds ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "18.4",
      "section_title": "Statistical Versus Practical Signi\ufb01cance",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.2 is considered important, at least for the purpose under considera-\ntion. If a 95 % con\ufb01dence interval were [0.23, 0.26], then with 95 % con\ufb01dence\nwe could conclude that there is an important di\ufb00erence. If instead the interval\nwere [0.13, 0.16], then we could conclude with 95 % con\ufb01dence that there is no\nimportant di\ufb00erence. If the con\ufb01dence interval were [0.1, 0.3], then we could\nnot state with 95 % con\ufb01dence whether the di\ufb00erence is important or not.\n\n\nA.19 Prediction\n\nSuppose that Y is a random variable that is unknown at the present time, for\nexample, a future change in an interest rate or stock price. Let X be a known\nrandom vector that is useful for predicting Y . For example, if Y is a future\nchange in a stock price or a macroeconomic variable, X might be the vector\nof recent changes in that stock price or macroeconomic variable.\n    We want to \ufb01nd a function of X, which we will call Y\u0002 (X), that best pre-\ndicts Y . By this we mean that the mean-squared error E[{Y \u2212Y\u0002 (X)}2 ] is made\nas small as possible. The function Y\u0002 (X) that minimizes the mean-squared er-\nror will be called the best predictor of Y based on X. Note that Y\u0002 (X) can\nbe any function of X, not necessarily a linear function as in Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.2",
      "section_title": "is considered important, at least for the purpose under considera-",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "11.9.1. The\nbest predictor is theoretically simple\u2014it is the conditional expectation of Y\ngiven X. That is, E(Y |X) is the best predictor of Y in the sense of minimiz-\ning E[{Y \u2212 Y\u0002 (X)}2 ] among all possible choices of Y\u0002 (X) that are arbitrary\nfunctions of X.\n\f698       A Facts from Probability, Statistics, and Algebra\n\n   If Y and X are independent, then E(Y |X) = E(Y ). If X were unobserved,\nthen E(Y ) would be used to predict Y . Thus, when Y and X are independent,\nthe best predictor of Y is the same as if X were unknown, because X contains\nno information that is useful for prediction of Y .\n   In practice, using E(Y |X) for prediction is not trivial. The problem is\nthat E(Y |X) may be di\ufb03cult to estimate whereas the best linear predictor\ncan be estimated by linear regression as described in Chap. 9. However, the\nnewer technique of nonparametric regression can be used to estimate E(Y |X).\nNonparametric regression is discussed in Chap. 21.\n\n\nA.20 Facts About Vectors and Matrices\n                                                          \u0017p\nThe norm of the vector x = (x1 , . . . , xp )T is \u0012x\u0012 = ( i=1 x2i )1/2 .\n    A square matrix A is diagonal if Ai,j = 0 for all i = j. We use the notation\ndiag(d1 , . . . , dp ) for a p \u00d7 p diagonal matrix A such that Ai,i = di .\n    A matrix O is orthogonal if O T = O \u22121 . This implies that the columns of\nO are mutually orthogonal (perpendicular) and that their norms are all equal\nto 1.\n    Any symmetric matrix \u03a3 has an eigenvalue-eigenvector decomposition,\neigen-decomposition for short, which is\n\n                                 \u03a3 = O diag(\u03bbi ) O T ,                            (A.50)\n\nwhere O is an orthogonal matrix whose columns are the eigenvectors of \u03a3\nand \u03bb1 , . . . , \u03bbp are the eigenvalues of \u03a3. Also, if all of \u03bb1 , . . . , \u03bbp are nonzero,\nthen \u03a3 is nonsingular and\n\n                              \u03a3 \u22121 = O diag(1/\u03bbi ) O T .\n\n      Let o1 , . . . , op be the columns of O. Then, since O is orthogonal,\n\n                                       oT\n                                        j ok = 0                                  (A.51)\n\nfor any j = k. Moreover,\n                                      oT\n                                       j \u03a3ok = 0                                  (A.52)\nfor j = k. To see this, let ej be the jth unit vector, that is, the vector\nwith a one in the jth coordinate and zeros elsewhere. Then, oT            T\n                                                                   j O = ej and\nO T ok = ek , so that for j = k,\n                             (                )\n               oT\n                j \u03a3ok = oj\n                           T\n                               O diag(\u03bbi ) O T ok = \u03bbj \u03bbk eT\n                                                           j ek = 0.\n\n    The eigenvalue-eigenvector decomposition of a covariance matrix is used\nin Sect. ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.9",
      "section_title": "1. The",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "7.8 to \ufb01nd the orientation of elliptically contoured densities. This de-\ncomposition can be important even if the density is not elliptically contoured\nand is the basis of principal components analysis (PCA).\n\f                        A.21 Roots of Polynomials and Complex Numbers            699\n\nExample A.6. An Eigendecomposition\n\n    In the next example, a 3 \u00d7 3 symmetric matrix Sigma is created and\nits eigenvalues and eigenvectors are computed using the function eigen().\nThe eigenvalues are in the vector decomp$values and the eigenvectors are\nin the matrix decomp$vectors. It is also veri\ufb01ed that decomp$vectors is an\northogonal matrix.\n   > Sigma = matrix(c(1, 3, 4, 3, 6, 2, 4, 2, 8), nrow = 3,\n      byrow = TRUE)\n   > Sigma\n        [,1] [,2] [,3]\n   [1,]    1    3    4\n   [2,]    3    6    2\n   [3,]    4    2    8\n   > decomp = eigen(Sigma)\n   > decomp\n   $values\n   [1] ",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "7.8",
      "section_title": "to \ufb01nd the orientation of elliptically contoured densities. This de-",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "Sigma = matrix(c(1, 3, 4, 3, 6, 2, 4, 2, 8), nrow = 3,\n      byrow = TRUE)\n   > Sigma\n        [,1] [,2] [,3]\n   [1,]    1    3    4\n   [2,]    3    6    2\n   [3,]    4    2    8\n   > decomp = eigen(Sigma)\n   > decomp\n   $values\n   [1]",
        "start": 647,
        "end": 884
      }
    ]
  },
  {
    "content": "11.59 4.79 -1.37\n\n   $vectors\n          [,1]    [,2]   [,3]\n   [1,] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "11.59",
      "section_title": "4.79 -1.37",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.426 -0.0479 0.903\n   [2,] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.426",
      "section_title": "-0.0479 0.903",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.499 -0.8203 -0.279\n   [3,] -",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.499",
      "section_title": "-0.8203 -0.279",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.754 0.5699 -0.326\n\n   > round(decomp$vectors %*% t(decomp$vectors), 5)\n        [,1] [,2] [,3]\n   [1,]    1    0    0\n   [2,]    0    1    0\n   [3,]    0    0    1                                                             \u0002\n\n\nA.21 Roots of Polynomials and Complex Numbers\nThe roots of polynomials play an important role in the study of ARMA pro-\ncesses. Let p(x) = b0 + b1 x + \u00b7 \u00b7 \u00b7 bp xp , with bp = 0, be a pth-degree polynomial.\nThe fundamental theorem of algebra states that p(x) can be factored as\n\n                         bp (x \u2212 r1 )(x \u2212 r2 ) \u00b7 \u00b7 \u00b7 (x \u2212 rp ),\n\nwhere r1 , . . . , rp are the roots of p(x), that is, the solutions to p(x) = 0. The\nroots need not be distinct and they can be complex numbers. In R, the roots\nof a polynomial can be found using the function polyroot().       \u221a\n    A complex number can be written    \u221a as a + b \u0131, where \u0131 = \u22121. The absolute\nvalue or magnitude of a + b \u0131 is a2 + b2 . The complex plane is the set of\nall two-dimensional vectors (a, b), where (a, b) represents the complex number\na + b \u0131. The unit circle is the set of all complex number with magnitude 1.\n\f700      A Facts from Probability, Statistics, and Algebra\n\nA complex number is inside or outside the unit circle depending on whether\nits magnitude is less than or greater than 1.\n\n\nExample A.7. Roots of a Cubic Polynomial\n\n   As an example, the roots of the cubic polynomial 1 + 2x + 3x2 + 4x3 are\ncomputed below. We see that there is one real root, \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.754",
      "section_title": "0.5699 -0.326",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "round(decomp$vectors %*% t(decomp$vectors), 5)\n        [,1] [,2] [,3]\n   [1,]    1    0    0\n   [2,]    0    1    0\n   [3,]    0    0    1                                                             \u0002",
        "start": 24,
        "end": 228
      }
    ]
  },
  {
    "content": "0.606 and two complex\nroots, \u2212",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.606",
      "section_title": "and two complex",
      "has_code": false
    },
    "code_blocks": []
  },
  {
    "content": "0.072 \u00b1 0.638\u0131. It is also veri\ufb01ed that these are roots.\n\n      > roots = polyroot(c(1, 2, 3, 4))\n      > roots\n      [1] -0.072+0.638i -0.606-0.000i -0.072-0.638i\n      > fn = function(x){1 + 2 * x + 3 * x^2 + 4 * x^3}\n      > round(fn(roots), 5)\n      [1] 0+0i 0+0i 0+0i                                                     \u0002\n\n\nA.22 Bibliographic Notes\n\nCasella and Berger (2002) covers in greater detail most of the statistical the-\nory in this chapter and elsewhere in the book. Wasserman (2004) is a modern\nintroduction to statistical theory and is also recommended for further study.\nAlexander (2001) is a recent introduction to \ufb01nancial econometrics and has\na chapter on covariance matrices; her technical appendices cover maximum\nlikelihood estimation, con\ufb01dence intervals, and hypothesis testing, including\nlikelihood ratio tests. Evans, Hastings, and Peacock (1993) provides a concise\nreference for the basic facts about commonly used distributions in statistics.\nJohnson, Kotz, and Kemp (1993) discusses most of the common discrete dis-\ntributions, including the binomial. Johnson, Kotz, and Balakrishnan (1994,\n1995) contain a wealth of information and extensive references about the nor-\nmal, lognormal, chi-square, exponential, uniform, t, F , Pareto, and many other\ncontinuous distributions. Together, these works by Johnson, Kotz, Kemp, and\nBalakrishnan are essentially an encyclopedia of statistical distributions.\n\n\nReferences\n\nAlexander, C. (2001) Market Models: A Guide to Financial Data Analysis,\n  Wiley, Chichester.\nCasella, G. and Berger, R. L. (2002) Statistical Inference, 2nd ed., Duxbury/\n  Thomson Learning, Paci\ufb01c Grove, CA.\nEvans, M., Hastings, N., and Peacock, B. (1993) Statistical Distributions, 2nd\n  ed., Wiley, New York.\nGourieroux, C., and Jasiak, J. (2001) Financial Econometrics, Princeton\n  University Press, Princeton, NJ.\n\f                                                          References   701\n\nJohnson, N. L., Kotz, S., and Balakrishnan, N. (1994) Continuous Univariate\n  Distributions, Vol. 1, 2nd ed., Wiley, New York.\nJohnson, N. L., Kotz, S., and Balakrishnan, N. (1995) Continuous Univariate\n  Distributions, Vol. 2, 2nd ed., Wiley, New York.\nJohnson, N. L., Kotz, S., and Kemp, A. W. (1993) Discrete Univariate Dis-\n  tributions, 2nd ed., Wiley, New York.\nWasserman, L. (2004) All of Statistics, Springer, New York.\n\fIndex\n\n\n\n\n\u2229, xxv                                     Alexander, G., 10, 36, 510\n\u222a, xxv                                     alpha, 507, 509\n\u0131, 699                                     analysis of variance table, 227, 229\n\u03c1XY , xxv, 64, 684                         Anderson, D. R., 126\n\u03c3XY , xxv, 684                             Anderson\u2013Darling test, 64\n\u223c, xxvi                                    ANOVA table, see analysis of variance\nx+ , 41                                         table\n                                           AOV table, see analysis of variance\nbias\u2013variance tradeo\ufb00, 483                      table\npackage in R, 664                          APARCH, 421\n                                           ar() function in R, 349, 385\nA-C skewed distributions, 102, 117\n                                           AR process, 325\nabcnon() function in R, 147\n                                             multivariate, 384\nabcpar() function in R, 147\n                                             potential need for many parameters,\nAbramson, I., 77\n                                                326\nabsolute residual plot, 258, 276\n                                           AR(1) process, 314\nabsolute value\n                                             checking assumptions, 319\n  of a complex number, 699\n                                             nonstationary, 317\nACF, see autocorrelation function\n                                           AR(1)+ARCH(1) process, 409\nacf() function in R, 387\n                                           AR(p) process, 325, 330\nADF test, 340\n                                           ARCH process, 405\nadf.test() function in R, 340, 341, 371\n                                           ARCH(1) process, 407\nadjust parameter, 598\n                                           ARCH(p) process, 411\nadjustment matrix (of a VECM), 457\n                                           ARFIMA, 391\nAER package in R, 77, 243, 263, 286, 403\n                                           arima() function in R, 318, 325, 337,\nAIC, 109, 110, 199, 232, 350, 654\n                                                348, 378\n  corrected, 112, 126, 342\n                                           ARIMA model\n  theory behind, 126\n                                             automatic selection, 342\n  underlying statistical theory, 126\n                                           ARIMA process, 104, 331, 343\nAlexander, C., 352, 433, 443, 460, 575,\n                                           arima.sim() function in R, 335, 640\n      700\n\n\n\n\u00a9 Springer Science+Business Media New York 2015                              703\nD. Ruppert, D.S. Matteson, Statistics and Data Analysis for Financial\nEngineering, Springer Texts in Statistics,\nDOI 10.1007/978-1-4939-2614-5\n\f704    Index\n\nARMA process, 328, 331, 332, 343, 405      Bera, A., 443\n  multivariate, 384                        Beran, J, 395\nARMAacf() function in R, 325, 330          Berger, J. O., 635\nArtzner, P., 573                           Berger, R., 700\nask price, 278, 298                        Bernardo, J., 635\nasymmetric power ARCH, see APARCH          Bernoulli distribution, 674\nasymmetry                                  Bernstein\u2013von Mises Theorem, 593\n  of a distribution, 87                    Best, N. G., 635\nAtkinson, A., 77, 300                      beta, 499, 500\nattach() function in R, 12                   estimation of, 507\nauto.arima() function in R, 326, 328,        portfolio, 503\n      337\u2013339, 342, 350, 369, 396          beta distribution, 586\u2013588, 679\nautocorrelation function, 308              bias, 139, 689\n  of a GARCH process, 408                    bootstrap estimate of, 139\n  of an ARCH(1) process, 407               bias\u2013variance tradeo\ufb00, 3, 49, 50, 86,\n  sample, 312                                    110, 536, 612\nautocovariance function, 308               BIC, 109, 110, 232, 235, 350\n  sample, 312                              bid price, 278, 298\nautoregressive process, see AR(1)          bid\u2013ask spread, 278, 298\n      process and AR(p) process            bimodal, 670\nAzzalini\u2013Capitanio skewed dis-             binary regression, 286\n      tributions, see A-C skewed           binary response, 286\n      distributions                        binomial distribution, 674\n                                             kurtosis of, 90\nB (MCMC diagnostic), 606                     skewness of, 88\nBu\u0308hlmann, P., 395                         Binomial(n, p), 674\nback-testing, 112                          Black Monday, 3, 47\nbackwards operator, 331, 333                 unlikely under a t model, 62\nbad data, 293                              Black\u2013Scholes formula, 10\nBagasheva, B. S., 635                      block resampling, 394, 395\nBailey, J., 10, 36, 510                    Bluhm, C., 274\u2013276, 282\nBalakrishnan, N., 700                      Bodie, Z., 36, 488, 510\nbandwidth, 48                              Bolance, C., 77\n  automatic selection, 50                  Bollerslev, T., 439, 443\nBARRA Inc., 540                            book value, 529\nBates, D., 300                             book-equity-to-market-equity, 528\nBauwens, L., 443                           book-to-market value, 529\nBayes estimator, 584                       Boos, D. D., 126\nBayes\u2019 law, 484                            boot package in R, 150, 394\nBayes\u2019s rule or law, see Bayes\u2019s theorem   bootstrap, 137, 139, 368, 560\nBayes\u2019s theorem, 582, 583                    block, 394\nBayesian calculations                        multivariate data, 175\n  simulation methods, 595                    origin of name, 137\nBayesian statistics, 581                   bootstrap approximation, 138\nbcanon() function in R, 147                bootstrap con\ufb01dence interval\nbcPower() function in R, 303                 ABC, 147\nBelsley, D., 262                             basic, 146\nBE/ME, see book-equity-to-market-            BCa , 147\n      equity                                 bootstrap-t interval, 143\u2013146\n\f                                                                   Index      705\n\n  normal approximation, 143             centering\n  percentile, 146, 147                     variables, 242\nbootstrap() function in R, 141          central limit theorem, 89, 682, 690\nbootstrap package in R, 141, 147, 150      for least-squares estimator, 258\nBox test, 313                              for sample quantiles, 54, 77, 561\nBox, G., 4, 126, 284, 352, 395, 635        for the maximum likelihood estima-\nBox\u2013Cox power transformation, 67, 69            tor, 105, 107, 126, 139, 142, 177,\nBox\u2013Cox transformation model, 284               594\nBox\u2013Jenkins model, 352                     for the posterior, 592, 594, 636\nBox.test() function in R, 320              in\ufb01nite variance, 682\nboxcox() function in R, 121, 284, 303      multivariate for the maximum\nBoxCox.Arima() function in R, 366               likelihood estimator, 175, 594\nboxplot, 65, 67                         Chan, K., 646, 662\nboxplot() function in R, 65             Change Dir, 11\nBritten-Jones, M., 488                  change-of-variables formula, 76\nBrockwell, P., 352                      characteristic line, see security\nBrooks, S., 635                                 characteristic line\nBrownian motion, 689                    Chernick, M., 150\n  geometric, 9                          chi-squared distribution, 681\nBUGS, 595                               \u03c72\u03b1,n , 681\nBurg, D., 11                            Chib, S., 635\nBurnham, K. P., 126\n                                        Chou, R., 443\nbuying on margin, see margin, buying\n                                        CKLS model, 300\n      on\n                                           extended, 665\nbwNeweyWest() function in R, 375\n                                        Clayton copula, see copula, Clayton\n                                        CML (capital market line), 496, 497,\nca.jo() function in R, 458, 460\n                                                506, 507\ncalibration\n                                           comparison with SML (security\n  of Gaussian copula, 200\n                                                market line), 500\n  of t-copula, 201\nCampbell, J., 10, 36, 510               co-monotonicity copula, see copula,\n                                                co-monotonicity\ncapital asset pricing model, see CAPM\ncapital market line, see CML            coda package in R, 599, 607\nCAPM, 2, 159, 495, 498\u2013500, 509, 527    coe\ufb03cient of tail dependence\n  testing, 507                             co-monotonicity copula, 197\ncar package in R, 245, 368                 Gaussian copula, 197\nCarlin, B. P., 635, 636                    independence copula, 197\nCarlin, J., 635, 636                       lower, 196\nCarroll, R., 77, 262, 300, 443, 664        t-copula, 197\nCasella, G., 635, 700                      upper, 197\nCCF, see cross-correlation function     coe\ufb03cient of variation, 283\nccf() function in R, 381                coherent risk measure, see risk measure,\nCDF, 669, 670                                   coherent\n  calculating in R, 669                 cointegrating vector, 453, 457\n  population, 673                       cointegration, 453\ncenter                                  collinearity, 234\n  of a distribution, 87                 collinearity diagnostics, 262\ncenter parameters                       components\n  A-C distributions, 103                   of a mixture distribution, 96\n\f706    Index\n\ncompounding                                   sample Spearman\u2019s, 195\n  continuous, 32                              Spearman\u2019s, 194, 195\nconcordant pair, 194                       correlation matrix, xxv, 157\nconditional least-squares estimator, 324      Kendall\u2019s tau, 195\ncon\ufb01dence coe\ufb03cient, 138, 690                 sample, 158\ncon\ufb01dence interval, 138, 559, 560, 690        sample Spearman\u2019s, 196\n  accuracy of, 143                            Spearman\u2019s, 196\n  for determining practical signi\ufb01cance,   corrlation\n      697                                     partial, 226\n  for mean using t-distribution, 143,      Corr(X, Y ), xxv\n      690                                  counter-monotonicity copula, see\n  for mean using bootstrap, 144                   copula, counter-monotonicity\n  for variance of a normal distribution,   coupon bond, 22, 25\n      692                                  coupon rate, 23\n  pro\ufb01le likelihood, 119                   COV, xxv\ncon\ufb01dence level                            covariance, xxv, 64, 160, 683, 684\n  of VaR, 553                                 sample, 219, 684\nCongdon, P., 635                           covariance matrix, xxv, 157, 160\nconjugate prior, 586                          between two random vectors, 162\nconsistent estimator, 370                     of standardized variables, 158\ncontaminant, 91, 293\n                                              sample, 158\nCook, R. D., 262\n                                           coverage probability\nCook\u2019s D, 251\n                                              actual, 142\nCook\u2019s D, 253, 254\n                                              nominal, 142\ncopula, 183, 193\n                                           covRob() function in R, 533\n  Archimedean, 187\n                                           Cov(X, Y ), xxv, 684\n  Clayton, 189, 190, 199, 204\n                                           Cox, D., 284\n  co-monotonicity, 185, 188, 189, 214\n                                           Cox, D. R., 126\n  counter-monotonicity, 185, 188, 189\n  Frank, 187, 189                          Cox, J., 646\n  Gaussian, 197, 200, 205                  Cp , 232\n  Gumbel, 191, 199, 204                    cp2dp() function in R, 117\n  independence, 185                        Crame\u0301r\u2013von Mises test, 64\n  Joe, 192, 199, 204                       credible interval, 585, 690\n  nonexchangeable Archimedean, 207         credit risk, 553\n  t, 197, 201                              critical value, 693\ncopula package in R, 187, 189, 205, 208,      exact, 108\n      210, 211                             cross-correlation, 533\ncor() function in R, 12                    cross-correlation function, 380, 382\nCORR, xxv                                  cross-correlations\ncorrelation, xxv, 683                         of principal components, 524\n  e\ufb00ect on e\ufb03cient portfolio, 472          cross-sectional data, 263\ncorrelation coe\ufb03cient, 162, 684            cross-validation, 111, 654\n  interpretation, 685                         K-fold, 111\n  Kendall\u2019s tau, 194                          leave-one-out, 112\n  Pearson, 64, 193, 684                    Crouhy, M., 575\n  rank, 193                                cumsum() function in R, 335\n  sample, 684, 685                         cumulative distribution function, see\n  sample Kendall\u2019s tau, 195                       CDF\n\f                                                                     Index     707\n\ncurrent yield, 23                             weekly interest rates, 219, 224\u2013228,\nCV, see cross-validation                          230, 232\u2013234, 240\n                                            data transformation, 67, 69\u201371\nDalgaard, P., 11                            Davis, R., 352\nDaniel, M. J., 635                          Davison, A., 150, 395\n                                            decile, 53, 670\ndata sets\n                                            decreasing function, 672\n  air passengers, 309, 365\n                                            default probability\n  Berndt\u2019s monthly equity returns, 529,\n                                              estimation, 274\u2013276\n      539\n                                            degrees of freedom, 229\n  BMW log returns, 320, 322, 323, 350,\n                                              of a t-distribution, 61\n      413, 415, 422, 427\n                                              residual, 229\n  CPI, 381, 385, 389, 528\n                                            Delbaen, F., 573\n  CPS1988, 263, 664\n                                            \u0394, see di\ufb00erencing operator and Delta,\n  Credit Cards, 286, 289, 291\n                                                  of an option price\n  CRSP daily returns, 158, 163, 166,\n                                            density\n      168, 169, 172\u2013174, 176, 564, 620\n                                               bimodal, 141\n  CRSP monthly returns, 531, 537\n                                               trimodal, 58\n  daily midcap returns, 110, 111, 149,\n                                               unimodal, 141\n      167, 460, 551, 613, 618\n                                            determinant, xxvi\n  default frequencies, 274, 276, 281, 283   deviance, 110, 111\n  DM/dollar exchange rate, 45, 58, 62,      df, see degrees of freedom\n      65                                    dged() function in R, 100\n  Dow Jones, 526                            diag(d1 , . . . , dp ), xxv, 698\n  Earnings, 75, 76                          DIC, 609\n  Equity funds, 524, 526, 543, 545          dic.samples() function in R, 601, 611,\n  EuStockMarkets, 77, 129                         612\n  excess returns on the food industry       Dickey\u2013Fuller test, 341\n      and the market, 221, 222                 augmented, 340, 341\n  Fama\u2013French factors, 531, 537             diffdic() function in R, 628\n  Flows in pipelines, 69, 117, 120, 202     di\ufb00erencing operator, 333\n  HousePrices, 302, 303                        kth-order, 334\n  housing starts, 361, 362, 364, 365        diffseries() function in R, 393\n  ice cream consumption, 377, 379           di\ufb00usion function, 646\n  Industrial Production (IP), 337, 381,     dimension reduction, 517, 519\n      385, 389, 528                         direct parameters\n  in\ufb02ation rate, 309, 313, 323, 326, 328,      A-C distributions, 103\n      330, 339, 340, 342, 346, 351, 391     discordant pair, 194\n  mk.maturity, 38                           discount bond, see zero-coupon bond\n  mk.zero2, 38                              discount function, 33, 34\n  Nelson\u2013Plosser U.S. Economic Time           relationship with yield to maturity,\n      Series, 235, 241, 424                       34\n  risk-free interest returns, 45, 62, 65,   dispersion, 122\n      67, 74, 113, 116, 124, 333, 646       distribution\n  S&P 500 daily log returns, 45, 47, 62,      full conditional, 596, 597\n      65, 556, 558, 569                       marginal, 46\n  Treasury yield curves, 455, 520, 522,       meta-Gaussian, 205\n      523                                     symmetric, 89\n  USMacroG, 243, 397, 403                     unconditional, 47\n\f708     Index\n\ndisturbances                                Ergashev, B., 635\n  in regression, 217                        ES, see expected shortfall\ndiversi\ufb01cation, 495, 503                    estimation\ndividends, 7                                  interval, 690\ndouble-exponential distribution, 678        estimator, 689\n  kurtosis of, 90                             e\ufb03cient, 689\nDowd, K., 575                                 unbiased, 689\ndpill() function in R, 661                  Evans, M., 700\nDraper, N., 243                             excess expected return, 496, 500\ndrift                                       excess return, 221, 507\n  of a random walk, 9                       exchangeable, 187\n  of an ARIMA process, 337, 338             expectation\ndstd() function in R, 100                     conditional, 645, 683\nDt , 8                                          normal distribution, 687\nDuan, J.-C., 443                            expectation vector, 157\nDunson, D. B., 635                          expected loss given a tail event, see\nDUR, see duration                                expected shortfall\nduration, 35, 36                            expected shortfall, 1, 65, 554, 555,\nduration analysis, 553                           557\u2013560\nDurbin\u2013Watson test, 368                     expected value\nDurbinWatsonTest() function in R, 368         nonexistent, 671\ndwtest() function in R, 368                 exponential distribution, 678\n                                              kurtosis of, 90\nEber, J-M., 573                               skewness of, 90\nEcdat package in R, 46, 47, 52, 58, 76,     exponential random walk, see geometric\n       124, 140, 158, 221, 222, 309, 361,        random walk\n       428, 531                             exponential tail, 94, 99\necdf() function in R, 52\nEDF, see sample CDF                         F -distribution, 681\nEdwards, W., 584                            F -test, 488, 681\ne\ufb00ective number of parameters, 610,         F-S skewed distributions, 102, 132\n       653                                  Fabozzi, F. J., 635\neffectiveSize() function in R, 599,         face value, see par value\n       607                                  factanal() function in R, 541\u2013543\ne\ufb03cient frontier, 468, 469, 472, 485        factor, 517, 527\ne\ufb03cient portfolio, 468, 470, 485            factor model, 504, 527, 530\nEfron, B., 150                                 BARRA, 540\neigen() function in R, 171, 172, 385,          cross-sectional, 538, 539\n       699                                     fundamental, 528, 529\neigenvalue-eigenvector decomposition,          macroeconomic, 528\n       171, 698                                of Fama and French, 529, 530\nellipse, 170                                   time series, 538, 539\nelliptically contoured density, 170, 171    F\u03b1,n1 ,n2 , 681\nempirical CDF, see sample CDF               Fama, E., 528, 529, 546\nempirical copula, 200, 206                  Fan, J., 664\nempirical distribution, 145                 faraway package in R, 234, 245\nEnders, W., 352, 460                        FARIMA, 391\nEngle, R., 439, 443                         fdHess() function in R, 175\nequi-correlation model, 200                 fEcofin package in R, 38, 110\n\f                                                                     Index       709\n\nFederal Reserve Bank of Chicago, 219       GARCH(1,1), 419\nFernandez\u2013Steel skewed distributions,      GARCH-in-mean model, 448\n       see F-S skewed distributions        Gauss, Carl Friedrich, 676\nfGarch package in R, 100\u2013102               Gaussian distribution, 676\n  std\nfged  (y|\u03bc, \u03c3 2 , \u03bd), 101                  GCV, 654\nFisher information, 105                    GED, see generalized error distribution\n   observed, 106                           Gelman, A., 635, 636\nFisher information matrix, 106, 174        gelman.diag() function in R, 599, 606,\nFitAR package in R, 366                          607\nfitCopula() function in R, 205, 213        gelman.plot() function in R, 599, 607\nfitdistr() function in R, 113              generalized cross-validation, see GCV,\nfitMvdc() function in R, 211                     654\n\ufb01tted values, 218, 223                     generalized error distribution, 99, 116\n   standard error of, 251                  generalized linear models, 286\n\ufb01xed-income security, 19                   generalized Pareto distribution, 575\nforecast() function in R, 396              generator\nforecast package in R, 326, 396              Clayton copula, 189\nforecasting, 342, 343                        Frank copula, 187\n   AR(1) process, 343                        Gumbel copula, 191\n   AR(2) process, 343                        Joe copula, 192\n   MA(1) process, 343                        non-strict of an Archimedean copula,\nforward rate, 29, 30, 33, 34                     207\n   continuous, 33                            strict of an Archimedean copula, 187\n   estimation of, 276                      geometric Brownian motion, 689\nfracdiff package in R, 393                 geometric random walk, 9\nfractionally integrated, 391                 lognormal, 9\nFrank copula, see copula, Frank            geometric series, 316\nFrench, K., 528, 529, 546                    summation formula, 23\n  std\nfged  (y|\u03bd), 99                            Gibbs sampling, 596\nfull conditional, see distribution, full   Giblin, I., 460\n       conditional                         Gijbels, I., 664\nfundamental factor model, see factor       GLM, see generalized linear model\n       model, fundamental                  glm() function in R, 286, 288\nfundamental theorem of algebra, 699        Gourieroux, C., 352, 443, 575\n                                           Gram\u2013Schmidt orthogonalization\nGalai, D., 575                                   procedure, 243\ngam() function in R, 661, 664              Greenberg, E., 635\ngamma distribution, 678                    growth stock, 531\n  inverse, 679                             Guille\u0301n, R., 77\ngamma function, 95, 678                    Gumbel copula, see copula, Gumbel\n\u03b3(h), 308, 310\n\u0002(h), 312\n\u03b3                                          half-normal plot, 254\nGARCH model, 294                           Hamilton, J. D., 352, 395, 443, 460\nGARCH process, 99, 104, 405\u2013409, 411,      Harrell, F. E., Jr., 243\n      413                                  Hastings, N., 700\n  as an ARMA process, 418                  hat diagonals, 251\n  \ufb01tting to data, 413                      hat matrix, 251, 270, 653\n  heavy tails, 413                         Heath, D., 573\n  integrated, 408                          heavy tails, 57, 257\nGARCH(p, q) process, 411                   heavy-tailed distribution, 93, 413\n\f710     Index\n\nhedge portfolio, 531                     influence.measures() function in R,\nhedging, 299                                    253\nHessian matrix, 106, 174                 information set, 342\n  computation by \ufb01nite di\ufb00erences, 175   Ingersoll, J., 646\nHeston, S., 443                          integrating\nheteroskedasticity, 258, 276, 405           as inverse of di\ufb00erencing, 335\n  conditional, 67, 406                   interest-rate risk, 35\nhierarchical prior, 612, 613             interest-rate spread, 527\nHiggins, M., 443                         interquartile range, 65, 103\nhigh-leverage point, 250                 intersection\nHill estimator, 567, 568, 570, 571          of sets, xxv\nHill plot, 568, 570, 571                 interval estimate, 690\nHinkley, D., 150, 395                    inverse Wishart distribution, 619\nhistogram, 47                            iPsi() function in R, 187\nHML (high minus low), 529                IQR, 65\nHoaglin, D., 77\nholding period, 5, 466                   Jackson, C., 635\nhomoskedasticity                         JAGS, 595\n  conditional, 407                       James, J., 36\nhorizon                                  Jarque\u2013Bera test, 64, 91\n  of VaR, 553                            jarque.bera.test() function in R, 92\nHosmer, D., 300                          Jarrow, R., 36, 443\nHsieh, K., 443                           Jasiak, J., 352, 443, 575\nHsu, J. S. J., 635                       Jenkins, G., 352, 395\nHull, J., 575                            Jobson, J., 488\n                                         Joe copula, see copula, Joe\nhyperbolic decay, 390\n                                         Johnson, N., 700\nhypothesis\n                                         Jones, M. C., 77, 664\n  alternative, 693\n                                         Jorion, P., 575\n  null, 693\nhypothesis testing, 137, 693\n                                         Kane, A., 36, 488, 510\n                                         Karolyi, G., 646, 662\nI, xxv                                   Kass, R. E., 635\nI(0), 335                                KDE, see kernel density estimator\nI(1), 335                                Kemp, A., 700\nI(2), 335                                Kendall\u2019s tau, see correlation coe\ufb03cient,\nI(d), 335                                      Kendall\u2019s tau, 194\ni.i.d., 673                              kernel density estimator, 48, 49, 52\nIeno, E., 11                               two-dimensional, 213\nilliquid, 298                              with transformation, 75\nimportance sampling, 635                 KernSmooth() package in R, 647\nincreasing function, 672                 KernSmooth package in R, 649, 661\nindependence                             Kim, S., 635\n    of random variables, 160, 162        Kleiber, C., 77\n    relationship with correlation, 686   knot, 655, 656\nindex fund, 495, 556                       of a spline, 654\nindicator function, xxvi, 52             Kohn, R., 646, 647\ninf, see in\ufb01num                          Kolmogorov\u2013Smirnov test, 64\nin\ufb01num, 670, 672                         Korkie, B., 488\n\f                                                                    Index     711\n\nKotz, S., 700                             Lintner, J., 510\nKPSS test, 340                            liquidity risk, 553\nkpss.test() function in R, 340            Little, R., 294\nKroner, K., 443                           Ljung\u2013Box test, 312, 320, 336, 383\nKuh, E., 262                              lm() function in R, 224, 226, 531\nkurtosis, 87, 89, 90                      lmtest package in R, 368\n  binomial distribution, 90               Lo, A., 10, 36, 510\n  excess, 91                              loading\n  sample, 91                                 in a factor model, 530\n  sensitivity to outliers, 91             loading matrix (of a VECM), 457, 458\nKutner, M., 243                           location parameter, 86, 88, 89, 675, 676\n                                             quantile based, 103\nlag, 308                                  locfit() function in R, 661\n   for cross-correlation, 381             locfit package in R, 651, 661\nlag operator, 331                         locpoly() function in R, 647, 649, 661\nLahiri, S. N., 395                        loess, 245, 259, 652\nLange, N., 294                            log, xxv\nLaplace distribution, see double          log10 , xxv\n       exponential distribution           log-drift, 9\nlarge-cap stock, 695                      log-mean, 9, 677\nlarge-sample approximation                log price, 6\n   ARMA forecast errors, 345              log return, see return, log\nLaurent, S., 443                          log-likelihood, 104\nlaw of iterated expectations, 683         log-standard deviation, 9, 677\nlaw of large numbers, 682                 log-variance, 677\nleaps() function in R, 239                Lognormal(\u03bc, \u03c3), 676\nleaps package in R, 232, 239              lognormal distribution, 676\nleast-squares estimator, 218, 221, 682       skewness of, 91\n   generalized, 271                       long position, 474\n   weighted, 259, 424                     longitudinal data, 263\nleast-squares line, 219, 298              Longsta\ufb00, F., 646, 662\nleast-trimmed sum of squares estimator,   Louis, T. A., 635, 636\n       see LTS estimator                  lower quantile, see quantile, lower\nLedoit, O., 636                           lowess, 245, 652\nLehmann, E., 77, 636                      LTS estimator, 293, 294\nLemeshow, S, 300                          ltsreg() function in R, 294\nlevel                                     Lunn, D. J., 635\n   of a test, 693\nleverage, 13                              MA(1) process, 328\n   in estimation, 653                     MA(q) process, 330\n   in regression, 251                     MacKinlay, A., 10, 36, 510\nleverage e\ufb00ect, 421                       macroeconomic factor model, see factor\nLi, W. K., 441                                 model, macroeconomic\nLiang, K., 109                            MAD, 51, 55, 65, 87, 122, 123\nlikelihood function, 104                  mad() function in R, 52, 79, 123\nlikelihood ratio test, 108, 681           magnitude\nlinear combination, 165                     of a complex number, see absolute\nlinear programming, 490                        value, of a complex number\nlinprog package in R, 490                 MAP estimator, 585\n\f712    Index\n\nMarcus, A., 36, 488, 510                   of an MCMC sample, 602\nmargin                                   mixing distribution, 99\n  buying on, 498                         mixture distribution\nmarginal distribution function, 46         normal scale, 98\nMark, R., 575                            mixture model, 96\nmarket capitalization, 695                 continuous, 99\nmarket equity, 529                         continuous scale, 99\nmarket maker, 298                          \ufb01nite, 99\nmarket risk, 553                         MLE, see maximum likelihood\nMarkov chain Monte Carlo, see MCMC             estimator\nMarkov process, 324, 688                 mode, 102, 670\nMarkowitz, H., 488                       model\nMarron, J. S., 77                          full, 108\nMASS package in R, 244, 284                parametric, 85\nmatrix                                     reduced, 108\n  diagonal, 698                            semiparametric, 566\n  orthogonal, 698                        model averaging, 126\n  positive de\ufb01nite, 161                  model complexity\n  positive semide\ufb01nite, 161                penalties of, 109\nMatteson, D. S., 436                     model selection, 231\nmaximum likelihood estimator, 85, 104,   moment, 92\n      108, 246, 324, 325, 682              absolute, 92\n  not robust, 122                          central, 92\n  standard error, 105                    momentum\nMCMC, 137                                  in a time series, 335\nmean                                     monotonic function, 672\n  population, 673                        Morgan Stanley Capital Index, 479\n  sample, 673                            Mossin, J., 510\n    as a random variable, 137, 690       Mosteller, 77\nmean deviance, 611                       moving average process, see MA(1) and\nmean-reversion, 309, 453                       MA(q) processes\nmean-squared error, 689                  moving average representation, 315\nmean sum of squares, 229                 MSCI, see Morgan Stanley Capital\nmean-squared error, 139                        Index\n  bootstrap estimate of, 139             MSE, see mean-squared error\nmean-variance e\ufb03cient portfolio, see     mst.mple() function in R, 173\n      e\ufb03cient portfolio                  multicollinearity, see collinearity\nmedian, 53, 670                          multimodal, 670\nmedian absolute deviation, see MAD       multiple correlation, 228\nMeesters, E., 11                         multiplicative formula\nMerton, R., 488, 510, 646                  for densities, 688\nmeta-Gaussian distribution, 186\nMetropolis\u2013Hastings algorithm, 597       Ne\ufb00 , 607\nmfcol() function in R, 12                N (\u03bc, \u03c3 2 ), 676\nmfrow() function in R, 12                Nachtsheim, C., 243\nmgcv package in R, 661, 664              Nandi, S., 443\nMichaud, R., 479                         Nelson, C. R., 243, 300\nmixed model, 659                         Nelson, D., 443\nmixing                                   Nelson\u2013Siegel model, 278, 281\n\f                                                                      Index   713\n\nnet present value, 25                     overdispersed, 596\nNeter, J., 243                            over\ufb01t\nNewey, W., 375                              density function, 50\nNeweyWest() function in R, 375, 424       over\ufb01tting, 109, 110, 649\nNielsen, J. P., 77                        oversmoothing, 50, 649\nnlme package in R, 175\nnlminb() function in R, 104               pD , 610\nnls() function in R, 273                  p-value, 64, 226, 693, 694\nnominal value                             PACF, see partial autocorrelation\n  of a coverage probability, 259                function\nnonconstant variance                      pairs trading, 459\n  problems caused by, 259                 Palma, W., 409, 420\nnonlinearity                              panel data, 263\n  of e\ufb00ects of predictor variables, 259   par() function in R, 12\nnonparametric, 555                        par value, 20, 22, 23\nnonrobustness, 71                         Pareto, Vilfredo, 680\nnonstationarity, 408                      Pareto constant, see tail index\nnorm                                      Pareto distribution, 571, 680\n  of a vector, 698                        Pareto tail, see polynomial tail, 571\nnormal distribution, 676                  parsimony, 3, 86, 307, 309, 312, 314,\n  bivariate, 687                                316, 325\n  kurtosis of, 90                         partial autocorrelation function,\n  multivariate, 164, 165                        349\u2013351\n  skewness of, 90                         PCA, see principal components analysis\n  standard, 676                           pca() function in R, 519\nnormal mixture distribution, 96           pD , 609\nnormal probability plot, 54, 98, 276      Peacock, B., 700\n  learning to use, 256                    Pearson correlation coe\ufb03cient, see\nnormality                                       correlation coe\ufb03cient, Pearson\n  tests of, 64                            penalized deviance, 611\n                                          percentile, 53, 670\nOpenBUGS, 598, 635                        Pfa\ufb00, B., 352, 460\nOpenBUGS, 595                             Phillips\u2013Ouliaris test, 454, 455\noperational risk, 553                     Phillips\u2013Perron test, 340\noptim() function in R, 104, 106, 113,     \u03c6(x), 676\n      180, 211                            \u03a6(y), 676\norder() function in R, 650                Pindyck, R., 443\norder statistic, 52, 53, 555              plogis() function in R, 304\northogonal polynomials, 243               Plosser, C., 243\nouter() function in R, 665                plus function, 656\noutlier, 256                                linear, 656\n  extreme, 257                              quadratic, 656\n  problems caused by, 258                   0th-degree, 657\n  rules of thumb for determining, 257     pnorm() function in R, 16\noutlier-prone, 57                         po.test() function in R, 455\noutlier-prone distribution, see heavy-    Poisson distribution, 283\n      tailed distribution                 Pole, A., 460\nOverbeck, L., 274\u2013276, 282                polynomial regression, see regression,\noverdi\ufb00erencing, 393, 394                       polynomial\n\f714    Index\n\npolynomial tail, 94, 99                    probability transformation, 196, 675\npolynomials                                pro\ufb01le likelihood, 119\n  roots of, 699                            pro\ufb01le log-likelihood, 119\npolyroot() function in R, 340, 699         pseudo-inverse\npooled standard deviation, 694                of a CDF, 670, 675\nportfolio, 159                             pseudo-maximum likelihood\n  e\ufb03cient, 470, 472, 475, 496                 for copulas, 199\n  market, 496, 500, 504, 505                  parametric for copulas, 200\n  minimum variance, 468                       semiparametric for copulas, 200\npositive part function, 41                 Pt , 5\nposterior CDF, 586                         pt , 6\nposterior distribution, 582\nposterior interval, 585, 593               qchisq() function in R, 692\nposterior probability, 584                 QQ plot, see quantile\u2013quantile plot\npotential scale reduction factor, 606      qqline() function in R, 55\npower                                      qqnorm() function in R, 54, 55\n  of a test, 695                           qqplot() function in R, 62\npower transformations, 67                  quadratic programming, 475\npp.test() function in R, 340               quantile, 53, 54, 670\npractical signi\ufb01cance, 697                   lower, 670\nprcomp() function in R, 521                  population, 673\nprecision, 589, 618                          respects transformation, 670\nprecision matrix, 618                        upper, 108, 670\nprediction, 295                            quantile function, 670, 675\n  best, 687, 697                           quantile() function in R, 53\n  best linear, 295, 297, 499, 687          quantile transformation, 675\n    relationship with regression, 298      quantile\u2013quantile plot, 61, 62\n  error, 297, 687                          quartile, 53, 670\n    unbiased, 297                          quintile, 53, 670\n  linear, 295\n  multivariate linear, 298                   , xxv\nprice                                      R-squared, 228, 298\n  stale, 278                               R2 adjusted, 231, 232\npricing anomaly, 529                       R2 , see R-squared\nprincipal axis, 518                        Rachev, S. T., 635\nprincipal components analysis, 517, 519,   rally\n      521, 523, 525\u2013527, 698                  bond, 19\nprincomp() function in R, 548              random sample, 673\nprior                                      random variables\n  noninformative, 582                         linear function of, 159\nprior distribution, 582                    random vector, 157, 687\nprior probability, 584                     random walk, 9, 317\nprobability density function                  normal, 9\n  conditional, 683                         random walk hypothesis, 1\n  elliptically contoured, 164              rank, 194\n  marginal, 682                            rank correlation, 194\n  multivariate, 687                        rational person\nprobability distribution                      de\ufb01nition within economics, 484\n  multivariate, 157                        rCopula() function in R, 189, 209\n\f                                                                      Index     715\n\nread.csv() function in R, 11                    nonconstant variance, 255, 258\nregression, 645                                 nonnormality, 255, 256\n   ARMA disturbances, 377                       raw, 252, 255\n   ARMA+GARCH disturbances, 424             return\n   cubic, 243                                   adjustment for dividends, 7\n   geometrical viewpoint, 229                   continuously compounded, see return,\n   linear, 645                                      log, 6\n   local linear, 647                            log, 6\n   local polynomial, 647, 648                   multiperiod, 7\n   logistic, 286, 303                           net, 1, 5\n   multiple linear, 217, 224, 298, 325          simple gross, 6\n   multivariate, 528                        return-generating process, 502\n   no-intercept model, 509                  reversion\n   nonlinear, 271, 274, 277, 300                to the mean, 335\n   nonlinear parametric, 274, 645            \u0002 607\n                                            R,\n   nonparametric, 259, 274, 645, 698        \u03c1(h), 308, 311\n   polynomial, 225, 242, 243, 246, 259,     \u03c1\u0002(h), 312\n       274                                  \u03c1XY , 64, 684\n     is a linear model, 274                 \u03c1\u0002XY , 684\n   probit, 286                              risk, 1\n   spurious, 372                                market or systematic component, 503\n   straight-line, 218                           unique, nonmarket, or unsystematic\n   transform-both-sides, 281                        component, 503, 504, 509\n   with high-degree polynomials, 243        risk aversion, 484\nregression diagnostics, 251                     index of, 498\nregression hedging, 298, 299                risk factor, 517, 527, 538\nregsubsets() function in R, 232             risk management, 553\nReinsel, G., 352, 395                       risk measure\nrejection region, 693                           coherent, 573\nREML, 659                                   risk premium, 465, 495, 496, 500\nreparameterization, 675                     risk-free asset, 465, 467, 495\nresampling, 54, 137, 138, 144, 559, 560     Ritchken, P., 443\n   block, 394                               rjags package in R, 598, 611, 628\n   model-based, 138                         rnorm() function in R, 13\n     for time series, 394, 395              Robert, C. P, 635\n   model-free, 138, 560                     robust estimation, 294\n   multivariate data, 175                   robust estimator, 52\n   time series, 394                         robust estimator of dispersion, 122\nresidual error MS, 537                      robust modeling, 294\nresidual error SS, 227                      robust package in R, 294, 533\nresidual mean sum of squares, 229, 653      Rombouts, J. V., 443\nresidual outlier, 250                       root \ufb01nder\nresiduals, 218, 255, 274, 318\u2013320               nonlinear, 38\n   correlation, 256, 368                    Ross, S., 646\n     e\ufb00ect on con\ufb01dence intervals and       Rossi, P., 443\n                                               p\n       standard errors, 368, 373, 424            , xxv\n   externally studentized, 253, 255         rstudent, 250, 251, 253\n   externally studentized (rstudent), 250   Rt , 5\n   internally studentized, 253              rt , 6\n\f716     Index\n\nRubin, D., 635, 636                        \u03c3XY , 64, 684\nRubinfeld, D., 443                         \u0002XY , 684\n                                           \u03c3\nrug, 48                                    sign function, 194\nrugarch package in R, 413, 424             signi\ufb01cance\nRuppert, D., 10, 77, 262, 300, 443, 664       statistical versus practical, 230\nrXY , 684                                  Silvennoinen, A., 443\nRyan, T. P., 243                           Silverman, B., 77\n                                           Sim, C. H., 64\nS&P 500 index, 508                         Simonato, J., 443\nsample ACF, 312                            simulation, 137\nsample CDF, 52                             simultaneous test, 312, 383\nsample median                              single-factor model, 504\n   as a trimmed mean, 122                  single-index model, see single-factor\nsample quantile, 52\u201354                            model\nSanders, A., 646, 662                      skewed-t distribution, 58\nsandwich package in R, 375, 395, 424       skewness, 87, 88, 90, 257\nscale matrix                                  lognormal distribution, 91\n   of a multivariate t-distribution, 166      negative or left, 89\nscale parameter, 86, 88, 675\u2013678              positive or right, 89\n   t-distribution, 95                         reduction by data transformation, 67\n   inverse, 86, 678                           sample, 91\n   quantile based, 103                        sensitivity to outliers, 91\nscatterplot, 684                           skewness parameter\nscatterplot matrix, 162                       quantile-based, 103\nscatterplot smoother, 259                  Sklar\u2019s theorem, 184\nscree plot, 522                            small-cap stock, 695\nSeber, G., 300                             SMB (small minus big), 529\nsecurity characteristic line, 501\u2013504,     Smith, A., 635\n       507                                 Smith, H., 243\nsecurity market line, see SML              SML (security market line), 499, 500\nSelf, S., 109                                 comparison with CML (capital\nself-in\ufb02uence, 653                                market line), 500\nselling short, see short selling           smooth.spline() function in R, 659\nSerling, R., 77                            smoother, 649\nset.seed() function in R, 14               smoother matrix, 653\nshape parameter, 86, 99, 675, 676, 681        for a penalized spline, 659\nShapiro\u2013Wilk test, 64, 65, 81              sn package in R, 102, 172\nshapiro.test() function in R, 64           sn.mple() function in R, 117\nSharpe, W., 10, 36, 470, 510               solve.QP() function in R, 476\nSharpe\u2019s ratio, 470, 472, 496              solveLP() function in R, 490\nShephard, N., 635                          source() function in R, 11\nshort position, 474                        sourcing a \ufb01le, 11\nshort rate, 300                            span\nshort selling, 98, 299, 473                   tuning parameter in lowess and loess,\nshoulder                                          245, 652\n   of a distribution, 87                   Spearman\u2019s rho, see correlation\nshrink factor, 606                                coe\ufb03cient, Spearman\u2019s rho\nshrinkage estimation, 482, 636             Spiegelhalter, D. J., 635\nSiegel, A. F., 300                         spline, 259\n\f                                                                     Index    717\n\n   cubic smoothing, 659                   supremum, 672\n   general degree, 657                    Svensson model, 278, 281\n   linear, 654\u2013656                        Svensson, L. E., 300\n   penalized, see penalized spline        sXY , 684\n   quadratic, 656                         s2Y , 673\n   smoothing, 259                         symmetry, 670\nspline() function in R, 647, 661\nspot rate, 25\u201327                          t-test\nspurious regression, 368, 454                independent samples, 694\nstable distribution, 682                     one-sample, 693\nstale price, 272                             paired samples, 695\nstandard deviation                           two-sample, 694\n   sample, 673                            t-distribution, 57, 61, 94, 95, 144\nstandard error, 225, 690                     classical, 95\n   Bayesian, 597, 608                        kurtosis of, 90\n   bootstrap estimate of, 139                multivariate, 165\n   of the sample mean, 690                   multivariate skewed, 172\nstandardization, 158                         standardized, 95\nstandardized variables, 158               t-meta distribution, 186\nstationarity, 46, 307, 380                t-statistic, 143, 225\n   strict, 308                            tail\n   weak, 308, 381                            of a distribution, 55\nstationary distribution, 689              tail dependence, 164, 165\nstationary process, 308                   tail independence, 164\nstatistical arbitrage, 459                tail index, 94, 680\n   risks, 459                                estimation of, 567, 569\nstatistical factor analysis, 540             regression estimate of, 567\nstatistical model, 307                       t-distribution, 96\n   parsimonious, 307, 309                 tail loss, see expected shortfall\nstatistical signi\ufb01cance, 697              tail parameter\nStefanski, L. A., 126                        quantile-based, 104, 148, 149\nStein estimation, 636                     t\u03b1,\u03bd , 94\nStein, C., 636                            tangency portfolio, 467, 470\u2013472, 488,\nstepAIC() function in R, 237, 244, 289,          495\n       303                                Taylor, J., 294\nStern, H., 635, 636                       TBS regression, see regression,\ns\u03b8\u0002, 690                                         transform-both-sides\nstochastic process, 307, 688              Tera\u0308svirta, T, 443\nstochastic volatility model, 452,         term structure, 20, 26\u201328, 33\n       623\u2013627, 629                       test bounds\nSTRIPS, 278                                  for the sample ACF, 312, 320, 383\nstudentization, 253                       test data, 110\nsubadditivity, 573                        Thomas, A., 635\nsum of squares                            Tiao, G., 635\n   regression, 227, 229                   Tibshirani, R., 150\n   residual, 227                          time series, 45, 46, 104, 405\n   total, 227                                multivariate, 380\nsupport                                      univariate, 307\n   of a distribution, 101                 time series plot, 46, 309\n\f718    Index\n\nt\u03bd [ \u03bc, {(\u03bd \u2212 2)/\u03bd}\u03c3 2 ], 95               urca package in R, 457, 460\ntotal SS, see sum of squares, total        utility function, 484\ntower rule, 683                            utility theory, 484\ntrace, xxvi\ntrace plot, 602                            validation data, 110\ntraining data, 110                         value investing, 531\ntransfer function models, 395              value stock, 531\ntransform-both-sides regression, 281,      value-at-risk, see VaR\n        283                                van der Linde, A., 635\ntransformation                             van der Vaart, A., 77, 636\n   variance-stabilizing, 67, 74, 283       VaR, 1, 65, 467, 553\u2013556, 559, 560, 563,\ntransformation kernel density estimator,         571, 573\n        75                                   con\ufb01dence interval for, 560\nTreasury bill, 467                           estimation of, 569\nTrevor, R., 443                              incoherent, 573\ntrimmed mean, 122                            nonparametric estimation of, 555\ntrimodal, 60                                 not subadditive, 573\ntrue model, 3                                parametric estimation of, 570\ntruncated line, 656                          semiparametric estimation of, 565,\nTsay, R. S., 352, 436, 443                       566\ntsboot() function in R, 394                  single-asset, 555\nTse, Y. K., 439                            VAR process, see AR process,\ntseries package in R, 340, 455                   multivariate\nTsui, A. K. C., 439                        VaR(\u03b1), 554\nTuckman, B., 36, 300                       VaR(\u03b1, T ), 554\nTukey, J., 77                              variance, xxv\ntype I error, 693                            conditional, 406, 408, 683, 687\ntype II error, 693                             normal distribution, 687\n                                             in\ufb01nite, 671\nugarchfit() function in R, 413, 422            practical importance, 672\nuncorrelated, 162, 685                       marginal, 408\nunder\ufb01t                                      population, 673\n  density function, 50                       sample, 219, 673\nunder\ufb01tting, 649                           variance function model, 407\nundersmoothed, 50                          variance in\ufb02ation factor, 234, 235, 237\nundersmoothing, 649                        varimax, 545, 546\nuniform distribution, 674                  \u0015 + (\u03c8 | Y ), 606\n                                           var\nuniform-transformed variables, 200         Vasicek, O., 646\nUniform(a, b), 674                         VECM, see vector error correction\nunimodal, 594, 670                               model\nunion                                      vector error correction model, 456\u2013458\n  of sets, xxv                             Vehtari, A., 635\nunique risks, 527                          Vidyamurthy, G., 460\nuniquenesses, 544, 545                     VIF, see variance in\ufb02ation factor\nuniroot() function in R, 38                vif() function in R, 234, 245\nunit circle, 699                           volatility, 1, 9\nunit root tests, 338\u2013341                   volatility clustering, 10, 46, 405\nupper quantile, see quantile, upper        volatility function, 646\n\f                                                         Index     719\n\nW (MCMC diagnostic), 606       WN(\u03bc, \u03c3), 310\nWagner, C., 274\u2013276, 282       Wolf, M., 636\nWand, M. P., 77, 664           Wolldridge, J., 443\nWasserman, L., 126, 664, 700   Wood, S., 664\nWasserman, W., 243\nWatts, D., 300                 y-hats, see \ufb01tted values\nweak stationarity, 308         Yap, B. W., 64\nWebber, N., 36                 Yau, P., 646, 647\nWeddington III, W., 460        Y , 673\nWeisberg, S., 262              yield, see yield to maturity\nWelsch, R., 262                yield curve, 635\nWest, K., 375                  yield to maturity, 23\u201327, 30, 33\nwhite noise, 310, 332             coupon bond, 26\n  Gaussian, 311                Yule\u2013Walker equations, 357\n  i.i.d., 311, 409\n  t, 311                       z\u03b1 , 676\n  weak, 310, 409               Zeileis, A., 77, 395\nWhite, H., 375                 zero-coupon bond, 20, 25, 30, 33, 34,\nWild, C., 300                        272\nWinBUGS, 595                   Zevallos, M., 409, 420\nWishart distribution, 618      Zuur, A., 11\n\f",
    "metadata": {
      "chapter_number": "4",
      "chapter_title": "of",
      "section_number": "0.072",
      "section_title": "\u00b1 0.638\u0131. It is also veri\ufb01ed that these are roots.",
      "has_code": true
    },
    "code_blocks": [
      {
        "language": "r",
        "code": "roots = polyroot(c(1, 2, 3, 4))\n      > roots\n      [1] -0.072+0.638i -0.606-0.000i -0.072-0.638i\n      > fn = function(x){1 + 2 * x + 3 * x^2 + 4 * x^3}\n      > round(fn(roots), 5)\n      [1] 0+0i 0+0i 0+0i                                                     \u0002",
        "start": 64,
        "end": 328
      }
    ]
  }
]